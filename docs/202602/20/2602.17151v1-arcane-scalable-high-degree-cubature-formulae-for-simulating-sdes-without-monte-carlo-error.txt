Title: ARCANE: Scalable high-degree cubature formulae for simulating SDEs without Monte Carlo error

URL Source: https://arxiv.org/pdf/2602.17151v1

Published Time: Fri, 20 Feb 2026 01:35:35 GMT

Number of Pages: 57

Markdown Content:
# ARCANE: Scalable high-degree cubature formulae for simulating SDEs without Monte Carlo error 

Peter Koepernik *1 Thomas Coxon *2 James Foster 3

February 20, 2026 

Abstract 

Monte Carlo sampling is the standard approach for estimating properties of solutions to stochastic differential equations (SDEs), but accurate estimates require huge sample sizes. Lyons and Victoir (2004) proposed replacing independently sampled Brownian driving paths with â€œcu-bature formulaeâ€, deterministic weighted sets of paths that match Brownian â€œsignature mo-mentsâ€ up to some degree D. They prove that cubature formulae exist for arbitrary D, but explicit constructions are difficult and have only reached D = 7 , too small for practical use. We present ARCANE, an algorithm that efficiently and automatically constructs cubature formulae of arbitrary degree. It reproduces the state of the art in seconds and reaches D = 19 within hours on modest hardware. In simulations across multiple different SDEs and error metrics, our cubature formulae robustly achieve an error orders of magnitude smaller than Monte Carlo with the same number of paths. 

Keywords. Stochastic differential equations, numerical simulation, cubature methods, Brownian motion, path signatures, orthogonal arrays, recombination, linear programs, dyadic intervals. 

# 1 Introduction 

Stochastic differential equations (SDEs) are commonly used to model continuous-time phenomena evolving under the influence of random noise. For example, SDEs with scalar noise (that is, driven by a single Brownian motion) have been applied across a range of topicsâ€”such as solar forecasting [6, 47, 15], wind speed modelling [5], physics [76], engineering [89], epidemiology [13, 64], population genetics [25], mathematical finance [21], systems biology [85], economics [17] and social sciences [20, chapter 2]. In addition, SDEs with low-dimensional noise (e.g. driven by d â‰¤ 3

independent Brownian motions) are natural models for physical objects whose motion exhibits random fluctuations. For example, they have been used to model aircraft dynamics [55, 58]. We consider low-dimensional SDEs, defined by Stratonovich integration, of the form: 

dyt = Î¼(yt) d t +

> d

âˆ‘

> i=1

Ïƒi(yt) â—¦ dW it , (1) 

> 1

Department of Statistics, University of Oxford, UK. Email: peter.koepernik@stats.ox.ac.uk .

> 2

Dept. Aeronautical and Automotive Engineering, Loughborough University, UK. Email: T.Coxon2@lboro.ac.uk .

> 3

Department of Mathematical Sciences, University of Bath, UK. Email: jmf68@bath.ac.uk .

> âˆ—

Equal contribution. 

1                 

> arXiv:2602.17151v1 [math.NA] 19 Feb 2026 t
> Ï‰(t)
> Brownian motion (discretised)
> t
> y(t)
> SDE Solution (discretised)
> E[yt] = exp ((Î»âˆ’12Î¼2)t)
> EQN
> MC [yt(Ï‰)]
> yt(Ï‰1)
> yt(Ï‰2)
> yt(Ï‰3)
> Scalar SDE
> dyt=Î»y tdt+Î¼y tdWt
> Pathwise solution
> yt(Ï‰i) = y0exp ((Î»âˆ’12Î¼2)t+Ï‰i(t))

Figure 1: Monte Carlo estimation for the mean of a scalar SDE, in the ideal case where there exists a closed-form solution that can be evaluated on a discretised time domain.                   

> Monte Carlo ARCANE Estimate E[f(yT)]â‰ˆ
> Mâˆ‘
> i=1
> Î»if(yT(Ï‰i))(2)
> Paths (Ï‰i)randomly sampled Brownian paths deterministic Brownian-like paths
> Weights (Î»i)uniform, Î»i=1
> Mnot necessarily uniform

Table 1: Comparison of the Monte Carlo and ARCANE estimators. Here, yT (Ï‰) âˆˆ Re denotes the solution at time T of the system (1) driven by a path Ï‰ : [0 , T ] â†’ Rd.where the initial condition y0 = Î¾ and solution y = {yt}tâˆˆ[0 ,T ] take values in Re, W = {Wt}tâˆˆ[0 ,T ]

is a standard d-dimensional Brownian motion, and Î¼, Ïƒ i : Re â†’ Re are the vector fields (referred to as the drift and diffusion respectively). Note that â€œlow-dimensionalâ€ refers to the number d of independent driving Brownian motions; the dimension e of the solution path is unrestricted. In practice, one is often interested in estimating key statistics of the solution to the SDE (1), such as its mean or variance, or more generally the expectation of f (yT ) for some functional f : Re â†’ R.This is traditionally achieved using Monte Carlo estimation (or Quasi-Monte Carlo, see Section 2), which takes the average over some number M of independently sampled numerical solutions of (1), see Table 1 (left) and Figure 1 for an illustration. However, by the Central Limit Theorem, the error of the Monte Carlo estimate is proportional to 1/âˆšM , where M is the number of samples. An accurate estimation of E[f (yT )] can therefore require an enormous number of sample paths and be computationally very expensiveâ€”especially for complex SDEs. In this paper, we aim to reduce this estimation error by replacing the uniformly weighted and inde-pendently sampled Brownian paths in (2) with a carefully designed deterministic set of Brownian-like paths with (not necessarily uniform) associated probability weights, see Table 1 (right). These paths are piecewise linear so that the SDE (1) simply reduces to an ODE along each piece, which can be solved by standard methods. Experimentally, the estimation error using our paths is con-sistently orders of magnitude lower than the error obtained by a similar number of Monte Carlo 2Figure 2: An illustration of our ARCANE cubature formulae with degrees 17, 19, and 5 (dyadic depth 8). As the formulae contain 3194, 8362, and 3952 paths, respectively, we have added noise to help distinguish paths. Here, the thickness and opacity of each path is taken to be proportional to its associated weight (i.e. paths with larger weights are more visible); a histogram of the weights is to the right of each cubature formula. 3samples. An example of such a collection of Brownian-like paths produced by our methodology is illustrated in Figure 2. More precisely, we propose a computational methodology called ARCANE 1, which is an efficient and scalable algorithm for producing what are known as cubature formulae : sets of weighted paths that match the â€œsignature momentsâ€ [19] of Brownian motion up to some degree D. It was first observed by Lyons and Victoir in their seminal 2004 paper [61] that cubature formulae could be used in place of Monte Carlo estimation for simulating SDEs in the way we outlined in Table 1. They proved that cubature formulae exist for arbitrary Brownian dimension d and arbitrary degree D,but were only able to find explicit constructions for D â‰¤ 5. Further constructions have been found in the subsequent literature [35, 78, 38, 65, 32], however, none have exceeded D = 7 . In the one-dimensional setting, cubature formulae of degree D = 11 have technically been constructed [35], but they consist of high-order â€œrough pathsâ€; solving the SDE (1) driven by such a path involves the computation of hundreds to thousands of high-order derivatives (up to eleventh order) of the drift and diffusion vector fields, as well as their evaluation at every step of the numerical solver. Even if the derivatives were efficiently computed by a symbolic program, solving (1) for one of these paths would take up to thousands of times longer than for an ordinary (e.g. piecewise linear) path. Our ARCANE algorithm can automatically construct cubature formulae of arbitrary dimension and degree. They consist of piecewise linear paths and can therefore be used plug-and-play in place of random Brownian sample paths. Our algorithm is optimised for GPU and highly parallelisable; using ARCANE, we were able to reproduce the state of the art D = 7 in seconds, and construct cubatures with degree up to D = 19 in one hour on a single GPU node. Using several real-world SDEs (from mathematical finance and population genetics), we demon-strate that our ARCANE cubature formulae consistently achieve orders of magnitude more accuracy when estimating statistics for SDEs compared to traditional (Quasi-)Monte Carlo simulation. Underlying the efficacy of our ARCANE algorithm is a series of innovations that drastically re-duce the run-time of a known and conceptually simple algorithm (from exponential to low-order polynomial in the relevant parameter, see Section 3 for details), combined with a highly optimized implementation of the algorithmâ€™s core components in JAX [11]. Code for our ARCANE algorithm along with datasets containing the cubature formulae used in our experiments can be found at: github.com/tttc3/ARCANE-Cubature We summarise our key contributions below: (1) We introduce ARCANE, a scalable, highly parallelisable and GPU-optimised algorithm for generating high-degree cubature formulae for SDE simulation. (2) We demonstrate that cubature formulae obtained with this algorithm on low-end compute (a few GPU hours) already improve drastically over the previously best known cubature formulae, and lead to an SDE estimation error several orders of magnitude smaller compared with traditional Monte Carlo methods. (3) We publish our constructed cubature formulae, as well as the code for the ARCANE algorithm, which can be used with more compute to construct even higher degree cubatures. (4) We prove several theoretical results on the correctness of our approach, which may be of inde-pendent interest to members of the stochastic analysis and rough path theory communities.      

> 1Algorithm for Recombination of Cubatures from Orthogonal Arrays that match Nested Expected signatures

42 Results 

In order to be able to estimate and compare the simulation errors of the cubature and the Monte Carlo SDE solvers, we need a ground truth to compare against. Most statistics of most SDEs are not available in closed form, but for a few SDEs, the mean and variance of the solution, m(t) = E [yt]

and v(t) = V(yt), are available in closed form (as a function of t, the initial condition y0, and the parameters of the SDE). Then, for a given set of Monte Carlo paths, or a given cubature (Î»i, Ï‰ i)Mi=1 ,we calculate the empirical mean and variance, 

Ë†m(t) = 

> Mâˆ‘
> i=1

Î»iyt(Ï‰i), Ë†v(t) = 

> Mâˆ‘
> i=1

Î»i

> (

yt(Ï‰i) âˆ’ Ë†m(t)

> )2

,

and combine them in the Mean-Variance error (MVE) 

MVE = 

> âˆš

( Ë† m(T ) âˆ’ m(T )) 2 + ( 

> âˆš

Ë†v(T ) âˆ’

> âˆš

v(T )) 2, (3) for a fixed time T (where T = 1 .0 unless stated otherwise). The motivation behind formula (3) is that it describes the 2-Wasserstein distance between two one-dimensional Gaussians N ( Ë† m(T ), Ë†v(T )) 

and N (m(T ), v (T )) . We study the MVE for four different one-dimensional SDEs summarised in Table 2, with results presented in Fig. 3. Model Stochastic Differential Equation (ItÃ´ form) Results Details Vasicek / OU dXt = a(b âˆ’ Xt) d t + Ïƒ dWt

Fig. 3 Appendix A.1 IGBM dXt = a(b âˆ’ Xt) d t + ÏƒX t dWt Appendix A.2 CIR dXt = a(b âˆ’ Xt) d t + ÏƒâˆšXt dWt Appendix A.3 Wrightâ€“Fisher dXt = sX t(1 âˆ’ Xt) d t + âˆšÎ³X t(1 âˆ’ Xt) d Wt Appendix A.4 Log-Heston 

dXt =

> (

Î¼ âˆ’ Vt

2

> )

dt + âˆšVt dW xt ,

dVt = a(b âˆ’ Vt) d t + ÏƒâˆšVt dW vt ,Ï = Correlation( W x, W v).

Fig. 4 Appendix A.5 Table 2: Examples of widely-used SDEs. For the CIR and Vasicek models, there are furthermore closed formulas for the Initial Bond Price 

B(T ) = E

> [

exp 

> (

âˆ’ 

> âˆ«T
> 0

Xs ds

> )]

. (4) If Xt models a short rate, then B(T ) is the fair price at time zero of a zero-coupon bond paying 1

at maturity time T (see e.g. [8, Ch. 15]). For a given cubature or set of Monte Carlo paths, we then compute the empirical expectation Ë†B(T ) and with it the relative error |B(T ) âˆ’ Ë†B(T )|/B (T ), which we call the Initial Bond Price Relative Error , see Figure 3. Again, we use T = 1 .0. An advantage of this error metric over the MVE is that the bond price depends on the entire solution path rather than just its final value. Finally, we also consider the logâ€“Heston model (see Table 2), which is driven by two independent Brownian motions and therefore requires two -dimensional cubature formulae to simulate. For the 5Figure 3: Error plots showing the performance of our cubature formulae in comparison with plain Monte Carlo and QMC methods across a range of different SDEs and error metrics. Unlabelled cubature formulae in orange are dyadic cubature formulae, see Section 3 as well as Appendix E for full-sized fully labelled versions of the same plots. 6Figure 4: Error plot showing the performance of our cubature formulae in comparison with plain Monte Carlo and QMC methods measured in Call Price Relative Error (see (5)) in the logâ€“Heston model. Superscripts in cubature labels refer to the dyadic depth of the cubature, see Section 3. logâ€“Heston model, there is a known closed formula for the Call Price 

C(T ) = e âˆ’Î¼T E

> [(

eXT âˆ’ K

> )
> +
> ]

, (5) which is the price at time zero of a European call option maturing at time T with strike price K

(we use T = 1 .0 and K = 2 .0). Analogously to the bond price, we use the Call Price Relative Error 

as the error metric for the logâ€“Heston model, see Fig. 4. In addition to plain Monte Carlo, the results in Fig. 3 and 4 also include two Quasiâ€“Monte Carlo (QMC) methods as baselines. When using QMC, one replaces the sequence of independent samples of Gaussian increments of the discretised Brownian motion driving the SDE in Monte Carlo with deterministic sequences that are deliberately constructed to explore the range of possible outcomes more uniformly. In low-dimensional problems, QMC is known to improve the convergence rate of Monte Carlo from O(1 /âˆšM ) to O(1 /M ), see e.g. [88, 69]. No guaranteed error bound is known in the infinite-dimensional setting of SDE simulation, but empirically QMC usually performs slightly better than plain Monte Carlo. In our experiments, we include Sobol sequences [79] and Latin hypercube sequences [66] as QMC baselines (dark and light green in Fig. 3, respectively). We also tested Halton sequences [36], but they were consistently inferior (typically close to ordinary Monte Carlo), so we omit them from the plots. As the results in Fig. 3 and 4 show, our cubature formulae consistently outperform both standard Monte Carlo and QMC by orders of magnitude. The only exception is the Wrightâ€“Fisher Diffusion (bottom right in Fig. 3), where cubature formulae only perform marginally better. We believe this is due to the fact that the Wrightâ€“Fisher SDE has non-smooth diffusion coefficients close to the boundary; we go into detail on this in the next section. Note further that the error in the CIR and Vasicek Initial Bond Price Relative Error plots (Fig. 3 right column, top and center row) plateaus between 10 âˆ’7 and 10 âˆ’8. We believe that this is because the cubature error becomes so small that 7Figure 5: Solution paths generated by the degree-19 cubature formula for the Wrightâ€“Fisher dif-fusion, simulated up to final times T = 0 .1, T = 1 .0, and T = 10 .0 (top row, left to right). Corresponding MVE plots are shown in the bottom row, with a shared y-axis scale. the overall accuracy is instead limited by numerical errors introduced by the ODE solver, which are of the same order of magnitude. For the IGBM model, we also illustrate the actual solution paths for one of our smaller cubatures, compared with an equivalent number of Monte Carlo paths, see Figure 7. 

2.1 Limitations 

Smoothness Just as standard quadrature formulae can lose accuracy when the test functions are non-smooth, the accuracy of our cubature formulae will depend on the smoothness of the SDE coefficients. For example, the diffusion coefficient of the Wrightâ€“Fisher diffusion, âˆšÎ³X t(1 âˆ’ Xt),is not smooth close to the boundaries at 0 and 1. But unlike in the CIR model, where a similar non-smoothness exists at the boundary 0 but most solution paths never get close to it, solution paths to the Wrightâ€“Fisher diffusion do interact strongly with and frequently get absorbed by the boundary. To illustrate this effect, Fig. 5 shows error plots for the Wrightâ€“Fisher diffusion when simulated until times T = 0 .1, T = 1 .0, and T = 10 .0, together with the associated solution paths from our degree 19 cubature. When simulated until time T = 0 .1, most solution paths have not reached the boundary and our cubatures perform much better than Monte Carlo, to a degree that is comparable with the other Meanâ€“Variance plots in Fig. 3. When simulated until time T = 1 .0 (as in Fig. 3), a significant proportion of the paths have been absorbed by or are close to the boundary where the diffusion coefficients are non-smooth, and the cubature formulae drop in performance. Interestingly, when simulated until time T = 10 .0, cubature formulae regain their advantage. We suspect this is because at that time, most solution paths have been absorbed by the boundary, which simplifies the distribution of yT , making its mean and variance easier to estimate. 8Figure 6: Solution paths generated by the degree-19 cubature formula for the IGBM SDE, simulated up to final times T = 1 .0, T = 3 .0, and T = 10 .0 (top row, left to right). Corresponding MVE plots are shown in the bottom row, with a shared y-axis scale. 

Long time horizons Our cubature formulae were constructed for a unit time interval T = 1 .0.Although they can be rescaled to longer horizonsâ€”as done in the experiments shown in Fig. 5â€”one would generally expect their accuracy to degrade as the time horizon increases. To assess this effect, we simulated the IGBM model over time intervals of length T = 1 .0 (as in Fig. 3), T = 3 .0, and T =10 .0. The resulting error plots are shown in Fig. 6. As anticipated, performance does deteriorate with increasing time horizon, but the effect is noticeably weaker than expected. In particular, the cubature formulae retain a consistent advantage over Monte Carlo and QMC methods across all horizons considered. We also observe that dyadic cubatures exhibit a more pronounced loss of accuracy than their non-dyadic counterparts, a phenomenon for which we currently have no clear explanation. 

# 3 Methods 

In this section, we give a detailed explanation of how the ARCANE algorithm works and present our main theoretical results. However, before we can do either, we start with a brief primer on the relevant mathematical background and notation. 

3.1 Background 

The mathematical foundation underlying our ARCANE algorithm relies on concepts from rough path theory and the theory of signatures. We provide here a minimal introduction to these concepts, referring the reader to Appendices B and C for a more comprehensive treatment. 9Figure 7: Illustration of the solution paths of IGBM, dXt = a(b âˆ’ Xt) d t + ÏƒX t dWt , when driven by our 91 cubature (degree 9, dyadic depth 1), which consists of 200 paths (top), and the solution paths to the same SDE when driven by 200 randomly sampled (Monte Carlo) paths (bottom). Despite consisting of the same number of paths, and hence having the exact same cost when used to solve the SDE, the mean of the cubature solution paths stays much closer to the true mean than that of the Monte Carlo solution paths. (The cubature looks as though it has fewer paths because many of its paths have small weight and are therefore less visible.) 10 Signatures and Cubature Formulae Given a path x : [0 , T ] â†’ Rd+1 , where x0(t) = t is fixed to be the time coordinate, the signature of x is the collection of all its iterated integrals: 

S( x) = 

> (

1, S1(x), S2(x), . . . 

> )

,

where Sm(x) = (S J (x) : J âˆˆ { 0, . . . , d }m) is the collection of m-fold iterated integrals of x. That is, if J = ( j1, . . . , j m) âˆˆ { 0, . . . , d }m, then 

SJ (x) = 

> âˆ«
> 0<u 1<Â·Â·Â· <u m<T

dxj1 (t) . . . dxjm (t) âˆˆ R.

For Brownian motion, these integrals are understood in the Stratonovich sense. The degree of a multi-index J = ( j1, . . . , j m) and its associated signature term SJ (x) is 

âˆ¥Jâˆ¥ = m + |{ i : ji = 0 }| , (6) that is, the number of terms in J with zeros (which correspond to the time coordinate) counted twice. This definition is rooted in the fact that the Brownian signature SJ

> 0,t

(W ) is equal in distri-bution to (âˆšt )âˆ¥Jâˆ¥ SJ

> 0,1

(W ).The signature captures important geometric information about a path; the first level is the incre-ment, S1(x) = x(T ) âˆ’ x(0) , the second level encodes the area enclosed between the path and the straight line from its startpoint to its endpoint, and the higher order terms can be viewed as higher order geometric information (e.g. volumes). A key insight of rough path theory (see Appendix B for a short and [62] for a comprehensive introduction) is that the signature of a path determines how it behaves when used as a driving signal in differential equations like (1). Another insight is that the expected signature of a random 

path captures its distribution (see Lemma C.1). In combination, this motivates the definition of a cubature formula on Wiener space . That is, a collection of deterministic paths (Ï‰i)Mi=1 with associated probability weights (Î»i)Mi=1 which seek to â€œemulateâ€ a Brownian motion by matching its expected signature up to some degree D:

> Mâˆ‘
> i=1

Î»i SJ (Ï‰i) = E [ SJ (B)],

for all J with âˆ¥Jâˆ¥ â‰¤ D. Then, D is called the degree of the cubature. 

Size of Cubature Formulae Lyons and Victoir [61, Thm. 2.4] proved that for any d, D âˆˆ N, a 

d-dimensional cubature formula of degree D exists that consists of at most 

M (d, D ) := |{ J âˆˆ { 0, . . . , d }âˆ— : âˆ¥Jâˆ¥ âˆˆ { D âˆ’ 1, D }}| (7) paths, where {0, . . . , d }âˆ— = â‹ƒâˆ

> k=0

{0, . . . , d }k is the set of all multi-indices. See Table 3 for a few example values of M (d, D ).Our ARCANE algorithm constructs cubature formulae with size at most M (d, D ), often even smaller (see Table 3). 11 Connection to rough path theory Related to cubature formulae for SDEs is rough path theoryâ€”specifically expected path signatures [19] and the universal limit theorem (Theorem B.2 in the appendix). This theorem guarantees that if a sequence of cubature formulae converges to Brownian motion in an appropriate sense, then solutions of SDEs driven by these cubatures will converge to the true solution driven by Brownian motion. This also underpins our theoretical results in Section 3.4, where we prove asymptotic correctness of our cubature formulae under certain assumptions. This provides a theoretical justification for replacing Monte Carlo sampling with deterministic cubature formulae for SDE simulation, as outlined in Table 1. Prior to our work, the universal limit theorem had seen applications in the context of SDE simula-tion. For example, in [33], it was used to show the pathwise convergence of numerical methods for simulating multidimensional SDEs with adaptive step sizes. 

3.2 The ARCANE algorithm 

As an input to the algorithm, we fix a Brownian dimension d â‰¥ 1 and a degree D â‰¥ 1. The ARCANE algorithm consists of the following core steps, each of which we explain in more detail below and illustrate in Fig. 8. 

â€¢ We start by constructing an approximate cubature from a binary orthogonal array (or, equiv-alently, from a kind of error-correcting code). 

â€¢ We then compress that approximate cubature to minimal size through an algorithm called â€œRecombinationâ€ [57, 82]â€”implemented efficiently on GPU. 

â€¢ Next, we use a linear program to modify the weights of the compressed, approximate cubature in such a way that it becomes an exact cubature. 

â€¢ In an optional final step, we take the product of the cubature with itself and recombine it again, yielding a cubature that matches the Brownian signature moments to the given degree 

D not only on [0 , 1] , but also on [0 , 0.5] and [0 .5, 1] . This step can be repeated for any number of times, yielding cubatures of arbitrary dyadic depth .We now describe each step in more detail. 

Step 1: Construction of Approximate Cubature Outcome : Approximate cubature of size (N d )âŒŠD/ 2âŒ‹ (with uniform weights). An obvious way of generating approximate cubatures are product constructions. In the simplest case, we could divide [0 , 1] into N equally sized subintervals, and consider the (uniformly weighted) set of 2N d paths that take a step of size Â±1/âˆšN in each of the d dimensions on each of the N

intervals, with linear interpolations in between. This is nothing but the simple symmetric random walk on [0 , 1] with N steps and Brownian scaling, which is well-known to converge to Brownian motion as N â†’ âˆ . Its expected signature can be shown to be within a relative error of O(1 /N ) of that of Brownian motion up to any fixed degree (see Lemma C.8), making it a valid yet impractically large approximate cubature. 12 1

> 1
> 1
> 1
> 0
> 0
> 0
> 0

# Symmetrise Paths 

(e.g. reflect increments )

> 1
> 0
> 1
> 0
> 1
> 0
> 1
> 0
> 1
> 1
> 1
> 1
> 0
> 0
> 0
> 0

OA  Piecewise Linear Paths    

> Î”ğ‘¡ Î”ğ‘¡ Î”ğ‘¡ Î”ğ‘¡
> +Î¾Î”ğ‘¡
> âˆ’Î¾Î”ğ‘¡
> 1
> 1
> 0
> 0
> 1
> 1
> 0
> 0
> 1
> 1
> 0
> 0
> 1
> 1
> 0
> 0
> 1
> 1
> 1
> 1
> 1
> 1
> 1
> 1
> 0
> 0
> 0
> 0
> 0
> 0
> 0
> 0
> 1
> 0
> 1
> 0
> 1
> 0
> 1
> 0

# Path Signature Computation 

# á‰ à¶± â‹¯ à¶±

0< ğ‘¡ 1< â‹¯ < ğ‘¡ ğ‘› < 1

# ğ‘‘ ğ‘‹ ğ‘¡ 1

ğ‘– 1 â‹¯ ğ‘‘ ğ‘‹ ğ‘¡ ğ‘› 

ğ‘– ğ‘› á‰‘

# Recombination  Sharpening 

(finds weights for an exact cubature) 

# Concatenate paths 

(and multiply their weights) 

Signatures of 

piecewise linear paths 

with uniform weights 

## Iterated integrals of paths with time 

Non -uniformly weighted 

subset with the same 

centre of mass 

# Ã— 1 Ã— ğ‘€ 

# Product 

# Cubature 

# Optional steps 

# Generate Orthogonal Arrays 

(and convert to paths) Figure 8: High-level overview of the ARCANE algorithm. 13 Our key observation is that constructions from the theory of error-correcting codes and orthogonal arrays [40, 46] can be adapted to find a subset of those 2N d paths of much smaller size (N d )âŒŠD/ 2âŒ‹

that, again with uniform weights, has the same expected signature up to the relevant degree D.This reduction in size is transformative; for example, for dimension d = 1 and degree D = 9 , if 

N = 64 (which we found to be minimal for the success of step 3 below), the naive construction would consist of 2N â‰ˆ 1.8 Ã— 10 19 paths, which our construction reduces to N 4 â‰ˆ 1.7 Ã— 10 7 paths. Further examples are given in Table 3. More specifically, suppose that A is a binary orthogonal array with N d columns and strength 

D. That is, A is an M Ã— N d matrix with Â±1 entries such that any M Ã— D submatrix obtained by selecting any D columns has the property that all 2D possible rows appear the same number of times. The probabilistic interpretation of a binary orthogonal array with strength D and N d 

columns is that of a coupling between N d Rademacher random variables (coin flips Â±1) such that any D of them are independent. We construct our cubature formulae from A by transforming each row of the matrix into a path, whose increments are equal to the rowâ€™s entries (in arbitrary order, and scaled by 1/âˆšN ). An orthogonal array can be constructed, for example, by letting its rows be the code words of a binary error-correcting code of length N d with â€œdual distanceâ€ D + 1 (see Definition 4 and Theorem D.3, or [39, Thm. 4.9]). If N d is a power of two, then using a certain code based on work of Bose and Ray [10] yields such an orthogonal array with M = ( N d )âŒŠD/ 2âŒ‹ rows. We use the â€œorrayâ€ python package [54], which contains efficient JAX [11] implementations of state-of-the-art orthogonal arrays, and was implemented primarily by the first author of this paper. Note that flipping all entries of an arbitrary column of an orthogonal array yields another orthogonal array. We have found that it increases the stability of the algorithm to randomly flip each column with probability 1/2 each, independently of each other. One advantage of this is that, regardless of the original array, the marginal distribution of every row will be a sequence of independent coin flips, matching the distribution of a simple random walk. The run-time of this step is O(n log n) where n is the size of the output (the approximate cubature). We note that orthogonal arrays have previously been used to construct cubature formulae for measures on Rn [87]. 

Step 2: Recombination Outcome : Approximate cubature of size at most M (d, D ) (with non-uniform weights) 

Recombination [57, 82] is an algorithm that takes a set of points (xi)Mi=1 in Rm for some m âˆˆ N,with probability weights (Î»i)Mi=1 , and returns a subset I âŠ‚ { 1, . . . , M } of size |I| â‰¤ m + 1 together with new probability weights (ËœÎ»i)iâˆˆI such that the weighted mean is preserved: 

> Mâˆ‘
> i=1

Î»ixi = âˆ‘

> iâˆˆI

ËœÎ»ixi .

CarathÃ©odoryâ€™s convex hull theorem guarantees that such a subset always exists, and that the size m + 1 of the subset is optimal in general (in the sense that there exist examples where no smaller subsets with the desired property exist; for example, consider the d + 1 corners of the standard simplex in Rd with e.g. uniform weights). The run-time of recombination is O(M m +log( M/m )m3) [82, page 61]. 14 Table 3: For different degrees D â‰¥ 5, and d = 1 fixed, this table lists the minimal N (restricted to powers of 2) for which the sharpening in Step 3 succeeds, the output size of Step 1 (naive and actual), and an upper bound on the output size of Steps 2 and 3 (with and without symmetrisation). Dimension Degree Minimum N Step 1 Output Size Steps 2 & 3 Output Size 

d D N naive  

> 2N d
> actual
> (N d )âŒŠD/ 2âŒ‹
> default
> M(d, D )
> symmetrised
> 2dMeven (d, D )

1 5 32 4.3 Ã— 10 9 1.0 Ã— 10 3 13 10 1 7 32 4.3 Ã— 10 9 3.3 Ã— 10 4 34 26 1 9 64 1.8 Ã— 10 19 1.7 Ã— 10 7 89 68 1 11 64 1.8 Ã— 10 19 1.1 Ã— 10 9 233 178 1 13 64 1.8 Ã— 10 19 6.9 Ã— 10 10 610 466 1 15 64 1.8 Ã— 10 19 4.4 Ã— 10 12 1597 1220 1 17 64 1.8 Ã— 10 19 2.8 Ã— 10 14 4181 3194 1 19 128 3.4 Ã— 10 38 9.2 Ã— 10 18 10976 8362 

Since signatures can be viewed simply as vectors of real numbers, we can apply recombination to the approximate cubature obtained in Step 1 to obtain a new approximate cubature with the same expected signature, but size at most M (d, D ) (recall (7)), and in general with non-uniform weights. For example, if d = 1 and D = 9 , then the size of the compressed cubature is at most 143 , down from 1.6 Ã— 10 7. Examples for more values of D and N are in Table 3. We use the Baryx library [22], which contains an efficient JAX [11] implementation of recombination that was implemented primarily by the second author of this paper. 

Symmetrisation. An optional speed-up in this step can be achieved by exploiting symmetries: a d-dimensional Brownian motion B = ( B(1) , . . . , B (d)) has the same distribution as (Îµ1B(1) , . . . , Îµ dB(d))

for any Îµ1, . . . , Îµ d âˆˆ {âˆ’ 1, +1 }. This implies that many signature terms in the expected Brownian signature are automatically zeroâ€”namely the odd terms, which are those corresponding to multi-indices J where at least one non-zero index j âˆˆ { 1, . . . , d } appears an odd number of times. By enforcing the same set of symmetries on our cubature paths, we can ensure that their expected signature is also zero at odd terms, and hence matches exactly. The recombination algorithms must then only be applied to even terms, which makes it significantly faster since recombination scales cubically in the number of terms whose weighted mean is preserved. Furthermore, the size of the resulting cubature is 2dMeven (d, D ), where 

[Meven (d, D ) = |{ I âˆˆ { 0, . . . , d }âˆ— : âˆ¥Iâˆ¥ â‰¤ { D âˆ’ 1, D } and âˆ¥Iâˆ¥ even }| .

(Cmp. (7).) A comparison with M (d, D ) is in Table 3 for a few example values. 

Step 3: Sharpening Outcome : If successful, exact cubature of size M (d, D ) or 2dMeven (d, D ).In this step, we check whether the weights of the approximate cubature obtained in Step 2 can be adjusted in such a way that the resulting reweighted cubature is exact. There are different ways of doing so, and we implemented several (including gradient descent, a linear program, or an alternating projections method; see our code base for details). We have no theoretical guarantee 15 for the success of this step, but in our experiments it always succeeded if N was sufficiently large. We record the minimum value of N (restricted to powers of 2) required for the sharpening to succeed as a function of the degree D in Table 3. We also found that it can be helpful to stop the recombination in Step 2 early, e.g. stop when the approximate cubature has been compressed to three or four times its minimal size, in order to give the linear program more degrees of freedom. 

Step 4: Dyadic Construction Outcome : Exact dyadic cubature. Given an exact cubature formula made up of paths (Ï‰i)Mi=1 and probability weights (Î»i)Mi=1 , we can construct the product of the cubature with itself: first we rescale each of the paths Ï‰i : [0 , 1] â†’ Rd

to a path ËœÏ‰i : [0 , 0.5] â†’ Rd as follows: 

ËœÏ‰i(t) = Ï‰i(2 t)

âˆš2 .

Then the product cubature is made up of M 2 paths (Ï‰prod  

> i,j

)Mi,j =1 , where Ï‰prod  

> i,j

: [0 , 1] â†’ Rd is the concatenation of ËœÏ‰i and ËœÏ‰j , with associated weight Î»prod  

> i,j

= Î»iÎ»j . This cubature has the property that it matches the expected Brownian signature on [0 , 0.5] and [0 .5, 1] in addition to [0 , 1] . We then apply recombination to compress this cubature to its minimal size. This process can be repeated iteratively, to obtain cubatures that match the expected Brownian signature on all intervals of the form [k2âˆ’nd , (k + 1)2 âˆ’nd ] for some nd â‰¥ 0, which we call the dyadic depth of the cubature (an ordinary cubature as produced by Step 3 has dyadic depth zero). We have a theoretical result (Theorem 1 below) indicating that scaling the dyadic depth of a cubature alongside its degree is guaranteed to yield an asymptotically consistent SDE estimator. 

3.3 Parameter Ablation 

The two free parameters of our algorithm are the number of steps N , and the seed for the randomi-sation in Step 1. We have always used the minimum number of steps N for which the sharpening step succeeds, but it would be reasonable to ask if increasing N further has an impact on perfor-mance. In Fig. 9, we show our error metrics on the CIR, IGBM, and Vasicek models with different cubature formulae; for each degree from 5 to 11 , we run the construction with several different values of N as well as several randomisation seeds. As Fig. 9 illustrates, increasing N beyond the minimum value does not seem to impact performance positively or negatively. But it does significantly increase the construction cost, justifying our approach of always using the minimum required value of N for successful sharpening. 

3.4 Theoretical results 

In this section, we want to establish conditions under which our cubatures are guaranteed to exhibit asymptotic consistency in the following sense: 

If yn is the solution to the SDE (1) driven by an ARCANE cubature (Î»ni , Ï‰ ni ), and 

y is the solution to the same SDE driven by Brownian motion, then yn â†’ y in distribution in the space of continuous paths. 

(C) 16 16 64 256 1024 

Number of time steps      

> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> Relative Error in Initial Bond Price

CIR    

> 16 64 256 1024

Number of time steps     

> 10 6
> 10 5
> 10 4
> 10 3
> Relative Error in Initial Bond Price

Vasicek    

> 16 64 256 1024

Number of time steps   

> 10 3
> 10 2
> Mean-Variance Error

IGBM    

> 16 64 256 1024

Number of time steps   

> 10 2
> 10 1
> Mean-Variance Error

CIR    

> 16 64 256 1024

Number of time steps   

> 10 3
> 10 2
> Mean-Variance Error

Vasicek 

degree 

5

7

9

11 Figure 9: Error plots for cubature formulae of degrees 5, 7, 9, and 11 , shown as a function of the number of steps N used in Step 1 of the construction (with multiple randomisation seeds each). Increasing N beyond the minimum required for successful sharpening has no discernible effect on performance. 17 We conjecture that (C) holds as long as Dn â†’ âˆ , with no assumptions on the dyadic depth. We fail to prove this (for the reader familiar with the language, we can show uniqueness of the subsequential limit, but not tightness), but what we can prove is that (C) holds if, in addition to 

Dn â†’ âˆ , either the dyadic depth goes to infinity along with the degree (Theorem 1), or we skip the recombination Step (Theorem 2). 

Theorem 1. Suppose (Î»ni , Ï‰ ni ) is an ARCANE cubature with degree Dn â†’ âˆ , and dyadic depth 

mn âˆˆ N such that 

mn â‰¥ 

> (

34 + Îµ

> )

log 2 Nn âˆ’ C

for some Îµ, C > 0. Then ( C) holds. Remark 1. For the assertion of Theorem 1 to hold, it is in fact sufficient if the cubature matches the expected Brownian signature up to degree Dn â†’ âˆ just on [0 , 1] , and only to degree D = 7 on all of the finer dyadic intervals. 

Theorem 2. Let (Î»ni , Ï‰ ni ) be the approximate cubature resulting from the first step of the ARCANE algorithm, prior to recombination, sharpening, and any dyadic constructions, with Nn â†’ âˆ and degree Dn â†’ âˆ . Then ( C) holds. 

For the reader familiar with rough path theory, we remark that the essence of the proofs is showing that the cubatures converge in distribution to canonical Brownian motion in the space of rough paths , in which case (C) follows from the universal limit theorem [62, Section 3.5] (convergence in the space of ordinary paths would not imply (C)). 

Remark 2. Work closely related to these types of theorems has been done on Donsker-type theorems on rough path space . Specifically, Breuillard, Friz, and Huesmann [12] give conditions under which a random walk with i.i.d. increments converges to Brownian motion in rough path space, and Bayer and Friz [7] prove convergence of product cubature formulae to Brownian motion in rough path space. The latter would imply (a stronger version of) our Theorem 1 if we didnâ€™t do the (essential) recombination step after the product construction. Nevertheless, the types of arguments used in the proof are very similar. Some of the facts established in the proofs may be of independent interest, such as Lemma C.2, which gives conditions under which pointwise convergence of expected signatures of random rough paths implies convergence in distribution and vice versa. The proofs of Theorems 1 and 2 are relatively involved and in Appendix C. 

# 4 Conclusion 

In this paper, we have developed a computational pipeline called â€œARCANEâ€ for constructing new state-of-the-art cubature formulae for numerically simulating SDEs. Using the ARCANE algorithm, we are able to construct high-degree cubature formulae with thou-sands of paths that empirically achieve orders of magnitude more estimation accuracy compared to standard Monte Carlo simulation across several real-world SDEs from mathematical finance and population genetics. 18 In addition, we showed that our cubature formulae are theoretically principled in the sense that, as their degree increases (and their degree on dyadic subintervals is â‰¥ 7), their distribution will converge to that of Brownian motion in a rough path sense. Importantly, and in contrast to past convergence results for SDE cubature formulae, this does not restrict the time interval on which the cubature formulae can be applied. To help facilitate applications, we have made our implementation of the ARCANE algorithm pub-licly availableâ€”along with datasets containing the cubature formulae used in our experiments. These can be found at github.com/tttc3/ARCANE-Cubature. 

# 5 Future work 

Neural SDEs Whilst SDEs have a wide range of uses, we expect that our state-of-the-art AR-CANE cubature formulae will be more impactful in computationally demanding applications. How-ever, many such applications (e.g. Langevin Markov Chain Monte Carlo [77]) require simulating a high-dimensional Brownian motionâ€”which is currently out of scope for our ARCANE cubature methodology. Instead, we believe that a more suitable application would be â€œNeuralâ€ SDEs [56, 52, 51] where only a low-dimensional Brownian motion is needed, but performing backpropagation through the steps of the SDE solver (for each sample path) can result in a significant computational cost during training. As an example, in [17], the following SDE model for wealth distribution is considered: 

dwt = ( rtwt âˆ’ cÎ¸(t, w t , Î· t) + Î·t) d t, 

dÎ·t = Î¼(t, Î· t) d t + Ïƒ(t, Î· t) d Wt , (8) where wt and Î·t denote the wealth and income of a household at time t, rt is the interest rate, cÎ¸ is the householdâ€™s consumption (depending on (t, w t, Î· t) and parameters Î¸) and Î¼, Ïƒ : [0 , T ] Ã— R â†’ R

are functions governing the dynamics of the household income. Since it is challenging to prescribe how a household determines its level of consumption, the authors of [17] make the assumption that a householdâ€™s consumption policy cÎ¸ is chosen to maximise a utility 

u1(cÎ¸) whilst leaving some wealth wT at terminal time T . More precisely, they model cÎ¸ using a neural networkâ€”which they train to maximise 

max 

> Î¸
> (

E 

> [ âˆ« T
> 0

eâˆ’Î´t u1(cÎ¸(t, w t, Î· t)) d t + Î»u 2(wT )

> ])

,

where Î´, Î» > 0 and u1 , u 2 are constant-relative-risk-aversion (CRRA) utility functions. Therefore, (8) can be seen as a real-world example of a Neural SDE within economics. Since backpropagating through the dynamics of a (Neural) SDE is computationally expensive [52], we expect that reducing the number of required simulations through ARCANE cubature formulae to be particularly beneficial for such policy-based models. Similarly, we expect that ARCANE cubature can reduce the number of simulations required for the simulation-based inference of diffusions [48]. 

Higher dimensions On the methodological side, we believe that the main area of future work would be to extend ARCANE cubatures to higher dimensions. In particular, it would be interesting to see if taking existing ARCANE cubature formulae (e.g. with high degree and/or dyadic depth) and â€˜reshapingâ€™ them into new multidimensional cubatures is an effective strategy. This could 19 potentially avoid the â€œcurse of dimensionalityâ€ inherent in the signatures of high-dimensional paths. In any case, our ARCANE methodology can be scaled further to produce cubature formulae with higher degrees and for SDEs with higher dimensional noise, requiring more computational power alongside higher precision floating point operations (particularly as the terms in signatures of paths with finite length decay factorially fast with depth [62, Proposition 2.2]). 

Efficient features Currently, we compute the (truncated) signatures of paths when constructing ARCANE cubature formulae. However, as path signatures contain various algebraic redundancies, we believe that the number of â€˜signatureâ€™ features used in our pipeline can be significantly reduced. For example, we expect that this feature reduction could be achieved by using log-signatures or by ignoring certain iterated integrals in the signature. In any case, such a feature reduction would particularly speed up the recombination stepâ€”which scales cubically with the number of features. 

Long time horizons The reduction in accuracy when simulating SDEs over longer time horizons with cubatures is well-known. One possible solution is to cut the time interval into some number 

N of equally sized sub-intervals, and apply the cubature formula on each of them. However, if the cubature formula has M paths, then applying it N times would result in M N paths. To avoid this exponential blow-up, [57] proposed to use the recombination algorithm on the remaining paths after each sub-interval to reduce the number of particles that represent the SDE solution. The challenge in this setting is to then identify a suitable collection of test functions so that the reduced collection of particles obtained by recombination remains accurate. We refer the reader to [70] for a recent article on applying cubature formulae to SDE simulation in this manner. In [70], the authors apply (high-order) recombination along with space partitioning to reduce the total number of particles used by their algorithm. A separate issue of reducing the number of particles during the SDE simulation is that it may be incompatible with automatic differentiation (e.g. for Neural SDEs [56, 52, 51]). Nevertheless, applying our new cubature formulae in this iterative fashion is left as a topic of future work. 

# 6 Acknowledgements 

PK acknowledges support from EPSRC grant EP/W523781/1 and from the Department of Statis-tics at the University of Oxford. TC acknowledges the support of the Department of Aeronautical and Automotive Engineering at Loughborough University. JF acknowledges the support of the Department of Mathematical Sciences at the University of Bath. 

# References 

[1] HansjÃ¶rg Albrecher et al. â€œThe Little Heston Trapâ€. In: Wilmott Magazine 2007.1 (Jan. 2007), pp. 83â€“92. [2] AurÃ©lien Alfonsi. â€œOn the discretization schemes for the CIR (and Bessel squared) processesâ€. In: Monte Carlo Methods and Applications 11.4 (2005), pp. 355â€“384. [3] AurÃ©lien Alfonsi. â€œHigh order discretization schemes for the CIR process: Application to Affine Term Structure and Heston modelsâ€. In: Mathematics of Computation 79.269 (2010), pp. 209â€“237. 20 [4] AurÃ©lien Alfonsi. â€œStrong order one convergence of a drift implicit Euler scheme: Application to the CIR processâ€. In: Statistics & Probability Letters 83.2 (2013), pp. 602â€“607. [5] J. Pablo Arenas-LÃ³pez and Mohamed Badaoui. â€œA Fokkerâ€“Planck equation based approach for modelling wind speed and its power outputâ€. In: Energy Conversion and Management 

222 (2020). [6] Jordi Badosa et al. â€œDay-Ahead Probabilistic Forecast of Solar Irradiance: A Stochastic Differ-ential Equation Approachâ€. In: Renewable Energy: Forecasting and Risk Management . 2018, pp. 73â€“93. [7] Christian Bayer and Peter K Friz. â€œCubature on Wiener space: pathwise convergenceâ€. In: 

Applied Mathematics & Optimization 67.2 (2013), pp. 261â€“278. [8] Tomas BjÃ¶rk. Arbitrage Theory in Continuous Time . Oxford University Press, Dec. 2019. 

isbn : 9780198851615. doi : 10.1093/oso/9780198851615.001.0001 .[9] Horatio Boedihardjo et al. â€œThe signature of a rough path: Uniquenessâ€. In: Advances in Mathematics 293 (2016), pp. 720â€“737. issn : 0001-8708. url : https://www.sciencedirect. com/science/article/pii/S0001870816301104 .[10] R. C. Bose and D. K. Ray-Chaudhuri. â€œOn a class of error correcting binary group codesâ€. In: Information and Control 3.1 (1960), pp. 68â€“79. issn : 0019-9958. url : https : / / www . sciencedirect.com/science/article/pii/S0019995860902874 .[11] James Bradbury et al. JAX: composable transformations of Python+NumPy programs . Ver-sion 0.3.13. 2018. url : https://github.com/jax-ml/jax .[12] Emmanuel Breuillard, Peter Friz, and Martin Huesmann. â€œFrom random walks to rough pathsâ€. In: Proceedings of the American Mathematical Society 137.10 (2009), pp. 3487â€“3496. [13] Yongli Cai et al. â€œA stochastic SIRS epidemic model with infectious force under intervention strategiesâ€. In: Journal of Differential Equations 259.12 (2015), pp. 7463â€“7502. [14] Luca Capriotti, Yupeng Jiang, and Gaukhar Shaimerdenova. â€œApproximation Methods for Inhomogeneous Geometric Brownian Motionâ€. In: International Journal of Theoretical and Applied Finance 22.2 (2019). [15] Khaoula Ben Chaabane et al. â€œData-driven uncertainty quantification for constrained stochas-tic differential equations and application to solar photovoltaic power forecast dataâ€. In: Statis-tics and Computing 35.163 (2025). [16] Ivan Cheltsov et al. Hadamard Langevin dynamics for sampling the l1-prior . 2025. arXiv: 

2411.11403 [math.NA] .[17] Cangxiong Chen et al. Stochastic Analysis of Overlapping Generations Models Under Incom-plete Markets . 2025. arXiv: 2509.05170 [math.PR] .[18] Ilya Chevyrev and Terry Lyons. â€œCharacteristic functions of measures on geometric rough pathsâ€. In: The Annals of Probability 44.6 (Nov. 2016). issn : 0091-1798. [19] Ilya Chevyrev and Harald Oberhauser. â€œSignature Moments to Characterize Laws of Stochas-tic Processesâ€. In: Journal of Machine Learning Research 23.176 (2022), pp. 1â€“42. url :

https://jmlr.org/papers/v23/20-1466.html .[20] Loren Cobb. Mathematical Frontiers Of The Social And Policy Sciences . New York: Rout-ledge, 1981. [21] John C. Cox, Jonathan E. Ingersoll, and Stephen A. Ross. â€œA Theory of the Term Structure of Interest Ratesâ€. In: Econometrica 53.2 (1985), pp. 385â€“407. 21 [22] Thomas Coxon. Baryx: Barycenter preserving measure reduction in JAX . Version 0.1.0. 2026. 

url : https://github.com/datasig-ac-uk/baryx .[23] Andrei Cozma and Christoph Reisinger. â€œStrong order 1/2 convergence of full truncation Euler approximations to the Coxâ€“Ingersollâ€“Ross processâ€. In: IMA Journal of Numerical Analysis 40.1 (2020), pp. 358â€“376. [24] Ricardo Crisostomo. An Analysis of the Heston Stochastic Volatility Model: Implementation and Calibration using Matlab . 2015. arXiv: 1502.02963 [q-fin.PR] .[25] Peter Czuppon and Arne Traulsen. â€œUnderstanding evolutionary and ecological dynamics using a continuum limitâ€. In: Ecology and Evolution 11.11 (2021), pp. 5857â€“5873. [26] P. Delsarte and J. M. Goethals. â€œAlternating bilinear forms over GF(q)â€. In: Journal of Combinatorial Theory, Series A 19.1 (1975), pp. 26â€“50. [27] Philippe Delsarte. â€œAn algebraic approach to the association schemes of coding theoryâ€. In: 

Philips Res. Rep. Suppl. 10 (1973). [28] Steffen Dereich, Andreas Neuenkirch, and Lukasz Szpruch. â€œAn Euler-type method for the strong approximation of the Coxâ€“Ingersollâ€“Ross processâ€. In: Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 468.2140 (2011), pp. 1105â€“1115. [29] Alison Etheridge. Some Mathematical Models from Population Genetics . Vol. 2012. Lecture Notes in Mathematics. Berlin, Heidelberg: Springer, 2011. doi : 10.1007/978-3-642-16632-7.[30] Warren J. Ewens. Mathematical Population Genetics: I. Theoretical Introduction . 2nd. Vol. 27. Interdisciplinary Applied Mathematics. New York: Springer, 2004. doi : 10.1007/978-0-387-21822-9 .[31] William Feller. â€œTwo Singular Diffusion Problemsâ€. In: Annals of Mathematics 54.1 (1951), pp. 173â€“182. doi : 10.2307/1969318 .[32] Emilio Ferrucci et al. â€œHigh-degree cubature on Wiener space through unshuffle expansionsâ€. In: Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 

482.2330 (2026). [33] James Foster and AndraÅ¾ JelinÄiÄ. On the convergence of adaptive approximations for stochas-tic differential equations . 2024. arXiv: 2311.14201 [math.NA] .[34] James M. Foster, GonÃ§alo dos Reis, and Calum Strange. â€œHigh Order Splitting Methods for SDEs Satisfying a Commutativity Conditionâ€. In: SIAM Journal on Numerical Analysis 62.1 (2024), pp. 500â€“532. [35] Lajos Gergely GyurkÃ³ and Terry Lyons. â€œEfficient and Practical Implementations of Cubature on Wiener Spaceâ€. In: Stochastic Analysis 2010 . Berlin Heidelberg: Springer, 2011, pp. 73â€“ 111. [36] J. H. Halton. â€œOn the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integralsâ€. In: Numerische Mathematik 2 (1960), pp. 84â€“90. [37] A. R. Hammons et al. â€œThe Z/sub 4/-linearity of Kerdock, Preparata, Goethals, and related codesâ€. In: IEEE Transactions on Information Theory 40.2 (1994), pp. 301â€“319. [38] Satoshi Hayakawa and Kenâ€™ichiro Tanaka. â€œMonte Carlo construction of cubature on Wiener spaceâ€. In: Japan Journal of Industrial and Applied Mathematics 39.2 (2022), pp. 543â€“571. [39] A Samad Hedayat, Neil James Alexander Sloane, and John Stufken. Orthogonal arrays: theory and applications . Springer Science & Business Media, 2012. 22 [40] A. S. Hedayat, N. J. A. Sloane, and John Stufken. Orthogonal Arrays: Theory and Applica-tions . Springer Series in Statistics. New York: Springer, 1999. [41] Mario Hefter and Arnulf Jentzen. â€œOn arbitrarily slow convergence rates for strong numerical approximations of Cox-Ingersoll-Ross processes and squared Bessel processesâ€. In: Finance and Stochastics 23 (2019), pp. 139â€“172. [42] Steven L. Heston. â€œA Closed-Form Solution for Options with Stochastic Volatility with Ap-plications to Bond and Currency Optionsâ€. In: The Review of Financial Studies 6.2 (1993), pp. 327â€“343. [43] Raymond Hill. â€œOn the largest size of cap in S53 â€. In: Rendiconti del Seminario Matematico della UniversitÃ  di Padova 54 (1973), pp. 378â€“380. url : https://www.bdim.eu/item?id= RLINA_1973_8_54_3_378_0 .[44] Raymond Hill. â€œOn Pellegrinoâ€™s 20-Caps in S43 â€. In: Combinatorics â€™81 in honour of Beni-amino Segre . Vol. 78. North-Holland Mathematics Studies. Amsterdam: North-Holland, 1983, pp. 433â€“447. [45] Yixuan Huang, Michael Tait, and Robert Won. â€œSidon sets and 2-caps in Fn 

> 3

â€. In: Involve, a Journal of Mathematics 12.6 (2019), pp. 995â€“1003. issn : 1944â€“4176. [46] W. Cary Huffman and Vera Pless. Fundamentals of Error-Correcting Codes . Cambridge: Cambridge University Press, 2003. [47] Emil B. Iversen et al. â€œProbabilistic forecasts of solar irradiance using stochastic differential equationsâ€. In: Environmetrics 25.3 (2014), pp. 152â€“164. [48] Petar Jovanovski et al. Simulation-based inference using splitting schemes for partially ob-served diffusions in chemical reaction networks . 2025. arXiv: 2508.11438 [stat.ME] .[49] CÃ³nall Kelly, Gabriel Lord, and Heru Maulana. â€œThe role of adaptivity in a numerical method for the Coxâ€“Ingersollâ€“Ross modelâ€. In: Journal of Computational and Applied Mathematics 

410 (2022). [50] A. M. Kerdock. â€œA class of low-rate nonlinear binary codesâ€. In: Information and Control 

20.2 (1972), pp. 182â€“187. issn : 0019-9958. [51] Patrick Kidger et al. â€œEfficient and Accurate Gradients for Neural SDEsâ€. In: Advances in Neural Information Processing Systems . Vol. 34. 2021, pp. 18747â€“18761. [52] Patrick Kidger et al. â€œNeural SDEs as Infinite-Dimensional GANsâ€. In: Proceedings of the 38th International Conference on Machine Learning (2021), pp. 5453â€“5463. [53] P.E. Kloeden and E. Platen. Numerical solution of stochastic differential equations . Appli-cations of Mathematics, Stochastic Modelling and Applied Probability, 23. Springer-Verlag, 1992. [54] Peter Koepernik. orray: Orthogonal Arrays in JAX . Version 0.1.0. 2025. url : https : / / github.com/peter-koepernik/orray .[55] Haiquan Li et al. â€œIdentification of a Stochastic Dynamic Model for Aircraft Flight Attitude Based on Measured Dataâ€. In: International Journal of Aerospace Engineering 2023.1 (2023). [56] Xuechen Li et al. â€œScalable Gradients for Stochastic Differential Equationsâ€. In: Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics . Vol. 108. 2020, pp. 3870â€“3882. [57] Christian Litterer and Terry Lyons. â€œHigh order recombination and an application to cubature on Wiener spaceâ€. In: Annals of Applied Probability 22.4 (2012), pp. 1301â€“1327. 23 [58] Ying Liu et al. â€œA stochastic approximation method for probability prediction of docking success for aerial refuelingâ€. In: Applied Soft Computing 103 (2021). [59] Roger Lord, Remmert Koekkoek, and Dick Van Dijk. â€œA comparison of biased simulation schemes for stochastic volatility modelsâ€. In: Quantitative Finance 10.2 (2010), pp. 177â€“194. [60] Terry Lyons and Zhongmin Qian. System control and rough paths . Oxford: Oxford University Press, 2002. [61] Terry Lyons and Nicolas Victoir. â€œCubature on Wiener Spaceâ€. In: Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 460.2041 (2004), pp. 169â€“198. [62] Terry J Lyons, Michael Caruana, and Thierry LÃ©vy. Differential equations driven by rough paths . Berlin: Springer, 2007. [63] â€œ9 BCH codesâ€. In: The Theory of Error-Correcting Codes . Ed. by F. J. MacWilliams and N. J. A. Sloane. Vol. 16. North-Holland Mathematical Library. Elsevier, 1977, pp. 257â€“293. 

doi : 10.1016/S0924-6509(08)70534-8 .[64] Yoshihiro Maki and Hideo Hirose. â€œInfectious Disease Spread Analysis Using Stochastic Dif-ferential Equations for SIR Modelâ€. In: 2013 4th International Conference on Intelligent Systems, Modelling and Simulation . 2013, pp. 152â€“156. [65] Anatoliy Malyarenko and Hossein Nohrouzian. â€œTesting Cubature Formulae on Wiener Space Versus Explicit Pricing Formulaeâ€. In: Stochastic Processes, Statistical Methods, and Engi-neering Mathematics . Cham: Springer International Publishing, 2022, pp. 223â€“248. [66] M. D. McKay, R. J. Beckman, and W. J. Conover. â€œA comparison of three methods for selecting values of input variables in the analysis of output from a computer codeâ€. In: Tech-nometrics 21.2 (1979), pp. 239â€“245. [67] P. W. Messer. â€œNeutral Models of Genetic Drift and Mutationâ€. In: Encyclopedia of Evo-lutionary Biology . Ed. by Richard M. Kliman. Oxford: Academic Press, 2016, pp. 119â€“123. 

isbn : 978-0-12-800426-5. [68] Grigori N. Milstein and John Schoenmakers. â€œUniform approximation of the Cox-Ingersoll-Ross processâ€. In: Advances in Applied Probability 47.4 (2015), pp. 1132â€“1156. [69] H. Niederreiter. Random Number Generation and Quasi-Monte Carlo Methods . Vol. 63. CBMSâ€“NSF Regional Conference Series in Applied Mathematics. SIAM, 1992. [70] Syoiti Ninomiya and Yuji Shinozaki. A high-order recombination algorithm for weak approx-imation of stochastic differential equations . 2025. arXiv: 2504.19717 [math.PR] .[71] Syoiti Ninomiya and Nicolas Victoir. â€œWeak Approximation of Stochastic Differential Equa-tions and Application to Derivative Pricingâ€. In: Applied Mathematical Finance 15.2 (2008), pp. 107â€“121. [72] Hossein Nohrouzian, Anatoliy Malyarenko, and Ying Ni. Constructing Trinomial Models Based on Cubature Method on Wiener Space: Applications to Pricing Financial Derivatives .2022. arXiv: 2204.10692 [q-fin.MF] .[73] Christine M. Oâ€™Keefe. â€œOvoids in PG(3, q): a surveyâ€. In: Discrete Mathematics 151.1 (1996), pp. 175â€“188. issn : 0012-365X. [74] Aaron Potechin. â€œMaximal caps in AG(6,3)â€. In: Designs, Codes and Cryptography 46.3 (Mar. 2008), pp. 243â€“259. issn : 1573-7586. doi : 10.1007/s10623-007-9132-z .24 [75] C. Radhakrishna Rao. â€œFactorial Experiments Derivable from Combinatorial Arrangements of Arraysâ€. In: Supplement to the Journal of the Royal Statistical Society 9.1 (1947), pp. 128â€“139. 

issn : 14666162. url : https://www.jstor.org/stable/2983576 (visited on 02/18/2025). [76] Hamood Ur Rehman et al. â€œAnalysis of Brownian motion in stochastic SchrÃ¶dinger wave equation using Sardar sub-equation methodâ€. In: Optik 289 (2023). [77] Maximilian Scott et al. Underdamped Langevin MCMC with third order convergence . 2025. arXiv: 2508.16485 [stat.ML] .[78] Yuji Shinozaki. â€œConstruction of a Third-Order K-Scheme and Its Application to Financial Modelsâ€. In: SIAM Journal on Financial Mathematics 8.1 (2017), pp. 901â€“932. [79] I. M. Sobolâ€™. â€œOn the distribution of points in a cube and the approximate evaluation of inte-gralsâ€. In: USSR Computational Mathematics and Mathematical Physics 7.4 (1967), pp. 86â€“ 112. [80] Yang Song et al. â€œScore-Based Generative Modeling through Stochastic Differential Equa-tionsâ€. In: International Conference on Learning Representations . 2021. [81] Douglas R. Stinson. â€œCombinatorial designs: constructions and analysisâ€. In: SIGACT News 

39.4 (2008), pp. 17â€“21. issn : 0163-5700. doi : 10.1145/1466390.1466393 .[82] Maria Tchernychova. â€œCarathÃ©odory cubature measuresâ€. PhD thesis. University of Oxford, 2015. [83] Irene Tubikanec et al. â€œQualitative properties of different numerical methods for the inhomo-geneous geometric Brownian motionâ€. In: Journal of Computational and Applied Mathematics 

406 (2022). [84] G. E. Uhlenbeck and L. S. Ornstein. â€œOn the Theory of the Brownian Motionâ€. In: Physical Review 36 (5 1930), pp. 823â€“841. [85] Marc Vaisband et al. â€œLoss formulations for assumption-free neural inference of SDE coeffi-cient functionsâ€. In: Nature Partner Journal: Systems Biology and Applications 11.22 (2025). [86] Oldrich Vasicek. â€œAn equilibrium characterization of the term structureâ€. In: Journal of Fi-nancial Economics 5.2 (1977), pp. 177â€“188. issn : 0304-405X. url : https://www.sciencedirect. com/science/article/pii/0304405X77900162 .[87] Nicolas Victoir. â€œAsymmetric Cubature Formulae with Few Points in High Dimension for Symmetric Measuresâ€. In: SIAM Journal on Numerical Analysis 42.1 (2004), pp. 209â€“227. 

doi : 10.1137/S0036142902407952 .[88] X. Wang. â€œLow discrepancy sequences in high dimensions: How well do they integrate?â€ In: 

Journal of Computational and Applied Mathematics 215.2 (2008), pp. 301â€“314. [89] Yasen Wang et al. â€œData-Driven Discovery of Stochastic Differential Equationsâ€. In: Engi-neering 17 (2022), pp. 244â€“252. 25 Appendix A Example SDEs 

This appendix provides the details for all the example SDEs used in our experiments. In all the examples, we first consider the ItÃ´ formulation of the SDE to obtain reference values. We then convert the SDE into Stratonovich form to perform the simulations. Throughout, we will use the following shorthand notation: 

â€¢ We use Ex0 [Xt] for E[Xt | X0 = x0], the expectation of Xt conditional on X0 = x0.

â€¢ We use Vx0 [Xt] for Var( Xt | X0 = x0), the variance of Xt conditional on X0 = x0.

A.1 Ornsteinâ€“Uhlenbeck (Vasicek) process 

The Ornsteinâ€“Uhlenbeck (OU) or Vasicek process is a Gaussian process originally developed as a model for the mean values of the velocity and displacement of a free particle in Brownian motion [84]. Beyond the usual applications in the natural sciences, the OU process has been applied in mathematical finance as a mean-reverting one-factor short-rate model (i.e. a model in which the interest rate is driven by a single source of randomness). For derivative pricing, this is also known as the Vasicek model [86]. More recently, multidimensional OU processes have seen applications in data science as the forward noising processes in SDE-based generative diffusion models [80]. 

Definition 

Let a > 0, b âˆˆ R, and Ïƒ > 0. Then the OU process with mean reversion rate a, long-term mean b,and volatility Ïƒ is 

dXt = a (b âˆ’ Xt) d t + Ïƒ dWt, (9) where t > 0 and Wt is a standard Brownian motion. In Stratonovich form, it becomes 

dXt = astrat (bstrat âˆ’ Xt) d t + Ïƒ â—¦ dWt, (10) where 

astrat := a, bstrat := b, 

which coincides with the ItÃ´ form since the diffusion coefficient is constant. 

Default parameters 

Unless otherwise specified, our results for the OU process use the parameters: 

X0 = 2 .0, a = 1 .0, b = 3 .0, Ïƒ = 0 .5, T = 1 .0.

Reference values 

All errors quoted in our results for the OU process are with respect to the following theoretically derived reference values: 26 1. Conditional mean [84]: 

Ex0 [Xt] = b + ( x0 âˆ’ b) exp( âˆ’at ). (11) 2. Conditional variance [84]: 

Vx0 [Xt] = Ïƒ2

2a

> (

1 âˆ’ exp( âˆ’2at )). (12) 3. Conditional distribution [84]: 

(Xt | X0) âˆ¼ N (Ex0 [Xt], Vx0 [Xt]). (13) 4. Price at time s of a zero-coupon bond maturing at time T [86]: 

E

> [

exp 

> (

âˆ’ 

> âˆ«Ts

Xt dt 

> ) âˆ£ âˆ£âˆ£âˆ£

Xs

> ]

= A(s, T ) exp (âˆ’XsB(s, T )), (14) where 

A(s, T ) = exp 

> (

Î³ (B(s, T ) âˆ’ (T âˆ’ s)) âˆ’ Ïƒ2

4a B(s, T )2

> )

,B(s, T ) = 1 âˆ’ exp (âˆ’a (T âˆ’ s))

a , Î³ = b âˆ’ Ïƒ2

2a2 .

Numerical considerations 

We do not take any special numerical considerations when simulating the OU process. 

A.2 Inhomogeneous Geometric Brownian Motion 

Inhomogeneous geometric Brownian motion (IGBM) is another example of a one-factor short-rate model used in mathematical finance [14]. However, unlike the OU process, IGBM has multiplicative noise and remains non-negative for all t â‰¥ 0. It is therefore suitable for modelling interest rates, stochastic volatilities and default intensities. From a theoretical viewpoint, IGBM is also one of the simplest SDEs that has no known method of exact simulation. As a consequence, it has been used as a test problem to study the properties of numerical methods for Monte Carlo simulation [83]. 

Definition 

Let a > 0, b > 0, and Ïƒ > 0. The inhomogeneous geometric Brownian motion (IGBM) with mean reversion rate a, long-term mean b, and volatility Ïƒ is given by 

dXt = a (b âˆ’ Xt) d t + ÏƒX t dWt, (15) where t > 0 and Wt is a standard Brownian motion. In Stratonovich form, this becomes 

dXt = astrat (bstrat âˆ’ Xt) d t + ÏƒX t â—¦ dWt, (16) where 

astrat := a + 12 Ïƒ2, bstrat := 2ab 

2a + Ïƒ2 .

27 Default parameters 

Unless otherwise specified, our results for the IGBM process use the parameters: 

X0 = 0 .06 , a = 0 .1, b = 0 .04 , Ïƒ = 0 .6, T = 1 .0.

Reference values 

All errors quoted in our results for the IGBM process are with respect to the following theoretically derived reference values: 1. Conditional mean [83]: 

Ex0 [Xt] = b + ( x0 âˆ’ b) exp( âˆ’at ). (17) 2. Conditional variance [83]: 

Vx0 [Xt] = 

> ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³

exp( âˆ’at )(2ab (tx 0 âˆ’ 1 

> a

x0 âˆ’ tb ) + x20

> )

âˆ’ exp( âˆ’2at )( x0 âˆ’ b)2 + b2, if Ïƒ2 

> a

= 1 ,

exp( âˆ’at )(4b(b âˆ’ x0)) âˆ’ exp( âˆ’2at )( x0 âˆ’ b)2

+ 2 b2at âˆ’ 3b2 + 2 bx 0 + x20, if Ïƒ2 

> a

= 2 ,

exp (âˆ’(2 a âˆ’ Ïƒ2)t)( x20 âˆ’ 2ab aâˆ’Ïƒ2 x0 + 2a2b2 

> (2 aâˆ’Ïƒ2)( aâˆ’Ïƒ2)
> )

+ b2Ïƒ2

> 2aâˆ’Ïƒ2

+ 2bÏƒ 2(x0âˆ’b) 

> aâˆ’Ïƒ2

exp( âˆ’at ) âˆ’ exp( âˆ’2at )( x0 âˆ’ b)2, otherwise .

(18) 

Numerical considerations 

We do not take any special numerical considerations for simulating the IGBM process. 

A.3 Coxâ€“Ingersollâ€“Ross Model 

The Coxâ€“Ingersollâ€“Ross (CIR) model is another one-factor short-rate model. It has similar prop-erties to the IGBM process (e.g. non-negativity), but is significantly more popular in financial applications (i.e. interest rates [21] and stochastic volatilities [42]). However, the key advantage of the CIR model is that it leads to analytical formulae for both bond and option prices (when used as the volatility in the Heston model). Due to its popularity, a wide range of numerical methods have been developed for simulating the CIR model [2, 3, 4, 23, 28, 34, 41, 49, 59, 68, 71]. Recently, one of these numerical methods [28] served as the inspiration for a Langevin SDE simulation algorithm for sampling from non-smooth target distributions (which often arise in Bayesian inverse problems) [16]. 

Definition 

Let a > 0, b > 0, and Ïƒ > 0. The Coxâ€“Ingersollâ€“Ross (CIR) model with mean reversion rate a,long-term mean b, and volatility Ïƒ is 

dXt = a (b âˆ’ Xt) d t + ÏƒâˆšXt dWt, (19) 28 where t > 0 and Wt is a standard Brownian motion [21]. In Stratonovich form, this is 

dXt = astrat (bstrat âˆ’ Xt) d t + ÏƒâˆšXt â—¦ dWt, (20) where 

astrat := a, bstrat := b âˆ’ Ïƒ2

4a .

Default parameters 

Unless otherwise specified, our results for the CIR model use the parameters: 

X0 = 2 .0, a = 1 .0, b = 3 .0, Ïƒ = 0 .5, T = 1 .0.

Reference values 

All errors quoted in our results for the CIR model are with respect to the following theoretically derived reference values [21, 31]: 1. Conditional mean [21]: 

Ex0 [Xt] = x0 exp( âˆ’at ) + b(1 âˆ’ exp( âˆ’at )). (21) 2. Conditional variance [21]: 

Vx0 [Xt] = Ïƒ2x0

a

(exp( âˆ’at ) âˆ’ exp( âˆ’2at )) + Ïƒ2b

2a

(1 âˆ’ exp( âˆ’at ))2. (22) 3. Conditional distribution [21]: 

(Xt | X0) âˆ¼ Ïƒ2

4a (1 âˆ’ exp( âˆ’at )) Â· Ï‡2(Î´, Î» ), (23) where Ï‡2(Î´, Î» ) denotes the non-central chi-squared distribution with Î´ degrees of freedom and non-centrality parameter Î»,

Î´ = 4ab Ïƒ2 , Î» = 4ax 0

Ïƒ2(exp( at ) âˆ’ 1) .

4. Price at time s of a zero-coupon bond maturing at time T [21]: 

E

[

exp 

(

âˆ’

âˆ« Ts

Xt dt

) âˆ£ âˆ£âˆ£âˆ£ Xs

]

= A(s, T ) exp (âˆ’XsB(s, T )), (24) where 

A(s, T ) = 

ï£«ï£¬ï£­

2Î³ exp 

( (Î³+a)( T âˆ’s)2

)

(Î³ âˆ’ a) + ( Î³ + a) exp (Î³(T âˆ’ s))ï£¶ï£·ï£¸

> 2ab Ïƒ2

,B(s, T ) = 2(exp (Î³(T âˆ’ s)) âˆ’ 1)

(Î³ âˆ’ a) + ( Î³ + a) exp (Î³(T âˆ’ s)) , Î³ = âˆš

a2 + 2 Ïƒ2.

29 Numerical considerations 

When simulating the CIR model using standard methods, it is possible to generate paths whose values are non-positive even if Ïƒ2 â‰¤ 2ab (the Feller condition [31] is met) and X0 > 0. These negative values prevent the computation of âˆšXt, causing the simulation to fail. To fix this problem, we instead compute 

> âˆš

X+ 

> t

:= âˆšmax ( Xt, 0) . This adjustment when simulating the CIR model is similar to the â€œfull-truncationâ€ scheme of [59]. 

A.4 Wright-Fisher diffusion with drift 

The Wright-Fisher diffusion is a continuous-time approximation to the discrete Wright-Fisher model used in population genetics, where the state variable models the relative frequency of an allele in a given population [30, 29, 67]. 

Definition 

Let s âˆˆ R and Î³ > 0. Then the Wright-Fisher diffusion with selection coefficient s and variance Î³

is given by 

dXt = sX t(1 âˆ’ Xt) d t +

> âˆš

Î³X t(1 âˆ’ Xt) d Wt,

where t > 0 and Wt is a standard Brownian motion. In Stratonovich form, this becomes 

dXt =

> (

sX t(1 âˆ’ Xt) âˆ’ Î³ 

> (

14 âˆ’ 12 Xt

> ))

dt +

> âˆš

Î³X t(1 âˆ’ Xt) â—¦ dWt.

Default parameters 

Unless otherwise specified, our results for the Wright-Fisher diffusion use the parameters: 

X0 = 0 .5, s = 0 , Î³ = 1 , T = 1 .0.

Reference values 

All errors quoted in our results for the Wright-Fisher diffusion are with respect to the following theoretically derived reference values: 1. Conditional mean (when s = 0 ) [30]: 

Ex0 [Xt] = x0. (25) 2. Conditional variance (when s = 0 ) [30]: 

Vx0 [Xt] = x0(1 âˆ’ x0)(1 âˆ’ exp( âˆ’Î³t )). (26) 3. Conditional boundary absorption probability [30]: 

Px0 (XÏ„ = 1) = 

> ï£±ï£´ï£²ï£´ï£³

x0, if s = 0 ,    

> 1âˆ’exp
> (âˆ’2sÎ³x0
> )
> 1âˆ’exp
> (âˆ’2sÎ³
> )

, if sÌ¸ = 0 , (27) where Ï„ = inf {t > 0 : Xt âˆˆ { 0, 1}} so that P(Ï„ < âˆ) = 1 for any s âˆˆ R and Î³ > 0.30 Numerical considerations 

To satisfy the absorbing boundary condition, we compute the state variable as X+ 

> âˆ’

:= min(max( X, 0) , 1) .

A.5 Log-Heston model 

The Heston model is a well-known stochastic volatility model for option pricing [42]. Equivalent to the Heston model is the log-Heston model, which can be obtained simply by applying a logarithm to the stock price component. Let a > 0, b âˆˆ R, Ïƒ > 0, Î¼ âˆˆ R, and Ï âˆˆ [âˆ’1, 1] . Then the log-Heston model with mean reversion rate a, long-term mean b, volatility-of-volatility Ïƒ, risk-free rate Î¼, and instantaneous correlation Ï is 

dXt = (Î¼ âˆ’ 12 Vt 

> )

dt + âˆšVt dW (1)  

> t

,

dVt = a (b âˆ’ Vt) d t + ÏƒâˆšVt dW (2)  

> t

,

(28) where t > 0, Xt := log ( St) is the log-asset price, Vt is its stochastic variance, and the Brownian motions have correlation Ï. In Stratonovich form, this becomes 

dXt = (Î¼strat âˆ’ 12 Vt 

> )

dt + âˆšVt â—¦ dW (1)  

> t

,

dVt = a (bstrat âˆ’ Vt) d t + ÏƒâˆšVt â—¦ dW (2)  

> t

,

(29) where 

Î¼strat := Î¼ âˆ’ 14 ÏÏƒ, bstrat := b âˆ’ Ïƒ2 

> 4a

.

The correction to b is the same as for the CIR model. The correction to Î¼ arises from the cross-term in the ItÃ´-to-Stratonovich conversion due to the correlated Brownian motions [53]. 

Default parameters 

Unless otherwise specified, our results for the log-Heston model use the parameters: 

X0 = log(20) , V 0 = 0 .4, Î¼ = 0 .1, a = 2 , b = 0 .1, Ïƒ = 0 .5, Ï = 0 , T = 1 .0.

Reference values 

All errors quoted in our results for the log-Heston model are with respect to the following theoret-ically derived reference value: 1. Price at time t = 0 of a European call option with expiration time T [42, 24]: 

C(t = 0 , T ) = E[exp( âˆ’Î¼T ) max (ST âˆ’ K, 0)] 

= S0Î 1 âˆ’ exp( âˆ’Î¼T )KÎ 2, (30) where 

Î 1 = 12 + 1

Ï€ 

> âˆ«âˆ
> 0

Re  

> (

exp( âˆ’iu log K)ÏˆXT (u âˆ’ i) iuÏˆ XT (âˆ’i) 

> )

du, 

Î 2 = 12 + 1

Ï€ 

> âˆ«âˆ
> 0

Re  

> (

exp( âˆ’iu log K)ÏˆXT (u)iu

> )

du, 

31 and T > 0, K is the strike price, Î¼ is the risk-free rate of return, and Ïˆ is the characteristic function 2 defined for u âˆˆ C as: 

ÏˆXT (u) = exp (AT (u)b + BT (u)V0 + i u(X0 + Î¼T )),AT (u) = a

[

T Â· râˆ’ âˆ’ 1

Î³ log 

( 1 âˆ’ g exp( âˆ’hT )1 âˆ’ g

)] 

,BT (u) = râˆ’ Â· 1 âˆ’ exp( âˆ’hT )1 âˆ’ g exp( âˆ’hT ) ,rÂ± = Î² Â± hÏƒ2 , h =

âˆš

Î²2 âˆ’ 4Î±Î³, g = râˆ’

r+

,Î± = âˆ’u2

2 âˆ’ iu

2 , Î² = a âˆ’ ÏÏƒ iu, Î³ = Ïƒ2

2 .

Numerical considerations 

The CIR model is used as the stochastic volatility component of the (log-)Heston model. Therefore, we apply the same numerical considerations as for the CIR model. That is, we compute 

âˆš

V + 

> t

:= 

âˆšmax ( Vt, 0) to prevent issues resulting from negativity. 

> 2While it is not a problem for our default parameters, the characteristic function as presented can become unstable for long maturity times and high volatilities due to the little Heston trap [1].

32 Appendix B Rough Path Theory 

B.1 Motivation 

We give a brief motivation of and introduction to rough path theory. A more comprehensive introduction is given in [62]. The goal of rough path theory is to study controlled differential equations (CDEs) of the form 

dYt = f (Yt) d Xt , Y0 = Î¾, (31) where X = ( X(0) , X (1) , . . . , X (d)) is the control or signal (living in V := Rd+1 for some d âˆˆ N,where X(0) (t) = t), and Y is the response (living in W := Rm for some m âˆˆ N). The solution, if it exists, is denoted Y = If (X, Î¾ ), and If is called the ItÃ´ map associated with f .Recall that if p â‰¥ 1 and X : [0 , 1] â†’ V is continuous, then the p-variation of X is 

âˆ¥Xâˆ¥p = sup  

> D
> [ âˆ‘

|Xti+1 âˆ’ Xti |p

> ]1/p

, (32) where the supremum is over all subdivisions of [0 , 1] (not to be confused with the Lp norm which we would denote by âˆ¥Â·âˆ¥ Lp ). If X is of bounded ( 1-)variation and f is sufficiently smooth (say Lipschitz), then classical theory ensures existence and uniqueness of the solution to (31), as well as continuity of the ItÃ´ map. More precisely, if Xn, n âˆˆ N, and X are bounded variation paths, and Xn â†’ X in 1-variation, then If (Xn, Î¾ ) â†’ If (X, Î¾ ) in 1-variation for any initial condition Î¾.However, in reality signals are often much more rough; a famous example is Brownian motion, whose paths almost-surely have finite p-variation if and only if p > 2, but the classical theory of controlled differential equation based on the Young integral fails as soon as p â‰¥ 2. Equations controlled specifically by Brownian motion can still be made sense of in an inherently stochastic way using ItÃ´ calculus, but the aim of rough path theory is to develop a flexible framework in which to study (31) for a broad class of rough signals in a pathwise (i.e. deterministic) manner. To summarise, what we would like to do is to extend the map If , say for a fixed, smooth f , to a space of functions that includes rough signals such as continuous paths that have finite p-variation only for p > 2, in such a way that If is continuous w.r.t. some complete metric. A naÃ¯ve approach, however, is doomed to fail: there exist many examples of sequences of smooth paths (x1

> n

) and (x2

> n

)

that converge to the same path x uniformly and in p-variation say for some p > 2, but such that 

If (x1

> n

, Î¾ ) and If (x2

> n

, Î¾ ) have different limits. The problem, it turns out, is not due to a wrong choice of metric, or an inherent impossibility of (deterministically) integrating against rough signals, but rather of conceptual nature: the mistake lies in thinking of a rough signal as â€œjustâ€ a continuous path [0 , 1] â†’ V ; to characterise its behaviour as a driving signal in (31), more information is required. It turns out that, in many of the examples alluded to at the beginning of the paragraph, (x1

> n

) and (x2

> n

) do in fact both have well-defined but 

distinct limits in an appropriate space of so-called rough paths that both â€œlive overâ€ the same classical path x : [0 , 1] â†’ V .To motivate what additional information may be required to characterise such a rough path, con-sider for a moment the case of CDEs driven by paths of bounded variation. If f is sufficiently regular, they can be solved through Picard iterationâ€”i.e. letting Y 0 

> t

= Y0 and iteratively solv-ing dY n+1  

> t

= f (Y nt ) d Xtâ€”which reveals that Yt is actually a function of the collection of iterated 33 integrals of the signal X on [0 , t ], that is its signature 

S0,t (X) := 

(

1, S10,t (X), S20,t (X), . . . 

)

âˆˆ T (( V )) = 

> âˆ

âŠ•

> k=0

V âŠ—k,

where V âŠ—k is the k-fold tensor product of V with itself, and 

Xk 

> 0,t

:= S k

> 0,t

(X) := 

âˆ« 

> 0<u 1<...<u k<t

dXu1 âŠ— . . . âŠ— dXuk âˆˆ V âŠ—k

can be thought of as compact notation for the collection of k-fold iterated integrals of X =(X(0) , X (1) , . . . , X (d)) : [0 , 1] â†’ V = Rd+1 , i.e. 

Xk 

> 0,t

= (Xk,I 

> 0,t

)  

> I=( i1...i k)âˆˆ[d]k

where Xk,I  

> 0,t

=

âˆ« 

> 0<u 1<...<u k<t

dX(i1) 

> u1

. . . dX(ik ) 

> uk

.

Note that the first level of the signature are just the increments X10,t = Xt âˆ’ X0, which determine the path up to a translation that doesnâ€™t affect its behaviour as a signal. From this perspective, it seems natural to think of a signal not as a path in V , but as a path in T (( V )) . If the underlying path has bounded variation, then this distinction is insubstantial because all levels of the signature are determined by the first level (the increments) through classical integration. But now say that we have a continuous path X : [0 , 1] â†’ V with finite p-variation only for p âˆˆ (2 , 3) , such as a realisation of Brownian motion. Then we can define X1 

> s,t

= Xt âˆ’ Xs, but for higher levels we run into the old problem that iterated integrals of X cannot be defined classically. However, it turns out that if we assign a second level X2 in such a way that 

X2 

> s,t

= X2 

> s,u

+ X2 

> u,t

+ X1 

> s,u

âŠ— X1 

> u,t

, âˆ€s < u < t, (33) an identity that holds automatically for the signature of a bounded variation path by standard properties of the integral, and such that X2 has bounded p-variation in an appropriate sense, then there is a unique way of extending the signature to all higher levels in such a way that the natural generalisation of (33) holds at all levels (and that the entire signature has bounded p-variation in an appropriate sense). Such an object is then called a ( p-) rough path over the original path 

X : [0 , 1] â†’ V , and, crucially, there can be many rough paths over the same classical path X.The central result of rough path theory is that there is a natural way to define integration against and CDEs driven by rough paths, in such a way that the ItÃ´ map is continuous with respect to a suitably chosen metric. 

B.2 Rough paths and the universal limit theorem 

Let â–³ = {(s, t ) : 0 â‰¤ s â‰¤ t â‰¤ 1}, and X : â–³ â†’ T (( V )) be a continuous mapping. To avoid confusion, we will use lower case letters such as x to denote classical paths [0 , 1] â†’ V in this section. The generalisation of (33) to higher levels is obtained by writing (the components of) Xs,t in terms of 

Xs,u and Xu,t for some s < u < t , in the case where X is the signature of a bounded variation path. This can be done using standard properties of integrals, and the result can be written compactly as 

Xs,t = Xs,u âŠ— Xu,t , âˆ€ 0 â‰¤ s â‰¤ u â‰¤ t â‰¤ 1, (34) also known as Chenâ€™s identity . The zeroth level of (34) is 1 = 1 , the first level is X1 

> s,t

= X1 

> s,u

+ X1

> u,t

,and the second level is (33). 34 Definition 1. Let p â‰¥ 1. A p-rough path is a continuous mapping X : â–³ â†’ T (( V )) that satisfies Chenâ€™s identity (34) and has finite p-variation in the sense that 

sup  

> D
> âˆ‘

âˆ¥Xksi,s i+1 âˆ¥p/k < âˆ, (35) for all k âˆˆ N, where the supremum is over finite subdivisions of [0 , 1] .Note that (34) implies that the first level of a p-rough path is always the increments of a continuous path [0 , 1] â†’ V , which by (35) has finite p-variation. Note further that if x : [0 , 1] â†’ V is a continuous path with finite p-variation for some p âˆˆ [1 , 2) , then its signature S( X) = (1 , X 1, X 2, . . . )

is classically defined in terms of iterated (Young) integrals, and is a p-rough path according to Definition 1. Denote by T (n)(V ) = âŠ•nk=0 V âŠ—k the truncated tensor algebra over V of degree n,equipped with the product ab = c where ck = a0 âŠ— bk + . . . + ak âŠ— b0.

Theorem B.1 (Extension Theorem) . Let p â‰¥ 1, and let X : â–³ â†’ T (âŒŠpâŒ‹)(V ) be a continuous mapping that satisfies Chenâ€™s identity and has finite p-variation in the sense that (35) holds for all 

k â‰¤ âŒŠ pâŒ‹. Then X has a unique extension to a p-rough path X : â–³ â†’ T (( V )) .

See Theorem 3.7 in [62]. In particular, a p-rough path is uniquely determined by its truncation at level âŒŠpâŒ‹, and we sometimes identify it with this truncation. Denote by Î©p(V ) the space of p-rough paths, which is a complete metric space when equipped with the p-variation metric [60, Sect. 3.3.1] 

dp(X, Y ) = max  

> i=1 ,..., âŒŠpâŒ‹

sup  

> D
> ( âˆ‘

âˆ¥Xksi,s i+1 âˆ’ Y ksi,s i+1 âˆ¥ pk 

> )1
> p

.

Note that Î©1(V ) can be identified with the space of (classical) continuous bounded variation paths 

[0 , 1] â†’ V (modulo constant translations) with the 1-variation metric. 

Definition 2. The space GÎ©p(V ) of geometric rough paths is the closure of Î©1(V ) in Î©p(V ) w.r.t. the p-variation metric. It is in particular complete. The inclusion GÎ©p(V ) âŠ‚ Î©p(V ) is strict except if p < 2. A simple example of a 2-rough path that is not geometric is Xs,t = (1 , 0, (t âˆ’ s)a) for any a âˆˆ V âŠ—2 \ { 0}. It is easy to check that Chenâ€™s identity holds and X has finite p-variation for any p â‰¥ 2, so X is a 2-rough path. But for any bounded variation path, and therefore any geometric p-rough path by approximation in dp, it must hold that the symmetric part of X2 

> s,t

is 12 (X1

> s,t

)âŠ—2, but this is not true except if a = 0 .

Remark 3. An alternative way to introduce (geometric) rough paths directly, without any reference to tensor algebras and Chenâ€™s relation, would have been to say that the space of (geometric) p-rough paths is the completion of the space of continuous paths [0 , 1] â†’ V of bounded variation w.r.t. a metric in which two paths are close if they and their first âŒŠpâŒ‹ iterated integrals are close in 

p-variation. The following theorem is the main result and justification of rough path theory. A function f : A â†’

B between two finite-dimensional Banach spaces A, B is said to be Lip (Î³) for some Î³ > 0 if it is bounded, âŒŠÎ³âŒ‹ times continuously differentiable and such that the âŒŠÎ³âŒ‹â€™th derivative is HÃ¶lder continuous with exponent Î³ âˆ’ âŒŠ Î³âŒ‹. Recall that V = Rd+1 and W = Rm.

Theorem B.2 (Universal Limit Theorem) . Let p â‰¥ 1 and Î³ > p , and f : W â†’ L (V, W ) be a Lip (Î³) function. Then for all X âˆˆ GÎ©p(V ) and Î¾ âˆˆ W , the equation 

dYt = f (Yt) d Xt , Y0 = Î¾, 

35 has a unique solution in GÎ©p(W ) (in an appropriate sense), and the map 

If : GÎ©p(V ) Ã— W â†’ GÎ©p(W )

that sends (X, Î¾ ) to Y is continuous and equal to the unique continuous extension of the classical ItÃ´ map If : Î© 1(V ) Ã— W â†’ Î©1(W ) (which is continuous in p-variation). 

A more precise statement and a proof are in Section 5.3 of [62]. 

B.3 Brownian motion as a rough path 

Since the main application of rough path theory is to differential equations driven by Brownian motion, it will be useful to consider for a moment the special case of a geometric p-rough path when 

p âˆˆ [2 , 3) . As mentioned before, signatures of bounded variation paths and therefore geometric p-rough paths have the property that the symmetric part Ss,t âˆˆ V âŠ—2 defined by 

S(ij ) 

> s,t

= 12

(

X2,(ij ) 

> s,t

+ X2,(ji )

> s,t

)

, (s, t ) âˆˆ â–³ ,

satisfies Ss,t = 12 (X1

> s,t

)âŠ—2. If we denote the anti-symmetric part by As,t âˆˆ V âŠ—2, i.e. 

Aij s,t = 12

(

X2,(ij ) 

> s,t

âˆ’ X2,(ji )

> s,t

)

, (s, t ) âˆˆ â–³ ,

then X2 = S + A, which means that X as a whole is fully determined by its increments X1 and the anti-symmetric part A of its second level, 

X =

(

1, X 1, 12 (X1)âŠ—2 + A

)

.

The tensor A can be interpreted as the area of X, in the following sense: if X is the signature of a path x âˆˆ Î©1(V ), then 

A(ij ) 

> s,t

= 12

âˆ«âˆ« 

> sâ‰¤u1â‰¤u2â‰¤t

dxiu1 dxju2 âˆ’ dxju1 dxiu2 ,

which is the area enclosed between the bounded variation path (xi, x j ) and its secant on the time interval [s, t ]. The area is â€œsignedâ€, e.g. the area enclosed by t 7 â†’ (t, sin( t)) on [0 , Ï€ ], [Ï€, 2Ï€], and 

[0 , 2Ï€] is, respectively, 2, âˆ’2, and 0. For genuine p-rough paths with p â‰¥ 2, it is still common to refer to A, the anti-symmetric part of X2, as its area .Brownian motion is a random continuous path which almost-surely has finite p-variation for p > 2.By the considerations in the previous section, a (geometric) rough path over a fixed realisation of the Brownian increments is determined by the choice of its area tensor A, and this choice is not unique! If we choose A to be (a continuous version of) the LÃ©vy area of Brownian motion, that is 

Aij s,t = 12

âˆ«âˆ« 

> sâ‰¤u1â‰¤u2â‰¤t

( dBiu1 dBju2 âˆ’ dBiu2 dBju1

), i, j âˆˆ [d], (s, t ) âˆˆ â–³ ,

where the integral is in the ItÃ³ or Stratonovich sense (which doesnâ€™t make a difference in this case), then we obtain what is called the canonical Brownian rough path . It can be obtained as the almost-sure limit of its piecewise linear approximation on an increasingly fine set of support points, 36 so it is indeed geometric. It can further be thought of as the signature of Brownian motion where iterated integrals are calculated in the Stratonovich sense. Indeed, if X : â–³ â†’ T (( V )) is a canonical Brownian rough path over a Brownian motion B, then for every k âˆˆ N and J = ( j1 . . . j k) âˆˆ [d]k,and every 0 â‰¤ s â‰¤ t â‰¤ 1, almost-surely 

Xk,J s,t = 

> âˆ«
> s<u 1<...<u k<t

â—¦ dB(j1) 

> u1

. . . â—¦ dB(jk ) 

> uk

, (36) where â—¦ dB denotes integration in the Stratonovich sense. A consequence of this is that the solution of a differential equation driven by Brownian motion when thought of as a rough path has the same law as the solution of the same equation when interpreted in the classical Stratonovich sense; hence why we use the Stratonovich interpretation of (1) (recall, however, that Stratonovich and ItÃ´ SDEs can easily be transformed into one another). The advantage of considering Brownian motion as a rough path is that, once a continuous version of the LÃ©vy area has been fixed, we can solve all differential equations driven by Brownian motion in a pathwise sense on the same set of probability one, and without requiring predictability or other such inherently probabilistic conditions common in stochastic integration. 37 Appendix C Theoretical Results 

C.1 (Re-)statement of results 

With the additional setup, we can state slightly stronger and more precise versions of Theorems 1 and 2. We assume that the reader is familiar with some standard notions in probability theory, such as weak convergence of probability measures (i.e. convergence in distribution), which we denote 

Î¼n =â‡’ Î¼, and tightness. 

Theorem 1*. Suppose (Î»ni , Ï‰ ni ) is an ARCANE cubature with degree Dn â†’ âˆ , and dyadic depth 

mn âˆˆ N such that 

mn â‰¥ 

> (

34 + Îµ

> )

log 2 Nn âˆ’ C, 

for some Îµ, C > 0. Then there exists a p âˆˆ (2 , 3) such that the cubature converges in distribution to Brownian motion as a p-rough path. In particular, if yn denotes the solution to an SDE with smooth vector fields driven by the cubature, and y denotes the solution to the same SDE driven by Brownian motion, then yn =â‡’ y in distribution as rough (and in particular as ordinary) paths. 

Theorem 2*. Let (Î»ni , Ï‰ ni ) be the approximate cubature resulting from the first step of the ARCANE algorithm, prior to recombination, sharpening, and any dyadic constructions, with Nn â†’ âˆ and degree Dn â†’ âˆ . Then the assertion of Theorem 1 holds for all p âˆˆ (2 , 3) .Remark 4. (i) The fact that the convergence of the cubature to Brownian motion holds on the level of rough paths is a much stronger assertion than just on the level of ordinary paths. In particular, the main assertion that solutions driven by the cubature paths converge to the real solutions only holds thanks to the universal limit theorem for rough paths (Theorem B.2), and would not follow from convergence on the level of ordinary paths. (ii) As mentioned in the main paper, for the assertion of Theorem 1* to hold, it is in fact sufficient if the cubature matches the expected Brownian signature up to degree Dn â†’ âˆ just on [0 , 1] ,and only to degree D = 7 on all of the finer dyadic intervals. 

C.2 Proofs 

In the remainder of this appendix, we prove Theorems 1* and 2*. We start by establishing condi-tions under which weak convergence and pointwise convergence of expected signatures imply each other for rough paths. Recall that V = Rd+1 , and GÎ©p(V ) denotes the space of geometric p-rough paths x = ( x0, x 1, . . . , x d)

in V defined on [0 , T ] for some fixed and arbitrary T > 0 with x0(t) = t. This is a complete and separable metric space [19, Proposition 39]. Recall that the signature is a map S : GÎ©p â†’ T (( V )) .Let 

M := {Î¼ âˆˆ M 1(GÎ©p) : Î¼(âˆ¥ Sm âˆ¥V âŠ—m 

> )

< âˆ, âˆ€m âˆˆ N},

where we used the shorthand Î¼(f ) for âˆ« f dÎ¼ if f is a function and Î¼ a probability measure such that the integral is well-defined, and M1(X) for a measurable space X denotes the set of probability measures on X. In particular, if Î¼ âˆˆ M , then Î¼(S) âˆˆ T (( V )) (exists and) is the expected signature of the random rough path defined by Î¼.38 Lemma C.1. Let Î¼ âˆˆ M such that its expected signature has an infinite radius of convergence, that is 

âˆ€Î» > 0 : 

> âˆ

âˆ‘

> m=1

Î»mÎ¼ (âˆ¥ Sm âˆ¥) < âˆ. (37) 

Then the expected signature of Î¼ characterises it, that is 

âˆ€Î½ âˆˆ M : Î¼(S) = Î½(S) = â‡’ Î¼ = Î½. (38) 

Furthermore, (37) holds if Î¼ is the law of canonical Brownian motion. Proof. Suppose Î¼ satisfies the assumptions of the lemma above, and Î½ âˆˆ M with Î¼(S) = Î½(S) .By [18, Prop. 6.1], this implies that the pushforward laws S # Î¼ and S # Î½ coincide. But by [9, Theorem 1.1], a rough path x âˆˆ GÎ©p is determined uniquely by its signature S( x) (we use here that x0(t) = t), and therefore Î¼ = Î½.Finally, (37) follows from standard exponential tail bounds on Brownian motion and e.g. [18, Theorem 6.3]. We say that a sequence (Î¼n) in M is uniformly integrable if 

âˆ€m âˆˆ N : sup 

> nâˆˆN

Î¼n

(

âˆ¥ Sm âˆ¥1{âˆ¥ Sm âˆ¥â‰¥ K}

)

âˆ’â†’ 0, K â†’ âˆ . (39) 

Lemma C.2. Let (Î¼n) be a uniformly integrable sequence in M, and Î¼ âˆˆ M 1(GÎ©p). Then the following hold. (i) If Î¼n =â‡’ Î¼ weakly, then Î¼ âˆˆ M and Î¼n(S) â†’ Î¼(S) pointwise. (ii) If (Î¼n) is tight, and Î¼ âˆˆ M satisfies (38) , then Î¼n(S) â†’ Î¼(S) pointwise implies Î¼n =â‡’ Î¼

weakly. Proof. For K > 0 and x âˆˆ R, write clamp K (x) := min( K, max( âˆ’K, x )) . To prove (i), assume that 

Î¼n =â‡’ Î¼. We first show that Î¼ âˆˆ M . Let J be a multi-index, then 

Î¼(| SJ |) â‰¤ lim inf  

> nâ†’âˆ

Î¼n

(| SJ |) â‰¤ lim inf  

> nâ†’âˆ

Î¼n

( clamp K

(| SJ |)) + sup 

> nâˆˆN

Î¼n

(| SJ |1{| SJ |â‰¥ K}

).

Since clamp K (| SJ |) is continuous and bounded, the first term on the right-hand side converges to 

Î¼(clamp K (| SJ |)) < âˆ, and by uniform integrability the second term is finite for some K > 0 (in fact goes to zero as K â†’ âˆ ). We showed that Î¼ âˆˆ M , in particular Î¼(S) exists. Next, again for any multi-index J and any K > 0,

âˆ£âˆ£Î¼n(S J ) âˆ’ Î¼(S J )âˆ£âˆ£ â‰¤ âˆ£âˆ£Î¼n(clamp K (S J )) âˆ’ Î¼(clamp K (S J )) âˆ£âˆ£

+ Î¼n

(| SJ |1{| SJ |â‰¥ K}

) + Î¼(| SJ |1{| SJ |â‰¥ K}

).

Since Î¼n =â‡’ Î¼, the first term on the right-hand side goes to zero as n â†’ âˆ for any fixed K > 0,so 

lim sup 

> nâ†’âˆ

âˆ£âˆ£Î¼n(S J ) âˆ’ Î¼(S J )âˆ£âˆ£ â‰¤ lim sup 

> nâ†’âˆ

Î¼n

(| SJ |1{| SJ |â‰¥ K}

) + Î¼(âˆ£âˆ£ SJ |1{| SJ |â‰¥ K}

).

39 By (39), the right-hand side goes to zero as K â†’ âˆ , so the left-hand side is zero. Since J was arbitrary, this implies Î¼n(S) â†’ Î¼(S) pointwise. Now assume that Î¼ âˆˆ M satisfies (38), Î¼n(S) â†’ Î¼(S) pointwise, and that (Î¼n) is tight. It suffices to prove uniqueness of subsequential limits, so take some convergent subsequence Î¼k(n) =â‡’ Î½.By (i) which weâ€™ve already proved, Î½ âˆˆ M and Î¼k(n)(S) â†’ Î½(S) , but also Î¼k(n)(S) â†’ Î¼(S) , so 

Î¼(S) = Î½(S) , which implies Î½ = Î¼ by (38). The next lemma shows that if we want to apply Lemma C.2 (ii) in the case where Î¼ is the distribution of (canonical rough) Brownian motion, then we get uniform integrability of (Î¼n) for free. 

Lemma C.3. If Î¼n, Î¼ âˆˆ M , n âˆˆ N such that Î¼n(S) â†’ Î¼(S) , and Î¼(âˆ¥ Sm âˆ¥2) < âˆ for all m âˆˆ N,then sup nâˆˆN Î¼n(âˆ¥ Sm âˆ¥2) < âˆ, and in particular (Î¼n) is uniformly integrable. Proof. For any fixed m âˆˆ N and multi-index J of length m, the shuffle product identity [62, Theorem 2.15] implies that there exists a set J of multi-indices of length 2m (the â€œshufflesâ€ of J with itself) such that 

| SJ |2 = (S J )2 = âˆ‘

> IâˆˆJ

SI .

Therefore Î¼n(S) â†’ Î¼(S) implies Î¼n(| SJ |2) â†’ Î¼(| SJ |2) < âˆ, in particular sup nâˆˆN Î¼n(| SJ |2) <

âˆ.To be able to apply Lemma C.2, we need to understand how to prove tightness for sequences of probability measures on GÎ©p, which is the content of the next lemma. For Î± âˆˆ (0 , 1] and a function f : [0 , T ] â†’ X for some normed space X, denote by 

[f ]Î± := sup 

> 0â‰¤s<t â‰¤T

âˆ¥f (t) âˆ’ f (s)âˆ¥|t âˆ’ s|Î± , âˆ¥f âˆ¥Î± := [ f ]Î± + sup  

> tâˆˆ[0 ,T ]

âˆ¥f (t)âˆ¥,

the Î±-HÃ¶lder seminorm and norm of f , respectively. We also write [f ]Lip for [f ]Î± if Î± = 1 , and similarly for âˆ¥f âˆ¥Lip . Sometimes it will be convenient to work with a version of the Î±-HÃ¶lder (semi-)norm that only evaluates f on subsequent dyadic points: 

[f ]Î±, D := sup  

> {

|f (t) âˆ’ f (s)||t âˆ’ s|Î± : 0 â‰¤ s < t â‰¤ 1, âˆƒk, n âˆˆ N0 : s = k2âˆ’n, t = s + 2 âˆ’n

> }

,

and similarly for âˆ¥f âˆ¥Î±, D.

Lemma C.4. Let (Î¼n) be a sequence in M, and suppose that, for some Î± > 0,

lim sup 

> nâ†’âˆ

Î¼n(x : âˆ¥xâˆ¥Î±, D â‰¥ K) âˆ’â†’ 0, K â†’ âˆ , (40) 

and 

lim sup 

> nâ†’âˆ

Î¼n

> (

x : sup 

> 0<s<t<T

âˆ¥ S2

> s,t

(x)âˆ¥|t âˆ’ s|2Î± â‰¥ K

> )

âˆ’â†’ 0, K â†’ âˆ . (41) 

Then (Î¼n) is tight in GÎ©p for every p > 1/Î± .

40 Proof. First of all, by Lemma C.5 below, (40) implies 

lim sup 

> nâ†’âˆ

Î¼n(x : âˆ¥xâˆ¥Î± â‰¥ K) âˆ’â†’ 0, K â†’ âˆ .

Now let 

AK := {x : âˆ¥xâˆ¥Î± â‰¤ K} âˆ© 

{

x : sup 

> 0<s<t<T

âˆ¥ S2

> s,t

(x)âˆ¥|t âˆ’ s|2Î± â‰¤ K

}

.

By assumption, lim sup nâ†’âˆ Î¼n(AcK ) â†’ 0 as K â†’ âˆ , so it suffices to show that AK for any K > 0

is relatively compact in GÎ©p.Let 1/Î± < q < min(3 , p ). Then by Proposition 1.15 in [62], if AK is a bounded subset of GÎ©q

and equicontinuous, then AK is relatively compact in GÎ©p. Equicontinuity follows from a uniform bound on the Î±-HÃ¶lder seminorm, and furthermore, for any x âˆˆ AK , and any subdivision (ti) of 

[0 , T ], âˆ‘

> i

|x(ti) âˆ’ x(tiâˆ’1)|q â‰¤ [x]Î±

âˆ‘

> i

|ti âˆ’ tiâˆ’1|Î±q â‰¤ K âˆ‘

> i

|ti âˆ’ tiâˆ’1| = KT. 

To prove boundedness of AK as a subset of GÎ©q, it remains to show that the q-variation of the second level of the signature is bounded in the sense of (35). Indeed, for any subdivision (ti) of 

[0 , T ], âˆ‘

> i

âˆ¥ S2

> s,t

(x)âˆ¥q/ 2 â‰¤ K âˆ‘

> i

|ti âˆ’ tiâˆ’1|Î±q â‰¤ KT. 

Lemma C.5. For f : [0 , T ] â†’ X for some normed space X, and Î± âˆˆ (0 , 1] ,

[f ]Î± â‰¤ 21 âˆ’ 2âˆ’Î± [f ]Î±, D.

Proof. Let M := [ f ]Î±, D, and let 0 â‰¤ s < t â‰¤ 1. Suppose that neither s nor t are dyadic, otherwise the proof simplifies. Let [v0, w 0] âŠŠ [s, t ] be the largest dyadic interval of the form [k2âˆ’n, (k + 1)2 âˆ’n]

that is fully contained in [s, t ]. Then, 

|f (t) âˆ’ f (s)||t âˆ’ s|Î± â‰¤ |f (t) âˆ’ f (w0)||t âˆ’ s|Î± + |f (w1) âˆ’ f (w0)||w1 âˆ’ w0|Î±

ï¸¸ ï¸·ï¸· ï¸¸

> â‰¤M

+ |f (v0) âˆ’ f (s)||t âˆ’ s|Î± .

Now let n1 â‰¥ n + 1 be the smallest integer such that s < v 1 := v0 âˆ’ 2âˆ’m. Then |t âˆ’ s| â‰¥ 2|v1 âˆ’ v0|,so |f (v0) âˆ’ f (s)||t âˆ’ s|Î± â‰¤ |f (v0) âˆ’ f (v1)||t âˆ’ s|Î±

ï¸¸ ï¸·ï¸· ï¸¸

> â‰¤2âˆ’Î±M

+ |f (v1) âˆ’ f (s)||t âˆ’ s|Î± .

Continuing in this way, we get 

|f (v0) âˆ’ f (s)||t âˆ’ s|Î± â‰¤ M

> m

âˆ‘

> i=1

2âˆ’Î±i + |f (vm) âˆ’ f (s)||t âˆ’ s|Î± ,

for all m. By construction, s < v m < s + 2 âˆ’nm where nm â‰¥ n + m â†’ âˆ , so f (vm) â†’ f (s) and therefore letting m â†’ âˆ gives 

|f (v0) âˆ’ f (s)||t âˆ’ s|Î± â‰¤ M

> âˆ

âˆ‘

> i=1

2âˆ’Î±i = M 2âˆ’Î±

1 âˆ’ 2âˆ’Î± .

41 Proceeding similarly with |f (t)âˆ’f (w1)||tâˆ’s|Î± , we obtain 

|f (t) âˆ’ f (s)||t âˆ’ s|Î± â‰¤ M

> (

1 + 2 2âˆ’Î±

1 âˆ’ 2âˆ’Î±

> )

â‰¤ 2M

1 âˆ’ 2âˆ’Î± .

Since s, t were chosen arbitrarily, this finishes the proof. The primary remaining ingredient in proving Theorems 1* and 2* is to confirm the conditions of Lemma C.4. This is the content of the next two lemmas. 

Lemma C.6. Let Î± âˆˆ (1 /3, 1/2) . Suppose we have probability measures Î¼n on C1([0 , 1] , R) that match the Brownian expected signature up to some degree D > 11/2âˆ’Î± on dyadic intervals up to order mn âˆˆ N (that is, on intervals of the form [k2âˆ’m, (k + 1)2 âˆ’m] for m â‰¤ mn), and 

lim sup 

> nâ†’âˆ

Î¼n

> (

x : âˆ¥xâˆ¥Lip â‰¥ 2mn(1 âˆ’Î±)C

> )

âˆ’â†’ 0, C â†’ âˆ . (42) 

Then, (Î¼n) is tight in GÎ©p for all p âˆˆ (1 /Î±, 3) .If Î¼n is supported on piecewise linear paths with maximum slope âˆšNn where Nn = 2 kn , then (42) 

is implied by kn â‰¤ 2(1 âˆ’ Î±)mn + C for some C > 0.

Lemma C.7. Let Nn, D n âˆˆ N such that Nn, D n â†’ âˆ , and suppose that Î¼n for n âˆˆ N is an atomic probability measure on C1([0 , 1] , R) consisting of piecewise linear paths of step length 1/N and step size Â±1/âˆšN such that every set of Dn increments are independent. Then (Î¼n) is tight in GÎ©p for all p âˆˆ (2 , 3) .

Before proving Lemmas C.6 and C.7, we show how they imply Theorems 1* and 2*. 

Proof of Theorems 1* and 2*. Let (Î»ni , Ï‰ ni ) be the ARCANE cubature specified in Theorem 1 (resp. Theorem 2), and let Î¼n be the associated atomic probability measure on GÎ©p:

Î¼n = âˆ‘

> i

Î»ni Î´Ï‰ni .

Let Î¼ denote the probability distribution of canonical Brownian motion on GÎ©p, then by the universal limit theorem for rough paths (Theorem B.2) it suffices to show that Î¼n =â‡’ Î¼. To do so, we verify the assumptions of Lemma C.2 (ii). By Lemma C.6 (resp. Lemma C.7), (Î¼n) is tight. Since Î¼n is atomic it is a member of M, and because Dn â†’ âˆ (and Nn â†’ âˆ for Theorem 2) we have Î¼n(S) â†’ Î¼(S) pointwise, and therefore by Lemma C.3 also uniformly integrability. Finally, by Lemma C.1 we have that Î¼ âˆˆ M and that it satisfies (38). 

Proof of Lemma C.7. Since Î¼n is atomic it is automatically in M. Thus by Lemma C.4 it suffices to prove (40) and (41) for arbitrary Î± âˆˆ (0 , 1/2) .We assume for simplicity that d = 1 ; the proof for d > 1 involves more notation but is otherwise the same. We further assume for simplicity that Nn = 2 mn for some mn âˆˆ N, mn â†’ âˆ . Fix n âˆˆ N,and abbreviate, if no confusion is possible m = mn, N = Nn, etc. Consider a particular dyadic interval [v, w ] of length 2âˆ’k. Note that if k â‰¥ m, then, for any x in the support of Î¼n,

|x(w) âˆ’ x(v)||w âˆ’ v|Î± â‰¤ N âˆ’1/2

2âˆ’kÎ± = 2 kÎ± âˆ’m/ 2 â‰¤ 2âˆ’(1 /2âˆ’Î±)m â†’ 0, n â†’ âˆ . (43) 42 Hence let k < m . Then the increment x(w) âˆ’ x(v), under the probability measure Î¼n, is a sum S

of 2mâˆ’k Rademacher random variables, any D of which are independent, multiplied with 1/âˆšN .In particular, moments of S up to order D coincide with the moments of the same sum where all 

Rademacher variables are independent, which has the same distribution as 1âˆšN

(2Bin (2mâˆ’k, 12

) âˆ’

2mâˆ’k). Thus, using Markovâ€™s inequality, for any l âˆˆ N with 2l â‰¤ D,

Î¼n

( |x(w) âˆ’ x(v)||w âˆ’ v|Î± â‰¥ C

)

= Î¼n

(

|S| â‰¥ C2âˆ’kÎ± )

â‰¤ Câˆ’2l22Î±lk E [ S2l ]

= Câˆ’2l22Î±lk (2/âˆšN )2l E

[(

Bin 

(

2mâˆ’k, 12

)

âˆ’ 2mâˆ’kâˆ’1

)2l ]

â‰¤ c(l)Câˆ’2l22Î±lk 2âˆ’ml (2mâˆ’kâˆ’1)l

â‰¤ c(l)Câˆ’2l2âˆ’(1 âˆ’2Î±)lk .

Here we used the well-known bound E[( Bin (n, p ) âˆ’ np )2l] â‰¤ c(l)( np )l. Using union bound over all 

2k such dyadic intervals, as well as over k âˆˆ { 0, . . . , m âˆ’ 1}, we get, as long as 2âˆ’(1 /2âˆ’Î±)m â‰¤ C

(recall (43)), 

Î¼n

(x : [ x]Î±, D â‰¥ C) â‰¤ c(l)

> mn

âˆ‘

> k=0

2kCâˆ’2l2âˆ’(1 âˆ’2Î±)lk 

= c(l)Câˆ’2lmnâˆ‘

> k=0

2âˆ’k[(1 âˆ’2Î±)lâˆ’1] .

Now if l > 1/(1 âˆ’ 2Î±), then we get an upper bound of the form 

Î¼n

(x : [ x]Î±, D â‰¥ C) â‰¤ c(l, Î± )Câˆ’2l

for some constant c = c(l, Î± ) > 0. Since Î± is fixed, we can choose l = l(Î±) = âŒˆ1/(1 âˆ’ 2Î±)âŒ‰ and then for all n â‰¥ n0(Î±) we have Dn â‰¥ l and therefore 

lim sup 

> nâ†’âˆ

Î¼n(x : [ x]Î±, D â‰¥ C) â‰¤ c(Î±)Câˆ’2l âˆ’â†’ 0, n â†’ âˆ .

It remains to show (41). Note first that S(00)  

> s,t

(x) = ( t âˆ’ s)2/2 and S(11)  

> s,t

(x) = ( x(t) âˆ’ x(s)) 2/2, so it suffices to study S(10) and S(01) . Since further S(10) + S (01) = S (1) S(0) , it in fact suffices to study only S(10) . Now, if (s, t ) = ( v, w ) is dyadic as above with w âˆ’ v = 2 âˆ’k, and the N â€² := 2 mâˆ’k, D-wise independent Rademacher variables determining the increments in [v, w ] are denoted Îµi, then 

S(10)  

> v,w

(x) = 

> 2mâˆ’k

âˆ‘

> i=1

Îµi

2N 3/2 + âˆ‘

> i<j

Îµj

N 3/2 .

The first term is just 12N (x(w) âˆ’ x(v)) â‰¤ (w âˆ’ v)( x(w) âˆ’ x(v)) , the absolute value of which is at most C|w âˆ’ v|1+ Î± â‰¤ C|w âˆ’ v|2Î± with high probability by what we already proved. The second term 43 is equal to N âˆ’3/2 âˆ‘ 

> j

(j âˆ’ 1) Îµj ; note that 

E

[( âˆ‘

> j

(j âˆ’ 1) Îµj

)2l ]

= 

> Nâ€²

âˆ‘

> j1,...,j 2l=1 2l

âˆ

> i=1

(ji âˆ’ 1) 

ï¸¸ ï¸·ï¸· ï¸¸ 

> â‰¤Nâ€²

E [Îµj1 . . . Îµ j2l ]

ï¸¸ ï¸·ï¸· ï¸¸

> âˆˆ{ 0,1}

â‰¤ 

> Nâ€²

âˆ‘

> j1,...,j 2l=1

N â€²2l E [Îµj1 . . . Îµ j2l

]

= E

[( âˆ‘

> j

N â€²Îµj

)2l ]

= N â€²2l E

[(

2Bin 

(

N â€², 12

)

âˆ’ N â€²

)2l ]

â‰¤ c(l)N â€²3l.

All together, this gives 

Î¼n

(âˆ£ âˆ£âˆ£âˆ£âˆ‘

> i<j

Îµj

N 3/2

âˆ£âˆ£âˆ£âˆ£ â‰¥ C|v âˆ’ w|2Î±

)

â‰¤ c(l)Câˆ’2l24lkÎ± 

( N â€²

N

)3l

â‰¤ c(l)Câˆ’2l2lk (4 Î±âˆ’3) ,

which, with the usual union bound over all 2âˆ’k-based dyadic intervals and k = 0 , . . . , m , gives an upper bound of 

c(l)Câˆ’2lmâˆ‘

> k=0

2k[1+ l(4 Î±âˆ’3)] ,

which again is summable for sufficiently large l = l(Î±).

Proof of Lemma C.6. Consider a particular dyadic interval [v, w ] of length 2âˆ’k with k â‰¥ m, then 

|x(w) âˆ’ x(v)||w âˆ’ v|Î± â‰¤ âˆ¥ xâˆ¥Lip |v âˆ’ w|1âˆ’Î± â‰¤ âˆ¥ xâˆ¥Lip 2âˆ’k(1 âˆ’Î±) â‰¤ âˆ¥ xâˆ¥Lip 2âˆ’m(1 âˆ’Î±).

Therefore, 

Î¼n

(

x : sup 

> |sâˆ’t|>2âˆ’mn

|x(t) âˆ’ x(s)||t âˆ’ s|Î± â‰¥ C

)

â‰¤ Î¼n

(

x : âˆ¥xâˆ¥Lip â‰¥ 2mn(1 âˆ’Î±)C

)

. (44) If Î¼n is supported on paths with Lipschitz constant âˆšNn = 2 kn/2, then it is sufficient if kn â‰¤

2(1 âˆ’ Î±)mn + C for some C > 0. By (42), the lim sup nâ†’âˆ of the left-hand side above goes to zero as C â†’ âˆ .Now if [v, w ] is a dyadic interval of length 2âˆ’k for some k â‰¤ m, then the expected signature on that interval matches that of Brownian motion up to order D; in particular, the moments of the increment |x(w) âˆ’ x(v)| matches up to order D, so 

Î¼n

( |x(w) âˆ’ x(v)||w âˆ’ v|Î± â‰¥ C

)

â‰¤ Câˆ’D2kÎ±D E

[

|x(w) âˆ’ x(v)|D]

â‰¤ Câˆ’D2kÎ±D E

[

|N (0 , 2âˆ’k)|D]

= Câˆ’D2kÎ±D 2âˆ’kD/ 2 E

[

|N (0 , 1) |D]

â‰¤ c(D)Câˆ’D2âˆ’kD (1 /2âˆ’Î±).

44 Given that D > 1/(1 /2 âˆ’ Î±), we can proceed just as in the proof of Lemma C.7 to obtain 

lim sup 

> nâ†’âˆ

Î¼n

(

x : sup 

> |sâˆ’t|â‰¤ 2âˆ’mn

|x(t) âˆ’ x(s)||t âˆ’ s|Î± â‰¥ C

)

âˆ’â†’ 0, C â†’ âˆ ,

complementing (44). For the second level, just like in the proof of Lemma C.7 it suffices to study S(10) . If [v, w ] is a dyadic interval of length 2âˆ’k for some k â‰¤ mn, then 

Î¼n

( | S(10)  

> v,w

||v âˆ’ w|2Î± â‰¥ C

)

â‰¤ Câˆ’D|v âˆ’ w|âˆ’2Î±D En

[

| S(10)  

> v,w

|D]ï¸¸ ï¸·ï¸· ï¸¸

> â‰¤c(D)|vâˆ’w|3D/ 2

â‰¤ c(D)Câˆ’D2âˆ’2kD (3 /4âˆ’Î±).

Again we proceed with a union bound over all dyadic intervals of length 2âˆ’k and over all k â‰¤ mn,which succeeds as long as 

2D(3 /4 âˆ’ Î±) > 1 â‡â‡’ 2D > 13/4 âˆ’ Î± â‡â‡’ D > 13/4 âˆ’ Î± ,

which follows from our assumption that D > 1/(1 /2 âˆ’ Î±).If v âˆ’ w < 2âˆ’m, then 

| S(10)  

> v,w

| â‰¤ âˆ¥ xâˆ¥Lip | S(00)  

> v,w

| = 12 âˆ¥xâˆ¥Lip |v âˆ’ w|2,

so 

lim sup 

> nâ†’âˆ

Î¼n

( | S(10)  

> v,w

||v âˆ’ w|2Î± â‰¥ C

)

â‰¤ lim sup 

> nâ†’âˆ

Î¼n

(

âˆ¥xâˆ¥Lip â‰¥ 2C22mn(1 âˆ’Î±))

âˆ’â†’ 0, C â†’ âˆ .

C.3 Product Cubatures 

We can think of the construction in Step 1 of the ARCANE algorithm in a slightly different way that lends itself to an obvious generalisation. The set of 2d paths on [0 , 1] that linearly interpolate between 0 and a point of the form (Â±1, . . . , Â±1) is itself a degree 3 cubature on d-dimensional Wiener space. What we have done is to scale this cubature onto an interval of length 1/N and concatenate N independent copies of it, obtaining a new cubature on [0 , 1] . From this perspective, we can more generally take any existing cubature, the base cubature , say of some degree m and size 

n, scale it onto an interval of length 1/N , and concatenate N independent copies to obtain a new cubature on [0 , 1] of size nN .

Lemma C.8. The product cubature obtained in this way is exact up to order m, and O(N âˆ’(mâˆ’1) /2)-approximate up to any fixed higher order. Proof. Denote by (Si)ni=1 the set of signatures of the paths in the base cubature, and by (Î»i)ni=1 

their weights. Fix N âˆˆ N and denote by ËœSi the Brownian scaling of Si onto an interval of length 

1/N , that is ËœSJi = N âˆ’| J|/2SJi .

45 Denote by ËœS(1) , . . . , ËœS(N ) independent random variables distributed according to the scaled cuba-ture, that is 

P

( ËœS(1) = ËœSi1 , . . . , ËœS(N ) = ËœSiN

)

=

> N

âˆ

> j=1

Î»ij , i1, . . . , i N âˆˆ [n].

Then the expected signature of the product cubature is 

E

[ ËœS(1) âŠ— . . . âŠ— ËœS(N )]

= E

[ ËœS(1) ]âŠ—N

,

due to the independence. Now, since (Si, Î» i) is a degree m cubature, E[ ËœS(1) ] coincides up to degree m

with the expected Brownian signature on an interval of length 1/N , which is exp 

( 12N

âˆ‘di=1 ei âŠ— ei

)

.Due to the Brownian scaling, all terms of degree higher than m are of order N âˆ’(m+1) /2, which we collectively denote O(>m )(N âˆ’(m+1) /2) implies 

E

[ ËœS(1) ]âŠ—N

=

(

exp 

( 12N

> d

âˆ‘

> i=1

ei âŠ— ei

)

+ O(>m )(N âˆ’(m+1) /2)

)âŠ—N

= exp 

( 12

> d

âˆ‘

> i=1

ei âŠ— ei

)

+ O(>m )

(

N âˆ’(mâˆ’1) /2)

.

That is, the expected signature is exact up to order m and has an absolute error of order N âˆ’(mâˆ’1) /2

in higher degree terms, which was exactly our claim. The example we gave in the beginning, which is used in Step 1 of ARCANE, is the special case where the base cubature is itself the d-fold product of the simplest non-trivial one-dimensional Gaussian cubature, a Rademacher random variable (i.e. Â±1 with probability 1/2 each), which has degree 3.46 Appendix D Orthogonal Arrays 

A matrix of the form we use to construct the cubature in Step 1 of the ARCANE algorithm is known as an orthogonal array .

Definition 3. Let q, s, R, C âˆˆ N with q â‰¥ 2 and 2 â‰¤ s â‰¤ C. A matrix A âˆˆ { 0, . . . , q âˆ’ 1}RÃ—C is called a q-ary orthogonal array of strength s, an OA( R, C, q, s ) for short, if the R Ã— s submatrix obtained by selecting any s columns has the property that all qs possible rows appear the same number of times. The probabilistic interpretation of a q-ary orthogonal array with strength s and C columns is that of a coupling of C uniform random variables on {0, . . . , q âˆ’ 1} that are s-wise independent. In the context of the Rademacher cubature, we are interested in coupling a set of C = N w 

Rademacher random variables in an s-wise independent way, where s is the desired degree of the approximate cubature. This is realised by an OA( R, C, 2, s ), and then each of the R rows corresponds to one path in the resulting cubature. Therefore, the parameter regime for orthogonal arrays that we are most interested in is that where q = 2 and s (e.g. 5 or 7) are fixed, C = N w is variable and large, and R is as small as possible as a function of C (given q and s). 

Remark 5. If we construct a product cubature based on a more complex base cubature with rational weights, then we can couple some number of copies of them in an s-wise independent way using a 

q-ary orthogonal array with strength s, where q is the smallest integer such that all of the weights of the base cubature can be written as p/q for an integer p.A well-known lower bound on the number of rows of an orthogonal array with given s, q, C is given by Raoâ€™s bound .

Theorem D.1 (Rao [75]) . Any OA( R, C, q, s ) satisfies 

R â‰¥

> âŒŠs/ 2âŒ‹
> âˆ‘
> i=0
> (

Ci

> )

(q âˆ’ 1) i = Î˜ 

> (

CâŒŠs/ 2âŒ‹)

. (45) The Rao bound can be asymptotically achieved when q = 2 (the case of most interest to us) as well as if s = 2 , but for most parameter combinations with s, q â‰¥ 3 the smallest possible asymptotic of R = R(C) is unknown, and in particular whether or not they achieve the asymptotic growth 

Î˜( CâŒŠs/ 2âŒ‹) suggested by (45). A known positive example is when q = 3 and s = 4 , see Theorem D.2 (4b)). In the following theorem, we summarise the best construction methods we are aware of (in the asymptotic regime we are interested in, i.e. s, q fixed and C large), together with their asymptotic relationship between R and C. Most of these are taken or adapted from existing literature, and we go into detail on both the constructions as well as the correct attributions in the subsequent sections. Efficient JAX [11] implementations of all of the following orthogonal arrays are implemented in the â€œorrayâ€ python package [54]. 

Theorem D.2. The following orthogonal arrays can be constructed. (1) Strength 2 : For a prime power q and n âˆˆ N,

OA 

> (

qn, qn âˆ’ 1

q âˆ’ 1 , q, 2

> )

, having R = 1 + ( q âˆ’ 1) C, 

47 which attains (45) exactly. (2) Strength 3: For a prime power q and n âˆˆ N,

OA 

(

q3n+1 , q 2n, q, 3

)

, having R = qC 3/2,

which does not attain (45) asymptotically. (3) Binary Arrays ( q = 2 ): All of the following arrays attain (45) asymptotically. (a) (Bose-Ray-Chaudhuri [10]) For n âˆˆ N and even s âˆˆ N,

OA 

(

2nâŒŠs/ 2âŒ‹, 2n âˆ’ 1, 2, s 

)

, having R = ( C + 1) âŒŠs/ 2âŒ‹.

(b) (Bose-Ray-Chaudhuri [10]) For n âˆˆ N and odd s âˆˆ N,

OA 

(

21+ nâŒŠs/ 2âŒ‹, 2n, 2, s 

)

, having R = 2 CâŒŠs/ 2âŒ‹.

(c) (Kerdock [50]) For s = 5 and an even integer n â‰¥ 4,

OA 

(

22n, 2n, 2, 5

)

, having R = CâŒŠs/ 2âŒ‹.

(d) (Delsarte-Goethals [26]) For s = 7 and an even integer n â‰¥ 4,

OA 

(

23nâˆ’1, 2n, 2, 7

)

, having R = 12 CâŒŠs/ 2âŒ‹.

(4) Ternary Arrays ( q = 3 ): 

(a) For s = 3 , and n âˆˆ N,

OA(3 4n+1 , 20 n, 3, 3) , having R = 3 C4 log 3 / log 20 â‰ˆ 3C1.467 ,

OA(3 5n+1 , 45 n, 3, 3) , having R = 3 C5 log 3 / log 45 â‰ˆ 3C1.443 ,

OA(3 6n+1 ,112 n, 3, 3) , having R = 3 C6 log 3 / log 112 â‰ˆ 3C1.397 ,

all three of which do not attain (45) asymptotically. (b) For s = 4 , and n âˆˆ N,

OA 

(

32n+1 , 3n, 3, 4

)

, having R = 3 C2,

which attain (45) asymptotically. (5) General: For a prime number q âˆˆ N, s âˆˆ N, and n âˆˆ N,

OA ( qns , q n, q, s ) , having R = Cs,

which does not attain (45) asymptotically. 

48 D.1 Linear Orthogonal Arrays 

With the exception of (3c) and (3d), all of the orthogonal arrays in Theorem D.2 belong to the special class of linear orthogonal arrays , which are orthogonal arrays OA( R, C, q, s ) whose rows form a vector space over a finite field with q elements. Such a field exists if and only if q is a prime powerâ€”in which case the field is unique up to isomorphism and denoted Fqâ€”and in that case R = qn where n is the dimension of the vector space spanned by the rows. This is why all of the constructions above require q to be a prime power. To construct a linear array, say OA( qn, C, q, s ), it suffices to specify the basis of the vector space spanned by the rows, which is a set of n vectors in FCq . If we arrange them as the rows of an n Ã— C

matrix M , then it turns out that they give rise to (i.e. are the basis of the row space of) a linear orthogonal array of strength s if and only if the columns of M are s-wise linearly independent. 

Lemma D.1. Let q be a prime power, and M an nÃ—C matrix with elements in Fq. Then the qn Ã—C

matrix whose rows are all possible Fqâ€“linear combinations of the rows of M form an orthogonal array of strength s if and only if any s columns of M are Fqâ€“linearly independent. In that case, we call M a generator of the orthogonal array. 

See e.g. Theorem 10.4 in [81]. They only state the â€œifâ€ direction, but the â€œonly ifâ€ statement follows from the same proof. Using Lemma D.1, the task reduces to finding a set of s-wise linearly independent vectors in Fnq that is as large as possible. The next section contains constructions based directly on this idea. 

D.1.1 Direct linear constructions 

An instructive example is the case of strength s = 2 : a set of vectors in Fnq for some n âˆˆ N is 

2-wise linearly independent if and only if no vector is a multiple of another. The qn âˆ’ 1 non-zero elements of Fnq can be partitioned into disjoint sets of q âˆ’ 1 vectors that are multiples of each other, and if we pick an arbitrary representative out of each of the C := ( qn âˆ’ 1) /(q âˆ’ 1) classes (for example the set of vectors whose first non-zero entry is equal to 1), then we obtain an n Ã— C

matrix over Fq whose columns are 2-wise linearly independent. By Lemma D.1, it generates an 

OA( qn, (qn âˆ’ 1) /(q âˆ’ 1) , 2, q ), and this is exactly construction (1) from Theorem D.2. A reference for this construction is Corollary 10.5 in [81]. 

Construction 1 (Theorem D.2 (1)) . For a prime power q and n âˆˆ N, there exists a linear 

OA 

> (

qn, qn âˆ’ 1

q âˆ’ 1 , 2, q 

> )

.

It is generated in the sense of Lemma D.1 by an n Ã— C matrix whose columns are all non-zero elements of Fnq whose first non-zero coordinate is equal to 1.

Another relatively simple construction is Tneorem D.2 (5). 

Construction 2 (Theorem D.2 (5)) . For a prime number q, strength s âˆˆ N, and n âˆˆ N, there exists a linear 

OA ( qns , q n, q, s ) .

It is generated by an ns Ã— qn matrix M that can be obtained as follows. First, create an s Ã— qn

matrix M â€² over Fqn whose columns are of the form (1 , x, x 2, . . . , x sâˆ’1), with x ranging through all 

49 qn elements of Fqn . Then, replace each entry in M â€² with a column vector of length n over Fq

through an(y) identification of Fqn with Fnq as vector spaces over Fq. The resulting ns Ã— qn matrix generates the array. Proof. It suffices to show that any s columns of M â€² are Fqn â€“linearly independent. Indeed, then it follows that they are also Fqâ€“linearly independent, and therefore any s columns of M are Fqâ€“linearly independent. Let x1, . . . , x s âˆˆ Fqn be distinct. Then we have to show that the sÃ—s submatrix of M â€² corresponding to those s columns is non-singular, and indeed its determinant is given through the Vandermonde formula by 

det 

> ï£«ï£¬ï£¬ï£¬ï£¬ï£­

1 . . . 1

x1 . . . xs

... . . . ...

xsâˆ’11 . . . xsâˆ’1 

> s
> ï£¶ï£·ï£·ï£·ï£·ï£¸

= âˆ

> i<j

(xj âˆ’ xi)Ì¸ = 0 ,

where multiplication etc. are performed in Fqn .If n = 1 , then this is an OA( qs, q, s, q ); by definition, any q-ary orthogonal array of strength s must have at least qs rows, so the number of rows is minimal, and if q > s then the number of columns is larger than that of the trivial array with the same N, q , and s (which has s columns). The largest possible number of columns given s, q > s , and N = qs is an open problem outside of a few known special cases; even if s = 2 , the answer is unknown for numerous values of q (see e.g. Section 2 in [39], particularly the discussion following Corollary 2.22). If n > 1, then this array stops being optimal in the same sense, and in fact the scaling of its number of rows, R = Cs, is worse than that of all the other arrays listed in Theorem D.2 in overlapping parameter regimes. But it is the best construction that we are aware of that works for arbitrary strength and q â‰¥ 3. For n = 1 , the construction is well-known, see e.g. [81, Corollary 10.7]. We havenâ€™t directly found the generalisation to n > 1 in the literature, but due to its simplicity assume that it is likewise well-known. 

Construction 3 (Theorem D.2 (3a)) . For n âˆˆ N and even s = 2 u âˆˆ N, there exists a linear 

OA(2 nu , 2n âˆ’ 1, 2, s ).

It is generated by an (nu ) Ã— (2 n âˆ’ 1) matrix M over F2 obtained in the following way. For every non-zero x âˆˆ F2n , create the vector (x, x 3, . . . , x 2uâˆ’1) âˆˆ Fu 

> 2n

. Replace each entry with a binary vector of length n through a (vector space) identification of F2n with Fn 

> 2

to yield a vector in Fnu  

> 2

.The 2n âˆ’ 1 vectors obtained in this way form the columns of M .Proof. It suffices to prove that the columns of M constructed in this way are s-wise linearly inde-pendent, which is in Section 4 of a paper by Bose and Ray-Chaudhuri [10]. This construction is the dual of the well-known BCH code, see e.g. [63]. Theorem D.2 (3b) is obtained from (3a) through the following general construction for binary arrays, see e.g. Theorem 2.24 in [39]. In words, any binary array of even strength s = 2 u can be â€œupgradedâ€ to strength s = 2 u + 1 at the cost of doubling the number of rows and the additional benefit of adding one column. 50 Lemma D.2. If A is an OA( R, C, 2, s ) for even s âˆˆ N, then the (2 R) Ã— (C + 1) matrix  

> [

A 0

A 1

> ]

is an OA(2 R, C + 1 , 2, s + 1) , where A denotes the array obtained from A by changing 1â€™s for 0â€™s and vice versa. 

D.1.2 Constructions from cap sets 

Let q be a prime power, and d âˆˆ N. A cap set in Fdq is a set of points such that no three of them are collinear, or, in other words, a 3-wise affinely independent set of points. 

Lemma D.3. If the columns of an nÃ—C matrix M over Fq are s-wise affinely independent, then the 

(n + 1) Ã— C matrix obtained by adding a row that only consists of 1â€™s generates an OA( qn+1 , C, q, s )

in the sense of Lemma D.1. Proof. A set of vectors that is affinely independent becomes linearly independent when an additional coordinate equal to 1 is added to each vector. Then the statement follows from Lemma D.1. Furthermore, it is easy to see that products of cap sets are cap sets. In particular, if S âŠ‚ Fdq is a cap set say of size k, then Sn âŠ‚ Fdn q is a cap set of size kn. Combining with Lemma D.3 gives the following. 

Corollary 1. If S âŠ‚ Fdq is a cap set of size C, then there exists an OA( qnd +1 , C n, q, 3) .

Using Corrolary 1, we can turn known cap-set constructions into orthogonal arrays. For instance, Theorem D.2 (2) is based on a known construction of a cap set in F3 

> q

for prime q.

Construction 4 (Theorem D.2 (2)) . For a prime number q and n âˆˆ N, there exists a linear 

OA( q3n+1 , q 2n, q, 3) .

To construct it, let aX 2 + X + b be an irreducible degree two polynomial over Fq, and put 

S = {(x, y, ax 2 + xy + by 2) : x, y âˆˆ Fq}.

Then S is a cap set of size q2 in F3 

> q

, which generates the orthogonal array in the sense of Corollary 1. Proof. The cap set above can be derived from the cap set (or ovoid ) in the projective space P G (3 , q )

constructed in Example 1.4(1) in [73]. The arrays in Theorem D.2 (4a) are based on known cap sets in Fd 

> 3

for d = 4 , 5, 6.

Construction 5 (Theorem D.2 (4a)) . The three classes of orthogonal arrays in Theorem D.2 (4a) are generated in the sense of Lemma D.3 by the following cap sets: 

â€¢ A cap set of size 20 in F43,

â€¢ A cap set of size 45 in F53,

51 â€¢ A cap set of size 112 in F63,which are explicitly given, constructed, or can be derived from the constructions given in [44, Fig. 1], [43, Section 2], and [74], respectively. 

The three cap sets above are all of maximal size [44, 43, 74], see also sequence A090245 in the OEIS. Sets of points in Fdq that are s-wise affinely independent for s > 3 are sometimes called generalised cap sets , and are much less well-studied. They also lack the convenient product property of (ordi-nary) cap sets: in fact, if S âŠ‚ Fdq contains at least two points and is 4-wise affinely independent, then S2 âŠ‚ F2dq is not. Indeed, let u, v âˆˆ S be distinct, then (u, u ), (u, v ), (v, u ), and (v, v ) are all in 

S2 and lie on a plane (i.e. are affinely dependent). The only result in this area that we are aware of is a construction by Huang [45] of 4-wise affinely independent sets in Fn 

> 3

.

Construction 6 (Theorem D.2 (4b)) . For every integer n âˆˆ N, there exists a linear 

OA(3 2n+1 , 3n, 3, 4) .

For x âˆˆ F3n , map the vector (x, x 2) âˆˆ F23n to a ternary vector of length 2n using an identification of F3n with Fn 

> 3

as vector spaces over F3. Looping through all 3n possible values for x, this gives 

3n points in F2n 

> 3

. They are 4-wise affinely independent [45] and generate the array in the sense of Corollary 1. 

D.1.3 Constructions from non-linear codes 

There is a connection between orthogonal arrays and codes, which we outline briefly. 

Definition 4. Let N, k âˆˆ N, and q âˆˆ N be a prime power. (i) A code of size N âˆˆ N and length k âˆˆ N over Fq is a matrix C âˆˆ FN Ã—kq , whose rows are called 

code words , and we slightly abuse notation by writing u âˆˆ C if u is a row in C.(ii) The distance d of C is the minimal Hamming distance between any two distinct codewords, that is, 

d = min 

> u,v âˆˆCuÌ¸=v

dH (u, v ), where dH (u, v ) = # {i âˆˆ [k] : uiÌ¸ = vi}.

(iii) A code C is called linear if its code words are a vector space over Fq. In that case, the dual code of C is its orthogonal complement, 

CâŠ¥ := {u âˆˆ Fkq : âˆ€v âˆˆ C : âŸ¨u, v âŸ© = 0 }.

The distance dâŠ¥ of CâŠ¥ is called the dual distance of C.It is possible to extend the notion of dual distance to non-linear codes, see e.g. section 4.4 of [39]. This is important for the statement of the following theorem, but it is not necessary in any way to understand the definition for the purpose of reading the remainder of this section. 52 Theorem D.3 (Delsarte 1973 [27]) . A matrix with elements in Fq for some prime power q âˆˆ N

is a code with dual distance at least t + 1 if and only if it is an orthogonal array with strength at least t.

See also Theorem 4.9 in [39] for a more recent exposition. The two remaining constructions, Theorem D.2 (3c) and (3d), are obtained from two non-linear codes called the Kerdock code [50] and the Delsarte-Goethals code [26], binary codes with dual distance 6 and 8, respectively. 

Construction 7 (Theorem D.2 (3c)) . For n â‰¥ 4 even, there exists a non-linear 

OA(2 2n, 2n, 2, 5) .

The rows of the array are the codewords of the Kerdock code, whose construction can be derived from sections 3 and 4 of [37]. 

Construction 8 (Theorem D.2 (3d)) . For n â‰¥ 4 even, there exists a non-linear 

OA(2 3nâˆ’1, 2n, 2, 7) .

The rows of the array are the codewords of the Delsarte-Goethals code, whose construction can be derived from sections 3, 4, and 6 of [37]. 

53 Appendix E Full-Sized Plots 

This appendix includes full-sized versions of the plots in Fig. 3. In addition to our cubatures, they include the degree 5 cubature constructed in the original paper by Lyons and Victoir [61], as well as the degree 5 and 7 cubatures constructed in [72]. 

[Left intentionally blank. Figures continue on the following page.] 

54 Figure 10: Error plot showing the performance of our cubature formulae in comparison with plain Monte Carlo and QMC methods measured in Initial Bond Price Relative Error (see (4)) in the CIR model. Superscripts in cubature labels refer to the dyadic depth of the cubature, see Section 3. 

Figure 11: Error plot showing the performance of our cubature formulae in comparison with plain Monte Carlo and QMC methods measured in Meanâ€“Variance Error (see (3)) in the CIR model. Superscripts in cubature labels refer to the dyadic depth of the cubature, see Section 3. 55 Figure 12: Error plot showing the performance of our cubature formulae in comparison with plain Monte Carlo and QMC methods measured in Meanâ€“Variance Error (see (3)) in the Vasicek model. Superscripts in cubature labels refer to the dyadic depth of the cubature, see Section 3. 

Figure 13: Error plot showing the performance of our cubature formulae in comparison with plain Monte Carlo and QMC methods measured in Initial Bond Price Relative Error (see (4)) in the Vasicek model. Superscripts in cubature labels refer to the dyadic depth of the cubature, see Section 3. 56 Figure 14: Error plot showing the performance of our cubature formulae in comparison with plain Monte Carlo and QMC methods measured in Meanâ€“Variance Error (see (3)) in the IGBM model. Superscripts in cubature labels refer to the dyadic depth of the cubature, see Section 3. 

Figure 15: Error plot showing the performance of our cubature formulae in comparison with plain Monte Carlo and QMC methods measured in Meanâ€“Variance Error (see (3)) in the Wrightâ€“Fisher diffusion model. Superscripts in cubature labels refer to the dyadic depth of the cubature, see Section 3. 57