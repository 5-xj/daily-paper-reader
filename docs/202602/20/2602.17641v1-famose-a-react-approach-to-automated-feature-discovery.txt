Title: FAMOSE: A ReAct Approach to Automated Feature Discovery

URL Source: https://arxiv.org/pdf/2602.17641v1

Published Time: Fri, 20 Feb 2026 02:06:55 GMT

Number of Pages: 23

Markdown Content:
# FAMOSE: A ReAct Approach to Automated Feature Discovery 

Keith Burghardt 1 Jienan Liu 1 Sadman Sakib 1 Yuning Hao 1 Bo Li 1

# Abstract 

Feature engineering remains a critical yet chal-lenging bottleneck in machine learning, particu-larly for tabular data, as identifying optimal fea-tures from an exponentially large feature space tra-ditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE 

(Feature Aug Mentation and Optimal Selection ag Ent), a novel framework that leverages the Re-Act paradigm to autonomously explore, generate, and refine features while integrating feature selec-tion and evaluation tools within an agent archi-tecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct frame-work to automated feature engineering, especially for both regression and classification tasks. Ex-tensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K in-stances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for re-gression tasks by reducing RMSE by 2.0% on av-erage, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE’s strong performance is because ReAct allows the LLM context window to record (via iterative fea-ture discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering. 

# 1. Introduction 

Despite the simplicity of tabular models, their performance is often difficult to optimize. Namely, while carefully engi-neered features are often the most important component to 

> 1

Amazon.com, Inc., 410 Terry Ave N, Seattle, WA 98109, United States. Correspondence to: Keith Burghardt 

<kaburg@amazon.com >.

Preprint. February 20, 2026. 

Figure 1. Types of AutoML. In traditional AutoML, feature engi-neering would include feature discovery, such as within OpenFE (Zhang et al., 2023), or iterative feature modification, such as the method of Piramuthu et al. (Piramuthu & Sikora, 2009). Although LLM methods, such as CAAFE, often excel at feature generation, iterative feature modification has not been explored as often be-fore now. FAMOSE offers a way to learn through trial-and-error which features work and which ones do not until a better feature is developed. 

model performance (Heaton, 2016; Tschalzev et al., 2024), their optimization remains a difficult and time-consuming bottleneck. This is because identifying those valuable fea-tures is extremely difficult: the space of potential new fea-tures (transformations or combinations of existing attributes) is enormous, and only a small fraction will ultimately prove useful. As a result, designing features has traditionally relied on human intuition and domain knowledge, and it continues to consume a significant portion of a data scientist’s effort (Bengio et al., 2013). To reduce the manual burden, a variety of automated fea-ture engineering techniques have been proposed, whether algorithmic methods, such as OpenFE (Zhang et al., 2023), or LLM-based methods, such as CAAFE (Hollmann et al., 2023). However, these methods struggle due to the combi-natorically large space of features to evaluate (Zhang et al., 2023; Horn et al., 2019). While LLM-based methods may offer a route to search this space more efficiently, these methods do not iteratively improve on their prior features by learning from their mistakes. This can be illustrated in Fig. 1. 1

> arXiv:2602.17641v1 [cs.LG] 19 Feb 2026 FAMOSE: A ReAct Approach to Automated Feature Discovery

To address these limitations, we introduce FAMOSE :

Feature Aug Mentation and Optimal Selection agEnt, a framework that utilizes ReAct agents for autonomous fea-ture engineering. The agent iteratively proposes, evaluates, and refines features, rather than generating a static list in a single pass. FAMOSE therefore simulates a data scientist who iteratively hypothesizes, tests, and refines features to improve predictions. Inspired by the Reasoning and Acting paradigm, the agent can reason about the task, execute code, and adapt its strat-egy based on validation performance, enabling a feedback-driven search process similar to how human practitioners ex-periment with features. Unlike one-shot or template-based approaches, the agent can directly interact with the dataset to compute statistics, inspect distributions, and test hypothe-ses about feature utility. As a result, feature proposals are grounded in the actual data rather than relying solely on prior knowledge. The iterative loop also allows the agent to detect and correct invalid or unhelpful features, reducing hallucinations and improving robustness. By learning from empirical outcomes, the agent can more effectively navigate the feature space and focus on transformations that are infor-mative in context. Moreover, during the ReAct process, the LLM explains the reason it chose this feature, improving feature interpretability. At the end of each round, the best feature that improves model performance (if one is found) is saved. At the next round, we re-start the ReAct agent to create yet more features. Each new feature’s performance is then evaluated conditional on the original features and all the saved features from prior rounds. Once performance stops increasing at the feature discovery step, FAMOSE then applies a minimal-redundancy maximal-relevance (mRMR) feature selection step (Ding & Peng, 2005) to produce a compact final feature set. This final step contrasts with prior work that used the LLM itself for feature selection (Holl-mann et al., 2023). We expect that algorithms might select features more accurately than LLMs, so mRMR is chosen as a reasonable feature selection tool. The entire process is fully automated and generalizes across data classifica-tion and regression with diverse dataset sizes and feature dimensionalities. In summary, our contributions are as follows: • ReAct-based feature discovery. We propose a ReAct agent framework to autonomously generate, evaluate, and refine features through iterative interaction with the data. • Comprehensive benchmarking. We evaluate our method on a diverse suite of classification and bench-mark datasets, and check model robustness via evaluat-ing its performance on multiple tabular models as well as an alternative LLM. • State-of-the-art performance and robustness. The proposed method matches or exceeds state-of-the-art performance across classification and regression tasks, and can create features in datasets where some autoML methods fail. This is especially true for large classifi-cation datasets (e.g., more than 10K instances, where FAMOSE’s performance is substantially higher than competing methods). • Feature interpretability through reasoning. We leverage LLM-based reasoning to provide human-interpretable explanations for why generated features are useful. Overall, this algorithm offers a simple, scalable, and robust method for feature discovery. Moreover, while LLMs have been understood as general pattern machines (Mirchandani et al., 2023), our work offers evidence that AI agents can be remarkably effective at solving problems that need highly innovative solutions, despite the limitations of LLMs at creating diverse outputs (Jiang et al., 2025). 

# 2. Related Work 

Automated feature engineering has long been studied to im-prove prediction performance, where carefully engineered features have been shown to outperform gains from model selection or hyperparameter tuning alone (Heaton, 2016). Early work focused on pre-processing data, namely creating compact or latent representations of data, including princi-pal component analysis (PCA) (Abdi & Williams, 2010). In PCA, a new set of orthogonal features are created that represent the largest amount of variance in the data. PCA can be used to reduce the feature space into a subset of distinct features, although this only creates features that are linear functions of the original features. A substantial body of work, however, has investigated com-binatorial and search-based feature engineering. This in-cludes AutoFeat (Horn et al., 2019), DIFER (Zhu et al., 2022), and OpenFE (Zhang et al., 2023), which each create a set of new features based on transformations and oper-ations on older features and then prune the resulting fea-ture set. Other examples include ExploreKit (Katz et al., 2016) and Deep Feature Synthesis (Kanter & Veeramacha-neni, 2015) (also cf. (Heaton, 2016; Nargesian et al., 2017; Davis & Foo, 2016; Nargesian et al., 2017)), which gener-ate candidate features by enumerating transformations over existing features, then follow a heuristic or model-based evaluation for feature selection. While these approaches can produce interpretable features, they often struggle to scale efficiently with memory or time because the search space increases enormously with the number of features in the original dataset. Moreover, because the transformation rules are predefined, these methods may miss domain-specific 2FAMOSE: A ReAct Approach to Automated Feature Discovery 

or multi-step feature constructions that fall outside their templates. An example of multi-step feature construction, however, includes (Piramuthu & Sikora, 2009), which it-eratively creates new features based on features it already created, but this idea has not been as carefully explored in autoML literature. Recent literature has begun to address the limitations of traditional autoML with LLMs. Namely, LLMs have re-cently been utilized for feature generation or transformation (Hollmann et al., 2023; Zhang et al., 2024; Han et al., 2024; Nam et al., 2024; He et al., 2025; Abhyankar et al., 2025), which expands on prior work utilizing LLMs for tabular data understanding (Fang et al., 2024; Wang et al., 2023; Dinh et al., 2022; Inala et al., 2024). A state-of-the-art LLM based feature engineering tool is CAAFE (Hollmann et al., 2023), which uses an LLM to suggest new features based on dataset metadata and task descriptions. In contrast, FeatLLM (Han et al., 2024) treats feature generation as an optimization problem guided by LLM proposals. Related efforts explore evaluating or benchmarking LLM-generated features (Zhang et al., 2024; Bordt et al., 2024) or guid-ing feature construction using model-based reasoning (Nam et al., 2024). These methods demonstrate that LLMs can propose creative and context-aware features that go beyond simple transformations. However, most existing LLM-based approaches follow a one-shot or template-driven workflow: features are generated once, evaluated externally, and not iteratively refined by the LLM itself. As a result, the model cannot learn from feedback, correct invalid feature defini-tions, or adapt its strategy based on empirical performance. Once features are generated, a large body of work has devel-oped feature selection tools (Chandrashekar & Sahin, 2014). Many heuristics have been explored (cf. (Zhang et al., 2023; Horn et al., 2019; Katz et al., 2016; Kanter & Veeramacha-neni, 2015), etc.), which we will not have space to explore in depth. However, one widely-used selection method is minimum redundancy–maximum relevance (mRMR) (Ding & Peng, 2005). This method selects a subset of features based on minimizing redundancy (e.g., minimizing corre-lations between features) while maximizing the relevance (e.g., finding features highly correlated with the target vari-able). Other methods of feature selection include LLMs (Hollmann et al., 2023). Although we expect algorithmic feature selection is more accurate than LLM-based methods, a comprehensive comparison between these methods have not been explored. In contrast to prior approaches, our work’s most substan-tial contribution is that FAMOSE uses a ReAct framework (Yao et al., 2022), in which the LLM iteratively proposes, evaluates, and refines features based on model feedback and data-driven analysis. By integrating reasoning, acting, and validation within a closed loop, this approach aims to ad-

Algorithm 1 FAMOSE:  

> 1:

Input: Dataset D = ( X, y ), base model M 

> 2:

Output: Selected engineered feature set F⋆ 

> 3:

Split D into K=5 folds (stratified if classification)  

> 4:

for Each fold do  

> 5:

Split training data into train and validation  

> 6:

Initialize feature set F ← ∅  

> 7:

Define E(Z), the error of a model in validation data  

> 8:

Evaluate baseline score on validation set  

> 9:

for r = 1 to 20 do  

> 10:

for step = 1 to 10 do  

> 11:

Agent proposes a feature, f , using metadata and tool feedback  

> 12:

Validate feature code  

> 13:

if validation fails then  

> 14:

Agent creates new code  

> 15:

end if  

> 16:

if 1−E(X ∩F ∩{ f })/E (X ∩F ) < 0.01 then  

> 17:

Continue  

> 18:

end if  

> 19:

end for 

Find f s.t. E(X ∩ F ∩ { f }) is maximized  

> 20:

if E(X ∩ F ∩ f ) > 0 then  

> 21:

F ← F ∩ { f } 

> 22:

end if  

> 23:

end for  

> 24:

Apply mRMR to select final features F⋆ 

> 25:

Train M using F⋆ and evaluate on test fold  

> 26:

end for 

dress the scalability, robustness, and adaptability limitations observed in prior automated feature engineering methods. In addition, our work conditions the performance on new features with the performance of the model with all prior picked features, allowing us to find features that add more signal to our prediction task than all previous features. Fi-nally, we utilize a traditional algorithm for feature selection (mRMR (Ding & Peng, 2005)) rather than an LLM (Holl-mann et al., 2023), which we believe allows for fast and accurate feature selection. 

# 3. Methodology 

In this section, we describe the FAMOSE methodology for feature generation, the datasets used to evaluate FAMOSE, the experimental setup, and the baseline algorithms. 

3.1. FAMOSE 

We outline FAMOSE in Algorithm 1, with an example of FAMOSE in Fig. 2. FAMOSE at it’s core is Smola-gents (Roucher et al., 2025) with additional tools and a post-processing evaluation algorithm to ensure that the best 3FAMOSE: A ReAct Approach to Automated Feature Discovery 

features are selected. Smolagents is an opensource AI agent that can think, act, and observe, but can hallucinate without appropriate tools. We removed its core tools except for the Python code compiler, and created a metadata generator tool that allows the agent to determine data file column names (features) and whether the column values represent numeri-cal, datetime, or categorical data. This allows the agent to better understand features without manually incorporating those details into the prompt. We also provided it with a feature evaluation tool, in which code generated by an LLM is used to generate a feature column. The model perfor-mance (ROC-AUC or RMSE) in held-out data containing the candidate feature, the original features, and all of the best features chosen so far is compared against a model without the candidate feature. If the change in performance is positive, the feature improves model performance. We negate RMSE so that a positive change always represents a better model. The tool is highly robust with the help of error correction and regex to extract all the feature names in the feature code; if the feature names include hallucinated features, or if the code cannot run, then the tool forces the agent to correct its mistakes, including the replacement of hallucinated features with correct ones. Data provided to the feature generation code also has the target variable removed to avoid data leakage. We provide the following prompt for the agent (see details of prompt and agent output in the Appendix): We first spec-ify a role (data scientist) and the relevant files and feature descriptions (see example feature descriptions in Appendix Tables S3 & S3). We then ask the agent to 1. Use all of these insights to create a large set of new features. It can use any mathematical operations or transforms. 2. Explain why this feature is useful 3. Check the performance of each feature with a feature evaluation tool 4. Set a goal to improve feature performance by 1%, and do not stop until this is achieved. 5. Return the best feature. The agent will then autonomously use the file meta-data tool, create Python code, and evaluate features with the appropriate tools. If the agent discovers that a feature did not improve performance by at least 1%, it will iteratively improve on and evaluate new features for up to 10 agent steps until it is satisfied (top right of Fig. 2). At this point, the agent will stop, and a feature evaluator algorithm will then evaluate all the features created by the agent and pick the best feature that improves model performance (if any are found). This post-agent step addresses situations in which             

> Figure 2. An illustration of FAMOSE applied to the
> balance-scale task (Siegler, 1976). FAMOSE first invents features and observes their performance, in this case using the difference in arm weights to see whether a scale is balanced, tilting right, or tilting left. If performance is insufficient, FAMOSE then thinks about how to create better features, ultimately discovering the feature, moment, that exactly predicts whether a scale is bal-anced. Because the performance cannot be improved further in this task, the feature discovery step eventually stops. The feature selection step then selects this feature using mRMR (Ding & Peng, 2005) and removes the four extraneous features in the task dataset:
> W1,W2,L1, and L2, thus instead of an error-prone model with four features, we discover a perfect predictor with only one feature.

the LLM hallucinates model performance in order to satisfy the 1% model performance improvement goal. Finally the agent repeats all these steps for up to 20 rounds or until the performance has not improved after 6 rounds (this stops the algorithm early to save time). Adding features, however, could contribute to overfitting a model, so we use mRMR (Gauch, 2003) to select the best features, as shown in the bot-tom right of Fig. 2. This contrasts with other SOTA feature engineering methods (Hollmann et al., 2023; P ´erez et al., 2025), in which the LLM itself decides what features to remove. We specifically use 5-fold cross-validation within the training dataset to find the optimal number of features to select (based on ROC-AUC or RMSE), then apply mRMR to the training dataset to select the specific features. 

3.2. Data 

We evaluate our approach on a diverse collection of public tabular datasets, including 20 classification tasks and 7 re-gression tasks, spanning over three orders of magnitude in size (all datasets are outline in Appendix Table S2). These dataset tasks cover binary and multi-class classification (e.g. income prediction, medical diagnosis, game outcomes) as well as regression problems (e.g. house prices, insurance costs). We specifically chose these datasets from a prior pa-per (Abhyankar et al., 2025), but added additional tasks from Kaggle that contain over 10K instances to ensure that ROC-AUC differences were more statistically significant. Most datasets analyzed are commonly used as benchmarks, and a smaller subset of these have been used in many prior papers (Horn et al., 2019; Hollmann et al., 2023; Abhyankar et al., 2025; Han et al., 2024). We borrow feature descriptions and 4FAMOSE: A ReAct Approach to Automated Feature Discovery 

task descriptions (i.e., what the model aims to predict) from (Abhyankar et al., 2025), although we correct feature names for vehicle (Siebert, 1987), balance-scale (Siegler, 1976), and cmc (Lim, 1999) whose feature names were mis-written in the Github code for (Abhyankar et al., 2025). For example, column names were lower-case when they should be capitalized, or contained spaces when they should con-tain underscores (“ ”). For datasets not mentioned in (Ab-hyankar et al., 2025), and for bank marketing (Moro et al., 2011), which also contained errors in feature descrip-tions, we instead use feature descriptions from each dataset’s respective webpage (e.g., Kaggle; all dataset links are ref-erenced in Appendix Table S2). Table S1 summarizes the key characteristics of the datasets, including the number of instances, number of features, (for classification tasks) class imbalance (fraction of data in the smallest class), and (for regression tasks) the target variable variance. We also list in Table S2 a brief description of each prediction task. For training/validation and testing, we use 

scikit-learn ’s K-Folds (if regression) or Strati-fiedKFold (if classification) with K =5, and use a random seed of 42 (Pedregosa et al., 2011). We further stratify training data into training and validation using K =5where validation is used to select FAMOSE features or to determine the number of features to select for mRMR. Test folds are only used for the final performance evaluation. 

3.3. LLMs and Resources 

For LLM-based feature engineering, we use AWS Bedrock’s 

Sonnet 3.5 V2 LLM and Deepseek-R1 , as they have reasonable speed, performance, and cost. While the main text describes results using Sonnet 3.5 V2 , we also report Deepseek-R1 in the Appendix and find it creates similar quality features. Following prior work (Ab-hyankar et al., 2025), we fix the LLM’s temperature to be 0.8. Fine-tuning such hyperparameters will be important research in the future. Analysis was made with AWS Sage-maker ml.r6i.32xlarge and ml.m7i.48xlarge 

instances for most data, but we use a GPU instance, 

ml.g5.48xlarge , to train Autogluon (Erickson et al., 2020) (Cf. https://instances.vantage.sh/ for a summary of instance specifications). We do not expect variations in instances will have a significant impact on model performance. Because the resources needed to run LLM inference are unknown, and could affect LLM speed, we do not compare algorithm inference times, especially the speed of LLM methods against classical methods. We would expect that if classical methods were given the same resources (perhaps 10s of GPUs) as an LLM like Sonnet 3.5 V2 , we should expect far different speeds. That said, because we have limited time to analyze data, we do not include performance for classical algorithms that created errors or take more than 10 hours for all five folds to fin-ish within a ml.m7i.48xlarge instance. LLM-based methods, meanwhile, typically finish a few minutes, and therefore do not have such restrictions. Even the longest time for FAMOSE to complete all five folds of a task is roughly 6 hours ( covtype , which contains 580K instances and 55 features). 

3.4. Prediction Models 

We compare different feature engineering approaches using XGBoost, as this is a common high-quality tabular model baseline. While XGBoost performance could vary based on hyperparameters, we use default parameters and set the seed to 42 for consistency. We also apply the XGBoost-derived features to Random Forest (Breiman, 2001) and Autogluon in order to determine the robustness of these features across models. Other models can also be tested on FAMOSE; we believe the models we choose, however, rep-resent exemplars of state-of-the-art tabular models: boosted trees (XGBoost), ensembles of trees (Random Forest), and weighted ensembles of deep-learning, boosted, and founda-tion models (Autogluon). 

3.5. Feature Engineering Baselines 

We compare our methods against two classical methods, AutoFeat (Horn et al., 2019), and OpenFE (Zhang et al., 2023), as well as SOTA LLM-based methods, CAAFE (Holl-mann et al., 2023) and FeatLLM (Han et al., 2024). For LLM-based methods, we replace a call to the Open AI API with a call to AWS Bedrock (a change of just a few lines) in order to easily switch between various candidate LLMs we aimed to use, but all other code was kept the same. We do not report comparisons against another recent base-line, LLM-FE (Abhyankar et al., 2025), as their results are hard to validate: metadata was miswritten in their code, as mentioned above, and details of model implementations, including model hyperparameters, are missing. Finally, they report results, such as normalized RMSE, but do not state how they normalized RMSE (e.g., normalize by the mean or range, within the test set or training set, etc.), making the results they report unreproducible. Smaller issues include the category pre-processing step in their code converts cate-gories into float numbers, so that the model would interpret the categories “dog”, “cat’, and “car” as 0, 1, 2, where 2 is greater than 0 (we believe this is not an adequate way to rep-resent non-cardinal categorical variables). Due to challenges reproducing their results, and the incorrect pre-processing step they use on data, we cannot be certain we replicated their work, so we leave out this model (that said, our best guess at reproducing their results shows significantly worse performance than our method or CAAFE). In some datasets, we find column names contain unusual characters (e.g., “)” or “ <”), which would confuse classi-5FAMOSE: A ReAct Approach to Automated Feature Discovery 

cal methods (AutoFeat or OpenFE). AutoFeat and OpenFE read and write feather-formatted files, which cannot na-tively parse those characters. For the classical feature en-gineering methods, we replaced column names, “(” or “)” with “ ”, “[” (“]”) with “left(right)bracket” and “ <”( >) with “less(greater)than”. Despite results reported in (Abhyankar et al., 2025), the simple change in the column names dra-matically improved the robustness of each of the classical methods, and the only errors were due to out-of-memory errors (e.g., creating and then selecting among millions of features would require TBs of memory), or more rarely out-of-time errors. 

# 4. Results 

In Table 2, we show classifier performance: the mean ROC-AUC after applying FAMOSE, and 4 competing SOTA methods: AutoFeat (Horn et al., 2019) and OpenFE (Zhang et al., 2023) (both classical methods), and FeatLLM (Han et al., 2024) and CAAFE (Hollmann et al., 2023) (both LLM-based methods). For any multi-class tasks, such as 

balance-scale , we calculate an unweighted average of one-versus-one ROC-AUC. We find that FAMOSE is statistically insignificantly different from the SOTA meth-ods, and consistently find AutoFeat and OpenFE fail to handle particularly complex datasets or data with more than 

∼10K rows. This is because the number of features these models analyze scale greatly with the number of columns, which in turn creates, e.g., TBs of memory for moderately large datasets. Unfortunately, this means classical meth-ods fail for many realistic datasets. FeatLLM, meanwhile, fails or performs poorly for different reasons, often due to issues including creating rules for multiple classes, or because the LLM output will end without completing the task. CAAFE and FAMOSE, meanwhile can work across all tasks. While we find OpenFE is strongest for tasks with fewer instances, FAMOSE pulls away for tasks with more than 10K instances, with a mean improvement of 0.23% .We chose 10K as a reasonable compromise between size and number of tasks (7 out of 21). We compare features de-veloped by CAAFE and FAMOSE in the Appendix, where we find that FAMOSE creates many more diverse functions. FAMOSE also performs strongly for regression tasks, where it is better than the competing methods, AutoFeat and OpenFE (LLM methods we evaluated do not create features for regression tasks). Namely, we find a 2.0% reduction in RMSE on average , as shown in Table 1 (Wilcoxon signed rank test p-value = 0 .07 between FAMOSE and the base-line). Importantly, FAMOSE again performs well in both small and large tasks, while classical methods often fail to evaluate larger tasks (success rate is < 90% ). Interest-ingly, OpenFE performs especially poorly for the bike 

task, although we are unsure as of yet why its performance is consistently worse across all folds. In addition, however to model performance, we see in Ap-pendix A.2 that FAMOSE’s output achieves human inter-pretability. For example, when applying FAMOSE to the 

balance-scale task, we see, much like 2, that the agent can test and discover moment difference is the only feature needed to capture whether a scale is balanced or tilts. We apply an alternative LLM, Deepseek-R1 (Guo et al., 2025), to FAMOSE and discover performance is similar (see Tables S5 & S6 in the Appendix). The model perfor-mance is therefore robust to the type of LLM chosen. We also see the features evaluated for XGBoost models work well in Random Forest (Breiman, 2001), and Autogluon models (see Tables S7 & S8 in the Appendix). The features FAMOSE discovers appear to work well across a variety of models. We can also test whether FAMOSE can be ablated to a simpler model (Appendix Tables S10 & S9). We find, for example, that removing the “goal” in our prompt (“improve performance by at least 1%”) can sometimes lead to lower performance. We believe this is because the agent decides to stop the results early without working “hard” to create ever better features. We similarly find, as expected, that feature selection alone is insufficient to boost model per-formance (thus motivating feature engineering). For exam-ple, balance-scale achieves ROC-AUC of 1.0 once an LLM feature related to torque is discovered, but feature selection would never discover this feature. Similarly, the model performance is reduced when we do not select fea-tures (which is needed to simplify an otherwise complex model). In conclusion, we find: (1) FAMOSE’s ReAct framework can create SOTA features in both classification and regres-sion tasks, especially for tasks with more than 10K instances (2) creating a goal for FAMOSE (“improve performance by at least 1%”) can further improve model performance, and (3) FAMOSE-based features derived from one model can improve the performance of other models. 

# 5. Discussion 

Our proposed ReAct framework, FAMOSE is a novel way to create SOTA model performance for classification tasks (especially large classification tasks) as well as regression tasks. Moreover, the features engineered by FAMOSE im-prove the performance of both weaker (e.g., Random Forest) and stronger (e.g., Autogluon) models. If FAMOSE is ap-plied to a fast and simple model, our results suggest that an end-user could be reasonably confident that FAMOSE-engineered features can also improve the performance of a more powerful, slower, or costlier model, thereby saving time or money in the process. 6FAMOSE: A ReAct Approach to Automated Feature Discovery 

Task Baseline OpenFE AutoFeat FAMOSE 

bike 40.3 ±1.03 92.09 ±6.35 41.47 ±1.01 40.05 ±0.99 

crab 2.32 ±0.13 2.26 ±0.15 – 2.34 ±0.08 

cybersecurity attacks 0.82 ±0.0 – – 0.82 ±0.0 

housing 409.58 ±10.54 432.87 ±10.22 403.96 ±9.61 408.56 ±26.34 

insurance 192.89 ±13.91 190.23 ±11.18 187.6 ±12.48 191.64 ±11.16 

forest-fires 92.7 ±5.34 88.52 ±5.91 93.49 ±5.42 79.49 ±5.87 

wine-quality 0.64 ±0.02 0.75 ±0.03 0.62 ±0.02 0.64 ±0.01 % Reduction ↑ 0.0% -20.7% 0.3% 2.0% 

% Tasks analyzed ↑ 100% 86% 71% 100% 

Table 1. RMSE comparison of FAMOSE to SOTA feature engineering algorithms across 5 folds: OpenFE (Zhang et al., 2023) and AutoFeat (Horn et al., 2019). The “% Reduction” is the mean percent reduction of RMSE across all folds and tasks. This is calculated by replacing any algorithms that could not run with the baseline performance. Bold indicates best model. Task Baseline OpenFE AutoFeat FeatLLM CAAFE FAMOSE 

balance-scale 0.914 ±0.01 0.999 ±0.001 0.977 ±0.025 1.0 ±0.0 0.965 ±0.022 1.0 ±0.0 

blood 0.698 ±0.03 0.681 ±0.048 0.677 ±0.033 0.426 ±0.063 0.697 ±0.03 0.704 ±0.027 

breast-w 0.989 ±0.007 0.993 ±0.004 0.993 ±0.006 0.994 ±0.004 0.99 ±0.01 0.989 ±0.008 

car 0.999 ±0.0 1.0 ±0.0 1.0 ±0.0 0.86 ±0.019 0.999 ±0.0 0.997 ±0.003 

cmc 0.704 ±0.019 0.693 ±0.013 0.683 ±0.019 0.709 ±0.031 0.7 ±0.02 0.696 ±0.023 

communities 0.866 ±0.008 – – 0.808 ±0.011 0.864 ±0.007 0.861 ±0.007 

credit-g 0.771 ±0.036 0.772 ±0.023 0.775 ±0.026 0.711 ±0.035 0.783 ±0.024 0.757 ±0.04 

eucalyptus 0.835 ±0.01 0.907 ±0.009 0.903 ±0.008 – 0.83 ±0.014 0.836 ±0.021 

heart 0.919 ±0.023 0.919 ±0.022 0.914 ±0.026 0.92 ±0.021 0.917 ±0.023 0.903 ±0.027 

myocardial 0.688 ±0.082 0.686 ±0.093 – 0.592 ±0.054 0.679 ±0.09 0.679 ±0.05 

pc1 0.822 ±0.05 0.795 ±0.104 0.82 ±0.049 0.701 ±0.055 0.834 ±0.033 0.832 ±0.064 

tic-tac-toe 0.999 ±0.002 1.0 ±0.0 0.999 ±0.002 – 0.999 ±0.002 1.0 ±0.001 

vehicle 0.929 ±0.007 0.952 ±0.007 0.951 ±0.003 0.792 ±0.008 0.93 ±0.008 0.918 ±0.021 Small Task % Improvement ↑ 0.0% 1.04% 0.92% -7.6% 0.47% 0.36% 

adult 0.929 ±0.002 0.896 ±0.002 – 0.887 ±0.003 0.929 ±0.002 0.929 ±0.002 

bank fraud base 0.886 ±0.004 0.837 ±0.005 – 0.718 ±0.007 0.887 ±0.003 0.886 ±0.003 

bank marketing 0.934 ±0.005 – – 0.738 ±0.02 0.933 ±0.004 0.928 ±0.013 

covtype 0.99 ±0.0 0.994 ±0.0 – 0.743 ±0.003 0.99 ±0.0 0.989 ±0.001 

diabetes 0.705 ±0.004 0.673 ±0.003 – – 0.705 ±0.005 0.705 ±0.003 

junglechess 0.974 ±0.001 0.999 ±0.0 – 0.718 ±0.004 0.974 ±0.001 0.994 ±0.006 Large Task % Improvement ↑ 0.0% -1.77% -0.018% -15.9% 0.008% 0.229% 

% Overall Improvement ↑ 0.0% 0.15% 0.62% -10.2%* 0.32% 0.32% % Tasks analyzed ↑ 100% 89% 58% 84% 100% 100% 

Table 2. ROC-AUC comparison of FAMOSE to SOTA feature engineering algorithms across 5 folds: AutoFeat (Horn et al., 2019), OpenFE (Zhang et al., 2023), FeatLLM (Han et al., 2024), and CAAFE (Hollmann et al., 2023). Algorithms that could not run due to errors are indicated by “–”. The “% Improvement” is the mean percent increase in ROC-AUC across folds and tasks. This is calculated by replacing any algorithms that could not run with the baseline performance. Bold indicates best model. 

In contrast to prior methods, FAMOSE effectively simulates a data scientist by iteratively hypothesizing, testing, and re-fining features to improve predictions. Moreover, FAMOSE self-corrects code to robustly develop new features for a rich variety of datasets. We hypothesize FAMOSE’s ability to create useful features is due to the LLM context window recording the features that did and did not improve model performance, which is akin to few-shot learning, allowing new features invented by an agent to improve on the mis-takes of features the agent invented in previous steps. Over-all, our results hint at ways that AI agents can be general problem solvers in situations that require highly inventive solutions, such as feature engineering. More specifically, with the help of post-processing checking (to reduce hallu-cinations), and the right tools and prompts, other agents can be able to similarly develop trustworthy, error-free outputs in highly inventive tasks. 

5.1. Limitations 

There are several limitations associated with FAMOSE. First, the ReAct framework can be expensive due to the number of tokens needed for agentic chain-of-though rea-soning. In addition, FAMOSE appears to perform more poorly on cheaper and smaller LLMs (based on our initial exploration of FAMOSE with Llama 3.1-8B). Third, the benefit of FAMOSE depends on the background knowledge available to the LLM; for example, the LLM will perform better on a common task than a task very different from the text it was trained on. A RAG framework might help in these scenarios by inserting more bespoke background 7FAMOSE: A ReAct Approach to Automated Feature Discovery 

knowledge of the task to the LLM prior to the feature gen-eration step. Finally, FAMOSE will need to be modified to create features for multi-label classification, but we expect such modifications would be minor. 

# 6. Conclusion 

We introduced FAMOSE, a novel ReAct-based algorithm to automate feature engineering for tabular machine learning tasks. This approach reduces reliance on deep domain ex-pertise, speeds up responses to emerging patterns, and lays the groundwork to automate the end-to-end machine learn-ing lifecycle. FAMOSE utilizes agents to create features and evaluates the features again after the agent finishes its task to control for LLM hallucinations. Finally, it selects a subset of features with mRMR to optimize model perfor-mance. Using these features, we found SOTA performance improvements across classification and regression tasks, es-pecially for large classification tasks. These features were found to work well across various models and came with explanations written by the agent’s LLM, which helps hu-mans better understand these features, in contrast to many non-LLM methods. 

# Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 

# References 

Abdi, H. and Williams, L. J. Principal component analysis. 

Wiley interdisciplinary reviews: computational statistics ,2(4):433–459, 2010. Abhyankar, N., Shojaee, P., and Reddy, C. K. Llm-fe: Automated feature engineering for tabular data with llms as evolutionary optimizers. arXiv preprint arXiv:2503.14434 , 2025. Agur, A. and Venugopal, U. Cyber security attacks. https: //www.kaggle.com/datasets/teamincrib o/cyber-security-attacks/data , 2024. Aha, D. Tic-Tac-Toe Endgame. UCI Machine Learning Repository, 1991. DOI: https://doi.org/10.24432/C5688J. Arunjangir245, K. U. Healthcare insurance expenses. http s://www.kaggle.com/datasets/arunjang ir245/healthcare-insurance-expenses/ ,2023. Becker, B. and Kohavi, R. Adult. UCI Ma-chine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20. Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. IEEE transac-tions on pattern analysis and machine intelligence , 35(8): 1798–1828, 2013. Blackard, J. Covertype. UCI Machine Learning Repository, 1998. DOI: https://doi.org/10.24432/C50K5N. Bohanec, M. Car Evaluation. UCI Machine Learning Repos-itory, 1988. DOI: https://doi.org/10.24432/C5JP48. Bordt, S., Nori, H., Rodrigues, V., Nushi, B., and Caruana, R. Elephants never forget: Memorization and learning of tabular data in large language models. arXiv preprint arXiv:2404.06209 , 2024. Breiman, L. Random forests. Machine learning , 45(1): 5–32, 2001. Bulloch, B. Eucalyptus soil conservation. https://www. kaggle.com/datasets/ishadss/eucalypt us , 1996. CDC. Diabetes health indicators dataset. https://www. kaggle.com/datasets/alexteboul/diabe tes-health-indicators-dataset , 2015. Chandrashekar, G. and Sahin, F. A survey on feature selection methods. Computers & Electrical Engineer-ing , 40(1):16–28, 2014. ISSN 0045-7906. doi: https: //doi.org/10.1016/j.compeleceng.2013.11.024. URL 

https://www.sciencedirect.com/scienc e/article/pii/S0045790613003066 . 40th-year commemorative issue. Cortez, P. and Morais, A. A data mining approach to predict forest fires using meteorological data. In Proceedings of the 13th EPIA 2007 , pp. 512–523, Guimaraes, Portugal, 2007. Cortez, P., Cerdeira, A., Almeida, F., Matos, T., and Reis, J. Modeling wine preferences by data mining from physico-chemical properties. In Decision Support Systems , vol-ume 47, pp. 547–553. Elsevier, 2009. Davis, J. J. and Foo, E. Automated feature engineering for http tunnel detection. computers & security , 59:166–185, 2016. Ding, C. and Peng, H. Minimum redundancy feature se-lection from microarray gene expression data. Journal of bioinformatics and computational biology , 3(02):185– 205, 2005. 8FAMOSE: A ReAct Approach to Automated Feature Discovery 

Dinh, T., Zeng, Y., Zhang, R., Lin, Z., Gira, M., Rajput, S., Sohn, J.-y., Papailiopoulos, D., and Lee, K. Lift: Language-interfaced fine-tuning for non-language ma-chine learning tasks. Advances in Neural Information Processing Systems , 35:11763–11784, 2022. Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., and Smola, A. Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint arXiv:2003.06505 , 2020. Fanaee-T, H. Bike sharing dataset, 2023. URL https: //www.kaggle.com/dsv/6194875 .Fang, X., Xu, W., Tan, F. A., Zhang, J., Hu, Z., Qi, Y. J., Nickleach, S., Socolinsky, D., Srinivasan Sengamedu, S., and Faloutsos, C. Large language models (llms) on tabular data: Prediction, generation, and understanding — a survey. Transactions on Machine Learning Research ,2024. URL https://www.amazon.science/p ublications/large-language-models-llm s-on-tabular-data-prediction-generat ion-and-understanding-a-survey .fedesoriano. Heart failure prediction dataset. https: //www.kaggle.com/datasets/fedesorian o/heart-failure-prediction , 2021. Ferretti, J. Expedia travel dataset. https://www.kagg le.com/datasets/jacopoferretti/exped ia-travel-dataset/data , 2022. Gauch, H. G. Scientific method in practice . Cambridge University Press, 2003. Golovenkin, S., Shulman, V., Rossiev, D., Shesternya, P., Nikulina, S., Orlova, Y., and Voino-Yasenetsky, V. Myocardial infarction complications. UCI Machine Learning Repository, 2020. DOI: https://doi.org/10.24432/C53P5M. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-centivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. Guvenir, H., Acar, B., Muderrisoglu, H., and Quinlan, R. Arrhythmia. UCI Machine Learning Repository, 1997. DOI: https://doi.org/10.24432/C5BS32. Han, S., Yoon, J., Arik, S. O., and Pfister, T. Large language models can automatically engineer features for few-shot tabular learning. arXiv preprint arXiv:2404.09491 , 2024. He, T., Huang, X., Du, Y., Long, Q., Qiao, Z., Wu, M., Fu, Y., Zhou, Y., and Xiao, M. Fastft: Accelerating rein-forced feature transformation via advanced exploration strategies. arXiv preprint arXiv:2503.20394 , 2025. Heaton, J. An empirical analysis of feature engineering for predictive modeling. In SoutheastCon 2016 , pp. 1–6, 2016. doi: 10.1109/SECON.2016.7506650. Hofmann, H. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI: https://doi.org/10.24432/C5NC77. Hollmann, N., M ¨uller, S., and Hutter, F. Large language models for automated data science: Introducing caafe for context-aware automated feature engineering. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Pro-cessing Systems , volume 36, pp. 44753–44775. Curran Associates, Inc., 2023. URL https://proceeding s.neurips.cc/paper_files/paper/2023/ file/8c2df4c35cdbee764ebb9e9d0acd519 7-Paper-Conference.pdf .Horn, F., Pack, R., and Rieger, M. The autofeat python li-brary for automated feature engineering and selection. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases , pp. 111–120. Springer, 2019. Inala, J. P., Wang, C., Drucker, S., Ramos, G., Dibia, V., Riche, N., Brown, D., Marshall, D., and Gao, J. Data analysis in the era of generative ai. arXiv preprint arXiv:2409.18475 , 2024. Jesus, S., Pombal, J., Alves, D., Cruz, A., Saleiro, P., Ribeiro, R., and et al. Turning the tables: Biased, imbal-anced, dynamic tabular datasets for ml evaluation, 2022. URL https://www.kaggle.com/datasets/ sgpjesus/bank-account-fraud-dataset-n eurips-2022 .Jiang, L., Chai, Y., Li, M., Liu, M., Fok, R., Dziri, N., Tsvetkov, Y., Sap, M., Albalak, A., and Choi, Y. Artificial hivemind: The open-ended homogeneity of language models (and beyond). arXiv preprint arXiv:2510.22954 ,2025. Kanter, J. M. and Veeramachaneni, K. Deep feature synthe-sis: Towards automating data science endeavors. 2015 IEEE International Conference on Data Science and Ad-vanced Analytics (DSAA) , pp. 1–10, 2015. URL https: //api.semanticscholar.org/CorpusID: 206610380 .Katz, G., Shin, E. C. R., and Song, D. X. Explorekit: Auto-matic feature generation and selection. 2016 IEEE 16th International Conference on Data Mining (ICDM) , pp. 979–984, 2016. URL https://api.semanticsc holar.org/CorpusID:4468932 .9FAMOSE: A ReAct Approach to Automated Feature Discovery 

Lim, T.-S. Contraceptive Method Choice. UCI Machine Learning Repository, 1999. DOI: https://doi.org/10.24432/C59W2D. Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M. G., Rao, K., Sadigh, D., and Zeng, A. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721 , 2023. Moro, S., Laureano, R., and Cortez, P. Using data mining for bank direct marketing: An application of the crisp-dm methodology. In Proceedings of the European Simulation and Modelling Conference - ESM’2011 , pp. 117–121, 2011. Nam, J., Kim, K., Oh, S., Tack, J., Kim, J., and Shin, J. Op-timized feature generation for tabular data via llms with decision tree reasoning. Advances in Neural Information Processing Systems , 37:92352–92380, 2024. Nargesian, F., Samulowitz, H., Khurana, U., Khalil, E. B., and Turaga, D. S. Learning feature engineering for clas-sification. In IJCAI , volume 17, pp. 2529–2535, 2017. Pace, R. K. and Barry, R. Sparse spatial autoregressions. 

Statistics and Probability Letters , 33:291–297, 1997. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-napeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825–2830, 2011. P ´erez, A. S., Boukhary, A., Papotti, P., Lozano, L. C., and Elwood, A. An llm-based approach for insight generation in data analysis. arXiv preprint arXiv:2503.11664 , 2025. Piramuthu, S. and Sikora, R. T. Iterative feature construc-tion for improving inductive learning algorithms. Expert Systems with Applications , 36(2):3401–3406, 2009. Redmond, M. Communities and Crime. UCI Machine Learning Repository, 2002. DOI: https://doi.org/10.24432/C53W3X. Roucher, A., Moral, A. V. D., T. Wolf, L. V. W., and Kau-nismaki, E. ’smolagents’: a smol library to build great agentic systems. https://github.com/hugging face/smolagents , 2025. Sayyad Shirabad, J. and Menzies, T. The promise repository of software engineering databases. https://www.op enml.org/search?type=data&sort=runs& id=1068&status=active , 2005. Sidhu, G. S. Crab age prediction, 2021. URL https: //www.kaggle.com/dsv/2834512 .Siebert, J. Vehicle recognition using rule based methods. 

Turing Institute Research Memorandum TIRM-87-018 ,1987. Siegler, R. Balance Scale. UCI Machine Learning Reposi-tory, 1976. DOI: https://doi.org/10.24432/C5488X. Tschalzev, A., Marton, S., L ¨udtke, S., Bartelt, C., and Stuck-enschmidt, H. A data-centric perspective on evaluating machine learning models for tabular data. arXiv preprint arXiv:2407.02112 , 2024. van Rijn, J. N. and Vis, J. K. Endgame analysis of dou shou qi. openml.org/search?type=data&id=409 98 , 2014. Wang, R., Wang, Z., and Sun, J. Unipredict: Large language models are universal tabular classifiers. arXiv preprint arXiv:2310.03266 , 2023. Wolberg, W. Breast Cancer Wisconsin (Original). UCI Machine Learning Repository, 1990. DOI: https://doi.org/10.24432/C5HP4Z. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations , 2022. Yeh, I.-C. Blood Transfusion Service Center. UCI Machine Learning Repository, 2008. DOI: https://doi.org/10.24432/C5GS39. Zhang, T., Zhang, Z. A., Fan, Z., Luo, H., Liu, F., Liu, Q., Cao, W., and Jian, L. Openfe: Automated feature genera-tion with expert-level performance. In International Con-ference on Machine Learning , pp. 41880–41901. PMLR, 2023. Zhang, Y., Li, N., Gan, Q., Zhang, W., Wipf, D., and Wang, M. Elf-gym: Evaluating large language mod-els generated features for tabular prediction. In Pro-ceedings of the 33rd ACM International Conference on Information and Knowledge Management , CIKM ’24, pp. 5420–5424, New York, NY, USA, 2024. Associa-tion for Computing Machinery. ISBN 9798400704369. doi: 10.1145/3627673.3679153. URL https: //doi.org/10.1145/3627673.3679153 .Zhu, G., Xu, Z., Yuan, C., and Huang, Y. Difer: differen-tiable automated feature engineering. In International Conference on Automated Machine Learning , pp. 17–1. PMLR, 2022. 10 FAMOSE: A ReAct Approach to Automated Feature Discovery 

# A. Appendix 

A.1. Full LLM Prompt 

The full prompt is below 

Prompt: 

You are a data analyst expert with full knowledge of data analysis methods. You have been given the following file: - <file name> , this is a CSV file useful to answer the question: <Ex: Determine if an individual has diabetes. No diabetes, prediabetes or diabetes?> 

In addition, we have the following descriptions of each column: <dict of each feature and description> 

Your tasks are the following. Only perform one task within your code. Do not write the code for multiple tasks at once: - Task 1. Use all of these insights to create a large set of new features. You can use use any mathematical opera-tions or transforms. Do not use any black box models, such as Random Forest, XGBoost, LightGBM, etc., and do not use the ”Diabetes” feature in your newly generated features because that trivially improves model performance. - Task 2. Explain why this feature should help answer the question: <Ex: Determine if an individual has diabetes. No diabetes, prediabetes or diabetes?> 

- Task 3. Check the performance of each feature by creating a dict of the form {”code”:str(code of function ”new feature(df)” that when run on a dataframe returns the feature as a Pandas series) }, saving this dict object with Pandas to pickle() as a file named ”new feature texttt¡dataset¿ 0 {np.random.randint(100000) }.pkl” and running the feature performance evaluator tool. - Task 4. Higher performance values are better. If the feature performance evaluator() score is over 0.01, move to the next task. Otherwise create more features and try again. - Task 5. Save the best performing feature use Pandas to pickle() to save a dict object of the form: ”code”:str(code of function ”new feature(df)” that when run on a dataframe returns the feature as a Pandas series); save as a file named ”new feature <dataset> 0 <random number> .pkl”, where you create a random integer with numpy. DO NOT USE pickle.dump() to save this dict object because you will get an error. 

A.2. Example implementation 

We show an example of how this algorithm works on one exemplar task: balance-scale . This task aims to predict whether a scale slides to the left, right, or is balanced, making it a 3-class classifier problem. We are given 4 features: left arm length, left arm weight, right arm length, and right arm weight. In theory, we only need one feature: the difference in torque (left arm length times left arm weight minus right arm length times right arm weight). The four features, when fed into SOTA methods gives us ROC-AUC of 0.8 for Random Forest (Table S7), or 0.91 for XGBoost (Table 2). Interestingly, the performance is 0.995 for Autogluon, demonstrating it is a very good, although not perfect, tabular model. FAMOSE’s output is as follows, given a prompt like what is seen above: 11 FAMOSE: A ReAct Approach to Automated Feature Discovery 

Output: 

I’ll help solve these tasks step by step. Thought: For Task 1, I need to create meaningful features for predicting the balance scale direction. Since this is a physics problem, I can use the principles of moments and leverage. The key insight is that the product of weight and distance gives the moment/torque on each side, and comparing these will determine which way the scale tips. 

<code >

import pandas as pd import numpy as np # Read the data df = pd.read csv(”../data/balance-scale train 3.csv”) # Create features based on physics principles def create features(df): # Calculate moments (torque) for each side left moment = df[’Left-Weight’] * df[’Left-Distance’] right moment = df[’Right-Weight’] * df[’Right-Distance’] # Calculate moment differences and ratios moment difference = left moment - right moment moment ratio = left moment / right moment # Calculate total weights and distances total weight = df[’Left-Weight’] + df[’Right-Weight’] total distance = df[’Left-Distance’] + df[’Right-Distance’] # Calculate squared terms left moment squared = left moment ** 2 right moment squared = right moment ** 2 # Calculate log transformations (adding 1 to avoid log(0)) log moment ratio = np.log1p(moment ratio) # Create feature dictionary features = ’moment difference’: moment difference, ’moment ratio’: moment ratio, ’total weight’: total weight, ’total distance’: total distance, ’left moment squared’: left moment squared, ’right moment squared’: right moment squared, ’log moment ratio’: log moment ratio return features 12 FAMOSE: A ReAct Approach to Automated Feature Discovery 

Output (cont.): 

# Create the features new features = create features(df) print(”Created features:”, list(new features.keys())) 

</code >

Observation: Created features: [’moment difference’, ’moment ratio’, ’total weight’, ’total distance’, ’left moment squared’, ’right moment squared’, ’log moment ratio’] Thought: For Task 2, I’ll explain why these features should help predict the balance scale direction. 

<code >

explanation = ””” The created features should help predict the balance scale direction for the following reasons: 1. moment difference (left moment - right moment): - If positive, the scale tips left - If negative, the scale tips right - If zero, the scale is balanced This is directly based on the physics principle of torque. 2. moment ratio (left moment / right moment): - If >1, scale tips left - If <1, scale tips right - If = 1, scale is balanced This gives a relative measure of the imbalance. 3. total weight and total distance: These provide context about the overall scale of the problem. 4. squared moments: Captures non-linear relationships and emphasizes larger differences. 5. log moment ratio: Transforms the ratio to handle skewed distributions better. ””” print(explanation) 

</code >

13 FAMOSE: A ReAct Approach to Automated Feature Discovery 

Output (cont.): 

Observation: Printed explanation of feature importance and physics principles. Thought: For Task 3, I’ll test the performance of one of our most promising features - the moment difference, as it directly captures the physical principle determining the scale’s direction. 

<code >

import pickle # Create the feature function for moment difference feature code = ””” def new feature(df): return df[’Left-Weight’] * df[’Left-Distance’] - df[’Right-Weight’] * df[’Right-Distance’] ””” # Create dictionary with the feature code feature dict = ”code”: feature code # Save the feature dictionary using pandas random number = np.random.randint(100000) filename = f”new feature balance-scale 0 random number.pkl” pd.to pickle(feature dict, filename) # Evaluate the feature performance performance = feature performance evaluator(filename) print(f”Feature performance score: performance”) 

</code >

Observation: Feature performance score: 0.815 Thought: For Task 4, since our feature performance evaluator score (0.815) is well above 0.01, we can proceed to Task 5. 

<code >

# Create the final feature dictionary with our best performing feature final feature code = ””” def new feature(df): return df[’Left-Weight’] * df[’Left-Distance’] - df[’Right-Weight’] * df[’Right-Distance’] ””” final feature dict = ”code”: final feature code # Save with pandas to pickle using a random number random number = np.random.randint(100000) final filename = f”new feature balance-scale 0 random number.pkl” pd.to pickle(final feature dict, final filename) final answer(f”Successfully completed all tasks. Final feature saved as final filename with performance score of 0.815”) 

</code >

After this output, it prints the following observation: 14 FAMOSE: A ReAct Approach to Automated Feature Discovery 

Observations: Created features: [’moment difference’, ’moment ratio’, ’total weight’, ’total distance’, ’left moment squared’, ’right moment squared’, ’log moment ratio’] The created features should help predict the balance scale direction for the following reasons: 1. moment difference (left moment - right moment): - If positive, the scale tips left - If negative, the scale tips right - If zero, the scale is balanced This is directly based on the physics principle of torque. 2. moment ratio (left moment / right moment): - If 

>

1, scale tips left - If 

<

1, scale tips right - If = 1, scale is balanced This gives a relative measure of the imbalance. 3. total weight and total distance: These provide context about the overall scale of the problem. 4. squared moments: Captures non-linear relationships and emphasizes larger differences. 5. log moment ratio: Transforms the ratio to handle skewed distributions better. Feature performance score: 0.09026 Last output from code snippet: Successfully completed all tasks. Final feature saved as new feature balance-scale 0 33885.pkl with performance score of 0.815 We notice that the LLM hallucinates with a statement “performance score of 0.815”, hence why all observations are checked post-facto (where we confirm the true ROC-AUC performance improvement of 0.09026 within a validation dataset, separated from the test dataset. This represents a change in ROC-AUC from 0.91 to 1.0 within that dataset. We then use mRMR to optimize the number of features needed, which turns out to exactly the above feature. The final model is simplified to become a single feature model with perfect prediction. For context, other methods, even with comparable accuracy, will often have more features. 15 FAMOSE: A ReAct Approach to Automated Feature Discovery 

Task # Datapoints # Features Imbalance Target Variance adult 48842 15 0.24 –arrhythmia 452 280 0.0 –balance-scale 625 5 0.08 –bank fraud base 1000000 32 0.01 –bank marketing 45211 17 0.12 –blood 748 5 0.24 –breast-w 699 10 0.34 –communities 1994 103 0.33 –credit-g 1000 21 0.3 –diabetes 253680 22 0.02 –car 1728 7 0.04 –covtype 581012 55 0.0 –cmc 1473 10 0.23 –eucalyptus 736 20 0.14 –heart 918 12 0.45 –junglechess 44819 7 0.1 –myocardial 686 92 0.22 –pc1 1109 22 0.07 –tic-tac-toe 958 10 0.35 –vehicle 846 19 0.24 –crab 3893 9 – 3.2 bike 17379 13 – 179.7 forest-fires 517 13 – 80.2 housing 20640 10 – 1024.5 insurance 1338 7 – 386.2 wine 6497 13 – 0.9 cybersecurity attacks 40000 25 – 0.8 

Table S1. Data statistics: number of datapoints, number of features, data imbalance (if a classification task) or target variance (if a regression task). 

16 FAMOSE: A ReAct Approach to Automated Feature Discovery 

Task Type Task Prediction Task 

Classification 

adult (Becker & Kohavi, 1996) Earn more than $50,000 per year (yes/no) 

arrhythmia (Guvenir et al., 1997) Presence of cardiac arrhythmia (16 classes) 

balance-scale (Siegler, 1976) Balance direction: left, right, or balanced 

breast-w (Wolberg, 1990) Breast mass malignant (M) or benign (B) 

blood (Yeh, 2008) Blood donation occurred (yes/no) 

car (Bohanec, 1988) Car acceptability rating (4 classes) 

diabetes (CDC, 2015) Diabetes status: none, prediabetes, diabetes 

cmc (Lim, 1999) Contraceptive method choice (3 classes) 

communities (Redmond, 2002) Violent crime rate: low, medium, or high 

covtype (Blackard, 1998) Forest cover type (7 classes) 

credit-g (Hofmann, 1994) Credit approved (yes/no) 

eucalyptus (Bulloch, 1996) Seedlot utility classification 

heart (fedesoriano, 2021) Heart disease present (yes/no) 

junglechess (van Rijn & Vis, 2014) Winner of Jungle Chess endgame 

myocardial (Golovenkin et al., 2020) Chronic heart failure present (yes/no) 

pc1 (Sayyad Shirabad & Menzies, 2005) Software defect present (1/0) 

tic-tac-toe (Aha, 1991) First player wins (positive/negative) 

vehicle (Siebert, 1987) Car acceptability rating (4 classes) 

bank fraud base (Jesus et al., 2022) Fraudulent applicant (yes/no) 

travel (Ferretti, 2022) Hotel booking vs. link click 

bank marketing (Moro et al., 2011) Subscribed to term deposit (yes/no) 

Regression 

bike (Fanaee-T, 2023) Total rental bike count 

crab (Sidhu, 2021) Crab age estimation 

housing (Pace & Barry, 1997) House price estimation 

insurance (Arunjangir245, 2023) Individual medical cost 

forest-fires (Cortez & Morais, 2007) Burned forest area (Portugal) 

wine-quality (Cortez et al., 2009) Wine quality score (0–10) 

cybersecurity attacks (Agur & Venu-gopal, 2024) Alert severity level 

Table S2. Tasks used to evaluate FAMOSE. age “the age of an individual” workclass “a general term to represent the employment status of an individual” fnlwgt “the number of units in the target population that the responding unit represents” education “the highest level of education achieved by an individual” educational-num “the highest level of education achieved in numerical form” marital-status “marital status of an individual” occupation “the general type of occupation of an individual” relationship “what this individual is relative to others” race “race” gender “gender” capital-gain “capital gain last year” capital-loss “capital loss last year” hours-per-week “the hours an individual has reported to work per week” native-country “country of origin for an individual” 

Table S3. Example feature descriptions for the adult task. 

17 FAMOSE: A ReAct Approach to Automated Feature Discovery 

age “age in years” job “type of job (categorical: ’admin.’, ’unknown’, ’unem-ployed’, ’management’, ’housemaid’, ’entrepreneur’, ’student’, ’blue-collar’, ’self-employed,retired’, ’techni-cian’, ’services’)” marital “marital status (categorical: ’married’,’divorced’,’single’; note: ’divorced’ means divorced or widowed)” education “(categorical: ’unknown’,’secondary’,’primary’,’tertiary’)” default “has credit in default? (binary: ’yes’,’no’)” balance “average yearly balance, in euros (numeric)” housing “has housing loan? (binary: ’yes’,’no’)” loan “has personal loan? (binary: ’yes’,’no’)” contact “contact communication type (categorical: ’un-known’,’telephone’,’cellular’)” day “last contact day of the month (numeric)” month “last contact month of year (categorical: ’jan’, ’feb’, ’mar’, ..., ’nov’, ’dec’)” duration “last contact duration, in seconds (numeric)” campaign “number of contacts performed during this campaign and for this client (numeric, includes last contact)” pdays “number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)” previous “number of contacts performed before this campaign and for this client (numeric)” poutcome “outcome of the previous marketing campaign (categori-cal: ’unknown’,’other’,’failure’,’success’)” 

Table S4. Example feature descriptions for the bank-marketing task. Baseline FAMOSE (Claude) FAMOSE (Deepseek) 

adult 0.929 ±0.002 0.929 ±0.002 0.929 ±0.002 

balance-scale 0.914 ±0.01 1.0 ±0.0 1.0 ±0.0 

bank fraud base 0.886 ±0.004 0.886 ±0.003 0.885 ±0.004 

bank marketing 0.934 ±0.005 0.928 ±0.013 0.933 ±0.004 

blood 0.698 ±0.03 0.704 ±0.027 0.687 ±0.009 

breast-w 0.989 ±0.007 0.989 ±0.008 0.986 ±0.008 

car 0.999 ±0.0 0.997 ±0.003 0.999 ±0.002 

communities 0.866 ±0.008 0.861 ±0.007 0.864 ±0.005 

covtype 0.990 ±0.0 0.989 ±0.001 0.992 ±0.001 

cmc 0.704 ±0.019 0.696 ±0.023 0.698 ±0.026 

credit-g 0.771 ±0.036 0.757 ±0.04 0.748 ±0.022 

diabetes 0.705 ±0.004 0.705 ±0.003 0.705 ±0.003 

eucalyptus 0.835 ±0.01 0.836 ±0.021 0.835 ±0.03 

heart 0.919 ±0.023 0.903 ±0.027 0.916 ±0.021 

junglechess 0.974 ±0.001 0.994 ±0.006 0.997 ±0.001 

pc1 0.822 ±0.05 0.832 ±0.064 0.832 ±0.068 

myocardial 0.688 ±0.082 0.679 ±0.05 0.676 ±0.09 

tic-tac-toe 0.999 ±0.002 1.0 ±0.001 0.997 ±0.004 

vehicle 0.929 ±0.007 0.918 ±0.021 0.930 ±0.009 

% Improvement ↑ 0.0% 0.32% 0.29% 

Table S5. FAMOSE classifier performance (ROC-AUC) for Claude and Deepseek. We notice that the algorithm works well for both closed-source (Claude 3.5 Sonnet V2) and open-source (Deepseek-R1) LLMs. 

18 FAMOSE: A ReAct Approach to Automated Feature Discovery 5 0 5 10 15 20 25 

# Number of Functions Within Code 

# 0.00 

# 0.02 

# 0.04 

# 0.06 

# 0.08 

# 0.10 

# 0.12 

> Density

# CAAFE 

# FAMOSE 

Figure S1. Number of functions used in CAAFE and FAMOSE code across all task folds (5 × number of tasks). Baseline FAMOSE (Sonnet 3.5 V2) FAMOSE (Deepseek) 

bike 40.3 ±1.03 40.05 ±0.99 39.79 ±0.73 

crab 2.32 ±0.13 2.34 ±0.08 2.31 ±0.16 

housing 409.58 ±10.54 408.56 ±26.34 396.44 ±19.22 

insurance 192.89 ±13.91 191.64 ±11.16 190.16 ±11.96 

forest-fires 92.7 ±5.34 79.49 ±5.87 79.47 ±4.6 

wine-quality 0.64 ±0.02 0.64 ±0.01 0.64 ±0.02 

cybersecurity attacks 0.82 ±0.0 0.82 ±0.0 0.82 ±0.0 

% Reduction ↑ 0.0% 2.0% 2.8% 

Table S6. FAMOSE regression performance (RMSE) for Claude and Deepseek. Bold are best-performing models. We notice that the algorithm works well for both closed-source (Claude 3.5 Sonnet V2) and open-source (Deepseek-R1) LLMs, with a slight edge for Deepseek-R1. 

19 FAMOSE: A ReAct Approach to Automated Feature Discovery df.drop 

> Add
> Mult
> Div
> Eq
> BitAnd
> Sub
> Gt
> df['DBH'].astype
> In

# Functions 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

17.5 

> % of Functions
> (Across Datasets)

Figure S2. Frequency each function used in CAAFE code across all task folds (5 × number of tasks). Autogluon Random Forest Baseline FAMOSE Baseline FAMOSE 

adult 0.929 ±0.003 0.929 ±0.002 0.903 ±0.002 0.901 ±0.005 

balance-scale 0.995 ±0.004 1.0 ±0.0 0.806 ±0.009 1.0 ±0.0 

bank marketing 0.938 ±0.004 0.937 ±0.006 0.929 ±0.004 0.928 ±0.003 

bank fraud base 0.877 ±0.01 0.884 ±0.006 0.834 ±0.003 0.835 ±0.004 

blood 0.757 ±0.035 0.747 ±0.029 0.686 ±0.042 0.706 ±0.03 

breast-w 0.993 ±0.005 0.994 ±0.005 0.991 ±0.006 0.992 ±0.006 

car 1.0 ±0.0 0.998 ±0.004 0.994 ±0.002 0.992 ±0.003 

communities 0.866 ±0.008 0.862 ±0.009 0.869 ±0.007 0.863 ±0.008 

cmc 0.726 ±0.014 0.708 ±0.019 0.698 ±0.014 0.688 ±0.024 

covtype 0.998 ±0.0 0.998 ±0.0 0.997 ±0.0 0.997 ±0.001 

credit-g 0.75 ±0.042 0.761 ±0.016 0.791 ±0.022 0.764 ±0.026 

diabetes 0.699 ±0.005 0.7 ±0.004 0.676 ±0.002 0.675 ±0.002 

eucalyptus 0.879 ±0.028 0.889 ±0.021 0.886 ±0.012 0.882 ±0.015 

heart 0.928 ±0.023 0.92 ±0.027 0.927 ±0.028 0.91 ±0.037 

junglechess 0.998 ±0.001 1.0 ±0.0 0.932 ±0.001 0.996 ±0.002 

myocardial 0.69 ±0.075 0.699 ±0.051 0.709 ±0.087 0.676 ±0.041 

pc1 0.824 ±0.067 0.82 ±0.079 0.84 ±0.044 0.855 ±0.053 

tic-tac-toe 0.999 ±0.001 0.999 ±0.002 0.998 ±0.004 0.999 ±0.002 

vehicle 0.938 ±0.014 0.933 ±0.021 0.93 ±0.009 0.923 ±0.019 % Improvement ↑ 0.0% 0.02% 0.0% 1.2% 

Table S7. Robustness of classifier performance (ROC-AUC) for FAMOSE XGBoost features applied to different models (Random forest and Autogluon). Bold indicates the best model. 

20 FAMOSE: A ReAct Approach to Automated Feature Discovery Add 

> Mult
> Sub
> Div
> Eq
> Pow
> abs
> np.log1p
> np.sqrt
> BitAnd

# Functions 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

17.5 

> % of Functions
> (Across Datasets)

Figure S3. Frequency each function used in FAMOSE code across all task folds (5 × number of tasks). Autogluon Random Forest Baseline FAMOSE Baseline FAMOSE 

bike 35.37 ±0.92 35.41 ±0.96 42.02 ±1.53 41.97 ±1.23 

crab 2.13 ±0.11 2.1 ±0.13 2.19 ±0.09 2.18 ±0.09 

housing 384.31 ±15.11 381.08 ±18.47 418.38 ±11.23 416.28 ±14.05 

insurance 166.59 ±13.31 174.56 ±16.7 178.82 ±8.87 183.44 ±11.0 

forest-fires 80.89 ±6.11 79.16 ±5.59 83.13 ±5.04 79.22 ±5.68 

wine-quality 0.61 ±0.01 0.61 ±0.01 0.61 ±0.01 0.61 ±0.02 

cybersecurity attacks 0.82 ±0.0 0.82 ±0.0 0.83 ±0.0 0.82 ±0.0 

% Reduction ↑ 0.0% -0.1% 0.0% 0.5% 

Table S8. Robustness of classifier performance (RMSE) for FAMOSE XGBoost features applied to different models (Random forest and Autogluon). Bold indicates the best model. 

21 FAMOSE: A ReAct Approach to Automated Feature Discovery CAAFE FAMOSE 

# 0.0 

# 0.2 

# 0.4 

# 0.6 

# 0.8 

# 1.0 

> % Folds with Feature Engineering

Figure S4. Percentage of task folds (5 × number of tasks) where FAMOSE or CAAFE features were added. No goal in prompt Only feature selection No feature selection 

bike 40.19 ±0.89 40.3 ±1.03 40.43 ±1.54 

crab 2.34 ±0.14 2.34 ±0.12 2.34 ±0.09 

cybersecurity attacks 0.82 ±0.0 0.82 ±0.0 0.83 ±0.0 

forest-fires 79.87 ±4.98 79.54 ±5.89 92.93 ±5.24 

housing 409.03 ±26.18 409.58 ±10.54 418.51 ±38.84 

insurance 186.53 ±12.52 184.43 ±16.86 193.95 ±13.82 

wine-quality 0.64 ±0.01 0.64 ±0.01 0.64 ±0.01 % Reduction ↑ 2.2% 2.4% -0.8% 

Table S9. Ablation for regression tasks. We see a slight improvement in the RMSE reduction when we only select features, although the difference is not significant (Wilcoxon signed rank test p-value = 0 .3), but the difference between FAMOSE and no feature selection is significant (Wilcoxon signed rank test p-value = 0 .007 ). Overall, our results show FAMOSE performs best when we have a goal and feature selection. 

22 FAMOSE: A ReAct Approach to Automated Feature Discovery 

No goal in prompt Only feature selection No feature selection 

adult 0.928 ±0.002 0.929 ±0.002 0.929 ±0.002 

balance-scale 1.0 ±0.0 0.914 ±0.01 1.0 ±0.0 

bank fraud base 0.885 ±0.004 0.885 ±0.005 0.881 ±0.01 

bank marketing 0.928 ±0.005 0.933 ±0.004 0.929 ±0.011 

blood 0.678 ±0.038 0.688 ±0.024 0.679 ±0.024 

breast-w 0.99 ±0.008 0.989 ±0.006 0.985 ±0.011 

car 0.999 ±0.001 0.997 ±0.002 0.998 ±0.002 

cmc 0.691 ±0.01 0.704 ±0.023 0.7 ±0.015 

communities 0.859 ±0.005 0.861 ±0.007 0.866 ±0.007 

covtype 0.992 ±0.0 0.989 ±0.0 0.989 ±0.002 

credit-g 0.755 ±0.021 0.756 ±0.036 0.758 ±0.04 

diabetes 0.705 ±0.004 0.705 ±0.003 0.706 ±0.004 

eucalyptus 0.831 ±0.024 0.836 ±0.027 0.825 ±0.023 

heart 0.913 ±0.02 0.916 ±0.019 0.914 ±0.023 

junglechess 0.999 ±0.001 0.974 ±0.001 0.994 ±0.006 

myocardial 0.664 ±0.077 0.663 ±0.067 0.677 ±0.076 

pc1 0.826 ±0.067 0.825 ±0.056 0.82 ±0.054 

tic-tac-toe 0.996 ±0.006 1.0 ±0.001 0.998 ±0.003 

vehicle 0.929 ±0.004 0.931 ±0.01 0.924 ±0.007 % Improvement ↑ 0.0% -0.38% 0.04% 

Table S10. Ablation for classifier tasks. Due to the small changes in ROC-AUC, the results are not statistically significant, except for only feature selection, which is significantly worse than the baseline and FAMOSE (Wilcoxon signed rank test p-value < 0.04 ), which suggests that we need to go beyond feature selection along to achieve high performance. Overall, our results show FAMOSE performs best when we have a goal and feature selection. 

23