Title: Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search

URL Source: https://arxiv.org/pdf/2602.04248v1

Published Time: Thu, 05 Feb 2026 01:32:30 GMT

Number of Pages: 9

Markdown Content:
# Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search 

Hao Lu 1 Haoyuan Huang 1 Yulin Zhou 1 Chen Li 1 Ningxin Zhu 1

## Abstract 

Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have signif-icantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, cur-rent approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteris-tic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Op-timization Agent. PE-EMP functions as a re-flexive optimizer within the local search, utiliz-ing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (sys-tem prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on com-plex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results under-score the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks. 

> 1

JianChengXingYun Technology Co., Ltd., Shenzhen, China. Correspondence to: Hao Lu <luhao@jianchengxingyun.com >,Chen Li <lichen@jianchengxingyun.com >.

Preprint. February 5, 2026. 

## 1. Introduction 

Large Language Models (LLMs) have demonstrated that increasing computational capability at inference time can significantly improve reasoning performance (OpenAI et al., 2024; 2025b). Techniques such as ”Best-of-N” sampling (Brown et al., 2024; Li et al., 2022; Lightman et al., 2023) and Monte Carlo Tree Search (MCTS) (Zhou et al., 2024; Feng et al., 2024; Xu, 2023; Zhang et al., 2024a;b; Inoue et al., 2025; Lu et al., 2025; Xie et al., 2024) allow models to explore multiple reasoning paths and self-correct, effec-tively scaling performance without the need for expensive parameter updates. However, a critical limitation of current inference-time scaling methods is that they are stateless. Whether using standard MCTS or adaptive branching strate-gies (Inoue et al., 2025), the agent treats each new problem as an isolated event. Once a search process concludes, the successful strategies or valid reasoning patterns discovered are discarded. This contrasts with human problem-solving, which is empirical: experts solve problems by combin-ing long-term experience (accumulated domain knowledge) with short-term experience (immediate feedback and context from the current problem). Existing methods attempting to address this lack of memory often fall short in integration. Systems like FLEX (Cai et al., 2025b) maintain an experience library but treat retrieval and reasoning as separate steps, preventing the model from evolving its search strategy dynamically. Other approaches, such as Training-Free GRPO (Cai et al., 2025a), use histor-ical data to adjust the generation probability but lack the structured exploration provided by tree search, limiting their effectiveness on complex, multi-step logic tasks. To address these limitations, we propose Empirical-MCTS, a framework that integrates continuous experience accumu-lation with structured tree search. Our approach unifies two distinct types of experience via a dual-loop evolutionary mechanism: Short-term Experience (PE-EMP): Within the local search process, we introduce Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP). Instead of using static prompts for node expansion, PE-EMP functions as a reflexive op-timizer. It analyzes pairwise response differences to syn-1

> arXiv:2602.04248v1 [cs.AI] 4 Feb 2026 Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search

thesize adaptive criteria and dynamically evolves the meta-prompt (system prompt). This ensures that immediate feed-back is not merely used for selection, but actively refines the generation policy for subsequent steps (Liu et al., 2025). Long-term Experience (Memory Optimization): To sustain learning across problems, we implement a Memory Opti-mization Agent. Rather than a static retrieval database, our repository is treated as a dynamic policy prior. We employ an optimizer-based update mechanism—utilizing atomic op-erations such as add, modify, and merge—to continuously distill high-quality insights from the search process into a global knowledge base (Cai et al., 2025a). Furthermore, to ensure rigorous evaluation of the evolved prompts, we integrate a hybrid preference model adapted from recent pairwise optimization strategies (Zhang et al., 2024b). By mapping the explicit ”self-principled” scores generated by PE-EMP into a globally consistent ranking via Enhanced Borda Count, we address the non-transitivity inherent in LLM preferences. This integration allows Empirical-MCTS to accurately distinguish and prioritize peak performance trajectories, ensuring that the qualitative insights gained from memory optimization are effectively translated into quantitative search guidance. We evaluate Empirical-MCTS on diverse benchmarks, in-cluding AIME25 (Zhang & Math-AI, 2025), ARC-AGI-2 (Chollet et al., 2026), and MathArena Apex (Balunovi ´cet al., 2025), using DeepSeek-V3.1-Terminus (DeepSeek-AI, 2024), gpt-oss-120b (OpenAI et al., 2025a) and Gemini 3 Pro (DeepMind, 2025b). Empirical-MCTS achieved better results than previous approaches, such as repeated sampling, LLaMA-Berry, FLEX and Training-free GRPO. Our key contributions are: • We introduce Empirical-MCTS, a novel paradigm that bridges the gap between structured search and continu-ous learning. By unifying local exploration with global memory optimization, we transform MCTS from a stateless inference technique into a non-parametric on-line learning agent capable of accumulating ”wisdom” across disparate problem instances without weight up-dates. • We propose Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP). Moving beyond static expan-sion, PE-EMP acts as a reflexive optimizer that synthe-sizes ”self-principles” from pairwise feedback. This mechanism dynamically evolves the meta-prompt (sys-tem prompt) in real-time, allowing the agent to actively refine its reasoning policy and navigate complex logic spaces with increasing precision during the search. • We show that Empirical-MCTS outperforms stan-dard MCTS and previous experience-based models. It achieves state-of-the-art results on high-difficulty benchmarks like AIME25, ARC-AGI-2, and Math-Arena Apex. Our results prove that ”remembering” past reasoning patterns is the key to solving complex, multi-step logic problems more efficiently than tradi-tional search methods. 

## 2. Related Work 

2.1. Inference-Time Scaling and Tree Search 

Inference-time scaling focuses on improving model outputs by allocating more compute during generation. Early meth-ods, such as Self-Consistency (Wang et al., 2023), generate multiple independent answers and select the most frequent one. To handle more complex tasks, tree-search algorithms like Tree of Thoughts (ToT) (Yao et al., 2023a) and MCTS applied to LLMs (Zhou et al., 2024; Feng et al., 2024; Xu, 2023; Zhang et al., 2024a;b; Inoue et al., 2025; Lu et al., 2025; Xie et al., 2024) were developed to structure the rea-soning process into steps. Recent work has focused on optimizing the search structure. AB-MCTS (Inoue et al., 2025) introduces adaptive branching and LLaMA-Berry (Zhang et al., 2024b) integrates MCTS with a pairwise reward model to improve selection accuracy. However, despite these structural improvements, these methods re-main stateless across problems. They do not retain the successful reasoning patterns discovered during the search. Empirical-MCTS builds upon the robust search backbone of LLaMA-Berry but introduces a stateful paradigm, per-sisting empirical experience to evolve the agent’s policy continuously. 

2.2. Experience-Driven Agents 

Several studies aim to enable agents to learn from interac-tion without parameter updates. FLEX (Cai et al., 2025b) proposes a ”Forward Learning” paradigm that stores suc-cessful trajectories in a library for future retrieval. However, FLEX largely relies on static retrieval mechanisms and does not deeply integrate this experience into the step-by-step decision-making of a search tree. Training-Free GRPO (Cai et al., 2025a) takes a different approach by acting as an ”inference-time optimizer,” using the LLM to analyze group results and update the context effectively. While this avoids gradient updates, it operates primarily as a policy adjustment tool rather than a reasoning search engine. Empirical-MCTS bridges these approaches by using MCTS to generate high-quality reasoning data and a global repository to store and refine this data, ensuring the agent becomes more efficient over time. 2Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search 

2.3. Meta-Reasoning and Refinement 

Iterative refinement techniques, such as Self-Refine (Madaan et al., 2023) and Reflexion (Shinn et al., 2023), al-low models to critique their own outputs. However, standard self-refinement can sometimes lead to the model discarding correct intermediate steps or drifting away from the original problem constraints. MCTSR-Zero (Lu et al., 2025) sug-gests using ”Meta-Prompts”—high-level instructions gen-erated by the model—to guide subsequent generation. We formalize this as our ”Short-term Experience” module. In our MCTS expansion step, we use explicit meta-prompts containing critiques and suggestions to generate child nodes. This ensures that the reasoning process remains focused and builds constructively on previous steps. 

## 3. Methodology: The Empirical-MCTS Framework 

3.1. Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) 

In Empirical-MCTS, the evolution of the meta-prompt (sys-tem prompt) is not an isolated heuristic but a derivative of the reward modeling process. Drawing inspiration from the Self-Principled Critique Tuning (SPCT) framework pro-posed for inference-time scaling of generalist reward mod-els (Liu et al., 2025), we introduce a unified mechanism termed Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP). SPCT demonstrates that generating specific ”self-principles” prior to critiquing significantly enhances the scalability and accuracy of reward models. We extend this paradigm by transforming the Judge model from a mere discriminator into a reflexive optimizer. While SPCT utilizes principles to scale inference accuracy via sampling, PE-EMP leverages these generated principles (Adaptive Criteria) to drive the evolution of the generation policy itself. Formally, let a node expansion result in a pairwise compari-son between a candidate state Sc = ( Pc, r c) and a baseline (parent) state Sp = ( Pp, r p), where P denotes the meta-prompt and r the model response. The PE-EMP module J

processes the query q, prior accumulated experiences Eprior ,and the two states to yield a triadic output: 

J (q, Eprior , S c, S p) → (s, Enew , Pevolved ) (1) where s is the raw score vector, Enew denotes the new em-pirical insights derived from practice, and Pevolved is the evolved meta-prompt. The internal logic of the PE-EMP executes a strict seven-stage cognitive process that mirrors and extends the SPCT pipeline: 

Algorithm 1 Empirical-MCTS: Continuous Agent Evolu-tion  

> 1:

Input: Query q, Initial Prompt P0, Global Experience Library D, Iterations T 

> 2:

Initialize: Search tree T with root v0(q, P0) 

> 3:

Pevolved ← P 0 {Initialize evolved prompt with base prompt } 

> 4:

Eprior ← Retrieve (D, q ) {Retrieve long-term experi-ence } 

> 5:

for t = 1 to T do  

> 6:

Sp ← Selection (T , UCB ) {Navigate tree nodes } 

> 7:

{— Local Loop: PE-EMP Expansion — } 

> 8:

Sc ← Expand (Sp, Pevolved , Eprior ) {Generate child state } 

> 9:

(s, Enew , Pevolved ) ← J (q, Eprior , S c, S p) {PE-EMP Reflexive Optimizer } 

> 10:

// PE-EMP internally: (1) Generate Criteria → (2) Critique → (3) Scoring → (4) Prompt Evolution  

> 11:

{— Value Backpropagation — } 

> 12:

R ← Hybrid-Reward (s, Borda-Count (T )) {Section 3.3 } 

> 13:

Update-Q-Values (T , S p, R ) {Decay-based back-prop } 

> 14:

{— Global Loop: Memory Update — } 

> 15:

if Enew contains high-value insights then  

> 16:

πmem ← Optimizer (D, Enew ) {Determine Add/Modify/Merge/Delete } 

> 17:

D ← Update-Library (D, π mem ) {Non-parametric policy update } 

> 18:

end if  

> 19:

end for  

> 20:

Output: Best response r∗ from T

1. Adaptive Criteria Generation (Self-Principled Phase) : Aligning with SPCT’s methodology, the model first acts as a principle generator. It dynam-ically formulates specific evaluation dimensions tai-lored to the user’s query, ensuring that the subsequent critique is grounded in context-aware standards rather than generic rules. 2. Comparative Chain-of-Thought (Critique Phase) :A step-by-step critical analysis is performed, contrast-ing the candidate and baseline responses against the generated criteria. 3. Dynamic Weight Allocation : Weights are assigned to each criterion based on their relevance to the specific problem context, ensuring P wi = 100% .4. Weighted Scoring : A quantitative calculation deter-mines the performance scores for both responses based on the weighted criteria. 3Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search  

> Figure 1. Concrete Instantiation of Empirical-MCTS Framework.

5. Raw Score Generation : The model performs a step-by-step arithmetic aggregation, calculating the weighted sum of dimension-specific scores derived in the previous step. This yields a precise scalar value within the range [0 , 10] for each response, serving as the high-fidelity signal for probabilistic preference modeling. 6. Context-Aware Insight Synthesis : Building upon the 

prior experiences retrieved from the RAG repository, the model synthesizes the immediate feedback with historical wisdom. It distills the current interaction into new, high-order empirical insights ( Enew ), explicitly identifying how the current case refines, expands, or corrects the existing knowledge boundary provided by the retrieval system. 7. Strategic Prompt Evolution : Finally, the model ana-lyzes the fundamental execution differences between 

Pc and Pp. By internalizing the Critical Success Fac-tors of the winner and the Failure Lessons of the loser, it synthesizes a more advanced meta-prompt Pevolved .This architecture ensures that Pevolved is not a random mu-tation but a conscious adaptation. By incorporating the 

self-principled logic from SPCT, the meta-prompt explic-itly encodes the high-quality evaluation criteria and lessons learned from the immediate pairwise battle, creating a di-rected evolutionary pressure toward prompts that produce higher-reward responses. To prevent semantic drift in abstract reasoning tasks, we further filter retrieved experiences by task taxon-omy alignment. Each experience in D is tagged with a coarse-grained task type (e.g., geometry-proof ,

combinatorial-optimization ) derived from the original problem source. Only experiences sharing the same task type as q (determined via zero-shot classification with the base LLM) are included in Eprior . We set k = 5 as the default retrieval size, balancing context richness against prompt length constraints. 

3.2. Experience Memory Optimization 

A critical contribution of Empirical-MCTS is its active man-agement of the experience library, a methodology directly inspired by the Training-Free GRPO (Cai et al., 2025a). Training-Free GRPO posits that LLM agents can achieve alignment effects analogous to parametric reinforcement learning (e.g., PPO (Schulman et al., 2017)) by iteratively refining experiential knowledge as a ”token prior” rather than updating model weights. Adopting this paradigm, we implement a Memory Optimiza-tion Agent that functions as the optimizer in the context space. Unlike static RAG systems that simply append in-puts, this agent treats the distilled insights Enew as gradients for updating the global knowledge state. Let Dt be the current experience library. Upon receiving the batch of new insights from the PE-EMP phase, the system invokes the optimization routine. Following the optimiza-tion protocol defined in Training-Free GRPO, the agent analyzes the new insights against the existing library Eexist 

4Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search 

and generates a structured update plan πmem consisting of four atomic operations: • Add : Append distinct, high-quality experiences de-scribed in the new insights to the library Dt, effectively expanding the policy’s action space. • Modify : Refine or correct an existing experience 

ei ∈ D t based on the fresh ”semantic advantage” de-rived from the current iteration, sharpening the policy’s precision. • Merge : Synthesize multiple fragmented experiences 

{ei, e j , ... } into a single, cohesive principle to reduce context redundancy and improve retrieval efficiency. • Delete : Prune obsolete or low-quality experiences that no longer align with high-reward outcomes, function-ing as a ”forgetting” mechanism for bad policies. Formally, this step represents a non-parametric policy up-date: 

Dt+1 ← Optimizer (Dt, π mem (Enew , Eexist )) (2) By continuously optimizing D, Empirical-MCTS shifts the output distribution π(y|q, D) towards higher rewards over time, achieving continuous self-improvement without the computational cost of gradient-based training. 

3.3. Value Estimation via Hybrid Preference Integration 

To transform the discrete insights from PE-EMP into a con-tinuous reward signal, we adapt the robust Pairwise Pref-erence Reward Model (PPRM) architecture from LLaMA-Berry (Zhang et al., 2024b). Unlike previous approaches that rely on opaque logits from a separate reward model, our framework uniquely leverages the explicit ”Self-Principled Scores” generated by PE-EMP (Section 3.1) as the high-fidelity input source. This integrates the fine-grained, instruction-following assessment of our reflexive optimizer with the global consistency guarantees of graph-theoretic ranking. Local and Global Valuation. We map the raw score vector 

s = ( Sc, S p) from PE-EMP to a transition probability via the Bradley-Terry model: Qlocal (sc) = exp( Sc)exp( Sc)+exp( Sp) .To address the non-transitivity inherent in pairwise LLM preferences, we employ the Enhanced Borda Count (EBC) method (Zhang et al., 2024b) to construct a global ranking from the pairwise transition graph. The final reward R(s)

is a weighted fusion of the local probability and the global Borda rank, ensuring that the evolved prompts are evaluated within the context of the entire search history. 

3.4. Search Strategy 

We employ a standard Upper Confidence Bound (UCB) applied to Trees for node selection. For value backpropaga-tion, we adopt the decay-based update rule from Self-Refine MCTS (Zhang et al., 2024b): Q(sp) ← (1 − γ)Q(sp) + 

γQ (sc). This ”soft-max” mechanism is particularly essen-tial for our framework, as it allows the sharp, high-reward insights discovered by the Memory Optimization Agent to propagate efficiently up the tree without being diluted by earlier, less experienced exploration attempts. 

## 4. Experiments 

4.1. Experimental Setup Benchmarks. We evaluated Empirical-MCTS on three high-difficulty benchmarks designed to test frontier reasoning and generalization capabilities: AIME25 (Zhang & Math-AI, 2025), MathArena Apex (Balunovi ´c et al., 2025), and ARC-AGI-2 (Chollet et al., 2026). AIME25 represents a standard high-school competition mathematics benchmark. Math-Arena Apex is a dynamic evaluation framework specifically curated to mitigate data contamination—a pervasive issue in current LLM evaluation—by utilizing problems from recent competitions (e.g., CMIMC 2025, IMO 2025). It is currently the only benchmark evaluating proof-writing capabilities. ARC-AGI (Abstraction and Reasoning Corpus) measures general fluid intelligence, requiring agents to syn-thesize transformation programs from minimal examples, a task where traditional LLMs historically struggle. 

Models. Experiments were conducted using a diverse set of frontier models to demonstrate model-agnostic appli-cability: DeepSeek-V3.1-Terminus (DeepSeek-AI, 2024), gpt-oss-120b (OpenAI et al., 2025a), and the Gemini 3 Flash (DeepMind, 2025a). 

Baselines. We benchmark Empirical-MCTS against a spec-trum of inference-time strategies: (1) LLM with In-Context Learning (ICL) (Dong et al., 2024), (2) LLM agent with reasoning-and-acting workflow (ReAct) (Yao et al., 2023b), (3) Repeated Sampling (Best-of-N ) (Brown et al., 2024; Li et al., 2022; Lightman et al., 2023), (4) FLEX (Cai et al., 2025b), (5) LLaMA-Berry (Zhang et al., 2024b), and (6) Training-Free GRPO (Cai et al., 2025a). 

4.2. Results on Mathematical Reasoning 

We first assess the impact of adding empirical memory to structured search on mathematical reasoning tasks. Table 1 summarizes the results on AIME25 and MathArena Apex. 

Surpassing Stateless Search. On AIME25, Empirical-MCTS achieves 73.3%, outperforming both the state-less LLaMA-Berry framework (63.3%) and the memory-augmented FLEX agent (66.6%). This result validates 5Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search 

Table 1. Performance comparison on AIME25 and MathArena-Apex. Baseline denotes DeepSeek-V3.1-Terminus. All evaluated frameworks utilize this model as the underlying backbone. METHOD / F RAMEWORK AIME25 MATH ARENA -A PEX 

BASELINE 56.7 0.00 (16 RUNS )ICL 53.3 -REACT 60.0 -FLEX 66.6 -REPEATED SAMPLING 70.0 0.00 (4 RUNS )LL AMA-B ERRY 63.3 2.08 (4 RUNS )

OURS 73.3 4.17 (4 RUNS )

our core hypothesis: By integrating memory optimization directly into the node expansion via PE-EMP, Empirical-MCTS allows the agent to navigate the solution space more efficiently. 

Solving the Unsolvable. The results on MathArena Apex are noteworthy. The base model, DeepSeek-V3.1-Terminus, scored 0.00% across 16 runs, indicating that the model lacks the requisite reasoning priors for these problems. While Repeated Sampling also failed (0.00%) and LLaMA-Berry achieved only marginal success (2.08%), Empirical-MCTS reached 4.17%. While the absolute score is low due to the extreme difficulty of the benchmark, the relative improve-ment is substantial. It demonstrates that our framework does not merely extract existing knowledge but synthesizes new solution paths through the accumulation of empirical wisdom during the search. 

4.3. Efficiency and Cost Analysis 

A critical barrier to inference-time scaling is cost. In Ta-ble 2, we compare Empirical-MCTS using the cost-efficient Gemini 3 Flash against state-of-the-art frontier models on MathArena Apex and ARC-AGI-2. 

High Performance at Low Cost. Empirical-MCTS enables smaller, faster models to punch above their weight class. On MathArena Apex, Ours (Gemini 3 Flash) achieves an accuracy of 35.42% with a total cost of only $5.24. This outperforms Gemini 3 Pro, which scores 23.44% at a cost of $3.40, and drastically outperforms GPT-5.2 (High), which scores 13.54% at a cost of $12.00. Similarly, on ARC-AGI-2, our method achieves 38.33%, surpassing the much more expensive Gemini 3 Pro (31.1%) and Grok 4 (16.0%). Figure 2 visualizes the Pareto frontier of reasoning effi-ciency. As shown in the upper panel (MathArena Apex), our approach strictly dominates all baselines, achieving the high-est accuracy while maintaining a cost significantly lower than the heavy-compute models (e.g., GPT-5.2 at $12.00). In the ARC-AGI-2 landscape, although GPT-5.2 holds a marginal accuracy lead, Empirical-MCTS resides on the ”knee” of the curve—delivering comparable frontier-level reasoning at a reduced price point. 0 5 10      

> Cost ($)
> 0510 15 20 25 30 35 40
> Accuracy (%) MathArena Apex Performance vs Cost
> Ours (Gemini 3 Flash) Pareto Frontier Other Models GPT-5.2 Gemini 3 Pro Gemini 3 Flash Grok 4 GPT-5.1 Claude Sonnet 4.5 Grok 4 Fast Gemini 2.5 Pro DeepSeek-R1-0528
> Ours (Gemini 3 Flash)
> 0.0 0.5 1.0 1.5 2.0
> Cost ($)
> 010 20 30 40
> Accuracy (%) ARC-AGI-2 Performance vs Cost
> GPT-5.2 Gemini 3 Pro Gemini 3 Flash Grok 4 GPT-5.1 Claude Sonnet 4.5 Grok 4 Fast Gemini 2.5 Pro DeepSeek-R1-0528
> Ours (Gemini 3 Flash)

Figure 2. Cost-Performance Pareto Frontier Analysis. We plot accuracy against inference cost for various models on MathArena Apex and ARC-AGI-2. The dashed blue line indicates the Pareto frontier, representing the optimal trade-off between cost and per-formance. 

4.4. Ablation Studies and Scalability Analysis 

To disentangle the contributions of PE-EMP, Memory Op-timization, and Meta-Prompting, we conducted extensive ablation studies using gpt-oss-120b on the AIME25 bench-mark. We also analyzed the scalability of our method with respect to the number of search rollouts. 

Component Contribution and Scaling Behavior. Figure 3 presents the performance trajectory across different rollout configurations. Key observations include: • Full Framework: Achieves a peak of 76.7% at 8 roll-outs, demonstrating continuous improvement (63.3% 

→ 76.7%) that suggests further gains with increased 6Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search 

Table 2. Performance and cost analysis on MathArena Apex and ARC-AGI-2. Empirical-MCTS with Gemini 3 Flash outperforms more expensive models (e.g., GPT-5.2, Grok 4), establishing a new Pareto frontier for cost-effective reasoning. MODEL MATH ARENA APEX ARC-AGI-2 ACC COST ($) ACC COST ($) 

OURS (G EMINI 3 F LASH ) 35.42% 5.24 38.33% 0.97 GPT-5.2 (H IGH ) (O PEN AI, 2025 B) 13.54% 12.00 43.3 % 1.39 GEMINI 3 P RO (D EEP MIND , 2025 B) 23.44% 3.40 31.1% 0.81 GEMINI 3 F LASH (D EEP MIND , 2025 A) 19.79% 1.51 26.75% 0.21 GROK 4 ( X.AI , 2025 A) 2.08% 6.21 16.0% 2.17 GPT-5.1 (H IGH ) (O PEN AI, 2025 A) 1.04% 6.58 17.6% 1.17 CLAUDE SONNET 4.5 (A NTHROPIC , 2025) 1.56% 4.56 13.6% 0.76 GROK 4 F AST (R EASONING ) ( X.AI , 2025 B) 5.21% 0.16 5.3% 0.06 GEMINI 2.5 P RO (G OOGLE , 2025) 0.52% 3.74 4.9% 0.76 DEEP SEEK -R1-0528 (G UO ET AL ., 2025) 1.04% 0.98 1.1% 0.05 

compute. • w/o Meta-Prompt: Removing explicit meta-prompting (with static prompts) in PE-EMP but retain-ing memory drops performance to 66.7% at 8 rollouts. This indicates that while memory provides the content ,meta-prompting is essential for instructing the model on how to apply that content effectively. • w/o Memory & Meta-Prompt: This configuration, which excludes the experience library and utilizes static prompts, peaks at 66.7% (rollout 3) but then regresses to 63.3% at rollout 8, failing to accumulate ”wisdom” from previous rollouts. • w/o PE-EMP & Memory: To rigorously isolate the efficacy of the self-principled and pairwise comparison mechanisms, this variant removes the PE-EMP mod-ule, effectively degenerating into a standard MCTS framework. In this setting, the reward model utilizes static pre-defined criteria and assesses node quality pointwise scoring of individual responses, rather than through relative pairwise comparison. The resulting performance drop (to 56.7% at rollout 8) mirrors that of naive repeated sampling, underscoring that the core en-gine of our improvement is the evolution of the prompt and experience driven by pairwise signals, not merely the tree structure. • Repeated Sampling: Plateaus at 56.7% after 2 roll-outs, confirming the limitations of stateless approaches. 

Empirical Memory Growth Dynamics. Figure 4 quanti-fies how the global experience repository expands during search. Starting from zero experiences at initialization, the repository grows monotonically with each rollout (110 →

311 experiences after 8 rollouts), demonstrating the frame-work’s capacity for continuous knowledge accumulation without parameter updates. 1 2 4 8

> Rollout count
> 50 60 70 80 90
> Accuracy (%)
> GPT-OSS-120b Ours Ours (w/o Meta-Prompt) Ours (w/o Memory & Meta-Prompt) Ours (w/o PE-EMP & Memory) Repeated Sampling

Figure 3. Ablation study on AIME25 using gpt-oss-120b. 

Performance-Experience Correlation. Figure 5 reveals a strong positive correlation between accumulated experi-ences and reasoning performance. The full framework (red) shows a clear upward trajectory (63.3% → 76.7%) as experi-ences grow from 110 to 311. In contrast, the variant without meta-prompting (yellow) exhibits diminishing returns after 240 experiences, while the baseline (w/o PE-EMP & Mem-ory at rollout 8) ultimately reached 56.7%. This confirms that both memory accumulation and meta-prompting are essential for translating experiences into performance gains. 

4.5. Qualitative Analysis of Policy Evolution 

To understand how Empirical-MCTS improves, we analyzed the evolution of meta-prompts in the MathArena Apex tasks. In initial rollouts, the prompt Pevolved focused on generic instructions (e.g., ”Check your calculations”). By Rollout 4, after the Memory Optimization Agent had distilled fail-7Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search 0 1 2 4 8

> Rollout count
> 050 100 150 200 250 300
> Number of experiences 0110 175 236 311

Figure 4. Growth of empirical memory repository during search. The number of distilled experiences increases mono-tonically with rollout count (0 → 8), enabling progressive policy refinement across problem instances. 100 150 200 250 300 350   

> Number of experiences
> 50 55 60 65 70 75 80
> Accuracy (%) 63.3% 70.0% 73.3% 76.7% 60.0% 60.0% 66.7% 66.7%
> Baseline (w/o PE-EMP & Memory)
> Ours Ours (w/o Meta-Prompt) Ours (w/o PE-EMP & Memory)

Figure 5. Performance vs. accumulated experiences on AIME25. The full framework (red) demonstrates a strong pos-itive correlation between experience count and accuracy, while ablated variants show limited improvement (yellow). Baseline performance shown as horizontal reference line (56.7%). 

ures from invalid proofs, the meta-prompt had evolved to include highly specific constraints: ”For geometry proofs involving cyclic quadrilaterals, explicitly verify Ptolemy’s inequality before assuming existence.” This transition from generic to domain-specific guidance, driven by the feedback loop between PE-EMP and the Memory Agent, explains the model’s ability to solve problems that were previously intractable for the base model. 

## 5. Conclusion 

We presented Empirical-MCTS, a framework that uni-fies structured search with non-parametric online learn-ing. By coupling Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) with a dynamic global memory opti-mizer, our approach enables agents to accumulate empirical wisdom and refine their reasoning policies in real-time with-out gradient updates. Empirical evaluations across AIME25, MathArena Apex, and ARC-AGI-2 confirm that this dual-loop mechanism establishes a new Pareto frontier for reasoning efficiency. Our results demonstrate that converting search history into actionable policy priors allows significantly smaller models to outperform larger, stateless baselines, validating that ”re-membering” reasoning patterns is as critical as the search capability itself. 

Limitations. A primary limitation of our framework is the assumption that the accumulation of experience and the evolution of meta-prompts are inherently positive. In prac-tice, this relies on the base model’s capability to accurately verify reasoning steps. For extremely complex tasks that exceed the model’s verification boundary, there is a risk of integrating low-quality insights or ”hallucinated wisdom.” This can lead to degenerative evolution, where the meta-prompt acts on false premises, steering the search towards suboptimal regions. Future research will focus on miti-gating this risk by introducing robust consistency checks and uncertainty quantification into the memory optimiza-tion loop to filter toxic experiences. Additionally, we aim to extend Empirical-MCTS to multi-turn interactions and longer-horizon planning tasks, investigating how empirical memory can persist and adapt over extended operational contexts. 

## References 

Anthropic. Claude sonnet 4.5. Technical report, Anthropic, September 2025. Balunovi ´c, M., Dekoninck, J., and Ivo Petrov, e. a. Math-arena: Evaluating llms on uncontaminated math competi-tions. Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmark , 2025. Brown, B., Juravsky, J., and Ryan Ehrlich, e. a. Large lan-guage monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787 , 2024. Cai, Y., Cai, S., and Yuchen Shi, e. a. Training-free group relative policy optimization. arXiv preprint arXiv:2510.08191 , 2025a. Cai, Z., Guo, X., and Yu Pei, e. a. Flex: Continuous agent evolution via forward learning from experience. arXiv preprint arXiv:2511.06449 , 2025b. Chollet, F., Knoop, M., and Gregory Kamradt, e. a. Arc-agi-2: A new challenge for frontier ai reasoning systems. 

arXiv preprint arXiv:2505.11831 , 2026. DeepMind, G. Gemini 3 flash model card. Technical report, Google DeepMind, December 2025a. DeepMind, G. Gemini 3 pro model card. Technical report, Google DeepMind, November 2025b. 8Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search 

DeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 , 2024. Dong, Q., Li, L., and Damai Dai, e. a. A survey on in-context learning. arXiv preprint arXiv:2301.00234 , 2024. Feng, X., Wan, Z., and Muning Wen, e. a. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179 , 2024. Google. Gemini 2.5 pro preview model card. Technical report, Google, May 2025. Guo, D., Yang, D., and Zhang, e. a. Deepseek-r1 incen-tivizes reasoning in llms through reinforcement learn-ing. Nature , 645(8081):633–638, September 2025. ISSN 1476-4687. Inoue, Y., Misaki, K., Imajuku, Y., and So Kuroki, e. a. Wider or deeper? scaling LLM inference-time compute with adaptive branching tree search. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. Li, Y., Choi, D., Chung, J., and Kushman, e. a. Competition-level code generation with alphacode. Science , 378(6624): 1092–1097, December 2022. ISSN 1095-9203. doi: 10.1126/science.abq1158. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let’s verify step by step. arXiv preprint arXiv:2305.20050 , 2023. Liu, Z., Wang, P., and Runxin Xu, e. a. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495 , 2025. Lu, H., Gu, Y., and Haoyuan Huang, e. a. Mctsr-zero: Self-reflective psychological counseling dialogues generation via principles and adaptive exploration. arXiv preprint arXiv:2505.23229 , 2025. Madaan, A., Tandon, N., and Gupta, e. a. Self-refine: It-erative refinement with self-feedback. In Advances in Neural Information Processing Systems , volume 36, pp. 46534–46594. Curran Associates, Inc., 2023. OpenAI. Introducing gpt-5.1. Technical report, OpenAI, November 2025a. OpenAI. Introducing gpt-5.2. Technical report, OpenAI, December 2025b. OpenAI, :, Jaech, A., and Adam Kalai, e. a. Openai o1 system card. arXiv preprint arXiv:2412.16720 , 2024. OpenAI, :, Agarwal, S., and Lama Ahmad, e. a. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925 , 2025a. OpenAI, :, El-Kishky, A., Wei, A., and Andre Saraiva, e. a. Competitive programming with large reasoning models. 

arXiv preprint arXiv:2502.06807 , 2025b. Schulman, J., Wolski, F., and Prafulla Dhariwal, e. a. Prox-imal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017. Shinn, N., Cassano, F., and Gopinath, e. a. Reflexion: lan-guage agents with verbal reinforcement learning. In Ad-vances in Neural Information Processing Systems , vol-ume 36, pp. 8634–8652. Curran Associates, Inc., 2023. Wang, X., Wei, J., and Dale Schuurmans, e. a. Self-consistency improves chain of thought reasoning in lan-guage models. arXiv preprint arXiv:2203.11171 , 2023. x.ai. Grok 4. Technical report, x.ai, July 2025a. x.ai. Grok 4 fast. Technical report, x.ai, September 2025b. Xie, Y., Goyal, A., and Wenyue Zheng, e. a. Monte carlo tree search boosts reasoning via iterative preference learning. 

arXiv preprint arXiv:2405.00451 , 2024. Xu, H. No train still gain. unleash mathematical rea-soning of large language models with monte carlo tree search guided by energy function. arXiv preprint arXiv:2309.03224 , 2023. Yao, S., Yu, D., and Zhao, Jeffrey, e. a. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems ,volume 36, pp. 11809–11822. Curran Associates, Inc., 2023a. Yao, S., Zhao, J., and Dian Yu, e. a. React: Synergizing reasoning and acting in language models, 2023b. Zhang, D., Huang, X., and Dongzhan Zhou, e. a. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394 , 2024a. Zhang, D., Wu, J., and Jingdi Lei, e. a. Llama-berry: Pair-wise optimization for o1-like olympiad-level mathemati-cal reasoning. arXiv preprint arXiv:2410.02884 , 2024b. Zhang, Y. and Math-AI, T. American invitational mathemat-ics examination (aime) 2025, 2025. Zhou, A., Yan, K., Shlapentokh-Rothman, M., and Hao-han Wang, e. a. Language agent tree search unifies rea-soning acting and planning in language models. arXiv preprint arXiv:2310.04406 , 2024. 9