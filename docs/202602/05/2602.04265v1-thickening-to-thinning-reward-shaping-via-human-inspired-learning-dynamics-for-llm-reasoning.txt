Title: Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning

URL Source: https://arxiv.org/pdf/2602.04265v1

Published Time: Thu, 05 Feb 2026 01:34:36 GMT

Number of Pages: 25

Markdown Content:
# Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning 

Wenze Lin * 1 Zhen Yang * 2 Xitai Jiang 1 Pony Ma 3 Gao Huang 1

## Abstract 

Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Mod-els (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T (Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes ”thickening” (longer trajecto-ries) to broaden the search space and explore novel solution paths; (2) Upon achieving cor-rectness, it shifts to ”thinning”, imposing length penalties to discourage redundancy, thereby fos-tering model confidence and crystallizing reason-ing capabilities. Extensive experiments on mathe-matical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demon-strate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance. 

## 1. Introduction 

Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a practical paradigm for enhancing the reasoning behavior of Large Language Models (LLMs), especially in domains with automatic verification such as math and code. Despite recent successes, existing RLVR methods provide only a limited account of how learning should unfold under finite computational budgets. Standard 

> *

Equal contribution 1Tsinghua University, Beijing, China 

> 2

Southern University of Science and Technology, Shenzhen, China 

> 3

Mind Lab, Beijing, China. Correspondence to: Gao Huang <gao-huang@tsinghua.edu.cn >.

Preprint. February 5, 2026. 

sequence-level RLVR assigns uniform rewards to all cor-rect outputs and zero reward otherwise, implicitly treating long exploratory reasoning and concise mastery as equally desirable. Extensions based on entropy regularization or ver-bosity control address symptoms such as entropy collapse or excessive length, but do not differentiate when exploration is beneficial versus when compression is appropriate. Mean-while, more theoretically motivated formulations often em-phasize unbiased or stable optimization objectives, yet lack an explicit characterization of how such objectives translate into effective capability acquisition over rare correct solu-tion trajectories. As a result, the dynamics of exploration and consolidation remain entangled rather than structurally separated. 

Design Philosophy of Thickening-to-Thinning. Effec-tive learning under finite cognitive or computational re-sources is inherently stage-wise. When confronted with unfamiliar or difficult problems, human learners rarely seek concise answers immediately. Instead, learning be-gins with an expansive phase characterized by broad explo-ration—trying multiple approaches, examining alternative decompositions, and tolerating verbosity and redundancy as a necessary cost of discovery. Only after a problem is successfully resolved does learning transition into a sec-ond phase, where reasoning is summarized and abstracted, and unnecessary details are stripped away to form com-pact, precise representations that can be efficiently retained and reused. This progression is succinctly captured by a pedagogical principle attributed to Hua Luogeng: effective learning proceeds by first “reading the book thick,” enrich-ing understanding through extensive exploration, and then “reading it thin,” distilling insights through abstraction and compression, as illustrated in Figure 1. Crucially, these two stages serve distinct and asymmetric roles—exploration increases the likelihood of uncovering rare correct solutions, while compression consolidates successful reasoning into stable knowledge. Treating them as simultaneously optimal obscures the dynamics of learning under finite compute. Motivated by this gap, we propose Thickening-to-Thinning (T2T), a reward shaping framework that explicitly embeds this stage-wise learning dynamic into RLVR. Rather than redefining the notion of correctness, T2T modulates reward 1

> arXiv:2602.04265v1 [cs.LG] 4 Feb 2026

T2T Gathering vast information, understanding context, grappling 

> with complexity. The ‘thick’ phase is about immersion.
> Condensing knowledge, mastering essentials, applying core
> principles. The ‘thin’ phase is about clarity and efficient recall.

Figure 1. A two-stage learning pattern in human learning, where understanding is first expanded through exploration and later re-fined into concise and efficient knowledge. Thinning 

> Thickening
> ……

Figure 2. (1) On incorrect attempts, T2T incentivizes ”thickening” to broaden the search space; (2) Upon correctness, it shifts to ”thinning” to discourage redundancy, fostering model confidence. 

magnitude based on correctness, difficulty, and response length, encouraging expansive, exploratory reasoning when correctness is not yet achieved and favoring concise, precise solutions once success is attained. Importantly, T2T preserves the simplicity of standard sequence-level RLVR: verification remains at the sequence level, no auxiliary models or token-level supervision are introduced, and no additional computational overhead is incurred during training. As a result, T2T can be seamlessly integrated into existing RLVR pipelines and extended to a broad range of reinforcement learning variants, providing a minimal yet principled mechanism for aligning optimization objectives with the natural progression from exploration to compression. 

## 2. Related Works 

Reinforcement Learning for Reasoning Reinforcement learning (RL) has become standard for aligning language models with complex logical constraints (Ouyang et al., 2022). Iterative training frameworks like STaR (Zelikman et al., 2022) and ReST (Singh et al., 2023) utilize the model’s own successful trajectories as bootstrap supervision. More recent policy optimization methods, such as PPO (Schul-man et al., 2017) and Group Relative Policy Optimization (GRPO) (Shao et al., 2024), directly optimize reasoning per-formance against outcome-based or process-based rewards (Lightman et al., 2023). However, standard RL objectives for reasoning typically treat the reasoning chain length as a byproduct or explicitly penalize verbosity to reduce costs (Dubois et al., 2023). In contrast, our T2T approach treats trajectory length as a dynamic control variable. By explic-itly incentivizing longer chains when the model is uncertain (low competence), we encourage the policy to allocate more “internal compute” to difficult problems, effectively learning to explore before learning to optimize. 

Variants and Extensions of RLVR. A growing body of work has sought to address limitations of standard RLVR, including the mismatch between pass@1 improvements and large-k behavior, entropy collapse, and excessive verbosity. One line of work explicitly targets pass@k-style objectives or their surrogates (Walder & Karkhanis, 2025; Bagirov et al., 2025; Zhang et al., 2025; Chen et al., 2025b; Peng et al., 2025; Jiang et al., 2025). Another line focuses on the-oretical or statistical corrections to the optimization process itself, for example via entropy control (Yang et al., 2025), structured pruning or brevity-promoting objectives (Hou et al., 2025; Liu et al., 2025a), or bias-aware alternatives to GRPO (Liu et al., 2025b; Yang et al., 2026). These approaches are primarily motivated by formal objective mis-match or optimization pathologies, and propose principled modifications to the learning objective or update rule. In a different vein, NSR demonstrates that training solely on negative samples can still yield substantial capability im-provements, highlighting the robustness of reward-driven learning signals (Zhu et al., 2025). Our work follows a dif-ferent design philosophy. Rather than deriving corrections from theoretical optimality criteria, we adopt a heuristic per-spective inspired by human learning behavior, and modify the reward objective accordingly. From this viewpoint, T2T represents a parallel approach to existing RLVR variants, addressing similar challenges through a distinct inductive bias rather than a unified or compositional framework. We include detailed empirical comparisons with several of these methods in our experiments. Beyond these variants, a num-ber of extensions enrich the RLVR framework itself. For instance, Luffy incorporates off-policy guidance to improve sample efficiency and stability (Yan et al., 2025). Since our proposed T2T method operates at the level of reward design, it is orthogonal to such framework-level extensions and can be applied alongside them without additional algorithmic overhead. 2T2T 

## 3. Preliminaries 

Reinforcement Learning with Verifiable Rewards 

Let V : Q × O → { 0, 1} denote a verifier that checks whether an output o solves query q:

V(q, o ) = 

(

1, if o is verified as correct for q, 

0, otherwise . (1) Fix an interaction protocol (prompting, decoding strategy, and any hard constraints) and a model policy πθ (o | q) over outputs o ∈ O given a query q. Define the single-sample success probability 

pθ (q) := Pr  

> o∼πθ(·| q)

[ V(q, o ) = 1 ] . (2) For clarity, we consider sequence-level reinforcement learn-ing with verifiable rewards and use it to illustrate the algo-rithms throughout this paper. Given a query q ∼ D and a sequence-level output o ∼ πθ (·| q), the learning objective is to maximize the expected reward: 

J (πθ ) = Eq∼D , o ∼πθ (·| q)

R(q, o ). (3) In GRPO and related RLVR methods, the reward function 

R(q, o ) is typically defined as a binary indicator reflecting the correctness of the generated output. Under this def-inition, the objective reduces to maximizing the model’s success probability under single-sample decoding: 

J (πθ ) = Eq∼D 

h

Eo∼πθ (·| q)

V(q, o )i

= Eq∼D 

pθ (q),

(4) where pθ (q) ≜ Pr o∼πθ (·| q)

 V(q, o ) = 1  denotes the prob-ability that the model produces a verifier-accepted output for query q.Using the policy gradient theorem, the gradient of the ob-jective can be written as 

∇θ J (πθ ) = Eq∼D , o ∼πθ (·| q)

h

Aπθ (q, o ) ∇θ log πθ (o | q)

i

,

(5) where the advantage function is defined as 

Aπθ (q, o ) = R(q, o ) − Eo′∼πθ (·| q)

R(q, o ′). (6) In practice, the expectation over the policy is approximated using N sampled outputs {oj }Nj=1 , yielding the empirical estimator 

ˆAπθ (q, o i) = R(q, o i) − 1

N

> N

X

> j=1

R(q, o j ), oj ∼ πθ (·| q).

(7) This group-wise baseline corresponds to the advantage esti-mator used in GRPO. To enable multiple optimization steps on the same batch of data, we introduce the importance sampling ratio between the current policy πθ and the behavior policy πθold :

rθ (q, o ) = πθ (o | q)

πθold (o | q) . (8) Consequently, the final surrogate objective with clip-ping—designed to constrain policy updates and ensure train-ing stability—is formulated as: 

JCLIP (θ) = Eq∼D ,o ∼πθold (·| q)



min 



rθ (q, o ) ˆA(q, o ), clip  rθ (q, o ), 1 − ϵ, 1 + ϵ ˆA(q, o )



,

(9) where ϵ is a hyperparameter determining the clipping range, and ˆA(q, o ) is the group-wise advantage estimator defined previously. 

## 4. Method 

4.1. Thickening to Thinning Reward Design 

In this subsection, we introduce the Thickening-to-Thinning (T2T) reward, a competence-aware reward shaping scheme for RLVR. The core idea is to modulate length-related incen-tives as a function of the model’s current success probability on each query: when the model is unlikely to solve a query, longer responses are encouraged to facilitate exploration, whereas once the query is reliably solved, shorter and more concise solutions are favored to promote compression. For a fixed query q,

pθ (q) = Eo∼πθ (·| q)

V(q, o ) = Pr  

> o∼πθ(·| q)

 V(q, o ) = 1 .

(10) When the context is clear, we write p for brevity. 

Length score. Let ℓ(o) be the length of o (e.g., number of tokens). We define a normalized length score sL(o) ∈ [0 , 1] 

via clipping: 

sL(o) ≜ clip 

 ℓ(o) − Lmin 

Lmax − Lmin 

, 0, 1



. (11) 

Length-regularized reward. To encourage exploration on incorrect attempts while favoring concise correct solu-tions, we first consider the following length-regularized reward: 

RLR (q, o ) ≜ V(q, o ) +  1−2V(q, o ) α s L(o), α ∈ (0 , 12 ).

(12) Equivalently, in a piecewise form, 

RLR (q, o ) = 

(1 − α s L(o), V(q, o ) = 1 ,α s L(o), V(q, o ) = 0 . (13) 3T2T 

The constraint α < 12 ensures that any verified-correct out-put receives strictly larger reward than any incorrect one: 

min V=1 RLR = 1 − α > α = max V=0 RLR .Taking expectation over o ∼ πθ (· | q) yields 

ERLR (q, o ) = E[V] + α E(1 − 2V) sL(o)

= p + α



(1 − p) E[sL(o) | V = 0] − p E[sL(o) | V = 1] 



.

(14) 

Thickening-to-thinning(T2T) reward. Eq. (14) shows that length shaping enters the objective through first-order factors in (1 − p) and p. To realize an effective “thicken-to-thin” learning dynamic, we replace these linear factors with quadratic terms. This design preserves the success proba-bility p as the primary objective, while using p-dependent weighting to adaptively modulate the importance of length-related shaping. At the objective level, we target 

ERT2T (q, o, θ ) = p + α(1 − p)2 E[sL(o) | V = 0] 

− αp 2 E[sL(o) | V = 1] , (15) so that the length-related correction becomes a second-order term in p and mainly reshapes learning dynamics under finite sampling. We implement Eq. (15) by the following competence-conditioned reward: 

RT2T (q, o, θ ) ≜ V(q, o ) + (1 − V (q, o )) α s L(o) (1 − p)

− V (q, o ) α s L(o) p. (16) Equivalently, 

RT2T (q, o, θ ) = 

(1 − α s L(o) p, V(q, o ) = 1 ,α s L(o) (1 − p), V(q, o ) = 0 . (17) In practice, p is estimated on-policy from the sampled group 

{oi}Ki=1 ∼ πθold (· | q) (and optionally an exponential mov-ing average); we treat this estimate as a stop-gradient statis-tic when updating θ.

On-policy estimation. Following GRPO, for each query 

q we sample a group of K outputs {oi}Ki=1 ∼ πθ (· | q) and compute the on-policy pass-rate 

ˆpθ (q) ≜ 1

K

> K

X

> i=1

V(q, o i). (18) The implemented T2T reward is then defined as 

ˆRT2T (q, o, θ ) = 

(1 − α s L(o) ˆ p(q; θ), V(q, o ) = 1 ,α s L(o)  1 − ˆp(q; θ), V(q, o ) = 0 .

(19) We compute advantages and perform policy optimization following the same procedure as in GRPO. 

4.2. Analysis on T2T 

The quadratic weighting in Eq. (15) induces a competence-conditioned learning dynamic. When p ≪ 1, the factor 

(1 − p)2 is close to one while p2 is negligible, so the length-based shaping term is dominated by incorrect samples. In this regime, T2T preferentially increases the probability of longer trajectories on failed attempts, effectively encourag-ing deeper exploration for hard queries. This behavior is analogous to increasing search depth at test time, but cru-cially, it is carried out during training and any successful long-horizon trajectory is distilled into the policy parame-ters. Conversely, when p ≈ 1, (1 − p)2 vanishes and the shaping term is dominated by p2 on correct samples. As a result, among already-solved queries, T2T penalizes unnec-essary length on correct outputs, favoring concise solutions and reducing redundant inference-time computation. Taken together, T2T realizes a “thickening-to-thinning” process: exploration through longer reasoning is emphasized when a query is hard, and progressively suppressed as the model becomes competent on that query. 

Reward-induced ordering. The T2T reward induces a consistent and interpretable preference over responses. For a fixed query q, the piecewise form in Eq. (17) implies that any verified-correct output receives higher reward than any incorrect one. Moreover, among correct outputs, shorter responses are preferred due to the −αs L(o) p term, while among incorrect outputs, longer responses are preferred due to the αs L(o)(1 − p) term. As a result, T2T induces the following ordering: correct short ≻ correct long 

≻ incorrect long ≻ incorrect short .

This ordering aligns with the intended learning behavior: prioritize correctness, encourage exploration through longer attempts when incorrect, and favor concise solutions once correctness is achieved. Notably, unlike standard GRPO with binary rewards, T2T provides non-trivial learning sig-nals even for queries whose sampled outputs are all correct or all incorrect, through the length-dependent shaping terms. 

## 5. Experiment 

5.1. Experimental Setup Models. To investigate the scalability and effective-ness of our proposed method across different model capacities, we conduct experiments on the Qwen and DeepSeek series, which are widely recognized for their mathematical reasoning capabilities. Specifically, we utilize DeepSeek-R1-Distill-Qwen-1.5B ,

Qwen2.5-3B , Qwen3-4B , and Qwen3-14B as our base policies. For the Qwen3 series, we utilize the non-thinking 

4T2T 

mode . These models serve as the initialization for all reinforcement learning experiments. 

Training Setup. We construct our training set using 

DAPO-MATH-17k (Yu et al., 2025), a high-quality dataset specifically filtered for mathematical reasoning. All models are trained using the Verl framework (Sheng et al., 2025). We employ a global train batch size of 256 and train for a total of 275 steps. The maximum new tokens is set to 8192. Detailed hyperparameter configurations are provided in Appendix B.1. 

Baseline Settings. We compare T2T against standard GRPO and three advanced baselines that explicitly differ-entiate between positive and negative samples to refine the learning signal. Detailed implementation specifics and hy-perparameter configurations are provided in Appendix B.2. The specific methods are as follows: • GRPO (Guo et al., 2025; Shao et al., 2024): The vanilla Group Relative Policy Optimization algorithm without additional length or gradient modifications. • LASER (Liu et al., 2025a): A reward shaping method that encourages efficient reasoning by augmenting the reward for positive samples with a length penalty term 

α · I(L < L target ), while keeping rewards for negative samples unchanged. • W-REINFORCE (Zhu et al., 2025): A gradient reweighting approach that retains the full gradient for negative samples to facilitate unlearning, while scaling down the gradient for positive samples by a factor ρ.• EntroPIC (Yang et al., 2025): An entropy-stabilization method that utilizes Proportional-Integral (PI) control to dynamically adjust the loss weights of positive and negative samples based on the discrepancy between the current policy entropy and a target entropy. We note that due to computational constraints, for the larger 

Qwen3-14B model, only the standard GRPO baseline is implemented. 

Evaluation Setup. We evaluate the models on four widely used and challenging mathematical reasoning benchmarks: 

MATH-500 (Hendrycks et al., 2021; Lightman et al., 2024), 

AIME’24 , AIME’25 , and AMC’23 . During evaluation, we sample n = 64 responses per prompt for MATH-500, and n = 128 for AIME’24, AIME’25, and AMC’23. The sampling temperature is set to 0.7 with a top-p of 0.95. We report the Pass@ k metric ( k ∈ { 1, 2, 4, 8, 16 , 32 , 64 }). Specifically, Pass@1 is calculated as the mean accuracy over the n generated samples. For k > 1, to avoid high variance from limited sampling, we employ the unbiased estimator proposed by Chen (2021). Given n generated samples with c correct responses, Pass@ k is calculated as: Pass @k = E

"

1 −

 n−ck

 nk

#

(20) .

5.2. Main Results on Mathematical Benchmarks 

The main results across four mathematical reasoning bench-marks are summarized in Table 1. The full experimental results are provided in Appendix D.1. Our experiments yield several key observations regarding the effectiveness and scalability of the proposed method: 

Competitive Performance and Robust Improvements. 

As shown in Table 1, T2T demonstrates highly competi-tive performance across most tested model capacities. No-tably, on the Qwen2.5-3B and Qwen3-4B models, our method yields consistent gains, particularly excelling in the AMC’23 and AIME benchmarks compared to the stan-dard GRPO baseline and other variants like LASER and W-REINFORCE. On the largest evaluated model, Qwen3-14B, T2T exhibits dominant performance, securing the top spot in all Pass@1 and Pass@64 metrics, which validates the method’s strong scalability. 

Effectiveness of RL Training. Comparing the results with the Initial Model , it is evident that Reinforcement Learning significantly boosts mathematical reasoning capa-bilities. All RL-based methods show a substantial jump in performance compared to the base model. However, T2T generally maximizes this gain on models with sufficient capacity (3B+), suggesting that our approach utilizes the training data more efficiently by better distinguishing be-tween high-quality and low-quality reasoning paths. 

Limitations on Smaller Models. While T2T performs well on larger models, we observe a different trend on the smallest model, DeepSeek-R1-Distill-Qwen-1.5B . As indi-cated in Table 1, T2T shows only marginal improvements or even slight degradation in certain metrics (e.g., MATH-500 Pass@1 and AMC’23 Pass@64) compared to the GRPO baseline. We attribute this to the limited capacity of the 1.5B model. When attempting to emulate the complex, extended reasoning chains encouraged by T2T, the 1.5B model is prone to entering repetitive generation loops, especially on difficult problems. This behavior undermines the benefits of the method, suggesting that a minimum model capacity is required to fully leverage the T2T training objective. 5T2T 

Table 1. Main Results on mathematical reasoning benchmarks. We report Pass@1 and Pass@64 performance using the unbiased estimator. The best results are highlighted in bold , and the second best results are underlined.                                                                                                                                                                                     

> Base Model Method MATH-500 AIME’24 AIME’25 AMC’23
> Pass@1 Pass@64 Pass@1 Pass@64 Pass@1 Pass@64 Pass@1 Pass@64 DeepSeek-R1-Distill-Qwen-1.5B Initial Model 66.0 85.0 20.5 46.9 14.8 28.2 50.9 86.4 GRPO 68.9 84.0 20.6 47.7 14.8 28.9 57.3 92.7
> LASER 66.5 85.4 20.8 48.3 14.9 28.4 57.8 91.5 W-REINFORCE 67.5 85.8 20.4 46.8 14.7 28.0 58.2 91.6 EntroPIC 66.2 84.8 21.0 49.5 15.0 28.5 58.5 91.4
> T2T (Ours) 66.9 86.1 21.3 50.7 15.2 28.8 59.0 91.7 Qwen2.5-3B Initial Model 40.6 49.1 7.1 14.7 3.4 4.6 43.9 59.0 GRPO 47.4 59.9 4.8 14.7 7.7 15.6 49.1 63.4 LASER 48.4 57.9 6.0 12.2 4.6 12.2 51.9 66.2 W-REINFORCE 46.5 60.2 12.6 22.8 5.8 15.9 52.5 70.9 EntroPIC 44.6 61.3 7.2 18.3 3.6 9.2 54.7 71.9
> T2T (Ours) 49.8 64.6 9.6 21.7 5.4 16.7 57.0 77.0
> Qwen3-4B Initial Model 80.2 91.2 22.2 37.0 14.7 35.4 71.4 92.4 GRPO 81.2 90.5 29.3 48.6 21.3 48.5 75.8 91.2 LASER 80.6 90.7 28.2 45.4 21.3 46.9 76.1 91.1 W-REINFORCE 80.9 90.6 28.5 50.7 20.8 50.9 75.8 93.9 EntroPIC 80.5 90.8 28.8 47.9 21.0 48.0 75.9 91.5
> T2T (Ours) 81.2 91.1 33.4 56.3 23.0 49.6 78.0 95.3
> Qwen3-14B Initial Model 82.2 87.2 27.1 48.3 19.6 47.3 74.6 91.2 GRPO 83.3 88.8 29.1 56.1 22.4 51.5 80.7 94.6
> T2T (Ours) 85.1 92.7 39.6 59.6 24.5 57.9 81.4 98.4

5.3. Analysis of Training Dynamics 

To investigate the learning behavior of T2T, we visualize the evolution of Training Accuracy and Policy Entropy during the fine-tuning process. Figure 3 and figure 4 presents the comparative results on Qwen2.5-3B and Qwen3-4B .

Emergent High Entropy without Explicit Regularization. 

A critical observation from training dynamics is the behav-ior of policy entropy, which varies across model capacities yet consistently favors our method. It is important to em-phasize that T2T does not employ any explicit entropy regularization terms .On Qwen2.5-3B , the standard GRPO baseline exhibits a relatively flat and static entropy curve, suggesting limited variation in its search strategy. In contrast, T2T demon-strates a distinct dynamic: it exhibits a steady increase in entropy that eventually converges to a stable level. This trajectory aligns with our “Thickening-to-Thinning” philos-ophy: the model first expands its search space to explore diverse reasoning paths (Thickening), and then stabilizes as it learns to distinguish correct solutions (Thinning). On Qwen3-4B , where the baseline shows a tendency to-wards entropy decay (signaling potential policy collapse), T2T effectively counters this drop , sustaining a signifi-cantly higher entropy level throughout training. Despite these different underlying dynamics, the common thread is that T2T implicitly incentivizes the model to main-tain a broader search space than the baseline. This emer-gent property directly supports the superior Pass@64 per-formance observed in our main results. 

Accelerated Learning Efficiency. Figure 3 demonstrate that T2T achieves a significantly increase in training accu-racy compared to the baseline during training. This rapid improvement stems from the synergy of the Thickening-to-Thinning (T2T) strategy . On one hand, the “Thickening” phase encourages the model to explore diverse trajectories for hard problems, increasing the likelihood of discovering correct reasoning chains that the baseline might miss. On the other hand, once a correct path is found, the “Thinning” phase applies a dynamic length penalty, filtering out verbose and irrelevant tokens. This dual mechanism ensures that the model not only finds the correct answers faster but also learns to refine them efficiently, leading to a consistently higher accuracy yield within the same training budget. 

Length Modulation Mechanics. We further analyze the evolution of response lengths. We observe that T2T adap-tively modulates generation length based on intrinsic model capability . Specifically, the method tends to increase length for less capable models (to encourage exploration of reason-ing paths), while decreasing it for highly proficient models that have already mastered the tasks (to encourage concise-ness). Moreover, a breakdown of positive and negative sam-6T2T 0 50 100 150 200 250              

> Step
> 0.175
> 0.200
> 0.225
> 0.250
> 0.275
> 0.300
> 0.325
> 0.350
> 0.375
> Accuracy
> T2T GRPO LASER W-REINFORCE ENTROPIC 050 100 150 200 250
> Step
> 0.600
> 0.625
> 0.650
> 0.675
> 0.700
> 0.725
> 0.750
> 0.775
> Accuracy
> T2T GRPO LASER W-REINFORCE ENTROPIC

Figure 3. Training Accuracy Evolution. The plot on the left corresponds to Qwen2.5-3B , and the plot on the right corresponds to 

Qwen3-4B . Across both model scales, T2T demonstrates superior learning efficiency compared to the baseline. 0 50 100 150 200 250              

> Step
> 0.6
> 0.8
> 1.0
> 1.2
> 1.4
> 1.6
> 1.8
> 2.0
> 2.2
> Entropy
> T2T GRPO LASER W-REINFORCE ENTROPIC 050 100 150 200 250
> Step
> 0.10
> 0.15
> 0.20
> 0.25
> 0.30
> 0.35
> 0.40
> Entropy
> T2T GRPO LASER W-REINFORCE ENTROPIC

Figure 4. Policy Entropy Evolution. The plot on the left corresponds to Qwen2.5-3B , and the plot on the right corresponds to 

Qwen3-4B . Regardless of the absolute trend, T2T consistently maintains a higher relative entropy level than the baseline, indicating sustained exploration capabilities. 

ples confirms that our method successfully learns a bi-modal strategy: shortening correct responses while elongating in-correct ones. Detailed plots and analyses of these length dynamics are provided in Appendix E. 5.4. Ablation Study 

To explicitly evaluate the contribution of each component in our proposed method, we conduct an ablation study using the Qwen2.5-3B model. We compare the full T2T against three variants: • w/o Difficulty Awareness: We remove the group pass rate term from the reward formulation, applying a static length reward/penalty regardless of problem difficulty. • w/o Thinning (Thickening Only): We retain difficulty awareness but only apply the positive reward for incor-rect responses (encouraging exploration on hard tasks). The penalty for correct responses is removed. • w/o Thickening (Thinning Only): We retain difficulty awareness but only apply the length penalty for correct responses (encouraging efficiency on easy tasks). The reward for incorrect responses is removed. • T2T (Full): The complete method incorporating difficulty-aware Thickening and Thinning. The results are summarized in Table 2. 

Impact of Difficulty Awareness. Comparing the full model with the w/o Difficulty Awareness variant, we ob-serve a clear performance drop when the adaptive pass-rate scaling is removed. This confirms that treating all samples equally is suboptimal. A static length bias fails to distin-guish between the need for exploration in hard problems and the need for conciseness in solved problems, leading to conflicting learning signals. 

Impact of Thickening and Thinning Mechanics. As shown in the ablation results, removing either the “Thicken-ing” or “Thinning” component leads to a general degrada-tion in performance across most metrics, validating that both phases are essential for the full effectiveness of T2T. The 7T2T                                                   

> Table 2. Ablation Study on Qwen2.5-3B. We investigate the impact of Difficulty Awareness, Thickening (exploration reward), and Thinning (efficiency penalty). Pass@1 and Pass@64 are reported. The best results are highlighted in bold .
> Method Variant MATH-500 AIME’24 AIME’25 AMC’23
> Pass@1 Pass@64 Pass@1 Pass@64 Pass@1 Pass@64 Pass@1 Pass@64 w/o Difficulty Awareness 49.5 64.4 8.9 20.5 4.9 15.8 56.2 75.8 w/o Thinning (Thickening Only) 49.1 63.3 8.5 20.8 4.6 16.0 55.1 75.2 w/o Thickening (Thinning Only) 48.3 60.1 8.2 18.5 4.2 13.9 55.8 72.5
> T2T (Full) 49.8 64.6 9.6 21.7 5.4 16.7 57.0 77.0

w/o Thinning variant (Thickening Only) retains the incentive for expansive exploration on difficult or incorrect problems. However, its performance drops indicate that without the subsequent “Thinning” phase—which encourages abstrac-tion and compression on correct solutions—the model fails to refine its verbose reasoning into efficient knowledge, mir-roring the human learning process where understanding must be condensed after expansion. Conversely, the w/o Thickening variant (Thinning Only) applies penalties for redundancy but lacks the reward mechanism to encourage longer chains when the model fails. The significant decline in performance here suggests that skipping the “Thicken-ing” phase prevents the model from engaging in the deep exploration necessary to solve hard problems, effectively trying to “thin” the knowledge before it has been sufficiently “thickened.” These findings confirm our hypothesis based on Hua Luogeng’s principle: mathematical reasoning capabili-ties are best developed by combining expansive exploration (Thickening) with disciplined abstraction (Thinning), rather than relying on either mechanism in isolation. 

## 6. Discussion 

From Test-Time Scaling to Its Training-Time Continua-tion via RLVR. Recent work in both academia and indus-try has revealed a close connection between Reinforcement Learning with Verifiable Rewards (RLVR) and Test-Time Scaling (TTS), particularly in verifiable domains such as mathematics and code. Empirically, the effectiveness of TTS suggests that correct solutions for hard queries often lie in low-probability regions that are reachable only through extended reasoning trajectories. When optimizing language models with RLVR using outcome-based verifiable rewards, policies are naturally incentivized to increase the proba-bility mass of such trajectories, leading to the emergence of multi-step reasoning behaviors without explicit supervi-sion. In this sense, RLVR can be viewed as a mechanism that partially amortizes the computational cost of test-time search into training. Our T2T reward design builds directly upon this perspective. While standard RLVR internalizes test-time scaling in a coarse and undifferentiated manner, it does not explicitly distinguish between regimes that require extensive exploration and those where the solution has al-ready been consolidated. T2T addresses this misalignment by dynamically allocating ”thinking budget”: it incentivizes extended reasoning for uncertain or high-difficulty queries, while enforcing conciseness for mastered tasks. Conse-quently, T2T offers a compute-aware evolution of the RLVR framework, preserving the exploratory power of scaling while curbing redundant verbosity and enhancing sample ef-ficiency. Related discussions and connections to prior work on test-time scaling and RLVR are provided in Appendix A. 

## 7. Conclusion 

This work introduces Thickening-to-Thinning (T2T), a com-petence and length conditioned reward shaping strategy that explicitly distinguishes between exploratory reasoning and efficient solution refinement within RLVR. Importantly, T2T is lightweight to implement, requires no additional models or inference-time overhead, and can be readily combined with algorithmic extensions that modify optimization or sampling procedures rather than reward semantics. Exten-sive experiments demonstrate that T2T not only improves final performance and generalization across diverse bench-marks, but also induces healthier training dynamics, mitigat-ing common RLVR pathologies such as brittle exploration or excessive verbosity. More broadly, our results suggest that mimicking key aspects of human learning—progressing from expansive exploration to deliberate compression—may offer a fruitful direction for advancing reasoning in large language models beyond static reward formulations. 

## 8. Limitations 

T2T represents a design choice rather than a uniquely opti-mal reward formulation; alternative stage-wise reward shap-ing schemes may yield similar effects. Our method relies on on-policy pass-rate statistics under finite sampling, which can be noisy for rare-event queries. Our experiments focus on verifiable reasoning domains, and extending stage-wise reward shaping to non-verifiable or subjective tasks remains an open direction. 8T2T 

## 9. Impact Statement 

This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. 

## References 

Bagirov, F., Arkhipov, M., Sycheva, K., Glukhov, E., and Bogomolov, E. The best of n worlds: Aligning rein-forcement learning with best-of-n sampling via max@ k optimisation. arXiv preprint arXiv:2510.23393 , 2025. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. 

Advances in neural information processing systems , 33: 1877–1901, 2020. Chen, H., Wang, Y., Han, K., Li, D., Li, L., Bi, Z., Li, J., Wang, H., Mi, F., Zhu, M., et al. Pangu embedded: An efficient dual-system llm reasoner with metacognition. 

arXiv preprint arXiv:2505.22375 , 2025a. Chen, M. Evaluating large language models trained on code. 

arXiv preprint arXiv:2107.03374 , 2021. Chen, Z., Qin, X., Wu, Y., Ling, Y., Ye, Q., Zhao, W. X., and Shi, G. Pass@ k training for adaptively balancing exploration and exploitation of large reasoning models. 

arXiv preprint arXiv:2508.10751 , 2025b. Dubois, Y., Li, C. X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P. S., and Hashimoto, T. B. Al-pacafarm: A simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems , 36:30039–30069, 2023. Feng, X., Wan, Z., Wen, M., McAleer, S. M., Wen, Y., Zhang, W., and Wang, J. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179 , 2023. Gershman, S. and Goodman, N. Amortized inference in probabilistic reasoning. In Proceedings of the annual meeting of the cognitive science society , volume 36, 2014. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-centivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-ematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 , 2021. Hou, B., Zhang, Y., Ji, J., Liu, Y., Qian, K., Andreas, J., and Chang, S. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296 , 2025. Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. Large language mod-els cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798 , 2023. Jiang, Y., Huang, J., Yuan, Y., Mao, X., Yue, Y., Zhao, Q., and Yan, L. Risk-sensitive rl for alleviating explo-ration dilemmas in large language models. arXiv preprint arXiv:2509.24261 , 2025. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. 

arXiv preprint arXiv:2001.08361 , 2020. Khan, Z., BG, V. K., Schulter, S., Fu, Y., and Chandraker, M. Self-training large language models for improved visual program synthesis with visual reinforcement. In 

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 14344–14353, 2024. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Ad-vances in neural information processing systems , 35: 22199–22213, 2022. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let’s verify step by step. In The Twelfth International Conference on Learning Representations ,2023. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let’s verify step by step. In The Twelfth International Conference on Learning Representations ,2024. URL https://openreview.net/forum? id=v8L0pN6EOi .Liu, W., Zhou, R., Deng, Y., Huang, Y., Liu, J., Deng, Y., Zhang, Y., and He, J. Learn to reason efficiently with adaptive length-based reward shaping. arXiv preprint arXiv:2505.15612 , 2025a. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A criti-cal perspective. In Second Conference on Language Mod-eling , 2025b. URL https://openreview.net/ forum?id=5PAF7PAY2Y .Magister, L. C., Mallinson, J., Adamek, J., Malmi, E., and Severyn, A. Teaching small language models to reason. In 

Proceedings of the 61st annual meeting of the association 

9T2T 

for computational linguistics (volume 2: short papers) ,pp. 1773–1781, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730–27744, 2022. Peng, R., Ren, Y., Yu, Z., Liu, W., and Wen, Y. Simko: Simple pass@ k policy optimization. arXiv preprint arXiv:2510.14807 , 2025. Polu, S., Han, J. M., Zheng, K., Baksys, M., Babuschkin, I., and Sutskever, I. Formal mathematics statement curricula for theorem proving. In International Conference on Learning Representations , 2022. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. 

arXiv preprint arXiv:1707.06347 , 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Push-ing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexi-ble and efficient rlhf framework. In EuroSys , pp. 1279– 1297, 2025. URL https://doi.org/10.1145/ 3689031.3696075 .Singh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Garcia, X., Liu, P. J., Harrison, J., Lee, J., Xu, K., et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585 , 2023. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effective than scal-ing model parameters. arXiv preprint arXiv:2408.03314 ,2024. Suzgun, M., Scales, N., Sch ¨arli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q., Chi, E., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023 , pp. 13003–13051, 2023. Walder, C. and Karkhanis, D. T. Pass@k policy opti-mization: Solving harder reinforcement learning prob-lems. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. URL https: //openreview.net/forum?id=W6WC6047X2 .Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency im-proves chain of thought reasoning in language models. 

arXiv preprint arXiv:2203.11171 , 2022. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824–24837, 2022. Wu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. An empir-ical analysis of compute-optimal inference for problem-solving with language models. 2024. Yan, J., Li, Y., Hu, Z., Wang, Z., Cui, G., Qu, X., Cheng, Y., and Zhang, Y. Learning to reason under off-policy guidance. In The Thirty-ninth Annual Conference on Neu-ral Information Processing Systems , 2025. URL https: //openreview.net/forum?id=vO8LLoNWWk .Yang, F., Chen, Z., Wang, X., Lu, X., Chai, J., Yin, G., Lin, W., Ma, S., Zhuang, F., Wang, D., et al. Your group-relative advantage is biased. arXiv preprint arXiv:2601.08521 , 2026. Yang, K., Xu, X., Chen, Y., Liu, W., Lyu, J., Lin, Z., Ye, D., and Yang, S. Entropic: Towards stable long-term train-ing of llms via entropy stabilization with proportional-integral control. arXiv preprint arXiv:2511.15248 , 2025. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems , 36:11809–11822, 2023. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., YuYue, Dai, W., Fan, T., Liu, G., Liu, J., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, R., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.-Y., Zhang, Y.-Q., Yan, L., Wu, Y., and Wang, M. DAPO: An open-source LLM reinforcement learning system at scale. In The Thirty-ninth Annual Conference on Neural Information Processing Systems ,2025. URL https://openreview.net/forum? id=2a36EMSSTp .Yuan, Z., Yuan, H., Li, C., Dong, G., Tan, C., and Zhou, C. Scaling relationship on learning mathemat-ical reasoning with large language models. CoRR ,abs/2308.01825, 2023. URL https://doi.org/10. 48550/arXiv.2308.01825 .Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Boot-strapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476–15488, 2022. 10 T2T 

Zhang, K., Gao, S., Hong, Y., Sun, H., Bao, J., Jiang, H., Song, Y., Dingqian, H., and Xiong, H. Rspo: Risk-seeking policy optimization for pass@ k and max@ k metrics in large language models. arXiv preprint arXiv:2508.01174 , 2025. Zhou, D., Sch ¨arli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625 ,2022. Zhu, X., Xia, M., Wei, Z., Chen, W.-L., Chen, D., and Meng, Y. The surprising effectiveness of negative rein-forcement in LLM reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems ,2025. URL https://openreview.net/forum? id=ftVlLG9cks .11 T2T 

## A. Related Works: Relation between TTS and RLVR 

Test-Time Scaling and Inference Search. The paradigm of scaling test-time computation has emerged as a complementary avenue to scaling model parameters and training data (Brown et al., 2020; Kaplan et al., 2020). Foundational work in Chain-of-Thought (CoT) prompting demonstrates that inducing intermediate reasoning steps significantly boosts performance on complex tasks (Wei et al., 2022; Kojima et al., 2022). Building on this, recent approaches explicitly structure inference as a search process. Techniques such as Self-Consistency (Wang et al., 2022) utilize majority voting over repeated sampling to marginalize out reasoning errors. More advanced methods adopt tree-based search algorithms, including Tree of Thoughts (ToT) (Yao et al., 2023) and Monte Carlo Tree Search (MCTS) strategies (Zhou et al., 2022; Feng et al., 2023), to explore the reasoning space systematically. While these methods effectively trade inference compute for accuracy—a phenomenon formalized as test-time scaling laws (Snell et al., 2024; Wu et al., 2024)—they rely on external scaffolding and often incur prohibitive latency costs. Our work aims to achieve similar benefits not by external search, but by shaping the policy to intrinsically perform “search-like” exploration during the generation process. 

Internalizing Search into Training. A growing body of work focuses on distilling the capabilities of heavy inference-time search into “System 1” models (Khan et al., 2024; Magister et al., 2023). Typically, this involves fine-tuning models on the successful traces generated by brute-force search or advanced reasoning agents (Polu et al., 2022; Yuan et al., 2023). While effective, these distillation approaches are offline and depend on the quality of the static dataset. In contrast, Reinforcement Learning with Verifiable Rewards (RLVR) provides an online alternative, where exploration and credit assignment are driven directly by outcome-based feedback, without requiring a pre-collected dataset of search trees. From this perspective, RLVR can be viewed as a form of amortized inference (Gershman & Goodman, 2014), shifting the cost of search from test time into training. In T2T, we use a competence-conditioned reward to drive an online curriculum: “thickening” reasoning to discover solutions for hard queries, and subsequently “thinning” them to internalize the efficiency of the final policy. This aligns with recent observations that effective reasoning models must learn to dynamically allocate computation based on problem difficulty (Huang et al., 2023; Chen et al., 2025a). 

## B. Implementation Details 

B.1. Hyperparameters 

We provide the detailed hyperparameter configurations used in our experiments in Table 3. All models are trained using the 

Verl (Sheng et al., 2025) framework with the settings specified below.  

> Table 3. Hyperparameter settings for T2T.

Hyperparameter Value 

Learning Rate 1e-6 Train Batch Size 256 PPO Mini-Batch Size 64 Max Response Length 8192 Max Prompt Length 1024 Rollout Temperature 1.0 Evaluation Temperature 0.7 Evaluation Top-p 0.95 Clip Ratio High 0.2 Clip Ratio Low 0.2 Total Training Steps 275 

T2T Reward Parameters 

Scaling Factor ( α) 0.2 Length Norm Max ( Lmax ) 4096 Length Norm Min ( Lmin ) 012 T2T 

B.2. Baseline Implementation Details 

To ensure a fair comparison and reproducibility, we strictly follow the hyperparameter recommendations provided in the respective original publications for all baseline methods. The specific configurations are detailed below: 

GRPO. We utilize the standard implementation of Group Relative Policy Optimization (Guo et al., 2025) without any additional reward shaping or gradient modification terms. 

LASER. Following Liu et al. (2025a), we configure the length-penalty reward shaping to encourage conciseness in correct responses. We set the penalty coefficient and the target length threshold as follows: • Penalty Coefficient ( α): 0.2

• Target Length ( Ltarget ): 4096 

This target length aligns with the maximum response length used in our main experiments. 

W-REINFORCE. We adopt the W-REINFORCE baseline as recommended by Zhu et al. (2025). This method scales down the gradients of positive samples while retaining full gradients for negative samples to encourage exploration. • Positive Sample Weight ( ρ): 0.1

EntroPIC. For the entropy-stabilization method EntroPIC (Yang et al., 2025), we employ the Proportional-Integral (PI) control mechanism to dynamically adjust loss weights. The specific control parameters are set as: • Target Entropy ( Htarget ): 0.1

• Proportional Gain ( Kp): 1.0

• Integral Gain ( Ki): 0.01 

These values correspond to the configuration for standard mathematical reasoning tasks as described in the original paper. 

## C. Generalization Capabilities 

A critical concern in domain-specific reinforcement learning is the potential risk of overfitting to the target domain (mathematics), which may degrade the model’s performance on other tasks—a phenomenon often referred to as the “alignment tax.” To evaluate the robustness and transferability of T2T, we conduct zero-shot evaluations on two out-of-domain benchmarks: BBH (Big Bench Hard) (Suzgun et al., 2023) for general logical reasoning and HumanEval (Chen, 2021) for code generation. We evaluate the generalization capabilities of the Initial Model , standard GRPO, and other baseline methods (LASER, W-REINFORCE, EntroPIC) on Qwen2.5-3B and Qwen3-4B. The results are summarized in Table 4. 

Generalization to Broader Reasoning Tasks. We observe that reinforcement learning demonstrates robust generalization capabilities beyond mathematical domains. As shown in Table 4, all evaluated RL methods maintain or slightly improve upon the Initial Model’s performance on out-of-domain benchmarks, such as BBH (general reasoning) and HumanEval (coding). This suggests that the reasoning patterns acquired during mathematical training effectively transfer to broader logical and algorithmic tasks. T2T aligns with this trend, confirming that our method enhances domain-specific performance while preserving general reasoning abilities. 

## D. Full Experimental Results 

D.1. Detailed Experimental Results on Mathematical Dataset 

In this section, we provide the comprehensive performance metrics for all evaluated models across the four benchmarks. To thoroughly analyze the exploration capabilities of different methods, we report Pass@ k for k ∈ { 1, 2, 4, 8, 16 , 32 , 64 }.Tables 5, 6, 7, and 8 present the results for MATH-500, AIME’24, AIME’25, and AMC’23, respectively. 13 T2T 

Table 4. Generalization Performance on Out-of-Domain Tasks. We report the average 3-shot Exact Match (EM) across all 27 subtasks of BBH (Reasoning) and Pass@k (k ∈ { 1, 5, 10 }) for HumanEval (Coding). Detailed breakdowns of performance on each BBH subtask are provided in Appendix D.2. 

Base Model Method BBH (Reasoning) HumanEval (Coding) 

3-shot EM Pass@1 Pass@5 Pass@10 Qwen2.5-3B Initial Model 55.3 61.5 78.2 84.5 GRPO 55.9 64.2 80.5 86.0 LASER 56.0 64.5 80.8 86.4 W-REINFORCE 55.9 64.1 80.4 86.1 EntroPIC 55.7 63.8 80.0 85.8 

T2T (Ours) 55.8 64.2 80.5 86.2 Qwen3-4B Initial Model 79.3 72.0 86.5 91.0 GRPO 80.8 75.2 88.5 92.8 LASER 80.7 75.5 88.8 93.0 W-REINFORCE 80.7 75.1 88.4 92.6 EntroPIC 80.0 74.8 88.0 92.2 

T2T (Ours) 80.6 75.7 89.0 93.5 

Table 5. Full Results on MATH-500 . Comparison of Pass@ k (k = 1 , 2, 4, 8, 16 , 32 , 64 ). 

Base Model Method Pass@1 Pass@2 Pass@4 Pass@8 Pass@16 Pass@32 Pass@64 

DeepSeek-R1-Distill-Qwen-1.5B Initial Model 66.0 72.2 76.6 79.7 82.1 83.8 85.0 GRPO 68.9 73.8 77.3 79.9 81.8 83.2 84.0 LASER 66.5 72.6 77.0 80.2 82.5 84.1 85.4 W-REINFORCE 67.5 73.5 77.5 80.5 82.8 84.5 85.8 EntroPIC 66.2 72.4 76.8 79.5 81.9 83.5 84.8 

T2T 66.9 73.3 77.8 81.0 83.4 85.0 86.1 

Qwen2.5-3B Initial Model 40.6 43.4 44.9 46.7 47.9 48.7 49.1 GRPO 47.4 51.6 53.3 56.6 58.3 59.5 59.9 LASER 48.4 50.9 53.2 55.2 56.5 57.3 57.9 W-REINFORCE 46.5 51.1 53.1 56.7 58.6 59.9 60.2 EntroPIC 44.6 49.0 52.7 55.8 58.1 60.0 61.3 

T2T 49.8 53.1 56.3 58.9 61.1 63.0 64.6 

Qwen3-4B Initial Model 80.2 83.3 85.7 87.6 89.1 90.3 91.2 

GRPO 81.2 84.1 86.3 88.0 89.1 89.9 90.5 LASER 80.6 83.9 86.4 88.3 89.4 90.2 90.7 W-REINFORCE 80.9 84.1 86.5 88.1 89.2 90.0 90.6 EntroPIC 80.5 83.8 86.1 87.9 89.0 90.1 90.8 

T2T 81.2 84.5 86.9 88.6 89.8 90.6 91.1 Qwen3-14B Initial Model 82.2 84.0 85.3 86.1 86.6 87.0 87.2 GRPO 83.3 85.1 86.4 87.3 88.0 88.5 88.8 

T2T 85.1 87.5 89.1 90.4 91.4 92.2 92.7 

14 T2T 

Table 6. Full Results on AIME’24 . Comparison of Pass@ k (k = 1 , 2, 4, 8, 16 , 32 , 64 ). 

Base Model Method Pass@1 Pass@2 Pass@4 Pass@8 Pass@16 Pass@32 Pass@64 

DeepSeek-R1-Distill-Qwen-1.5B Initial Model 20.5 25.2 30.0 34.9 39.7 43.7 46.9 GRPO 20.6 22.5 27.1 32.7 38.8 44.1 47.7 LASER 20.8 22.9 27.6 33.2 39.4 44.8 48.3 W-REINFORCE 20.4 22.1 26.8 32.0 38.1 43.2 46.8 EntroPIC 21.0 23.5 28.5 34.5 40.8 45.9 49.5 

T2T 21.3 23.7 29.9 36.2 42.1 47.3 50.7 

Qwen2.5-3B Initial Model 7.1 7.4 8.1 9.1 10.8 12.7 14.7 GRPO 4.8 5.8 7.0 8.4 10.0 12.1 14.7 LASER 6.0 6.9 7.7 8.7 9.9 11.2 12.2 W-REINFORCE 12.6 14.5 16.4 17.6 18.8 20.3 22.8 

EntroPIC 7.2 7.9 8.9 10.4 12.9 15.9 18.3 

T2T 9.6 11.3 13.6 15.8 18.1 20.0 21.7 Qwen3-4B Initial Model 22.2 24.2 26.1 28.2 31.2 34.2 37.0 GRPO 29.3 32.2 35.7 39.6 44.7 47.0 48.6 LASER 28.2 31.0 32.6 34.5 39.8 43.3 45.4 W-REINFORCE 28.5 31.1 34.1 37.6 44.2 48.2 50.7 EntroPIC 28.8 31.5 33.8 36.2 42.5 45.8 47.9 

T2T 33.4 37.6 41.9 46.1 50.7 54.5 56.3 

Qwen3-14B Initial Model 27.1 32.1 35.9 38.7 41.4 44.6 48.3 GRPO 29.1 34.3 39.3 44.4 49.3 53.5 56.1 

T2T 39.6 45.7 50.6 53.6 55.4 57.3 59.6 

Table 7. Full Results on AIME’25 . Comparison of Pass@ k (k = 1 , 2, 4, 8, 16 , 32 , 64 ). 

Base Model Method Pass@1 Pass@2 Pass@4 Pass@8 Pass@16 Pass@32 Pass@64 

DeepSeek-R1-Distill-Qwen-1.5B Initial Model 14.8 17.5 19.0 20.5 22.6 25.8 28.2 GRPO 14.8 17.6 20.2 22.4 24.4 26.8 28.9 

LASER 14.9 17.8 19.5 21.2 23.1 26.1 28.4 W-REINFORCE 14.7 17.4 19.1 20.8 22.9 25.5 28.0 EntroPIC 15.0 17.9 20.0 21.9 23.5 26.4 28.5 

T2T 15.2 18.4 21.4 23.8 25.7 27.5 28.8 Qwen2.5-3B Initial Model 3.4 3.4 3.5 3.6 3.8 4.1 4.6 GRPO 7.7 8.5 9.7 11.1 12.5 14.2 15.6 LASER 4.6 6.9 7.7 8.7 9.9 11.1 12.2 W-REINFORCE 5.8 7.2 8.8 10.5 12.6 14.4 15.9 EntroPIC 3.6 4.0 4.5 5.4 6.7 8.1 9.2 

T2T 5.4 7.1 9.3 11.7 13.8 15.2 16.7 

Qwen3-4B Initial Model 14.7 17.5 21.6 26.4 31.0 33.9 35.4 GRPO 21.3 24.6 29.2 33.8 39.3 43.9 48.5 LASER 21.3 23.3 27.0 32.0 37.7 43.7 46.9 W-REINFORCE 20.8 24.3 27.7 35.4 40.2 45.2 50.9 

EntroPIC 21.0 24.0 28.5 33.5 39.0 44.5 48.0 

T2T 23.0 26.7 31.4 36.1 41.2 46.3 49.6 Qwen3-14B Initial Model 19.6 22.3 26.3 31.7 37.7 43.2 47.3 GRPO 22.4 24.5 28.0 33.3 39.2 45.7 51.5 

T2T 24.5 29.1 34.7 41.1 48.1 54.2 57.9 

15 T2T 

Table 8. Full Results on AMC’23 . Comparison of Pass@ k (k = 1 , 2, 4, 8, 16 , 32 , 64 ). 

Base Model Method Pass@1 Pass@2 Pass@4 Pass@8 Pass@16 Pass@32 Pass@64 

DeepSeek-R1-Distill-Qwen-1.5B Initial Model 50.9 59.7 67.7 74.6 79.7 83.7 86.4 GRPO 57.3 65.3 73.5 80.6 86.1 89.9 92.7 

LASER 57.8 65.8 73.9 80.8 85.8 89.2 91.5 W-REINFORCE 58.2 66.1 74.0 81.0 86.0 89.4 91.6 EntroPIC 58.5 66.4 74.2 81.1 85.9 89.3 91.4 

T2T 59.0 66.9 74.5 81.3 86.2 89.5 91.7 Qwen2.5-3B Initial Model 43.9 47.7 50.8 53.4 55.8 57.8 59.0 GRPO 49.1 52.9 55.8 57.5 59.2 61.3 63.4 LASER 51.9 54.5 57.1 60.0 62.4 64.6 66.2 NSR 52.5 56.6 60.4 63.7 66.5 68.8 70.9 EntroPIC 54.7 59.0 62.7 65.9 68.7 70.6 71.9 

T2T 57.0 61.6 65.6 69.8 73.2 75.8 77.7 

Qwen3-4B Initial Model 71.4 76.3 80.0 83.3 85.7 90.6 92.4 GRPO 75.8 79.1 82.6 87.3 88.9 89.7 91.2 LASER 76.1 78.1 80.1 86.4 88.8 90.2 91.1 W-REINFORCE 75.8 78.7 81.2 88.3 89.1 92.6 93.9 EntroPIC 75.9 78.5 81.5 87.0 88.5 90.0 91.5 

T2T 78.0 81.7 85.1 90.0 92.0 93.5 95.3 

Qwen3-14B Initial Model 74.6 80.1 84.6 87.5 89.0 90.0 91.2 GRPO 80.7 85.1 87.4 89.4 91.6 93.5 94.6 

T2T 81.4 86.7 90.0 92.2 94.4 96.7 98.4 

D.2. Detailed BBH Experimental Results 

In this section, we present the comprehensive breakdown of the Big Bench Hard (BBH) benchmark results in Table 9 and Table 10. We report the exact scores for all 27 subtasks across Qwen2.5-3B and Qwen3-4B .16 T2T  

> Table 9. Full BBH Results on Qwen2.5-3B.

Subtask Initial Model GRPO LASER W-REINFORCE EntroPIC T2T (Ours) 

Boolean Expressions 78.00 82.00 82.40 82.80 76.80 82.40 Causal Judgement 59.89 60.96 60.96 60.43 58.82 61.50 

Date Understanding 59.60 60.40 60.00 59.20 58.40 58.80 Disambiguation QA 58.40 58.40 58.40 58.40 57.60 58.40 Dyck Languages 4.00 3.60 3.60 4.00 4.00 4.00 Formal Fallacies 54.40 55.20 55.60 52.80 56.40 52.00 Geometric Shapes 32.80 33.20 33.20 32.80 31.60 32.00 Hyperbaton 80.40 80.40 80.00 80.40 81.20 80.80 Logical Deduction (Five) 40.40 41.60 40.40 40.80 40.80 41.20 Logical Deduction (Seven) 31.20 30.00 30.40 31.20 30.00 30.40 Logical Deduction (Three) 67.20 67.20 67.20 67.20 67.60 67.20 Movie Recommendation 66.80 66.40 66.40 66.40 67.20 66.40 Multi-Step Arithmetic 76.00 77.60 78.00 76.80 78.40 77.60 Navigate 84.40 84.40 84.40 84.00 84.80 84.00 Object Counting 78.40 78.80 78.80 79.20 76.80 79.20 

Penguins in a Table 63.70 65.07 64.38 65.07 63.70 65.07 

Reasoning about Colored Objects 72.80 71.60 74.00 72.80 73.20 72.40 Ruins Names 48.00 47.60 47.20 47.20 49.60 47.20 Salient Translation Error 30.80 40.00 40.00 40.40 35.60 40.80 

Snarks 60.11 60.11 60.11 59.55 61.24 58.99 Sports Understanding 73.60 73.60 73.60 73.20 74.40 74.00 Temporal Sequences 32.40 30.40 30.80 30.80 31.60 30.80 Tracking Shuffled Objects (Five) 36.80 38.00 38.80 37.60 38.40 37.60 Tracking Shuffled Objects (Seven) 27.60 26.80 27.20 29.60 28.00 28.00 Tracking Shuffled Objects (Three) 60.80 60.80 61.20 61.20 61.60 61.20 Web of Lies 99.60 99.60 99.60 99.60 99.60 99.60 Word Sorting 14.40 14.80 15.20 14.40 15.20 14.80 

Average Score 55.28 55.87 55.99 55.85 55.65 55.79 

17 T2T  

> Table 10. Full BBH Results on Qwen3-4B.

Subtask Initial Model GRPO LASER W-REINFORCE EntroPIC T2T (Ours) 

Boolean Expressions 95.70 97.60 97.60 96.80 97.20 96.00 Causal Judgement 59.70 58.82 59.89 55.08 61.50 56.68 Date Understanding 81.60 83.60 83.20 83.20 84.00 83.60 Disambiguation QA 58.50 60.80 60.40 62.00 59.20 62.40 

Dyck Languages 38.50 39.60 38.80 39.20 36.80 39.60 

Formal Fallacies 75.00 73.60 72.80 74.00 78.40 72.80 Geometric Shapes 51.50 54.40 52.80 53.20 46.80 51.20 Hyperbaton 91.50 90.80 91.60 90.80 91.20 91.60 

Logical Deduction (Five) 67.50 71.60 68.80 68.40 70.40 69.60 Logical Deduction (Seven) 37.80 27.60 30.80 28.40 31.60 27.20 Logical Deduction (Three) 95.50 96.00 96.00 96.00 96.80 96.40 Movie Recommendation 67.50 68.80 69.20 69.20 69.60 68.80 Multi-Step Arithmetic 98.20 98.40 98.80 98.80 99.60 98.40 Navigate 90.50 92.80 92.80 92.80 93.60 92.80 Object Counting 84.50 86.00 86.00 86.00 84.80 85.60 Penguins in a Table 90.80 94.52 93.15 94.52 91.78 95.21 

Reasoning about Colored Objects 94.80 96.00 96.00 96.40 96.80 96.40 Ruins Names 68.00 70.00 72.40 68.00 69.60 66.80 Salient Translation Error 60.80 62.80 61.60 62.00 63.60 62.40 Snarks 69.50 69.66 66.85 68.54 67.98 69.66 

Sports Understanding 81.80 84.40 84.40 84.40 83.60 84.80 

Temporal Sequences 84.00 86.00 86.00 86.40 86.40 86.00 Tracking Shuffled Objects (Five) 95.50 96.40 97.60 97.60 97.60 97.20 Tracking Shuffled Objects (Seven) 89.00 92.80 92.80 93.20 89.60 93.20 

Tracking Shuffled Objects (Three) 85.50 91.60 91.20 94.40 75.60 92.40 Web of Lies 100.00 100.00 100.00 100.00 100.00 100.00 

Word Sorting 35.80 37.20 36.40 38.00 37.60 39.60 Average Score 79.33 80.82 80.73 80.70 80.05 80.57 

## E. Detailed Analysis of Response Length Dynamics 

In this section, we provide a granular analysis of how T2T modulates the response length across different model scales and correctness outcomes. 

E.1. Adaptive Length Modulation across Model Scales 

Figure 5 illustrates the evolution of average response length during training. A striking divergence is observed between the two model scales, which strongly validates the adaptive nature of our T2T mechanism. On Qwen2.5-3B , the response length for T2T increases relative to the initial stage. This suggests that for the smaller model, the training queries pose a higher level of intrinsic difficulty. Consequently, the “Thickening” incentive dominates the reward signal, encouraging the model to generate longer chains of thought to bridge reasoning gaps. This aligns with the entropy increase observed in Figure 4, confirming that the model is actively exploring via expansion. Conversely, on 

Qwen3-4B , the T2T response length exhibits a downward trend. Since the 4B model possesses stronger intrinsic reasoning capabilities, it achieves high success rates earlier in training. As the model gains confidence, the “Thinning” penalty takes precedence, incentivizing the pruning of redundant tokens to converge on more concise solutions. This contrast highlights a key advantage of T2T: it does not enforce a static length bias (e.g., always longer or always shorter). Instead, it acts as a 

competence-aware regulator , dynamically allocating the computational budget based on the model’s mastery of the task. 18 T2T 0 50 100 150 200 250              

> Step
> 1000
> 1500
> 2000
> 2500
> 3000
> 3500
> Response Length
> T2T GRPO LASER W-REINFORCE ENTROPIC 050 100 150 200 250
> Step
> 1500
> 1750
> 2000
> 2250
> 2500
> 2750
> 3000
> 3250
> Response Length
> T2T GRPO LASER W-REINFORCE ENTROPIC

Figure 5. Response Length Evolution. The plot on the left corresponds to Qwen2.5-3B , and the plot on the right corresponds to 

Qwen3-4B .0 20 40 60 80 100 120 140 

> Step
> 1250
> 1300
> 1350
> 1400
> 1450
> 1500
> 1550
> 1600
> Length (Tokens)
> Correct Response Length

(a) Correct Responses (Positive) 0 20 40 60 80 100 120 140  

> Step
> 2500
> 2600
> 2700
> 2800
> 2900
> 3000
> 3100
> Length (Tokens)
> Incorrect Response Length

(b) Incorrect Responses (Negative) 

Figure 6. Divergent Length Dynamics based on Correctness (Qwen3-14B). Analysis of token length evolution for (a) Correct and (b) Incorrect samples. Correct responses systematically shorten (Thinning) while incorrect responses lengthen (Thickening), confirming the bi-modal optimization strategy of T2T. 

E.2. Disentangling Correct and Incorrect Response Dynamics 

To rigorously verify that the observed length changes are causally linked to response correctness—as dictated by our T2T reward formulation—we decompose the response length evolution into positive (correct) and negative (incorrect) samples. Figure 6 presents this breakdown for the Qwen3-14B model. As shown in Figure 6, the model exhibits a striking divergence in behavior depending on the outcome: • Thinning for Correctness: The length of positive samples shows a consistent downward trend. This confirms that when the model successfully solves a problem, the adaptive length penalty effectively encourages the policy to converge towards the most efficient reasoning path. • Thickening for Failure: Conversely, the length of negative samples increases over time. This indicates that when the model fails to reach the correct answer, the reward formulation incentivizes it to generate longer sequences,forcing the model to explore alternative reasoning steps. 

Impact of Transitioning Samples. It is worth noting that the divergence shown in Figure 6 likely underestimates the true strength of the T2T mechanism due to the dynamic nature of training. As the model improves (see accuracy curves), many difficult problems that initially resulted in long, incorrect responses eventually transition into the ”correct” category. Since these newly solved problems often require complex (and thus longer) reasoning chains compared to easy problems, their migration into the positive pool partially offsets the length reduction of the originally correct samples. Similarly, their removal from the negative pool dampens the average length increase of the remaining incorrect samples. Therefore, 19 T2T 0 50 100 150 200  

> Step
> 0.525
> 0.550
> 0.575
> 0.600
> 0.625
> 0.650
> 0.675
> 0.700
> Accuracy
> Alpha = 0.2 Alpha = 0.1

(a) Training Accuracy 0 50 100 150 200   

> Step
> 1700
> 1800
> 1900
> 2000
> 2100
> 2200
> response_length/mean
> Alpha = 0.2 Alpha = 0.1

(b) Average Response Length 

Figure 7. Training Dynamics with different α. Comparing α = 0 .1 and α = 0 .2 on Qwen-14B. (a) Both settings achieve similar convergence rates and final accuracy. (b) The response length dynamics show that a larger α induces a slightly stronger modulation in token generation, yet both stabilize effectively. 

the intrinsic drive of T2T to shorten correct paths and lengthen incorrect exploration is likely even stronger than what the aggregated statistics suggest. This clear separation of dynamics provides strong empirical evidence that T2T creates a bi-modal strategy : efficient execution for known knowledge, and extensive exploration for unknown territories. 

## F. Hyperparameter Sensitivity Analysis 

A practical reinforcement learning algorithm should be robust to hyperparameter choices and not require exhaustive tuning. To verify the sensitivity of T2T to the length scaling factor α, we conducted experiments on the Qwen-14B model comparing two distinct settings: α = 0 .1 and α = 0 .2.The parameter α controls the magnitude of the length incentive relative to the correctness reward. As derived in Eq. (12) ,the theoretical constraint α < 0.5 ensures that any correct response yields a higher reward than any incorrect response, regardless of length. Within this valid range, α merely adjusts the ”strength” of the shaping signal rather than its direction. The results are summarized in Table 11. We observe that the performance differences between α = 0 .1 and α = 0 .2 are marginal across benchmarks. For instance, the Pass@1 score on MATH-500 fluctuates by less than 1%, and the exploration capability (Pass@64) remains consistently high. This indicates that our method is robust to the selection of α: as long as the parameter is set within a reasonable range (sufficiently large to provide a gradient signal but satisfying α < 0.5), the T2T mechanism effectively modulates the reasoning length without requiring precise tuning. 

Table 11. Sensitivity Analysis of α on Qwen3-14B. We compare the performance with α = 0 .1 and α = 0 .2. The method demonstrates high stability, showing that precise tuning of the scaling factor is not critical for success. 

Configuration MATH-500 AIME’24 AIME’25 AMC’23 

Pass@1 Pass@64 Pass@1 Pass@64 Pass@1 Pass@64 Pass@1 Pass@64 

α = 0 .1 85.0 92.2 39.9 60.1 24.9 58.3 80.6 98.2 

α = 0 .2 85.1 92.7 39.6 59.6 24.5 57.9 81.4 98.4 

Analysis of Training Dynamics. To further understand how the scaling factor α influences the optimization process, we visualize the evolution of training accuracy and response length in Figure 7. As illustrated in Figure 7, the model exhibits highly consistent behavior across different shaping magnitudes ( α = 0 .1 and 

α = 0 .2). Both the learning trajectories (Figure 7a) and the response length dynamics (Figure 7b) follow nearly identical patterns. This stability confirms that the proposed T2T method is robust to the hyperparameter α, consistently guiding the model towards an efficient reasoning policy without requiring precise tuning. 20 T2T 

## G. Case Study 

We present detailed comparisons between the baseline GRPO and our method to illustrate the adaptive nature of the Thickening-to-Thinning (T2T) mechanism. The following visualizations display two contrasting scenarios: (1) a Thickening Case on a hard trigonometric problem, where our method is incentivized to explore a rigorous derivation path—solving a system of linear equations—to correct the baseline’s failure; and (2) a Thinning Case on a simple arithmetic problem, where our method effectively prunes the baseline’s redundant conversational fillers to achieve inference efficiency without compromising accuracy. 

Case 1: Thickening Case Prompt 

There exist constants a, b, c, and d such that 

(sin x)7 = a sin 7 x + b sin 5 x + c sin 3 x + d sin x

for all angles x. Find d. Let’s think step by step and output the final answer in boxed format. 

GRPO 

We are given the identity: 

(sin x)7 = a sin 7 x + b sin 5 x + c sin 3 x + d sin x

for all angles x, and we are to find the constant d.

Step 1: Use the identity for sin 7 x

We can use the multiple-angle identity or power-reduction formulas to express sin 7 x in terms of sines of multiple angles. Alternatively, we can use the complex exponential form of sine: 

sin x = eix − e−ix 

2i .

So: 

sin 7 x =

 eix − e−ix 

2i

7

.

This will expand into a sum of terms involving eikx , and we can then take the imaginary part to get a sum of sines. But this might be tedious. Instead, we can use a known identity or the orthogonality of sine functions .

Step 2: Use the identity for sin 7 x

We can use the identity: 

sin 7 x = 78 sin x − 78 sin 3 x + 78 sin 5 x − 18 sin 7 x. 

This is a known identity derived from the multiple-angle expansions of sine powers. Let us verify this identity by comparing coefficients. 

Step 3: Compare with the given expression 

We are given: 

(sin x)7 = a sin 7 x + b sin 5 x + c sin 3 x + d sin x. 

From the identity above: 

sin 7 x = − 18 sin 7 x + 78 sin 5 x − 78 sin 3 x + 78 sin x. 

So matching terms: • a = − 18

• b = 78

21 T2T 

• c = − 78

• d = 78

Final Answer: 

78

T2T 

We are given the identity: 

(sin x)7 = a sin 7 x + b sin 5 x + c sin 3 x + d sin x

for all angles x, and we are to find the constant d.

Step 1: Use the identity for sin 7 x

We can use the multiple-angle identities or power-reduction formulas to express sin 7 x in terms of sines of multiple angles. Alternatively, we can use the complex exponential form of sine: 

sin x = eix − e−ix 

2i .

So: 

sin 7 x =

 eix − e−ix 

2i

7

.

This will expand into a sum of terms involving eikx , and we can then take the imaginary part to get back to sines. But this might be tedious. Instead, we can use a known identity or use symmetry and evaluate the equation at specific values of x to find the coefficients. 

Step 2: Use symmetry and evaluate at specific values of x

Let us consider the original equation: 

(sin x)7 = a sin 7 x + b sin 5 x + c sin 3 x + d sin x. 

We can plug in specific values of x to get equations involving a, b, c, d .

Case 1: x = 0 

• sin 0 = 0 , so LHS = 0 7 = 0 

• RHS = a sin 0 + b sin 0 + c sin 0 + d sin 0 = 0 

So this gives no new information. 

Case 2: x = π

> 2

• sin π 

> 2

= 1 , so LHS = 1 7 = 1 

The RHS is: 

a sin 7π

2 + b sin 5π

2 + c sin 3π

2 + d sin π

2 .

We compute each term: • sin 7π 

> 2

= sin  3π + π

> 2

 = −1

• sin 5π 

> 2

= sin  2π + π

> 2

 = 1 

• sin 3π 

> 2

= −1

22 T2T 

• sin π 

> 2

= 1 

So: RHS = −a + b − c + d. 

Setting RHS equal to LHS gives: 

−a + b − c + d = 1 (1) 

Case 3: x = π

> 6

• sin π 

> 6

= 12 , so LHS =   12

7 = 1128 

The RHS is: 

a sin 7π

6 + b sin 5π

6 + c sin 3π

6 + d sin π

6 .

Compute each sine: • sin 7π 

> 6

= sin  π + π

> 6

 = − 12

• sin 5π 

> 6

= sin  π − π

> 6

 = 12

• sin 3π 

> 6

= sin π 

> 2

= 1 

• sin π 

> 6

= 12

So: RHS = − a

2 + b

2 + c + d

2 .

Setting equal to LHS: 

− a

2 + b

2 + c + d

2 = 1128 (2) 

Case 4: x = π

> 3

• sin π 

> 3

= √32

• LHS =

 √32

7

= 27 √3128 

The RHS is: 

a sin 7π

3 + b sin 5π

3 + c sin π + d sin π

3 .

Compute each sine: • sin 7π 

> 3

= sin  2π + π

> 3

 = √32

• sin 5π 

> 3

= − √32

• sin π = 0 

• sin π 

> 3

= √32

So: RHS =

√32 (a − b + d).

Setting equal to LHS: √32 (a − b + d) = 27 √3128 

⇒ a − b + d = 27 64 (3) Now we have three equations: 

23 T2T 

• (1) −a + b − c + d = 1 

• (2) − a 

> 2

+ b 

> 2

+ c + d 

> 2

= 1128 

• (3) a − b + d = 27 64 

We are to find d.From (3): 

a = b − d + 27 64 (4) Substitute into (1): 

2d − c = 91 64 (5) Substitute into (2): 

d + c = 732 (6) Adding (5) and (6): 

3d = 105 64 ⇒ d = 35 64 .

Final Answer: 

35 64 

Case 2: Thinning Case Prompt 

How many positive whole-number divisors does 196 have? Let’s think step by step and output the final answer in boxed format. 

GRPO 

We are asked to find how many positive whole-number divisors the number 196 has. 

Step 1: Prime Factorization 

We start by factoring 196 into its prime factors. 

196 = 2 × 98 = 2 × 2 × 49 = 2 2 × 72

So, the prime factorization of 196 is: 

196 = 2 2 × 72

Step 2: Use the Divisor Count Formula 

If a number n has the prime factorization: 

n = pe1 

> 1

× pe2 

> 2

× · · · × pek 

> k

,

then the number of positive divisors of n is: 

(e1 + 1)( e2 + 1) . . . (ek + 1) .

For 196, the exponents are: • 22 → exponent = 2 • 72 → exponent = 2 

24 T2T 

So the number of positive divisors is: 

(2 + 1)(2 + 1) = 3 × 3 = 9 .

Final Answer: 

9

So, 196 has 9 positive whole-number divisors .

T2T 

To find the number of positive whole-number divisors of 196, we start by finding its prime factorization .

Step 1: Prime Factorization of 196 

196 = 2 × 98 = 2 × 2 × 49 = 2 2 × 72

So, the prime factorization is: 

196 = 2 2 × 72

Step 2: Use the Formula for Number of Divisors 

If a number 

n = pe1 

> 1

× pe2 

> 2

× · · · × pek 

> k

,

then the number of positive divisors is: 

(e1 + 1)( e2 + 1) . . . (ek + 1) .

For 196: • Exponent of 2 is 2 • Exponent of 7 is 2 So, the number of divisors is: 

(2 + 1)(2 + 1) = 3 × 3 = 9 

Final Answer: 

9

25