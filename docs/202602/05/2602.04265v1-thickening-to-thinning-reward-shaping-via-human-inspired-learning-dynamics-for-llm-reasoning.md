# Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning
# 由厚入薄：受人类学习动力学启发的奖励塑造方法，用于提升大语言模型推理能力

**Authors**: Wenze Lin, Zhen Yang, Xitai Jiang, Pony Ma, Gao Huang \\
**Date**: 2026-02-04 \\
**PDF**: https://arxiv.org/pdf/2602.04265v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:LNS</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 6.0 \\
**Evidence**: Dynamic reward framework to broaden search space during problem solving \\

---

## Abstract
Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes "thickening" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to "thinning", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.

## 摘要
带有可验证奖励的强化学习（RLVR）已成为增强大语言模型（LLM）推理能力的一种极具前景的范式。然而，它经常面临诸如熵坍

---

## 速览摘要（自动生成）

**问题**：LLM推理训练（RLVR）面临探索不足、冗余严重及难以平衡搜索深度与效率的问题。

**方法**：提出T2T动态奖励框架