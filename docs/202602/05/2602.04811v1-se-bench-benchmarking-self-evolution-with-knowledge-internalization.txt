Title: SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization

URL Source: https://arxiv.org/pdf/2602.04811v1

Published Time: Thu, 05 Feb 2026 02:18:30 GMT

Number of Pages: 20

Markdown Content:
# SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

Jiarui Yuan * 1 Tailin Jin * 1 Weize Chen * 1 Zeyuan Liu 1 Zhiyuan Liu 1 Maosong Sun 1

# Abstract 

True self-evolution requires agents to act as life-long learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hin-dered by two obstacles: the entanglement of prior knowledge , where “new” knowledge may appear in pre-training data, and the entangle-ment of reasoning complexity , where failures may stem from problem difficulty rather than an in-ability to recall learned knowledge. We introduce SE-B ENCH , a diagnostic environment that ob-fuscates the NumPy library and its API doc into a pseudo-novel package with randomized identi-fiers. Agents are trained to internalize this pack-age and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our inves-tigation reveals three insights: (1) the Open-Book Paradox , where training with reference documen-tation inhibits retention, requiring ”Closed-Book Training” to force knowledge compression into weights; (2) the RL Gap , where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the 

viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-BENCH establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https: //github.com/thunlp/SE-Bench .

# 1. Introduction 

Self-evolution, the capacity for an autonomous agent to re-cursively improve its own capabilities, is often viewed as a prerequisite for Artificial General Intelligence (AGI) (Go-  

> *Equal contribution 1Tsinghua University. Correspondence to: Weize Chen <chenwz21@mails.tsinghua.edu.cn >, Zhiyuan Liu
> <liuzy@tsinghua.edu.cn >.

ertzel & Pennachin, 2007; Legg & Hutter, 2006). An ideal self-evolving agent acts as a lifelong learner, continuously assimilating information from its environment, optimizing its solutions, and expanding its skill set without human in-tervention. However, current approaches often limit the scope of this evolution to transient or localized adaptations, such as inference-time response refinement (Novikov et al., 2025; Wang et al., 2025b) or iterative self-code modifi-cation (Zhang et al., 2025a; Wang et al., 2025a). While valuable, these mechanisms differ fundamentally from the expansive definition we explore here: the self-evolution requires agents to actively learn from experience by inter-nalizing novel skills or knowledge, akin to a human expert accumulating domain knowledge over time (Wang et al., 2024; Zhang et al., 2025b; Ouyang et al., 2025). Despite rapid progress in large language model (LLM) rea-soning capabilities, we lack a rigorous measurement for this foundational internalization ability. Existing bench-marks have made strides in evaluating specific sub-skills related to self-evolution, such as long-horizon information retrieval (Wei et al., 2025a; Li et al., 2025), iterative re-sponse refinement (Lee et al., 2025), and complex task execution (Team, 2025c; Jimenez et al., 2024) However, current evaluations fail to cleanly isolate an agent’s ability to process and restore experience due to two fundamental obstacles. First, the entanglement of prior knowledge :when a model solves a task involving “novel” knowledge, it is indistinguishable whether the agent learned from the relevant experience or merely recalled pre-training data. Second, the entanglement of reasoning complexity : if an agent fails a complex task, it is ambiguous whether it failed to internalize the necessary knowledge or failed to reason 

over it. This mirrors a student who memorizes a textbook but fails a frontier math problem due to logical difficulty rather than memory gaps. To address these limitations, we argue that the community needs a “Needle in a Haystack” test (Kamradt, 2023) for self-evolution: an environment where tasks are algorithmi-cally trivial if knowledge is internalized, and impossible if it is not. To this end, we introduce SE-B ENCH . SE-B ENCH 

relies on a knowledge obfuscation mechanism to create this clean environment. We employ a knowledge obfuscation 

mechanism, mapping the core functions of the NumPy (Har-ris et al., 2020) library to randomized, nonsense identifiers 1

> arXiv:2602.04811v1 [cs.CL] 4 Feb 2026 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization

(e.g., numpy.mean → zwc.kocito ) and rewriting the documentation to describe a “new” package. At training time, agents have access to the documentation, but at test time, agents are tasked with solving simple problems using this obfuscated package, with the strict constraint that any use of the original NumPy library is deemed to fail. This design grants SE-B ENCH three diagnostic properties: (1) Impossible without information : Without documentation, the probability of guessing the correct API is mathematically zero, eliminating prior knowledge confounds. (2) Trivial with information : Because the underlying logic maps 1-to-1 to standard NumPy, tasks are trivial for any agent that internalizes the mapping. Any ideal self-evolving method should theoretically achieve a near-100% success rate, thus cleanly isolating the internalization capability. (3) Com-positional generalization : While the training set consists of tasks solvable with single function calls, the test set re-quires composing multiple internalized functions, assessing generalization beyond simple memorization. Beyond serving as a rigorous metric, SE-B ENCH functions as a clean testbed that enables us to dissect the fundamen-tal mechanisms of self-evolution. We investigate whether standard parameter-optimization paradigms, specifically Su-pervised Fine-Tuning (SFT) and Reinforcement Learning (RL), can genuinely support the internalization capability. Our experiments uncover three critical insights: (1) The Open-Book Paradox: We find that the presence of refer-ence material during parameter update inhibits long-term retention. True internalization requires Closed-Book Train-ing : removing the documentation during parameter updates forces the model to compress external logic into its weights, significantly outperforming standard SFT. (2) The RL Gap :While SFT effectively internalizes new knowledge, standard RL fails even under the Closed-Book training setting. We identify that the negative gradient and PPO clipping (Schul-man et al., 2017) both are factors that impact the knowl-edge internalization for RL. (3) Viability of Self-Play: By applying SFT instead of RL to self-generated tasks and corresponding responses, models successfully internalize knowledge from their own noisy, unverified data, proving that self-evolution on knowledge internalization is viable if the correct optimization mechanism is used and that RL is not a one-size-fits-all solution. We position SE-B ENCH as a diagnostic testbed for the self-evolving agent community. Just as long-context models must at least demonstrate near-perfect retrieval on Needle-in-a-Haystack tests to establish basic competency, we argue that self-evolving agents should also demonstrate the ability to pass SE-B ENCH before they can be trusted to evolve in complex, open-ended environments. And because SE-BENCH provides a clean, controlled environment, it also serves as an ideal platform for studying the fundamental mechanisms for knowledge internalization, potentially facil-itating future research. 

# 2. SE-B ENCH 

We argue that a fundamental, yet often overlooked, compo-nent of self-evolution is knowledge internalization . While current methods often focus on transient adaptation, opti-mizing a solution within a single context window (Wang et al., 2025b; Qi et al., 2025; Wei et al., 2025b; Team, 2025b), genuine evolution requires transitioning from a stateless processor to a lifelong learner . A concrete exam-ple of such a process is a human software engineer learn-ing a new library: initially relying on documentation, but eventually internalizing the logic to solve problems fluently without external aid through repeated practice. Measuring such a capability of LLM agents in a similar scenario, however, presents a fundamental dilemma. We cannot evaluate the agent’s ability to internalize any exist-ing library (like Numpy), as they may have already been embedeed in the LLM’s pre-training weights (Shao et al., 2025; Wu et al., 2025). Furthermore, simply adopting a newly-released library is a fragile solution: as knowledge cutoff of the LLMs advances, the benchmark quickly be-comes obsolete. To rigorously measure the internalization 

ability of self-evolving methods, we require a domain that is permanently out-of-distribution: one that effectively does not exist on the Internet, ensuring that no further model can solve it with its pre-training knowledge. To this end, we introduce SE-B ENCH , a synthetic domain constructed by systematically obfuscating the NumPy li-brary paired with trivial coding problems. By mapping function names to nonsense identifiers while preserving the underlying logic, we create a “novel” package that remains structurally realistic yet alien to any model’s training distri-bution. This construction enforces three critical properties: • Impossible without Information: The randomized namespace guarantees a mathematical zero-shot baseline, eliminating pre-training confounds. • Trivial with Information: Because the logic is isomor-phic to standard NumPy, tasks are algorithmically trivial if the new API doc is provided, cleanly isolating memory 

failures from reasoning failures. • Compositional Generalization: By retaining the li-brary’s original structure, we can evaluate whether agents can compose internalized functions to solve multi-step problems beyond their specific training examples that in-volves only a single function. 

2.1. Benchmark Construction 

To ensure that SE-B ENCH serves as a rigorous evaluation of internalization, we implement a three-stage construction pipeline: Obfuscation , Question Generation , and Filtering .2SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

Figure 1: Overview of the SE-B ENCH construction pipeline. The process consists of three main stages: (1) Obfuscation ,where we implement a wrapper package zwc that renames selected NumPy functions and translates API documentation; (2) Generation , where Claude-4.5-sonnet generates valid tasks and test cases based on the original NumPy library; and (3) Filtering , where tasks are validated through strict consensus between three strong LLMs, followed by human verification. This process is illustrated in Figure 1. 

Stage I: Obfuscation. We select NumPy as our source domain due to its functional richness and simplicity. To construct our target library ZWC (a randomly generated package name), we identify a set of 268 common NumPy functions (see Section D.2) to serve as the core of the new package. Rather than simply renaming functions, we imple-ment ZWC as a wrapper package. Each function in ZWC is assigned a randomized, semantically void identifier ( e.g. ,

zwc.kocito ) which internally calls the corresponding NumPy function. And to prevent models from bypassing the obfuscation by invoking standard methods on the re-turned NumPy array objects (e.g., calling .mean() directly on an array), we wrap all inputs and outputs in a custom 

ZWCArray class. This ensures that the agent must strictly rely on the obfuscated functional API to manipulate data. To rewrite the accompanying documentation, we employ Gemini-2.5-Pro (Team, 2025a). We provide the model with the original NumPy docstrings and the global function map-ping, instructing it to translate the documentation into the context of the new ZWC package. This results in an API documentation that describes the “new” package. 

Step II: Question Generation. Prompting a model to gener-ate tasks directly using the obfuscated ZWC library is prone to hallucination, as the model lacks prior exposure to the new syntax. We therefore prompt Claude-4.5-sonnet (An-thropic, 2025) to generate simple coding problems relevant to the sampled original NumPy functions along with at least 8 test-cases. We employ a stratified generation strategy to create two distinct task categories: • Single-Function Tasks: We iterate through every func-tion in our function list. For each function, we prompt the model to create a self-contained problem that requires specifically that function to solve. This ensures 100% coverage of the functions included in SE-B ENCH .• Multi-Function Tasks: To test generalization, we ran-domly sample sets of 10 functions and prompt the model to generate a complex problem that requires the composi-tion of at least three functions from the sampled set. 

Step III: Filtering. To ensure the Trivial with Informa-tion property, we must verify that the generated tasks are not accidentally difficult or erroneous. We employ a strict 

Consensus Filtering protocol. We provide the generated questions (in their original NumPy form) to three distinct state-of-the-art models: Qwen3-Coder-480B (Team, 2025b), Gemini-2.5-Pro (Team, 2025a), and GPT-OSS-120B (Ope-nAI, 2025). A task is retained only if all three models can independently solve it and pass all test cases using standard NumPy. If three distinct model families can solve the ques-tion easily, we can be confident that a failure of a testing model on SE-B ENCH is due to a failure to learn the new API, not any error in the problem itself. Finally, we perform human verification on a 10% random subset of the filtered 3SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

data to ensure the clarity of the problem descriptions; all sampled tasks were found to be valid. 

2.2. Dataset Splits & Protocol 

We partition the filtered tasks into training and testing sets to rigorously evaluate the knowledge internalization. For 

training set , it only includes Single-Function tasks, and ensures that every function in the ZWC library appears at least once. For test set , it comprises both Single-Function 

tasks (to test retention on unseen problems) and Multi-Function tasks (to test compositional generalization). The core objective of SE-B ENCH is to measure internaliza-tion. Therefore, the information availability differs strictly between phases: • Training Phase: The agent is provided with the training set, each task includes a problem description and the 

relevant documentation for the involved function. The agent may use this phase to practice, memorize, or update its parameters. • Testing Phase: The agent is provided with the test set that includes only the problem descriptions without API documentation. To solve the task, the agent must rely entirely on the knowledge internalized during training. 

2.3. Metrics 

To ensure rigorous evaluation, we employ a strict Abstract Syntax Tree (AST) verification protocol. A solution is not judged merely on output correctness, but on its adherence to the benchmark constraints. Since ZWCArray is convertible to NumPy arrays, models might attempt to bypass the task by converting data to NumPy, performing operations, and converting back. To prevent this, we explicitly prohibit the import or usage of the original Numpy package. A solution is considered correct ( R(s) = 1 ) if and only if it meets three conditions: (1) it passes all provided test cases, (2) AST analysis confirms that the returned value relies on ZWC APIs, and (3) it contains zero imports of NumPy: 

R(s) = 

(

1, if all 3 conditions are met ,

0, otherwise . (1) 

2.4. Statistics and Validation 

SE-B ENCH comprises 1,417 tasks, partitioned into a train-ing set of 718 instances and a test set of 699 instances. To evaluate generalization capabilities, we stratify the test set by compositional complexity: it contains 259 Single-Function tasks, which require only single API calls, and 440 Multi-Function tasks that demand the composition of multiple APIs. Both splits maintain comprehensive cover-age of the ZWC surface area, and an illustrative example is provided in Section D.1. Table 1: Validation of SE-B ENCH Design Properties. 

Pass@64 of Qwen3-8B. Standard NumPy confirms the tasks are reasoning-trivial. ZWC Zero-Shot confirms no pre-training leakage. ZWC In-Context establishes a solv-ability ceiling when documentation is provided without training.          

> Evaluation Setting Single Multiple
> Reasoning Upper Bound (Using Standard NumPy)
> Standard NumPy 97.4 93.6 SE-B ENCH Strict Evaluation (Using ZWC)
> ZWC Zero-Shot 0.0 0.0
> ZWC In-Context 85.3 70.5

To validate the structural integrity of our design, we con-ducted a preliminary evaluation on our test set using Qwen3-8B under three distinct settings, as reported in Table 1. First, we evaluate the Standard NumPy setting, where the the model is allowed to use the original NumPy library. The high accuracy ( > 90%) across both splits confirms that the tasks are reasoning-trivial. Second, we test the ZWC Zero-Shot setting, where the model must use the obfuscated library and is denied access to API documentation. The re-sulting 0% accuracy confirms that our obfuscation is robust and prevents any leakage of pre-training knowledge. Finally, we examine the ZWC In-Context setting, where the model is provided with the API documentation relevant to the prob-lem. The recovery of performance demonstrates that the benchmark is solvable. Notably, the remaining gap (com-pared to >90% on NumPy) is primarily due to hallucination: qualitative analysis reveals that Qwen3-8B frequently tries to use NumPy namespace ( e.g., zwc.mean ) despite the provided relevant documentation. This underscores a core objective for self-evolution: methods must enable agents to strictly adhere to real, internalized knowledge. 

# 3. Experiment 

Baselines: We evaluate a diverse suite of self-evolution strategies across three paradigms. (1) Memory-based: ACE (Zhang et al., 2025b) and Expel (Zhao et al., 2024), which summarize experience into memory to improve future task performance. (2) Parameter-Optimization (SFT/RL): 

We consider two fundamental training protocols. In the 

Open setting, API documentation remains in the context during both trajectory collection and parameter updates. In the Closed setting, documentation is available for trajectory collection but is stripped during training. See Section C.3 for the difference between the Open and Closed settings. We also evaluate the fully autonomous self-play method 

Absolute-Zero (Zhao et al., 2025). (3) Hybrid: Closed-SFT-RL applies standard RL without API documentation on top of the Closed-SFT. 

Training Setup: We use Qwen3-8B, 4B and 1.7B (Team, 4SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

Table 2: Average performance over 5 rollouts of different models and methods . The best results are highlighted in bold ,and the second-best results are underlined. 

Single Multiple Method Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-8B Qwen3-4B Qwen3-1.7B 

Memory Based 

ACE (Zhang et al., 2025b) 11.2 12.0 2.0 4.1 6.6 0.0 

Expel (Zhao et al., 2024) 47.1 39.4 28.2 15.5 13.6 4.8 

SFT Based 

Open-SFT 0.0 0.0 0.0 0.0 0.0 0.0 

Closed-SFT 39.6 25.1 16.9 11.6 5.4 2.0 

RL Based 

Open-RL 0.0 0.0 0.0 0.0 0.0 0.0 

Closed-RL 0.0 0.0 0.0 0.0 0.0 0.0 

Absolute-Zero (Zhao et al., 2025) 0.0 0.0 0.0 0.0 0.0 0.0 

Hybrid 

Closed-SFT-RL 54.4 43.1 21.9 17.9 9.2 3.2 2025b) families as base models. All RL-based methods utilize the GRPO algorithm (Shao et al., 2024) within the veRL (Sheng et al., 2024) framework. Experiments were conducted on 8 NVIDIA A100 GPUs; further details are provided in Section C.1. 

Results. Table 2 showcases baseline performance on SE-BENCH . Memory-based methods achieve non-trivial results across all model sizes, with Expel attaining the highest accuracy on the Multi-Function split. This is intuitive, as memory allows the model to map SE-B ENCH identifiers to NumPy correspondents, and then revise the NumPy solution based on the mapping. However, even these methods re-main far from perfect, indicating that autonomous memory management is still in its infancy. Even more surprisingly, among parameter-update methods, only Closed-SFT and the hybrid Closed-SFT-RL achieve success; all other methods fail completely. This striking gap suggests that standard RL is fundamentally ill-suited for knowledge internalization, a failure we analyze mechanisti-cally in Section 4.2. Furthermore, the comparison between 

Open-SFT and Closed-SFT reveals the Open-Book Para-dox : despite using the same training trajectories, success depends entirely on removing documentation during param-eter updates. This forces the model to encode logic into its weights rather than relying on context. As we will show in Section 4.1, this reflects genuine internalization rather than simple alignment to the test-time prompt distribution. 

# 4. Analysis and Insight 

Beyond serving as a rigorous metric for current methods, SE-B ENCH functions as a clean testbed for investigating the fundamental mechanisms of self-evolution with knowledge Test w/ Doc Test w/o Doc    

> 0
> 10
> 20
> 30
> 40
> 50
> 60
> Acc
> 46.3
> 0.0
> 50.3
> 39.6
> Single
> Open-SFT Closed-SFT Test w/ Doc Test w/o Doc
> 0
> 5
> 10
> 15
> 20
> 25
> Acc
> 18.2
> 0.0
> 21.4
> 11.6
> Multiple
> Open-SFT Closed-SFT

Figure 2: Closed-SFT vs. Open-SFT . Open-SFT’s com-plete failure without documentation reveals strict context dependency, whereas Closed-SFT successfully internalizes knowledge, maintaining performance even when documen-tation is absent. internalization. In this section, we leverage this controlled environment to investigate three fundamental research ques-tions (RQs) about knowledge internalization with parameter update methods: (RQ1) : Does SFT induce true internal-ization, or merely context dependence? (RQ2) : Can RL internalize knowledge presented in the context? (RQ3) :Can Self-Play Enable Knowledge Internalization? (RQ4) :How does knowledge evolve from SFT to RL? 

4.1. RQ1: Does SFT Induce True Internalization, or Merely Context Dependence? 

In Section 3, we had a surprising observation: removing API documentation during parameter update significantly boosts performance on SE-B ENCH . We hypothesize that this train-ing condition forces the model to rely on parametric memory rather than context. A critical question remains: does this 5SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

Table 3: Ablation of RL components. Internalization col-lapses when using PPO clip loss or including negative ad-vantage. 

Setting Single Multiple SFT-like Closed-RL 51.0 9.4 

w/o Larger LR 31.7 1.4 w/o Larger BSZ 31.7 0.2 w/ PPO Clip Loss 0 0w/ GRPO Advantage 0 0improvement stem from genuine knowledge internalization ,or is it merely an artifact of distribution consistency between training and testing prompts? To isolate the mechanism, we evaluate all models with API documentation provided at test time, strictly aligning with the Open-SFT training prompt. As shown in Figure 2, Closed-SFT still outperforms Open-SFT even in this setting. This rules out prompt consistency as the driver; if it were, Open-SFT would lead. Instead, the results confirm that with-holding documentation during training forces the model to encode knowledge directly into its parameters, resulting in robust internalization independent of context availability. 

Takeaway 

SFT can internalize knowledge, but only when knowl-edge is absent from the context and utilized in the re-sponse. Therefore, use rich context (hints, docs) to generate high-quality data, but strip that context during parameter updates may foster the internalization. 

4.2. RQ2: Can RL Internalize Knowledge in the Context? 

While Closed-SFT effectively internalizes knowledge, we find that applying RL in the similar off-policy settings, i.e., 

generating rollout with API doc, and training without it, fails completely. As shown in Table 2, Closed-RL achieves zero performance. This stark contrast suggests that the mechanism for knowledge internalization is fundamentally different or more restricted in RL than in SFT. To isolate the cause of this failure, we perform a systematic ablation study bridging the mathematical gap between SFT and RL. For a prompt x ∼ D , let {yi}Ni=1 denote N trajec-tories. The SFT objective maximizes the log-probability of successful trajectories: 

LSFT (θ) = Ex∼D 

"

1

N

> N

X

> i=1

log πθ (yi | x)

#

. (2) For RL, we omit the KL regularization. And although the off-policy (rollout with doc, train without) theoretically ne-cessitates an importance sampling ratio πθ (y|xno doc )  

> πθ(y|xdoc )

, we ex-clude it in practice as the numerator vanishes for randomized ZWC function names. The training objective is thus formal-ized as: 

LRL (θ) = Ex∼D 

"

1

N

> N

X

> i=1

ρi(θ)Ai

#

. (3) By instantiating the policy term ρi(θ) and the advantage term Ai differently, we recover both paradigms: • SFT Instantiation: ρi(θ) = log πθ (yi | x) and Ai =

I(yi is correct ). This treats every correct trajectory as a positive reinforcement signal without regularization. • GRPO Instantiation: ρi(θ) uses the clipped probabil-ity ratio min( rt(θ), clip (rt(θ), 1 − ϵ, 1 + ϵ)) to constrain updates, and Ai is the group-normalized advantage (con-taining both positive and negative values). We start by constructing a SFT-like Closed-RL baseline: we configure the RL approach to use SFT-style hyperpa-rameters (High LR, Large Batch), the SFT-style objective (ρ = log π), and Binary Advantage ( A ∈ { 0, 1}). As shown in Table 3, this configuration successfully recovers Closed-SFT performance, proving that the RL framework itself is not the issue. We then systematically revert each component to its stan-dard GRPO setting to identify the bottleneck. Reverting to standard RL hyperparameters ( w/o Larger LR/BSZ ), drops performance to 31.7%, but the model still learns , indicating that optimization efficiency is affected rather than funda-mental capability. In sharp contrast, the training objective and advantage formulation prove critical. Reintroducing the PPO clipping mechanism ( w/ PPO Clip Loss ) causes immediate collapse to 0% . Similarly, reintroducing the stan-dard normalized advantage ( w/ GRPO Advantage ), which introduces negative reinforcement signals, also results in a total collapse to 0% .This isolates the failure to two specific ”safety” mechanisms in standard RL. First, Clipping prevents internalization. In-ternalizing a new vocabulary item (e.g., mapping np.mean 

to zwc.kocito ) requires a radical shift in probability mass, effectively a ”trust region violation.” By penalizing large shifts, the clipping term actively prevents the model from encoding new definitions. Second, standard normal-ized advantage generates negative signals for below-average responses. In the fragile early stages of memorization, these negative gradients likely erase tentative associations before they can solidify. The concrete underlying mechanisms im-pacting this internalization process remain an interesting direction for future work, and SE-B ENCH provides a clean test-bed for it. 6SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization ZWCArray 

> Attrib.
> Hallu.
> (37.0%)
> ZWC Func.
> Hallu.
> (23.0%)
> Param.
> Signature
> Misal.
> (29.0%)
> Return Value
> Misint.(3.0%)
> Native Python
> Incomp.(8.0%)
> Closed-SFT ZWCArray
> Attrib.
> Hallu.
> (10.0%)
> ZWC Func.
> Hallu.
> (33.0%)
> Param.
> Signature
> Misal.
> (31.0%)
> Return Value
> Misint.(18.0%)
> Native Python
> Incomp.(8.0%)
> Closed-SFT-RL

Figure 3: Error type distribution for Closed-SFT and Closed-SFT-RL. Table 4: Self-play ablation . While Absolute Zero fails, ap-plying SFT to the similar autonomous curriculum ( Closed-SFT self ) yields significant performance, confirming that self-generated data is sufficient for learning.           

> Method Qwen3-8B (Single) Qwen3-8B (Multiple) Absolute Zero 0.0 0.0
> Closed-SFT 39.6 11.6
> Open-SFT self 0.0 0.0
> Closed-SFT self 22.5 8.7

Takeaway 

Standard RL cannot internalize new knowledge within context. Its clipping mechanism and negative gradient prevent the parameter shifts required to encode new knowledge. 

4.3. RQ3: Can Self-Play Enable Knowledge Internalization? 

So far, our investigation has relied on carefully curated problems and test cases. A far more compelling scenario is 

self-play : can a model propose its own problems and test cases to internalize knowledge autonomously? Absolute Zero (Zhao et al., 2025) is a method that reflect such philosophy, a pure RL loop where the agent self-proposes tasks, and learns to solve. However, as shown in Table 2, this yields 0.0% accuracy . Given our finding that RL struggles with internalization (RQ2), a critical am-biguity arises: Is this failure due to poor self-generated data (the model cannot teach itself), or simply the improper RL? To isolate the cause, we investigate whether the model can learn from its own self-proposed curriculum if we switch to SFT. We construct the Open-SFT self and Closed-SFT self 

settings, which are similar to the corresponding settings in Section 3, but the questions and test-cases are now gener-ated by the base model itself. The results in Table 4 are decisive. While Absolute-Zero fails completely, Closed-SFT self recovers significant perfor-mance ( 22.5% ). Although this falls behind the Closed-SFT ,which trains on curated problems and test-cases, it demon-strates non-trivial learning. This result explains the failure of standard self-play. The barrier is the optimization method ,not the self-play paradigm. The model is fully capable of generating valid data conditioned on API documentation to teach itself. The failure occurs strictly because RL can hardly internalize knowledge. By switching the optimiza-tion method to SFT, the model now successfully internalizes knowledge autonomously .Besides, consistent with our earlier analysis in Section 4.1, 

Closed-SFT self outperforms Open-SFT self regardless of whether we provide documentation during test-time (Fig-ure 5), reinforcing the Open-Book Paradox in the self-play setting. 

Takeaway 

Current LLMs can propose meaningful training data to help themselves internalize knowledge. However, the autonomous curriculum should be coupled with an appropriate optimization method ( e.g., Closed-SFT). 

4.4. RQ4: How Knowledge Evolves from SFT to RL? 

While RQ2 confirms that RL cannot internalize knowledge from scratch, Table 2 shows that the hybrid Closed-SFT-RL achieves state-of-the-art performance among parameter-update methods. This suggests that once SFT internalizes the foundational knowledge, RL acts as a powerful amplifier. To understand the mechanism of this amplification, we ana-lyze the shift in error patterns from the SFT stage to the RL stage. Specifically, we investigate how RL impact the behav-ior . We classify the errors into five fine-grained categories (see Section E for examples of each category): • ZWCArray Attribute Hallucination: The agent as-sumes non-existent methods for the ZWCArray object (e.g., guessing .tolist() ), reflecting incorrect intu-ition about the data structure’s interface. • ZWC Function Hallucination: The agent invents func-tions that do not exist in the library ( e.g., zwc.mean() 

instead of zwc.kocito() ), indicating a failure to recall the correct API name. • Parameter Signature Misalignment: The agent cor-rectly identifies the function but misremembers its param-eter list, leading to execution errors. • Return Value Misinterpretation: The agent misunder-stands the output format of a function, causing down-stream errors. • Native Python Incompatibility: The agent applies un-supported native Python operations to ZWC objects. We randomly sampled 100 failed trajectories from both Closed-SFT and Closed-SFT-RL, and used Gemini-3-Flash to classify their error types. Figure 3 illustrates the dramatic shift in error distribution. In the SFT stage, errors are domi-7SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

nated by hallucinations, particularly ZWCArray Attribute Hallucination (37.0%). This suggests that SFT induces a ”probabilistic” form of memory: the model learns the general shape of the library but often fills in gaps with plau-sible guesses (like assuming ZWCArray has a .tolist() 

method). After applying RL, the proportion of ZWCArray Attribute Hallucination collapses to just 10.0%. Qualitative analysis in our case study (Section B) reveals the mechanism behind this shift: RL does not merely suppress errors; it drives the model to replace uncertain API calls with alternative, valid implementations. For instance, when the agent is unsure if a specific ZWC method exists, RL encourages it to fallback to robust primitives ( e.g., using explicit loops rather than hallucinated array methods). This results in code that is more disciplined and executable. However, RL does not significantly reduce Parameter Sig-nature Misalignment or ZWC Function Hallucination .This confirms the ”RL Gap” identified in RQ2: RL can-not correct fundamental memory errors. If the model mis-memorized a function name or signature during SFT, RL lacks the supervised signal to fix it. Instead, RL optimizes 

utilization , pruning “lazy” guesses to ensure that what is 

known is applied robustly. 

Takeaway 

Knowledge evolves from fragile acquisition (SFT) to 

robust consolidation (RL). SFT internalizes raw knowl-edge but leaves it prone to guessing. RL consolidates this by compelling the model to replace uncertain hallu-cinations with disciplined, grounded implementations, thereby ensuring robust utilization. 

4.5. Discussion: Connections to Recent Advancements 

The mechanisms analyzed in this study, specifically the ne-cessity of information starvation for internalization (RQ1) and the distinct roles of SFT and RL (RQ2/3), offer mech-anistic insights that complement several recent research directions. SE-B ENCH serves as a controlled environment to isolate and further investigate these dynamics. For example, our finding that removing documentation dur-ing SFT is critical for internalization provides empirical validation for strategies like OpenAI’s Deliberative Align-ment (Guan et al., 2024) under knowledge internalization. We quantify the underlying mechanism: removing the rele-vant information in the context is not merely data cleaning, but a functional requirement that forces the compression of external logic into model parameters. And recent works in “Prefix-RL” (Huang et al., 2025b; Qu et al., 2026; Setlur et al., 2026) observe that RL trained with privileged prefixes can generalize improvements to un-prefixed settings. our analysis adds crucial nuance. While we confirm RL optimizes knowledge utilization, we high-light its distinct limitation in internalizing new factual con-tent compared to SFT. SE-B ENCH enables researchers to rigorously disentangle these two effects, behavioral general-ization versus factual internalization, potentially facilitating the design of more targeted hybrid algorithms. 

# 5. Related Work 

LLM powered Agent. LLM-powered agents have demon-strated strong effectiveness across a wide range of real-world scenarios. They are capable of performing deep research tasks (Jin et al., 2025; Zheng et al., 2025) and handling code engineering problems (Zhang et al., 2024; Yang et al., 2024). However, current approaches for enhanc-ing these agentic capabilities primarily rely on pre-training or post-training with human-annotated data. As a result, continual improvement requires scaling up the amount of labeled data, which is both inefficient and costly, and may eventually encounter a performance ceiling. 

Self-Evolving Agent. To address the limitations above, an agent needs to possess the capability of self-evolution, con-tinuously learning from its own trajectories to improve its performance (Silver & Sutton, 2025). This ability is widely regarded as a necessary component toward achieving AGI. Methodologically, self-evolution can be realized in several ways. One approach is memory engineering (Zhao et al., 2024; Zhang et al., 2025b), where the agent summarizes in-sights from past trajectories and retrieves them as contextual knowledge when answering similar questions in the future. Another approach is post-training (Wang et al., 2025b; Fan et al., 2025), which updates the model parameters using pre-viously successful trajectories. In addition, co-evolutionary training methods can be employed to further reduce reliance on real-world annotated data (Zhao et al., 2025; Huang et al., 2025a). 

Evaluating Agents’ Evolving Capabilities. However, existing evaluations of agent capabilities focus on code gen-eration (Jin et al., 2025; Jimenez et al., 2024), search (Wei et al., 2025a), and tool usage (Mialon et al., 2024). A cru-cial foundation of self-evolution—the ability to memorize and leverage knowledge—has not been adequately assessed. Current memory benchmarks often focus on learning from user feedback (Ai et al., 2025), which is weakly verifiable and does not directly reflect improvements in the model’s un-derlying capabilities. In contrast, our benchmark provides a clean evaluation environment with no data leakage, thereby filling this gap and enabling a more faithful assessment of an agent’s memorize-and-leverage ability. 8SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

# 6. Conclusion 

We introduce SE-B ENCH , a diagnostic testbed that obfus-cates NumPy to test knowledge internalization . Our ex-periments reveal three key insights: (1) the Open-Book Paradox , demonstrating that true retention requires remov-ing knowledge during training, as accessible context inhibits internalization; (2) the RL Gap , showing that standard RL acts as a behavioral optimizer but fails to internalize new facts; and (3) the viability of Self-Play , which, when seeded with SFT, enables models to successfully distill knowledge from their own noisy curricula. We position SE-B ENCH as a critical unit test for future self-evolving agents, ensuring they possess the genuine ability to learn from experience. 

# Impact Statement 

This paper presents work whose goal is to advance the field by elucidating the mechanisms of knowledge internalization in LLMs. The benchmark itself serves as a basic test for the self-evolution methods, and our analysis of different self-evolution methods offers insights into the fundamental dynamics of how models acquire and utilize new informa-tion. While the development of self-evolving agents carries long-term implications for AI safety and control, this study is conducted in a purely synthetic, controlled environment designed to isolate these variables. We believe that a deeper mechanistic understanding of evolving mechanism is es-sential for developing more reliable, interpretable, and safe training protocols. There are no immediate negative societal consequences or specific ethical issues that we feel must be highlighted. 

# References 

Ai, Q., Tang, Y., Wang, C., Long, J., Su, W., and Liu, Y. Memorybench: A benchmark for memory and continual learning in LLM systems. CoRR , abs/2510.17281, 2025. doi: 10.48550/ARXIV.2510.17281. URL https:// doi.org/10.48550/arXiv.2510.17281 .Anthropic. Introducing claude sonnet 4.5. https://www. anthropic.com/news/claude-sonnet-4-5 ,September 2025. Fan, Y., Zhang, K., Zhou, H., Zuo, Y., Chen, Y., Fu, Y., Long, X., Zhu, X., Jiang, C., Zhang, Y., Kang, L., Chen, G., Huang, C., He, Z., Wang, B., Bai, L., Ding, N., and Zhou, B. SSRL: self-search reinforce-ment learning. CoRR , abs/2508.10874, 2025. doi: 10.48550/ARXIV.2508.10874. URL https://doi. org/10.48550/arXiv.2508.10874 .Goertzel, B. and Pennachin, C. (eds.). Artificial Gen-eral Intelligence . Cognitive Technologies. Springer, 2007. ISBN 978-3-540-23733-4. doi: 10.1007/ 978-3-540-68677-4. URL https://doi.org/10. 1007/978-3-540-68677-4 .Guan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Helyar, A., Dias, R., Vallone, A., Ren, H., Wei, J., Chung, H. W., Toyer, S., Heidecke, J., Beutel, A., and Glaese, A. Deliberative alignment: Reasoning enables safer language models. CoRR , abs/2412.16339, 2024. doi: 10.48550/ARXIV.2412.16339. URL https:// doi.org/10.48550/arXiv.2412.16339 .Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del R ´ıo, J. F., Wiebe, M., Peterson, P., G ´erard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. 

Nature , 585(7825):357–362, September 2020. doi: 10. 1038/s41586-020-2649-2. URL https://doi.org/ 10.1038/s41586-020-2649-2 .Huang, C., Yu, W., Wang, X., Zhang, H., Li, Z., Li, R., Huang, J., Mi, H., and Yu, D. R-zero: Self-evolving reasoning LLM from zero data. CoRR , abs/2508.05004, 2025a. doi: 10.48550/ARXIV.2508.05004. URL https: //doi.org/10.48550/arXiv.2508.05004 .Huang, Z., Cheng, T., Qiu, Z., Wang, Z., Xu, Y., Ponti, E. M., and Titov, I. Blending supervised and reinforcement fine-tuning with prefix sampling. CoRR , abs/2507.01679, 2025b. doi: 10.48550/ARXIV.2507.01679. URL https: //doi.org/10.48550/arXiv.2507.01679 .Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenRe-view.net, 2024. URL https://openreview.net/ forum?id=VTF8yNQM66 .Jin, B., Zeng, H., Yue, Z., Wang, D., Zamani, H., and Han, J. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. 

CoRR , abs/2503.09516, 2025. doi: 10.48550/ARXIV. 2503.09516. URL https://doi.org/10.48550/ arXiv.2503.09516 .Kamradt, G. Needle in a haystack - pressure testing llms, 2023. URL https://github.com/gkamradt/ LLMTest_NeedleInAHaystack .Lee, Y., Kim, S., Lee, B., Moon, M., Hwang, Y., Kim, J. M., Neubig, G., Welleck, S., and Choi, H. Refinebench: Evaluating refinement capability of language models via checklists. CoRR , abs/2511.22173, 2025. doi: 10.48550/ 9SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

ARXIV.2511.22173. URL https://doi.org/10. 48550/arXiv.2511.22173 .Legg, S. and Hutter, M. A collection of definitions of intel-ligence. In Goertzel, B. and Wang, P. (eds.), Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms - Proceedings of the AGI Workshop 2006 [May 20-21, 2006, Washington DC, USA] , volume 157 of Frontiers in Artificial Intelligence and Applications ,pp. 17–24. IOS Press, 2006. URL https://ebooks. iospress.nl/volumearticle/3471 .Li, S., Bu, X., Wang, W., Liu, J., Dong, J., He, H., Lu, H., Zhang, H., Jing, C., Li, Z., Li, C., Tian, J., Zhang, C., Peng, T., He, Y., Gu, J., Zhang, Y., Yang, J., Zhang, G., Huang, W., Zhou, W., Zhang, Z., Ding, R., and Wen, S. Mm-browsecomp: A comprehensive benchmark for multimodal browsing agents. CoRR , abs/2508.13186, 2025. doi: 10.48550/ARXIV.2508.13186. URL https: //doi.org/10.48550/arXiv.2508.13186 .Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. GAIA: a benchmark for general AI assistants. In 

The Twelfth International Conference on Learning Repre-sentations, ICLR 2024, Vienna, Austria, May 7-11, 2024 .OpenReview.net, 2024. URL https://openreview. net/forum?id=fibxvahvs3 .Novikov, A., Vu, N., Eisenberger, M., Dupont, E., Huang, P., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J. R., Mehrabian, A., Kumar, M. P., See, A., Chaudhuri, S., Holland, G., Davies, A., Nowozin, S., Kohli, P., and Balog, M. Alphaevolve: A coding agent for scientific and algorithmic discovery. CoRR , abs/2506.13131, 2025. doi: 10.48550/ARXIV.2506.13131. URL https:// doi.org/10.48550/arXiv.2506.13131 .OpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925 .Ouyang, S., Yan, J., Hsu, I., Chen, Y., Jiang, K., Wang, Z., Han, R., Le, L. T., Daruki, S., Tang, X., Tirumalashetty, V., Lee, G., Rofouei, M., Lin, H., Han, J., Lee, C., and Pfister, T. Reasoningbank: Scaling agent self-evolving with reasoning memory. CoRR , abs/2509.25140, 2025. doi: 10.48550/ARXIV.2509.25140. URL https:// doi.org/10.48550/arXiv.2509.25140 .Qi, Z., Liu, X., Iong, I. L., Lai, H., Sun, X., Sun, J., Yang, X., Yang, Y., Yao, S., Xu, W., Tang, J., and Dong, Y. Webrl: Training LLM web agents via self-evolving online cur-riculum reinforcement learning. In The Thirteenth Inter-national Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025 . OpenReview.net, 2025. URL https://openreview.net/forum? id=oVKEAFjEqv .Qu, Y., Setlur, A., Smith, V., Salakhutdinov, R., and Kumar, A. Pope: Learning to reason on hard prob-lems via privileged on-policy exploration, 2026. URL 

https://arxiv.org/abs/2601.18779 .Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. 

CoRR , abs/1707.06347, 2017. URL http://arxiv. org/abs/1707.06347 .Setlur, A., Wang, Z., Cohen, A., Rashidinejad, P., and Xie, S. M. Reuse your flops: Scaling rl on hard problems by conditioning on very off-policy prefixes, 2026. URL 

https://arxiv.org/abs/2601.18795 .Shao, R., Li, S. S., Xin, R., Geng, S., Wang, Y., Oh, S., Du, S. S., Lambert, N., Min, S., Krishna, R., Tsvetkov, Y., Hajishirzi, H., Koh, P. W., and Zettlemoyer, L. Spu-rious rewards: Rethinking training signals in RLVR. 

CoRR , abs/2506.10947, 2025. doi: 10.48550/ARXIV. 2506.10947. URL https://doi.org/10.48550/ arXiv.2506.10947 .Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR , abs/2402.03300, 2024. doi: 10.48550/ ARXIV.2402.03300. URL https://doi.org/10. 48550/arXiv.2402.03300 .Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexi-ble and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024. Silver, D. and Sutton, R. S. Welcome to the era of experi-ence. Google AI , 1, 2025. Team, G. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next gen-eration agentic capabilities. CoRR , abs/2507.06261, 2025a. doi: 10.48550/ARXIV.2507.06261. URL https: //doi.org/10.48550/arXiv.2507.06261 .Team, Q. Qwen3 technical report, 2025b. URL https: //arxiv.org/abs/2505.09388 .Team, T. T.-B. Terminal-bench: A benchmark for ai agents in terminal environments, Apr 2025c. URL 

https://github.com/laude-institute/ terminal-bench .Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large language models. 

Trans. Mach. Learn. Res. , 2024, 2024. URL https: //openreview.net/forum?id=ehfRiF0R3a .10 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

Wang, W., Piekos, P., Li, N., Laakom, F., Chen, Y., Os-taszewski, M., Zhuge, M., and Schmidhuber, J. Huxley-g ¨odel machine: Human-level coding agent development by an approximation of the optimal self-improving ma-chine. CoRR , abs/2510.21614, 2025a. doi: 10.48550/ ARXIV.2510.21614. URL https://doi.org/10. 48550/arXiv.2510.21614 .Wang, Y., Su, S., Zeng, Z., Xu, E., Ren, L., Yang, X., Huang, Z., He, X., Ma, L., Peng, B., Cheng, H., He, P., Chen, W., Wang, S., Du, S. S., and Shen, Y. Thetaevolve: Test-time learning on open problems. CoRR , abs/2511.23473, 2025b. doi: 10.48550/ARXIV.2511.23473. URL https: //doi.org/10.48550/arXiv.2511.23473 .Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. Browsecomp: A simple yet challenging benchmark for browsing agents. CoRR , abs/2504.12516, 2025a. doi: 10.48550/ARXIV.2504.12516. URL https://doi. org/10.48550/arXiv.2504.12516 .Wei, Y., Duchenne, O., Copet, J., Carbonneaux, Q., Zhang, L., Fried, D., Synnaeve, G., Singh, R., and Wang, S. I. SWE-RL: advancing LLM reasoning via reinforcement learning on open software evolution. 

CoRR , abs/2502.18449, 2025b. doi: 10.48550/ARXIV. 2502.18449. URL https://doi.org/10.48550/ arXiv.2502.18449 .Wu, M., Zhang, Z., Dong, Q., Xi, Z., Zhao, J., Jin, S., Fan, X., Zhou, Y., Fu, Y., Liu, Q., Zhang, S., and Zhang, Q. Reasoning or memorization? unreliable re-sults of reinforcement learning due to data contamination. 

CoRR , abs/2507.10532, 2025. doi: 10.48550/ARXIV. 2507.10532. URL https://doi.org/10.48550/ arXiv.2507.10532 .Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., and Press, O. Swe-agent: Agent-computer interfaces enable automated software engineering. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), 

Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024 , 2024. URL http://papers. nips.cc/paper_files/paper/2024/hash/ 5a7c947568c1b1328ccc5230172e1e7c-Abstract-Conference. html .Zhang, J., Hu, S., Lu, C., Lange, R. T., and Clune, J. Darwin godel machine: Open-ended evolution of self-improving agents. CoRR , abs/2505.22954, 2025a. doi: 10.48550/ ARXIV.2505.22954. URL https://doi.org/10. 48550/arXiv.2505.22954 .Zhang, K., Li, J., Li, G., Shi, X., and Jin, Z. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. 

CoRR , abs/2401.07339, 2024. doi: 10.48550/ARXIV. 2401.07339. URL https://doi.org/10.48550/ arXiv.2401.07339 .Zhang, Q., Hu, C., Upasani, S., Ma, B., Hong, F., Kama-nuru, V., Rainton, J., Wu, C., Ji, M., Li, H., Thakker, U., Zou, J., and Olukotun, K. Agentic context en-gineering: Evolving contexts for self-improving lan-guage models. CoRR , abs/2510.04618, 2025b. doi: 10.48550/ARXIV.2510.04618. URL https://doi. org/10.48550/arXiv.2510.04618 .Zhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy, J. G., and Natarajan, S. (eds.), 

Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Appli-cations of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelli-gence, EAAI 2014, February 20-27, 2024, Vancouver, Canada , pp. 19632–19642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936. URL https://doi. org/10.1609/aaai.v38i17.29936 .Zhao, A., Wu, Y., Yue, Y., Wu, T., Xu, Q., Yue, Y., Lin, M., Wang, S., Wu, Q., Zheng, Z., and Huang, G. Abso-lute zero: Reinforced self-play reasoning with zero data. 

CoRR , abs/2505.03335, 2025. doi: 10.48550/ARXIV. 2505.03335. URL https://doi.org/10.48550/ arXiv.2505.03335 .Zheng, Y., Fu, D., Hu, X., Cai, X., Ye, L., Lu, P., and Liu, P. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. 

CoRR , abs/2504.03160, 2025. doi: 10.48550/ARXIV. 2504.03160. URL https://doi.org/10.48550/ arXiv.2504.03160 .11 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 0 100 200 300 400 600 800 1000 1200         

> Step
> 0
> 5
> 10
> 15
> 20
> 25
> 30
> 35
> 40
> Acc
> Question Diversity
> 1/2Q
> 3/4Q
> Q0100 200 300 400 600 800 1000 1200
> Step
> 0
> 10
> 20
> 30
> 40
> Acc
> Response Diversity
> 2/5R
> 3/5R
> 4/5R
> 5/5R

Figure 4: Effects of diversity on knowledge internalization. The left panel shows the impact of question diversity, while the 

right panel shows the impact of response diversity. Question diversity has a substantially larger influence on knowledge internalization. 

# A. The Effect of Question and Trajectory Diversity 

As demonstrated in Section 4.1, SFT is indeed capable of internalizing knowledge. To further investigate the impact of diversity on knowledge internalization, we design the following experiments. Under the Closed-SFT setting, we vary the number of questions and the number of responses per question to separately investigate the effects of question diversity and response diversity on knowledge internalization. When studying question diversity, we keep the number of responses generated for each question fixed, while varying the total number of questions. Conversely, when studying response diversity, we fix the total number of questions and vary the number of responses generated for each question. Figure 4 shows the results of the above experiments. The left plot illustrates the effect of different levels of question diversity. As the number of distinct questions decreases, the training efficiency gradually drops and the final performance also degrades, indicating that question diversity plays a crucial role in knowledge internalization. The right plot shows the impact of response diversity. Both the training efficiency and the final performance remain largely consistent across different levels of response diversity, suggesting that once a sufficient number of correct responses is reached, further increasing response diversity has little influence on the training outcome. These results indicate that, when studying knowledge internalization in self-evolution, greater attention should be paid to the quality and diversity of questions. 

# B. Case Study on Continue RL 

Table 2 shows that applying RL after SFT-Internalized can further improve the degree of knowledge internalization. To better understand how RL contributes to this improvement, we present a case study in this section to analyze the specific ways in which RL influences the internalization process. Table 5 presents representative cases from the Single test set, comparing agent behavior before and after applying RL. Before RL, the agent incorrectly assumes that ZWCArray provides a sum() method. After RL, the agent learns that such an API does not exist in the ZWC library and instead resorts to appropriate Python built-in operations to achieve the desired functionality. Table 6 presents representative cases from the Multiple test set. Through RL, the agent explores and attempts different ZWC APIs, which enables it to better understand their functionality and apply them correctly in more complex, multi-API scenarios. 

# C. Experiment Details 

C.1. Hyper Parameters of Main Experiment 

For ACE , we use a temperature of 0.6 and a maximum response length of 8192 tokens during rollout. For each query, the agent is allowed to interact with the sandbox environment for up to three turns. During training, in order to enable parallel processing, the insights obtained from different queries are kept independent from each other. Afterward, these insights are aggregated to form a unified skillbook. At test time, we retrieve the 100 insights that are most similar to the given query from the skillbook using cosine similarity, and use them as context to assist the agent in solving the task. 12 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization Query                         

> You are developing a resource allocation system for a cloud computing platform. The system needs to determine how many virtual machine instances are required to handle incoming computational workloads. Each workload requires a certain amount of computational units (given as floating-point values), and each VM instance can provide exactly 1 computational unit. Since you cannot allocate partial VM instances, you must round up to ensure sufficient capacity. Given a nested list where each sublist represents workloads for different time periods, calculate the total number of VM instances needed for each time period.
> OLD Response: Execution Error, ZWCArray have no attribute named ’sum’
> import zwc def calculate_vm_instances(workloads): result =[] for period in workloads: ceiled =zwc.qojaxef(period) total =ceiled.sum() result.append(int(total)) return result
> NEW Response
> import zwc def calculate_vm_instances(workloads): result =[] for sublist in workloads: ceilings =zwc.pekap(sublist) total =sum(ceilings) result.append(total) return result

Table 5: Case study example of continue RL on Single .For ExpeL framework, we adapt it to our ”Think + Code” scenario. We conduct experiments using the Qwen3 family (8B, 4B, and 1.7B) as the backbone for both the Policy LLM and the Insight Extraction LLM. During experience gathering, we employ a decoding temperature of 0.6 and a maximum response length of 8192 tokens. Since reasoning models can implicitly explore diverse solution space, we replace the iterative ReAct approach in Expel with a direct generation process, where the agent produces a unified trajectory comprising both the internal thought process and the code solution. To ensure sufficient exploration, the agent is permitted up to 10 attempts per task. For insight extraction, we adhere to the prompt templates from the original ExpeL paper. Specifically, we prompt the Insight Extraction LLM to distill generalizable insights by either contrasting a failed trajectory with a successful one for the same task, or by identifying common patterns across a set of 5 successful trajectories from different tasks. During validation, we implement a dynamic few-shot strategy based on semantic relevance. We utilize all-mpnet-base-v2 to embed task descriptions and compute the cosine similarity between validation and experience tasks. We retrieve the top k=2 most relevant successful experiences to serve as in-context demonstrations, alongside the top k=1 insight to serve as an in-context guiding rule. For SFT-based methods, we use a batch size of 32 and set the temperature to 1.0. The learning rate linearly warms up from 

1 × 10 −6 to 1 × 10 −5, followed by cosine decay, and the max context length is set to 16384. For RL-based methods, we use a batch size of 32 and set the temperature to 1.0. The learning rate is fixed at 1 × 10 −6, and the rollout number in GRPO is set to n = 8 .

C.2. Hyper Parameters of RL Ablation 

To investigate the conditions under which RL-based algorithms are able to internalize knowledge, we conduct a series of ablation studies in Section 4.2. Table 7 reports the detailed hyperparameter settings used in these ablation experiments. 13 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization Test w/ Doc Test w/o Doc    

> 0
> 10
> 20
> 30
> 40
> 50
> 60
> 70
> Acc
> 50.3
> 0.0
> 54.0
> 22.5
> Single
> Open-SFT self Closed-SFT self Test w/ Doc Test w/o Doc
> 0
> 5
> 10
> 15
> 20
> 25
> Acc
> 19.1
> 0.0
> 19.3
> 8.7
> Multiple
> Open-SFT self Closed-SFT self

Figure 5: The performance of Closed-SFT self vs. Open-SFT self on test set with or without relevant API documentation. 

C.3. Difference between Open and Closed 

In this section, we provide a more detailed explanation of the differences between the Open and Closed settings to facilitate better understanding. Table 9 and Table 8 present the prompts used in the Open and Closed settings, respectively, which we term as prompt open and prompt closed .Under both the Open and Closed settings, trajectories are collected using prompt Open , Let T = {ti} denote the set of collected trajectories. In the Open setting, model parameters are updated using pairs {prompt Open , t i}, where the API documentation is explicitly provided during training. In contrast, in the Closed setting, parameter updates are performed using {prompt Closed , t i}. That is, the ZWC API documentation required for solving the task is removed during the training stage. This design forces the model to rely on knowledge internalized in its parameters rather than direct access to external API documentation. 

# D. Details of Benchmark 

D.1. Examples 

In this section, we present examples from both the Single and Multiple settings in Table 10 and Table 11. In the Single setting, each problem is constructed around a core API from the ZWC library. In contrast, the Multiple setting involves composing multiple ZWC APIs within a single problem. For all test cases, both the input and output are formatted as lists. 

D.2. Selected NumPy Functions 

Table 12 and Table 13 list the NumPy functions used to construct the ZWC library, covering both the main and linalg namespaces. For functions that share the same name in NumPy’s main and linalg modules, we map them to different obfuscated names in ZWC, in order to eliminate naming conflicts and to guarantee a one-to-one correspondence between function names and their underlying semantics in our benchmark. 

# E. Example of Error Types 

To provide a more concrete understanding of the error categories defined in the Section 4.4, we present representative examples for each type of error in this section, including ZWCArray Attribute Hallucination (Table 14), Return Value Misinterpretation (Table 15), Parameter Signature Misalignment (Table 16), Native Python Incompatibility (Table 17), and ZWC Function Hallucination (Table 18). These examples are randomly sampled from failed trajectories during evaluation and are manually verified to reflect typical failure patterns observed in practice. For each category, we provide a minimal code snippet together with the corresponding runtime error message to highlight the root cause of failure. 14 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization Query 

You are developing a computer graphics application that performs 3D transformations on batches of vertices. Given a batch of 3D vertices represented as a matrix where each row is a vertex (x, y, z ), and a set of transformation matrices, you need to: (1) Apply each transformation matrix to all vertices in the batch; (2) Compute the dot product between each transformed vertex and a lighting direction vector; (3) Create a result matrix where each element represents the lighting intensity for a specific vertex under a specific transformation. The function takes vertices, transformations, and a light direction vector, and returns a 2D list where result[i][j] represents the lighting intensity of vertex i after applying transformation j.

OLD Response: float() argument must be a string or a real number, not ’ZWCArray’ 

import zwc def solve_lighting_problem(vertices, transformations, light_direction): result = [] for i, vertex in enumerate(vertices): row = [] for j, transformation in enumerate(transformations): v = zwc.yitaf(vertex) transformed_v = zwc.majikih(transformation, v) light_dir = zwc.yitaf(light_direction) dot = zwc.majikih(transformed_v, light_dir) row.append(float(dot)) result.append(row) return result 

NEW Response 

import zwc def solve_lighting_problem(vertices, transformations, light_direction): light_dir = zwc.yitaf(light_direction) result = [] for vertex in vertices: vertex_array = zwc.yitaf(vertex) row_result = [] for trans in transformations: trans_mat = zwc.yitaf(trans) transformed_vertex = zwc.tosiha(trans_mat, vertex_array) dot = zwc.tosiha(light_dir, transformed_vertex) row_result.append(float(dot)) result.append(row_result) return result 

Table 6: Case study example of continue RL on Multiple .Setting train batch size mini batch size temperature max response length lr n

SFT-like Curated-IS-RL 10560 64 1 8192 1e − 5 1w/o Cliploss Ablation 10560 64 1 8192 1e − 5 1w/o Binary Advantage 1320 8 1 8192 1e − 5 8w/o Larger lr 10560 64 1 8192 1e − 6 1w/o Larger bsz 64 16 1 8192 1e − 5 1Table 7: Hyperparameter configurations for the ablation study of SFT-like Curated-IS-RL. The first row shows the default setting, while the following rows correspond to variants that remove the clipping loss, binary advantage, larger learning rate, and larger batch size, respectively. 15 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

You are given a coding problem along with a set of input-output test cases. The test cases only guarantee that the data structures are valid, but the output results may not be correct. Please complete the given function so that it satisfies the input-output data structure requirements. ### Problem ${question }

### Test Cases ${example test cases }

### Function to Complete ${function }

### Requirements - You **must** solve the problem **strictly by using the zwc library. Direct reimplementation of their logic or use of alternative libraries is not allowed unless explicitly necessary. - Only complete the body of the given function. **Do not** change the function name, parameters, or their order. - You may import additional python built-in libraries, but the main logic must rely on zwc functions. - At the end of your response, return the final implementation as a **single fenced Python code block** (‘‘‘python‘‘‘), containing all required imports and the completed function. - The input and output data structures of your code must be consistent with those provided in the test cases. For example, if the output in the test cases is a list, your code must also return a list. Please write your final implementation below, **ensuring that the zwc functions are explicitly used** in your solution. 

Table 8: Prompt Template of Closed Setting 16 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

You are given several helper functions from the zwc codebase along with a programming problem and a set of input-output test cases. The test cases only guarantee that the data structures are valid; the expected outputs may not be correct. Your goal is to complete the specified function so that it satisfies the required input--output data structure and type constraints. ### zwc Codebase Functions ${ref code }

### Problem ${question }

### Test Cases ${example test cases }

### Function to Complete ${function }

### Requirements - You **must** solve the problem **strictly by using the zwc library and the functions provided** in the "zwc Codebase Functions" section. Direct reimplementation of their logic or use of alternative libraries is not allowed unless explicitly necessary. - Only complete the body of the given function. **Do not** change the function name, parameters, or their order. - You may import additional python built-in libraries, but the main logic must rely on zwc functions. - At the end of your response, return the final implementation as a **single fenced Python code block** (‘‘‘python‘‘‘), containing all required imports and the completed function. - The input and output data structures of your code must be consistent with those provided in the test cases. For example, if the output in the test cases is a list, your code must also return a list. Please write your final implementation below, **ensuring that the zwc functions are explicitly used** in your solution. 

Table 9: Prompt Template of the Open Setting Selected functions: zwc.lenelo (np.bitwise and )Given two lists of equal length representing collision masks of sprites from two layers, compute the overlapping collision areas by applying a bitwise AND to each corresponding pair. 

Input: x1 = [255 , 170 , 85] , x2 = [15 , 240 , 51] . Output: [15 , 160 , 17] .

Input: x1 = [0 , 127 , 31] , x 2 = [255 , 128 , 16] . Output: [0 , 0, 16] .

Input: x1 = [1023 , 512 , 256] , x 2 = [511 , 768 , 384] . Output: [511 , 512 , 256] .

Input: x1 = [7 , 14 , 28 , 56] , x 2 = [3 , 6, 12 , 24] . Output: [3 , 6, 12 , 24] .

Input: x1 = [65535 , 32768 , 16384] , x 2 = [43690 , 21845 , 10922] . Output: [43690 , 0, 0] .

Input: x1 = [4095] , x 2 = [2730] . Output: [2730] .

Input: x1 = [255 , 255 , 255 , 255 , 255] , x 2 = [1 , 2, 4, 8, 16] . Output: [1 , 2, 4, 8, 16] .

Input: x1 = [1 , 3, 7, 15 , 31 , 63 , 127] , x 2 = [128 , 64 , 32 , 16 , 8, 4, 2] . Output: [0 , 0, 0, 0, 8, 4, 2] .Table 10: An Example of Single 

17 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

Selected functions: zwc.yisuvow (np.diag ), zwc.yopir (np.copy ), zwc.qubime (np.cosh )You are working on a signal processing application that needs to analyze stability matrices. Given a square matrix representing a system’s transfer function coefficients, you need to: 1. Extract the main diagonal elements to analyze the primary system parameters 2. Apply a hyperbolic cosine transformation to these diagonal elements (which represents a stabilization filter commonly used in control systems) 3. Create a independent copy of the transformed diagonal values for further processing Write a function process stability matrix(matrix) that takes a square matrix as input and returns the trans-formed diagonal elements as a separate array. The input matrix will be a nested list representing a 2D square matrix, and the output should be a list of the transformed diagonal values. 

Input: matrix = [[1 .0, 2.0] , [3 .0, 4.0]] . Output: [1 .5430806348152437 , 27 .308232836016487] .

Input: matrix = [[0 .0, 1.0, 2.0] , [3 .0, 0.5, 4.0] , [5 .0, 6.0, 1.0]] . Output: [1 .0, 1.1276259652063807 , 1.5430806348152437] .

Input: matrix = [[ −1.0, 2.0] , [1 .0, −2.0]] . Output: [1 .5430806348152437 , 3.7621956910836314] .

Input: matrix = [[0 .1, 0.2, 0.3] , [0 .4, 0.2, 0.6] , [0 .7, 0.8, 0.3]] . Output: [6 .132289479663686] .Table 11: An Example of Multy 

Functions in the main namespace abs absolute acos acosh add all allclose amax amin any arange arccos arccosh arcsin arcsinh arctan arctan2 arctanh argmax argmin argpartition argsort argwhere around array array equal array equiv ascontiguousarray asin asinh atan atan2 atanh atleast 1d atleast 2d atleast 3d average bincount bitwise and bitwise count bitwise left shift bitwise not bitwise or bitwise right shift bitwise xor block broadcast arrays broadcast to cbrt ceil choose clip compress concatenate conj conjugate convolve copy copysign copyto corrcoef correlate cos cosh count nonzero cov cross cumprod cumsum deg2rad degrees delete diag diagflat diagonal diff digitize divide divmod dot empty empty like equal exp exp2 expand dims expm1 extract eye fabs flip floor floor divide fmax fmin fmod frexp full full like gcd geomspace gradient greater greater equal heaviside histogram hstack hypot identity imag inner insert interp intersect1d invert isclose isfinite isinf isnan isreal ix kron lcm ldexp left shift less less equal lexsort linspace log log10 log1p log2 logaddexp logaddexp2 logical and logical not logical or logical xor logspace matmul max maximum mean median meshgrid min minimum mod modf moveaxis multiply nan to num negative nextafter nonzero not equal ones ones like outer pad partition percentile permute dims piecewise place polyfit polyval positive pow power prod ptp put putmask quantile rad2deg radians ravel real reciprocal remainder repeat reshape resize right shift rint roll rollaxis roots rot90 round searchsorted select shape sign signbit sin sinc sinh size sort sort complex spacing split sqrt square squeeze stack std subtract sum swapaxes take tan tanh tensordot tile trace transpose trapz tri tril triu true divide trunc union1d unique unravel index unwrap var vdot vectorize vstack where zeros zeros like 

Table 12: Functions in the main namespace 18 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

Functions in the linalg namespace cholesky cond cross det diagonal eig eigh eigvals eigvalsh inv lstsq matmul matrix norm matrix power matrix rank matrix transpose multi dot norm outer pinv qr slogdet solve svd svdvals tensordot tensorinv trace vecdot vector norm 

Table 13: Functions in the linalg namespace 

Code: 

import zwc def solve(data): arr = zwc.array(data) return arr.tolist() 

Error: 

AttributeError: ZWCArray has no attribute ’tolist’ 

Table 14: An Example of ZWCArray Attribute Hallucination 

Code: 

import zwc def solve(x, tolerance): result bool = [] result rank = [] for matrix in x: result = zwc.rfx.gosubab(matrix) s = result.s rank = result.rank is deficient = any(val < tolerance for val in s) result bool.append(is deficient) result rank.append(rank) return (result bool, result rank) 

Error: 

AttributeError: ‘SVDResult‘ object has no attribute ‘s‘. Did you mean: ‘S‘? 

Table 15: An Example of Return Value Misinterpretation 

Code: 

import zwc def solve(measurements): mantissas, exponents = zwc.vutodu(measurements) max exponent = max(exponents) + 1scaled mantissas = [m * 2**(e - max exponent) for m, e in zip(mantissas, exponents)] return (scaled mantissas, max exponent) 

Error: 

TypeError: ldexp() takes from 2 to 3 positional arguments but 1 were given 

Table 16: An Example of Parameter Signature Misalignment 

19 SE-B ENCH : Benchmarking Self-Evolution with Knowledge Internalization 

Code: 

import zwc def solve(y coordinates): # Convert the input list to a format suitable for zwc.kanol # Since kanol expects array like input, we can pass the list directly result = zwc.kanol(y coordinates) # Since we need to return a list of floats, we’ll convert the result # We can iterate through the zwc array and collect the values output = [] for i in range(len(result)): output.append(float(result[i])) return output 

Error: 

TypeError: float() argument must be a string or a real number, not ‘ZWCArray‘ 

Table 17: An Example of Native Python Incompatibility 

Code: 

import zwc def solve(x1, x2): result = zwc.cecim(x1, x2) return [list(row) for row in result] 

Error: 

AttributeError: module ’zwc’ has no attribute ’cecim’. Did you mean: ’cicip’? 

Table 18: An Example of ZWC Function Hallucination 

20