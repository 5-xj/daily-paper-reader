# POP: Prior-fitted Optimizer Policies
# POP：预拟合先验的优化器策略

**Authors**: Jan Kobiolka, Christian Frey, Gresa Shala, Arlind Kadra, Erind Bedalli, Josif Grabocka \\
**Date**: 2026-02-17 \\
**PDF**: https://arxiv.org/pdf/2602.15473v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 7.0 \\
**Evidence**: meta-learned optimizer policies for efficient automatic algorithm design \\

---

## Abstract
Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their performance relies on carefully tuned learning rates, momentum, and gradient accumulation. To address these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned optimizer that predicts coordinate-wise step sizes conditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimization functions of various complexity, where it consistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimization, and a recent meta-learned competitor under matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning.

## 摘要
优化是指寻找目标函数极值的任务。经典的基于梯度的优化

---

## 速览摘要（自动生成）

**问题**：传统梯度优化器高度依赖超参数微调，在复杂非凸场景下