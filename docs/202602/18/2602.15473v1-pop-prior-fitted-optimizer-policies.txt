Title: POP: Prior-fitted Optimizer Policies

URL Source: https://arxiv.org/pdf/2602.15473v1

Published Time: Wed, 18 Feb 2026 01:42:35 GMT

Number of Pages: 23

Markdown Content:
# POP: Prior-fitted Optimizer Policies 

Jan Kobiolka 1 Christian Frey 1 Gresa Shala 2 Arlind Kadra 1 Erind Bedalli 3 Josif Grabocka 1

Abstract 

Optimization refers to the task of finding extrema of an objective function. Classical gradient-based optimizers are highly sensitive to hyperparameter choices. In highly non-convex settings their per-formance relies on carefully tuned learning rates, momentum, and gradient accumulation. To ad-dress these limitations, we introduce POP (Prior-fitted Optimizer Policies), a meta-learned opti-mizer that predicts coordinate-wise step sizes con-ditioned on the contextual information provided in the optimization trajectory. Our model is learned on millions of synthetic optimization problems sampled from a novel prior spanning both convex and non-convex objectives. We evaluate POP on an established benchmark including 47 optimiza-tion functions of various complexity, where it con-sistently outperforms first-order gradient-based methods, non-convex optimization approaches (e.g., evolutionary strategies), Bayesian optimiza-tion, and a recent meta-learned competitor un-der matched budget constraints. Our evaluation demonstrates strong generalization capabilities without task-specific tuning. 

1. Introduction 

Optimization lies at the core of modern machine learning, scientific computing, and engineering. In practice, most con-tinuous optimization problems are solved using gradient and momentum-based methods, which scale efficiently to high-dimensional settings (Robbins & Monro, 1951; Kingma & Ba, 2015; Daoud et al., 2022; Liu et al., 2020). Despite their success, the performance of these optimizers is highly sen-sitive to hyperparameter choices, in particular the learning-rate schedule and momentum coefficients (Tian et al., 2023; Keskar & Socher, 2017; Hassan et al., 2022). Selecting    

> 1Department of Computer Science and Artificial Intelligence, University of Technology Nuremberg, Germany 2Department of Computer Science, Albert Ludwig University of Freiburg, Ger-many 3Faculty of Natural Sciences, University of Elbasan, Albania. Correspondence to: Jan Kobiolka <jan.kobiolka@utn.de >.
> Preprint. February 18, 2026.

suitable hyperparameters often requires extensive hyperpa-rameter optimization, which is costly, time-consuming, and difficult to transfer across tasks and domains (Mary et al., 2025; Liao et al., 2022; Schlotthauer et al., 2025). More recently, learned optimizers have gained increasing interest in the community (Tang & Yao, 2024; Yang et al., 2023; Metz et al., 2022; Cao et al., 2019; Gomes et al., 2021; Vishnu et al., 2020; Ma et al., 2024; Goldie et al., 2024; Lan et al., 2023). In contrast to classical optimization techniques, learned optimizers are typically composed of black-box function approximators (Metz et al., 2020; Vishnu et al., 2020), or they build on hand-designed update rules whose hyperparameters are learned (Kristiansen et al., 2024; Moudgil et al., 2025). Despite promising results, an open challenge remains how to learn an optimizerâ€™s behavior that (i) reduces the need for extensive tuning across tasks and (ii) maintains effective exploration of the objective landscape under constrained budgets, rather than becoming strongly local once the learning rate is annealed over time. We introduce POP, a meta-learned optimizer trained as a continuous-control agent leveraging RL meta-learning with optimization specific reward shaping. POP learns a pol-icy that outputs coordinate-wise step size decisions condi-tioned on the optimization history accumulated over time, which enables our model to be adaptive towards exploration-exploitation trade-offs automatically. Our model leverages a transformer-based backbone (Vaswani et al., 2017) to en-code historical long-range trajectory context. Inspired by prior-data fitted networks (PFNs) (M Â¨uller et al., 2022), we meta-train our model on a distribution of objective functions drawn from a novel prior. We construct this prior using a Gaussian process (Rasmussen & Williams, 2006), ap-proximated via Random Fourier Features (Rahimi & Recht, 2007), that allows scalable training on a set of diverse ob-jective landscapes at low computational cost. We illustrate the benefit of a state-conditioned optimizer in Figure 1. POP increases the learning rate when the opti-mization landscape is sufficiently smooth, enabling rapid convergence to a local minimum. Upon reaching such a min-imum, POP can either refine the solution through smaller updates or promote exploration by leveraging information from other regions of the landscape. This adaptive behavior emerges from the policy that is meta-learned to escape local 1

> arXiv:2602.15473v1 [cs.LG] 17 Feb 2026

POP: Prior-fitted Optimizer Policies 2               

> 1
> 0
> 1
> 2
> x1
> 2
> 1
> 0
> 1
> 2
> x2
> 0
> 10
> 20
> 30
> 40
> 50
> f(x)
> Six-Hump Camel Function
> 21012
> x1
> 2
> 1
> 0
> 1
> 2
> x2
> Adam Trajectory
> 21012
> x1
> 2
> 1
> 0
> 1
> 2POP Trajectory
> 2
> 0
> f(x) Adam
> POP
> 10 4
> 10 1
> f(x)
> 020 40 60 80
> Step
> 1
> 2
> Learning Rate
> 0.0
> 7.5
> 15.0
> 22.5
> 30.0
> 37.5
> 45.0
> 52.5

Figure 1. POP (blue) adapts its learning rate based on the optimization landscape, enabling rapid convergence, escape from local minima, and improved global optimization compared to Adam (red). Yellow diamond represents the global minima, the white cross represents the start state, and the square represents the end state. 

optima over a plethora of pretraining functions. A thorough evaluation highlights POPâ€™s generalization on the Virtual Library of Simulation Experiments (Surjanovic & Bingham) benchmark consisting of 47 diverse optimiza-tion functions. We further evaluate our modelâ€™s capabili-ties to transfer to longer optimization horizons and higher-dimensional problems. A statistical ranking analysis reveals that our method outperforms classical optimizers as well as a meta-learned optimizer introduced recently under matched budget constraints. The key contributions of this paper are: â€¢ We introduce POP, a novel optimizer that is meta-trained over millions of functions and learns step size policies conditioned on the optimizationâ€™s contextual information, enabling effective explo-rationâ€“exploitation behavior during optimization. â€¢ A Gaussian processâ€“based prior that combines convex and non-convex objectives, enabling scalable meta-training on diverse optimization landscapes and sup-porting strong generalization. â€¢ An empirical evaluation on 47 benchmark functions showing superior performance over conventional op-timizers, with ablations on reward design, evaluation budget, and scalability to higher dimensions. 

2. POP: Prior-fitted Optimizer 

In the following, we consider unconstrained optimization problems of the form xâˆ— âˆˆ argmin xâˆˆRd f (x), where each task instance is an objective function f sampled from a prior 

f âˆ¼ p(f ) over the space of compact functions. We refer to an episode as optimizing a single sampled function f for a fixed budget of T time steps. We define an optimization trajectory as: 

Definition 2.1 (Optimization trajectory) . For an objective function f , we define the optimization trajectory that in-cludes the first t update steps of a maximum budget of T

updates as: 

Ï„ (t) := 

 

xi, y i, âˆ‡f (xi), iT

 ti=1 

, (1) where xi denotes the parameters of the optimization prob-lem in step i, while yi:= f (xi) is the function value, âˆ‡f (xi)

the gradient information, and iT the normalized timestep. First-order optimizers proceed by applying an update rule based on local gradient information, e.g., GD applies xt =

xtâˆ’1 âˆ’ Î·tâˆ‡f (xtâˆ’1), where Î·t is the step size at iteration 

t. In this work, we replace a preset step size schedule by a meta-learned policy Î·t âˆ¼ Ï€Î¸ (Î·t|Ï„ (tâˆ’1) ), where at each iteration, our optimizer outputs the next step size Î·t via a state-conditioned control signal Ï„ (tâˆ’1) based on the opti-mization trajectory (cf. Theorem 2.1) up to step tâˆ’1.

2.1. MDP for Optimization 

We formulate the optimization problem as a Markov Deci-sion Process (MDP) defined by the tuple (S, A, P, r, p 0, Î³ ),where S are the states, A the actions, P the transition dy-namics, r : A Ã— S â†’ R denotes the reward signal, p0 is the probability distribution of initial elements in the optimiza-tion trajectory, and Î³ denotes the discount factor. Our goal is to train a parameterized policy Ï€Î¸ : S Ã— A â†’ R+.In our meta-learning setting, an episode corresponds to an optimization trajectory Ï„ on a sampled task instance from a prior, i.e., f âˆ¼ p(f ). In the sequential decision problem, the policy receives a representation of the current optimization context and outputs a continuous coordinate-wise step size action at = Î·t âˆˆ Rd > 0. The state st âˆˆ S is simply the trajectory of updates so far st := Ï„ (t), for t âˆˆ [1 , . . . , T ].

2.2. Optimization Step 

Given an optimization trajectory Ï„ (tâˆ’1) , our stochastic policy is a network with parameters Î¸ that outputs the 2POP: Prior-fitted Optimizer Policies 

update steps at iteration t for each function dimension 

Î¼Î¸

 Ï„ (tâˆ’1)  âˆˆ Rd and the standard deviation ÏƒÎ¸ âˆˆ Rd.

Ï€Î¸



Î·t | Ï„ (tâˆ’1) 

= N



Î¼Î¸



Ï„ (tâˆ’1) 

, diag  Ïƒ2

> Î¸



(2) To conduct an optimization, POP applies three steps. First, it samples a coordinate-wise next step size from our pol-icy network (Equation 3). Then, POP conducts a gradient update (Equation 4), and finally, it updates the trajectory (Equation 5). 

Î·t âˆ¼ Ï€Î¸



Î·t | Ï„ (tâˆ’1) 

(3) 

xt â† xtâˆ’1 âˆ’ Î·tâˆ‡f (xtâˆ’1) (4) 

Ï„ (t) â† Ï„ (tâˆ’1) âˆª

 

xt, y t, âˆ‡f (xt), tT

 

(5) 

2.3. Reward Signal 

We use a dedicated reward function for our optimization, defined as the improvement to the best observed function value so far in a trajectory. Let yâˆ—

> tâˆ’1

= min (Â·,y, Â·,Â·)âˆˆÏ„ (tâˆ’1) y,we define: 

R



yt, Ï„ (tâˆ’1) 

= max  0, y âˆ— 

> tâˆ’1

âˆ’ yt

 . (6) This reward assigns a positive signal only when an action yields a new observed minimum, directly linking policy op-timization to objective improvement. Since non-improving steps receive no penalty, the agent can explore the function freely, and meaningful improvements guide learning. 

2.4. Meta-Learning Objective 

We optimize the policy parameters Î¸ to maximize the ex-pected return over tasks sampled from a prior (cf. Section 3). We formulate the objective as follows: 

max  

> Î¸

Ef âˆ¼p(f ) EÏ„ âˆ¼POP (f ;Î¸)

> T

X

> t=0

Î³tR



yt, Ï„ (tâˆ’1) 

, (7) where Ï„ âˆ¼ POP (f ; Î¸) is computed by running Equations 3-5 for T steps on the function f sampled from the prior. We train the policy parameters using the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017). 

3. A Prior for Optimization Problems 

In order to meta-train our method, we sample optimiza-tion problems from a prior distribution f âˆ¼ p(f ). This distribution must be sufficiently diverse and complex to cover a wide range of optimization problems, including both convex and non-convex functions, so that learned op-timizers generalize well across problems. Therefore, we define a prior over optimization objectives as a combina-tion of a separable quadratic (convex) component and a Gaussian process prior with an RBF kernel, approximated via Random Fourier Features (RFF) for computational effi-ciency (Rahimi & Recht, 2007). We define a mixing weight 

Î± âˆ¼ U (Î±min , Î± max ) that interpolates between the convex and non-convex components. For a given dimensionality D,functions are constructed as: 

f (x) := Î±f C (x) + (1 âˆ’ Î±)fRFF (x), (8) with the convex component defined as: 

fC (x) = 

> D

X

> d=1

Î²1 

> d

(xd âˆ’ Î²2 

> d

)2, (9) where Î²1 

> d

âˆ¼ U (Î²1min , Î² 1max ) controls the curvature and en-sures convexity, and Î²2 

> d

âˆ¼ U (Î²2min , Î² 2max ) determines the location of the quadratic minimum. With probability pconvex 

we set Î± = 1 , yielding strictly convex quadratic objectives. The RFF Gaussian process prior is constructed as: 

fRFF (x) = âˆš2

> M

X

> m=1

Ï‰1 

> m

cos(( Ï‰2

> m

)âŠ¤x + Ï‰3

> m

). (10) The Gaussian process approximation uses M Random Fourier Features with phases Ï‰3 

> m

âˆ¼ U(0 , 2Ï€), weights 

Ï‰1 

> m

âˆ¼ N (0 , Ïƒ 2/M ), and frequencies Ï‰2 

> m

âˆ¼ N (0, â„“ âˆ’2ID ).The lengthscale â„“ âˆ¼ U (â„“min , â„“ max ) controls the smooth-ness of the resulting functions, while the output scale 

Ïƒ âˆ¼ U (Ïƒmin , Ïƒ max ) controls their amplitude. The 1/M 

scaling of Ï‰1 

> m

ensures that the variance of fRF F remains bounded as M increases. To sample a new function, we simply sample the new hy-perparameters Î±, â„“, and Ïƒ. For the new hyperparameters, we then sample new Î² âˆˆ R2Ã—D and Ï‰ âˆˆ R3Ã—D . In Sec-tion A, we prove that drawing functions from a GP-based prior is universal and every possible function in a compact set can be approximately drawn from a GP with random hyperparameters. 

4. In-Distribution Generalization 

One of the major challenges when meta-learning an opti-mizerâ€™s policy is the mismatch of domains and ranges across functions, leading to out-of-distribution failures (i.e., naively meta-learned policies fail for functions having very similar shapes but different scales). To ensure that our policy will be meta-learned and transferred to matching functions under an in-distribution assumption, we transform the domains and ranges of all training and test functions identically. 

Initial Context. POP trains deep parametric policies that output the next learning rate based on the trajectory of ob-servations so far, therefore, transferring knowledge from 3POP: Prior-fitted Optimizer Policies 

previously-seen functions with a similar landscape. There-fore, it needs an initial trajectory as an input before it con-ducts the first update step. As the initial context for our policy at the beginning of the optimization, we sample c âˆˆ N+ observations. Let 

p0 âˆˆ P (x) denote a random initial probability measure on the parameter space, where P(x) is the set of probability distributions on a functionsâ€™ parameter space. We define 

sc âˆ¼ p0 â‡” sc := Ï„ (c). After sampling the initial random context, we use the xi with the lowest f (xi) as the start-ing point of the gradient descent update steps leveraging coordinate-wise step sizes. 

Boundary Scaling. For each function, we assume known domain coordinate-wise boundaries x âˆˆ [xmin , x max ] and map coordinates to a fixed symmetric interval via boundary scaling per dimension. 

xbnd  

> t

= Vx Â·



2 Â· xt âˆ’ xmin 

xmax âˆ’ xmin âˆ’ 1



with Vx âˆˆ R+.

(11) where Vx refers to the target scale. Analogously, we scale function values using running extrema yt, min := min jâ‰¤t yj

and yt, max := max jâ‰¤t yj , initialized from the random con-text sc and update them online when the functionâ€™s land-scape is explored and the context information increases: 

ybnd  

> t

= Vy Â·



2 Â· yt âˆ’ ymin 

> t

ymax  

> t

âˆ’ ymin 

> t

âˆ’ 1



with Vy âˆˆ R+,

(12) where Vy refers to the target scale. 

Z-transformation. Boundary-scaled xbnd  

> t

and ybnd  

> t

are then standardized by applying an online Z-transformation using the evaluation data of the current optimization trajec-tory, yielding Ëœxt and Ëœyt.

Gradient Scaling. Because Ëœxt, Ëœyt are transformed before being passed to the model, gradients defined in the original coordinates are no longer consistent with the transformed space. We therefore apply the corresponding Jacobian to obtain gradients that reflect the correct local sensitivities: 

âˆ‡ Ëœf (xt) = 

 Ïƒx,t 

> Ïƒy,t

  Vy

> Vx

  xmax âˆ’xmin  

> ymax
> tâˆ’ymin
> t



âŠ™ âˆ‡ f (xt),

(13) with Vx, V y âˆˆ R+ denoting coordinate-wise scaling param-eters, and Ïƒx,t , Ïƒ y,t denoting the standard deviation of the respective dimension up to time step t. The optimizer main-tains one state per coordinate, defined exclusively in the transformed space. 

Transformed Update. Since the model operates on the transformed variables ËœÏ„ (tâˆ’1) , the gradient update is performed in the transformed space. Specifically, Ëœxt =Ëœxtâˆ’1 âˆ’ Î·tâˆ‡ Ëœf (xtâˆ’1). The updated iterate Ëœxt is then mapped back to the original domain by applying the inverse Z-transformation followed by inverse boundary scaling, after which it is evaluated using the original objective function 

f (xt). Following the evaluation of the new xt, yt, and 

âˆ‡f (xt), the entire transformed set ËœÏ„ (t) is re-transformed using the updated statistics. 

5. Experimental Protocol 

We train our proposed optimizer (cf. Section 2) on our newly proposed prior (cf. Section 3), with parameter distributions detailed in Table 4 in Appendix B.1. Notably, we restrict our prior to two-dimensional ( D=2 ) objective functions. Visu-alizations of randomly sampled functions from this prior are shown in Section B.2. Rather than conditioning on the full optimization trajectory Ï„ (tâˆ’1) , we decompose trajectories along dimensions and restrict the policy input to the history of the corresponding coordinate. The optimizer thus op-erates independently per dimension, enforcing conditional independence during training while enabling efficient batch-ing and scalability to higher-dimensional problems. We open-source our implementation 1 to foster future research. 

5.1. Architecture and Training 

We parameterize both the actor and critic using a shared transformer backbone with separate MLP heads, for a total of 166 K trainable parameters. Full architectural and train-ing hyperparameters are reported in Tables 5 and 6. The actor outputs the mean of a Gaussian policy and samples actions using a learned, state-independent log standard de-viation. For numerical stability, the log standard deviation is passed through a tanh nonlinearity and rescaled to lie in [âˆ’3.0, 2.0] . To enforce strictly positive learning rates, sampled actions are exponentiated via exp( Î·t).We sample B = 256 optimization problems per training iter-ation and train for 50 ,000 iterations, yielding approximately 

1.28 Ã— 10 7 optimization problem instances. The objective function is resampled from the prior every 8 iterations, re-sulting in 1.6 Ã— 10 6 distinct functions. At the beginning of each iteration, a random initial context of c = 10 points is generated, after which the agent is rolled out for T = 40 

optimization steps to solve a single optimization problem and collect trajectories. Following each rollout, the policy and value networks are updated using PPO on the collected transitions. Training requires approximately 10 hours on a single NVIDIA H100 GPU. 

5.2. Benchmarks In-distribution Performance. We construct a validation set for ablations consisting of 1024 optimization problems in D=2 , and test sets for one-shot evaluation with 1024 problems each for D âˆˆ { 2, 8, 16 , 32 }. All problems use a 

> 1https://anonymous.4open.science/r/ pop-F342/README.md

4POP: Prior-fitted Optimizer Policies 

fixed initial context of size c=10 .

Out-of-distribution Experiments. We further evaluate the out-of-distribution performance of our method on 47 optimization problems from the widely used Virtual Library of Simulation Experiments (VLSE) benchmark (Surjanovic & Bingham). It covers a diverse range of optimization landscapes, including functions with many local minima, as well as bowl-shaped, plate-shaped, and valley-shaped geometries, in addition to steep ridges and drops. Each benchmark problem is evaluated at its native dimensionality, which ranges from 1D to 6D across the suite. 

5.3. Baselines 

We compare our method against a diverse set of optimiza-tion baselines. Gradient Descent (GD) (Cauchy, 1847) and 

Adam (Kingma & Ba, 2015) represent first-order gradient-based methods, with Adam using adaptive learning rates. 

L-BFGS (Liu & Nocedal, 1989) is a quasi-Newton method leveraging approximate second-order information. Random Search provides a simple derivative-free baseline, while 

Genetic Algorithms (Holland, 1992) and Differential Evo-lution (Storn & Price, 1997) are population-based evolution-ary methods for global optimization. TPE (Bergstra et al., 2011) is a Bayesian optimization approach that models the objective to guide sample selection. Lastly, VeLo (Metz et al., 2022) is a learned optimizer which has been trained on a prior of neural network architectures. All baselines are evaluated under the same optimization budget. 

6. Evaluation 

Hypothesis 1: Learned optimizers trained on synthetic priors can generalize to unseen problems from the same distribution. 

Initially, we evaluate if POP can generalize to unseen prob-lems drawn from the same distribution. Performance is measured using normalized improvement, which captures how much the optimizer improves beyond the best value observed in the initial context, while normalizing by the contextâ€™s objective range to ensure comparability across problems: 

NI = ymin  

> t

âˆ’ yâˆ—

> t

ymax  

> t

âˆ’ ymin  

> t

+ Ïµ (14) where yt, min := min jâ‰¤t yj denotes the smallest loss value in the initial context and yâˆ— 

> t

= min (Â·,y, Â·,Â·)âˆˆÏ„ (tâˆ’1) y is the best loss value achieved in the optimization trajectory so far. We include a small Ïµ for numerical stability when the context range is close to zero. In Figure 2, we provide the episode rewards and the normalized improvement on the validation set during training. As observed, the normalized improvement increases together with the training reward,              

> (a) Episode rewards with moving average 010K 20K 30K 40K 50K
> Meta-Learning Iteration
> 0.0
> 0.5
> 1.0
> 1.5
> Reward
> Raw rewards
> Moving Avg (100)
> (b) Mean normalized improvement (NI) with 95% CI 010K 20K 30K 40K 50K
> Meta-Learning Iteration
> 0.05
> 0.10
> 0.15
> 0.20
> 0.25
> Normalized Improvement
> Figure 2. Meta-learning reward and evaluation validation perfor-mance of our POP agent.

indicating successful within-distribution generalization on unseen problems. We further find that POP is capable of in-ferring the shape of the objective function from context and adjusting its behavior accordingly. For convex functions, POP rapidly converges to a local minimum and subsequently refrains from further exploration of the function landscape. In contrast, for non-convex functions, POP explores local minima, strategically escapes them, and repeatedly con-verges to new local minima. A visual illustration of this behavior is provided in Section D.1. Next, we evaluate the competitiveness of our method by comparing it against a set of baselines such as GD, Adam, L-BFGS, and Random search on unseen test problems drawn from the same prior distribution. Since we require c ini-tial context points, we initialize all methods with identical context sets. For gradient-based methods, we select the best-performing context point as the initialization. We addi-tionally tune the learning rate for GD, Adam and L-BFGS using grid search on the validation set (hyperparameter set-tings are detailed in Section C) and leave the other parameter values at their default setting. We evaluate all methods on the test set. As shown in Figures 3 and 4, POP outperforms all base-lines by achieving higher normalized improvement during optimization, while gradient-based baselines plateau earlier, and random search improves more slowly. The initial strong 5POP: Prior-fitted Optimizer Policies 10 20 30 40 50 

> Step
> 0.3
> 0.2
> 0.1
> 0.0
> 0.1
> 0.2
> 0.3
> Normalized Improvement
> POP
> Random
> Adam
> GD
> L-BFGS
> Context/Opt boundary

Figure 3. In-distribution test set performance vs. baselines. Mean normalized improvement over steps; shading indicates 95% CIs. Dashed line marks the context/optimization boundary. 12345

> Random
> SGD
> L-BFGS
> Adam
> POP
> CD
> 100% Method Ranks

Figure 4. Method rankings on the in-distribution test set at 100% budget. Lower ranks correspond to better performance, while hori-zontal bars indicate differences that are not statistically significant. 

performance of L-BFGS can be attributed to its substan-tially higher number of function evaluations per iteration compared to the other methods, which provides an early advantage but leads to earlier plateauing. We next conduct a series of ablation studies to evaluate our design choices, focusing on the reward formulation and action parameterization. 

Reward Formulation. We compare five reward functions that differ in how they incentivize improvement. Let Ëœyt de-note the boundary scaled and Z-transformed current objec-tive value and Ëœyâˆ— 

> tâˆ’1

the boundary scaled and Z-transformed best value observed in the optimization trajectory. The cor-responding reward functions are defined as : 

RCurrent = Ëœ yt, (15) 

RGlobal Imp = Ëœ yâˆ— 

> tâˆ’1

âˆ’ Ëœyt, (16) 

RGlobal Imp c = max  0, Ëœyâˆ— 

> tâˆ’1

âˆ’ Ëœyt

 , (17) 

RSMAPE = max 0, 2 Â· max  0, Ëœyâˆ— 

> tâˆ’1

âˆ’ Ëœyt



|Ëœyâˆ—

> tâˆ’1

| + |Ëœyt|

!

, (18) 

RMix ,Î± = Î± max  0, Ëœyâˆ— 

> tâˆ’1

âˆ’ Ëœyt

 + (1 âˆ’ Î±) (Ëœ ytâˆ’1 âˆ’ Ëœyt) .

(19) We evaluate each on the within-distribution validation set with identical starting context. Table 1 shows that 

RGlobal Imp c achieves the highest mean normalized improve-

Table 1. Reward ablation on in-distribution validation set. Mean normalized improvement (higher is better) after 50k training itera-tions with 95% CIs.                           

> Method Mean CI 95% low CI 95% high
> RCurrent 0.1917 0.1797 0.2037
> RGlobal Imp 0.1270 0.1179 0.1360
> RGlobal Imp c0.2127 0.1998 0.2255
> RSMAPE 0.1193 0.1112 0.1274
> RMix ,Î± =0 .10.1454 0.1347 0.1562
> RMix ,Î± =0 .20.1969 0.1846 0.2093
> RMix ,Î± =0 .30.1817 0.1698 0.1935

ment. We attribute this to the fact that clipped rewards do not penalize exploratory actions, allowing the agent to tem-porarily move away from the current best solution without incurring negative reward, enabling to find better solutions. 

Table 2. Action ablation. 

Action Mode Mean LR 0.2127 

Next x 0.1986 

Action parameterization. 

We ablate the choice of ac-tion representation used by the policy. In particular, we compare two alternatives: (i) predicting a per-coordinate scalar learning rate and ap-plying a gradient descent update in the transformed state space, and (ii) directly predicting the next iterate Ëœxt+1 on a per-coordinate basis in the transformed state space. All other components of the model and training procedure are kept fixed. Table 2 reports the mean performance of both variants, where, predicting the learning rate yields a higher mean, indicating that parameterizing the action as a learning rate is more effective. 

Hypothesis 2: Learned coordinate optimizers trained on a low-dimensional (2D) prior for a fixed number of iterations generalizes to longer optimization horizons and higher-dimensional problems. 

To test whether the POP optimizer trained on 2D trajectories generalizes beyond its training regime, we first evaluate performance under a longer optimization budget on unseen test functions sampled from the same prior distribution. Since, we utilize last-token pooling in our architecture, the model is not constrained to the sequence length seen during training. Therefore, we extend the evaluation budget from 50 to 100 iterations, setting c=10 and allowing the model to predict the subsequent T =90 updates. As shown in Figure 5, POP continues to reduce the loss beyond the training horizon and outperforms all baselines over the extended budget. Next, we evaluate whether POP generalizes to higher-dimensional test problems compared to the 2D problems encountered during training. Due to its coordinate-wise formulation, the optimizer can be applied to higher dimen-sions by batching across coordinates without architectural modification. We evaluate the model on 8D, 16D, and 32D 6POP: Prior-fitted Optimizer Policies 20 40 60 80 100 

> Step
> 0.3
> 0.2
> 0.1
> 0.0
> 0.1
> 0.2
> 0.3
> Normalized Improvement
> POP
> Random
> Adam
> GD
> L-BFGS
> Context/Opt boundary

Figure 5. In-distribution test set performance vs. baselines at twice the training budget. Mean normalized improvement over steps; shading indicates 95% CIs. Dashed line marks the con-text/optimization boundary. 

test problems, running each method for 100 optimization steps 2. For the baselines we zero-shot apply the best found LRâ€™s for the 2D validation set. As shown in Table 3, POP maintains strong performance relative to the baselines even as dimensionality increases. 

Table 3. Mean performance across different dimensionalities. 

Method 8D 16D 32D POP 0.2433 0.2467 0.2508 

Random 0.1788 0.1789 0.1809 Adam 0.1588 0.1553 0.1622 GD 0.1655 0.1634 0.1642 L-BFGS 0.1833 0.1829 0.1776 

Hypothesis 3: Learned coordinate optimizers trained on a low-dimensional (2D) synthetic prior can generalize to out-of-distribution problems. 

We further validate the generalization capabilities of our method on the Virtual Library of Simulation Experiments 

benchmark. Since the optimal hyperparameters for each algorithm on the benchmark are unknown, all baseline meth-ods are evaluated using their default settings. Initially, we compare all methods in terms of per-task normalized regret: 

ropt ,t = yâˆ— 

> t

âˆ’ ymin 

ymax âˆ’ ymin , (20) where yâˆ— 

> t

represents the best function value achieved by an optimizer at iteration t for a given function, while ymin (ymax )represent the minimal (maximal) value of that function. In Figure 6, we present the average normalized regret for all methods across the different optimization tasks, where POP manages to outperform the majority of methods, and is sec-ond only to TPE after 100 optimization trials. For detailed per-function results, we point the reader to Appendix E. 

> 2

To maintain comparable smoothness across di-mensions, the number of parameters is increased to 

M âˆˆ { 8000 , 16000 , 32000 }.10 0 10 1 10 2

> Step
> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> Average Normalized Regret  POP
> Random Search
> Adam
> GD
> L-BFGS
> Differential Evolution
> Genetic Algorithm
> TPE
> VELO

Figure 6. The average normalized regret for all methods. Solid lines represent the mean value. 123456789

> GD
> Adam
> Genetic Algorithm
> VELO
> Random Search
> Differential Evolution
> TPE
> L-BFGS
> POP
> CD
> 50% Method Ranks 123456789
> GD
> Adam
> Random Search
> Genetic Algorithm
> VELO
> Differential Evolution
> TPE
> L-BFGS
> POP
> CD
> 100% Method Ranks

Figure 7. The critical difference diagram, where horizontal bars indicate a lack of statistical significance. A lower rank depicts a better performance. Top: Results after 50% of the optimization process has elapsed, Bottom: Results at the end of the optimization process. 

In Figure 7 we provide critical difference (CD) diagrams, to investigate the method ranks and the statistical signifi-cance of the results. To build the CD diagrams, we use the autorank (Herbold, 2020) package that runs a Friedman test with a Nemenyi post-hoc test, at a 0.05 significance level. The results indicate that POP wins in the majority of tasks and achieves the best rank across all methods. During half of the optimization process (Figure 7 top), POP not only outperforms all baselines, but the difference in results with SGD, Adam, genetic algorithms, VELO, and random search is statistically significant. After the full optimization pro-cess has finished (Figure 7 bottom), POP still manages to outperform all baselines in an out-of-distribution scenario, with a significant difference in results to SGD, Adam, and random search. Lastly, in Figure 8, we investigate the average rank obtained by every method during the optimization procedure, where, after the first 10 random observations for POP, it manages to not only achieve the best final performance, but it has better any-time performance compared to other baselines. 7POP: Prior-fitted Optimizer Policies 20 40 60 80 100  

> Step
> 2
> 4
> 6
> 8
> Method Rank
> POP
> Random Search
> Adam
> GD
> L-BFGS
> Differential Evolution
> Genetic Algorithm
> TPE
> VELO
> Figure 8. Method ranks during the optimization procedure. The solid lines represent the average rank, while, the shaded region represents the standard error.

7. Related Work 

Optimizers. Optimization aims to find the mini-mum/maximum of a given objective function. Depending on the underlying structure of the function, different opti-mization methods may be more or less effective. For convex functions, second-order methods such as Newtonâ€™s method (Newton, 1736) or quasi-Newton approaches like L-BFGS (Liu & Nocedal, 1989) can be highly efficient. In contrast, for non-convex problems, population-based methods such as Genetic Algorithms (Holland, 1992) and Differential Evo-lution (Storn & Price, 1997) are often effective at escaping local minima. For low-dimensional problems, Bayesian Optimization (Bergstra et al., 2011) can be particularly pow-erful, although it scales poorly with increasing dimensional-ity. In higher-dimensional settings, such as training neural networks, first-order methods including Gradient Descent 

(Cauchy, 1847) and adaptive variants like Adam (Kingma & Ba, 2015) are widely used. 

Learned Optimizers. A growing field that tries to automate the design and tuning of optimization algorithms is known as Learning to Optimize (L2O) (Chen et al., 2021; Tang & Yao, 2024). Most L2O approaches meta-train optimizers on neural network training tasks (Metz et al., 2022; Xu et al., 2019; Shu et al., 2020). These methods differ in what as-pects of the optimization process are learned, ranging from direct parameter updates (Metz et al., 2022) to the prediction of a global learning rate schedule (Xu et al., 2019; Shu et al., 2020). They also vary in their meta-learning methodology, with some approaches relying on reinforcement learning (Xu et al., 2019), while others employ gradient-based meta-training (Metz et al., 2022; GÂ¨ artner et al., 2023). 

Prior-fitted Models. Prior-Data Fitted Networks (PFN) (M Â¨uller et al., 2022; Hollmann et al., 2025) was introduced as a transformer-based model that is meta-learned by train-ing on millions of synthetic datasets generated from speci-fied priors. 

Novelty and Delineation from Prior Work 

Our approach, POP, differs from prior work along sev-eral key dimensions. Instead of meta-training on neural network losses (Shu et al., 2020; Xu et al., 2019; Metz et al., 2022), we train on a novel prior over convex and non-convex optimization problems, enabling meta-training over millions of synthetic functions at low computational cost. Rather than learning a global learning-rate schedule, POP predicts coordinate-wise learning rates. Using carefully designed input transformations such as boundary scaling, Z-transformation, and gradient scaling, POP exhibits strong generalization. Finally, coordinate-wise independence al-lows POP to scale naturally to high-dimensional settings via batching, while the transformer backbone efficiently processes long per-coordinate optimization histories. Our work falls within meta-learning optimizers, where a model is trained to solve a distribution of optimization problems and generalize to unseen ones. We employ a transformer-based backbone that processes optimization trajectories and gradients to predict adaptive step sizes, ef-fectively replacing classical solvers such as gradient descent. Operating at the algorithmic level, the learned model acts as a controller over the optimization process, modulating update directions and removing the need for hand-designed learning rate schedules. 

8. Future Work 

While our proposed method works well for optimization, its application to machine learning tasks remains an open direction that we leave to future work. A key challenge is designing synthetic priors that better reflect the structure of ML loss landscapes and generalize beyond function op-timization. In addition, we plan to train the method on longer optimization trajectories and develop more compute-efficient policy architectures. 

9. Conclusion 

Optimization algorithms are central to minimizing func-tions, yet gradient-based methods that rely on first-order information often become trapped in local minima, limit-ing their ability to explore complex landscapes. We pro-pose POP, a meta-learned optimizer that learns step size policies conditioned on the accumulated optimization his-tory, enabling strategic escapes from local minima. POP is meta-trained on millions of synthetic optimization problems drawn from a novel Gaussian process prior, covering both convex and non-convex regimes. In a thorough evaluation, our model generalizes well and achieves strong performance on a well-established benchmark suite encompassing 47 di-verse optimization functions. Overall, POP shows that state-conditioned learning rate policies, meta-trained on synthetic priors, enable robust and general-purpose optimization ca-pabilities. 8POP: Prior-fitted Optimizer Policies 

Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 

Acknowledgments 

Josif Grabocka and Arlind Kadra acknowledge the funding support from the â€Bayerisches Landesamt fur Steuerâ€ for the Bavarian AI Taxation Laboratory. Gresa Shala and Josif Grabocka acknowledge the funding by The Carl Zeiss Foundation through the research network â€Responsive and Scalable Learning for Robots Assisting Humansâ€ (ReScaLe) of the University of Freiburg Erind Bedalli and Josif Grabocka acknowledge the financial support from the Albanian-American Development Founda-tion through the READ program. Moreover, we gratefully acknowledge the scientific sup-port and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universit Â¨at Erlangen-N Â¨urnberg (FAU). NHR@FAU hardware is partially funded by the German Research Foundation (DFG) â€“ 440719683. 

References 

Bergstra, J., Bardenet, R., Bengio, Y., and K Â´egl, B. Algo-rithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems , volume 24, 2011. Cao, Y., Chen, T., Wang, Z., and Shen, Y. Learning to optimize in swarms. Advances in neural information processing systems , 32:15044â€“15054, 2019. Cauchy, A.-L. M Â´ethode g Â´en Â´erale pour la r Â´esolution des syst `emes dâ€™ Â´equations simultan Â´ees. In Comptes Rendus Hebdomadaires des S Â´eances de lâ€™Acad Â´emie des Sciences ,1847. Chen, T., Chen, X., Chen, W., Heaton, H., Liu, J., Wang, Z., and Yin, W. Learning to optimize: A primer and a benchmark. J. Mach. Learn. Res. , 23:189:1â€“189:59, 2021. Daoud, M. S., Shehab, M., Al-Mimi, H. M., Abualigah, L., Zitar, R. A., and Shambour, M. K. Y. Gradient-based opti-mizer (gbo): A review, theory, variants, and applications. 

Archives of Computational Methods in Engineering , 30 (4):2431â€“2449, December 2022. ISSN 1886-1784. doi: 10.1007/s11831-022-09872-y. Defazio, A., Yang, X., Khaled, A., Mishchenko, K., Mehta, H., and Cutkosky, A. The road less scheduled. Ad-vances in Neural Information Processing Systems , 37: 9974â€“10007, 2024. Goldie, A. D., Lu, C., Jackson, M., Whiteson, S., and Fo-erster, J. Can learned optimization make reinforcement learning less difficult? ArXiv , abs/2407.07082, 2024. doi: 10.48550/arxiv.2407.07082. Gomes, H., L Â´eger, B., and Gagnâ€™e, C. Meta learning black-box population-based optimizers. ArXiv , abs/2103.03526, 2021. G Â¨artner, E., Metz, L., Andriluka, M., Freeman, C. D., and Sminchisescu, C. Transformer-based learned optimiza-tion, 2023. Hassan, E., Shams, M. Y., Hikal, N. A., and Elmougy, S. The effect of choosing optimizer algorithms to im-prove computer vision tasks: a comparative study. Mul-timedia Tools and Applications , 82(11):16591â€“16633, September 2022. ISSN 1573-7721. doi: 10.1007/ s11042-022-13820-0. Herbold, S. Autorank: A python package for automated ranking of classifiers. Journal of Open Source Software ,5(48):2173, 2020. doi: 10.21105/joss.02173. Holland, J. H. Adaptation in Natural and Artificial Systems .University of Michigan Press, Ann Arbor, 1992. Hollmann, N., M Â¨uller, S. G., Purucker, L., Krishnakumar, A., KÂ¨ orfer, M., Hoo, S. B., Schirrmeister, R., and Hutter, F. Accurate predictions on small data with a tabular foundation model. Nature , 637:319 â€“ 326, 2025. doi: 10.1038/s41586-024-08328-6. Keskar, N. and Socher, R. Improving generalization performance by switching from adam to sgd. ArXiv ,abs/1712.07628, 2017. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Confer-ence Track Proceedings , 2015. Kristiansen, G., Sandler, M., Zhmoginov, A., Miller, N., Goyal, A., Lee, J., and Vladymyrov, M. Narrowing the focus: Learned optimizers for pretrained models. ArXiv ,abs/2408.09310, 2024. doi: 10.48550/arxiv.2408.09310. Lan, Q., Mahmood, R., Yan, S., and Xu, Z. Learning to optimize for reinforcement learning. RLJ , 2:481â€“497, 2023. doi: 10.48550/arxiv.2302.01470. 9POP: Prior-fitted Optimizer Policies 

Liao, L., Li, H., Shang, W., and Ma, L. An empirical study of the impact of hyperparameter tuning and model optimization on the performance properties of deep neural networks. ACM Transactions on Software Engineering and Methodology (TOSEM) , 31:1 â€“ 40, 2022. doi: 10. 1145/3506695. Liu, D. C. and Nocedal, J. On the limited memory bfgs method for large scale optimization. Mathematical Pro-gramming , 45(1):503â€“528, 1989. Liu, Y., Gao, Y., and Yin, W. An improved analysis of stochastic gradient descent with momentum. In Pro-ceedings of the 34th International Conference on Neu-ral Information Processing Systems , NIPS â€™20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Ma, X., Zhong, Z., Li, Y., Li, D., and Qiao, Y. Anovel reinforcement learning based heap-based opti-mizer. Knowl. Based Syst. , 296:111907, 2024. doi: 10.1016/j.knosys.2024.111907. Mary, A., Vincent, Member, I. P. J. S., and Member, I. A. A. B. S. Optimizing hyperparameters in meta-learning for enhanced image classification. IEEE Access , 13:130816â€“ 130831, 2025. doi: 10.1109/access.2025.3591142. Metz, L., Maheswaranathan, N., Freeman, C., Poole, B., and Sohl-Dickstein, J. N. Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves. ArXiv , abs/2009.11243, 2020. Metz, L., Harrison, J., Freeman, C., Merchant, A., Beyer, L., Bradbury, J., Agrawal, N., Poole, B., Mordatch, I., Roberts, A., and Sohl-Dickstein, J. N. Velo: Train-ing versatile learned optimizers by scaling up. ArXiv ,abs/2211.09760, 2022. doi: 10.48550/arxiv.2211.09760. Moudgil, A., Knyazev, B., Lajoie, G., and Belilovsky, E. Celo: Training versatile learned optimizers on a compute diet. Trans. Mach. Learn. Res. , 2025, 2025. M Â¨uller, S., Hollmann, N., Arango, S. P., Grabocka, J., and Hutter, F. Transformers can do bayesian inference. In 

International Conference on Learning Representations ,2022. Newton, I. Method of Fluxions and Infinite Series . Henry Woodfall, 1736. Rahimi, A. and Recht, B. Random features for large-scale kernel machines. In Neural Information Processing Sys-tems , 2007. Rasmussen, C. E. and Williams, C. K. I. Gaussian Processes for Machine Learning . The MIT Press, 2006. Robbins, H. and Monro, S. A stochastic approximation method. Annals of Mathematical Statistics , 22:400â€“407, 1951. Schlotthauer, J., Kroos, C., Hinze, C., Hangya, V., Hahn, L., and K Â¨uch, F. Pre-training llms on a budget: A comparison of three optimizers. ArXiv , abs/2507.08472, 2025. doi: 10.48550/arxiv.2507.08472. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. 

CoRR , abs/1707.06347, 2017. Shu, J., Zhu, Y., Zhao, Q., Meng, D., and Xu, Z. Meta-lr-schedule-net: Learned lr schedules that scale and general-ize, 07 2020. Storn, R. and Price, K. Differential evolution â€“ a simple and efficient heuristic for global optimization over continuous spaces. Journal of Global Optimization , 11(4):341â€“359, 1997. Surjanovic, S. and Bingham, D. Virtual library of sim-ulation experiments: Test functions and datasets. Re-trieved December 14, 2025, from http://www.sfu. ca/Ëœssurjano .Tang, K. and Yao, X. Learn to optimizeâ€”a brief overview. 

National Science Review , 11(8), April 2024. ISSN 2053-714X. doi: 10.1093/nsr/nwae132. Tian, Y., Zhang, Y., and Zhang, H. Recent advances in stochastic gradient descent in deep learning. Mathematics ,2023. doi: 10.3390/math11030682. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. At-tention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. Vishnu, T., Malhotra, P., Narwariya, J., Vig, L., and Shroff, G. M. Meta-learning for black-box optimization. Ma-chine Learning and Knowledge Discovery in Databases ,2020. Xu, Z., Dai, A. M., Kemp, J., and Metz, L. Learn-ing an adaptive learning rate schedule. arXiv preprint arXiv:1909.09712 , 2019. Yang, J., Chen, T., Zhu, M., He, F., Tao, D., Liang, Y., and Wang, Z. Learning to generalize provably in learning to optimize. ArXiv , abs/2302.11085, 2023. doi: 10.48550/ arxiv.2302.11085. 10 POP: Prior-fitted Optimizer Policies 

A. On the Universality of Gaussian Process Prior for Pretraining Learnable Optimizers 

The statement that a Gaussian Process (GP) is a universal approximator is a consequence of the properties of its covariance function, or kernel. The universality is not inherent to all GPs but depends on the choice of a universal kernel . The proof relies on the connection between GPs and Reproducing Kernel Hilbert Spaces (RKHS). The approximation of the GP with Random Fourier Features introduces an additional approximation term absorbed into Ïµ.

Theorem. Let K âŠ‚ Rd be a compact set. Let k : K Ã— K â†’ R be a universal kernel. Let f âˆˆ C(K) be any continuous function on K. Then, for any Ïµ > 0, there exists a set of points Xn = {x1, . . . , x n} âŠ‚ K and corresponding values 

Yn = {f (x1), . . . , f (xn)} such that the posterior mean Î¼n(x) of a Gaussian Process with prior mean m(x) = 0 and kernel 

k(x, x â€²) satisfies: 

sup 

> xâˆˆK

f (x) âˆ’ Î¼n(x) < Ïµ. 

A.1. Preliminaries: Gaussian Processes and RKHS 

A Gaussian Process GP( m(x), k (x, x â€²)) is a stochastic process where any finite collection of random variables has a multivariate normal distribution. It is fully specified by its mean function m(x) and covariance function (kernel) k(x, x â€²).For simplicity, we assume a zero prior mean, m(x) = 0 .Given a set of n noiseless observations (Xn, Y n) = {(xi, y i)}ni=1 where yi = f (xi), the posterior distribution of the GP at a new point xâˆ— is also Gaussian. The posterior mean is given by: 

Î¼n(xâˆ—) = k(xâˆ—, X n) Kâˆ’1 

> n

Yn,

where k(xâˆ—, X n) = [ k(xâˆ—, x 1), . . . , k (xâˆ—, x n)] is a row vector and Kn is the n Ã— n Gram matrix with entries (Kn)ij =

k(xi, x j ).Associated with every positive definite kernel k is a unique Reproducing Kernel Hilbert Space (RKHS), denoted Hk. This is a Hilbert space of functions on K with two key properties: 1. For every x âˆˆ K, the function kx(Â·) := k(x, Â·) is in Hk.2. ( Reproducing property ) For any g âˆˆ H k and x âˆˆ K, we have âŸ¨g, k xâŸ©Hk = g(x).A crucial result from GP theory is that the posterior mean Î¼n(x) is the function in Hk that minimizes the RKHS norm 

âˆ¥gâˆ¥Hk subject to the interpolation constraints g(xi) = yi for all i = 1 , . . . , n .

A.2. Universal Kernels 

The approximation power of a GP is entirely determined by the richness of its RKHS. This leads to the definition of a universal kernel. 

Definition (Universal Kernel). A continuous kernel k on a compact set K is called universal if its corresponding RKHS 

Hk is dense in the space of continuous functions C(K) with respect to the uniform norm ( Lâˆž). This means that for any continuous function f âˆˆ C(K) and any Î´ > 0, there exists a function g âˆˆ H k such that: 

sup 

> xâˆˆK

|f (x) âˆ’ g(x)| < Î´. 

Examples of universal kernels include: â€¢ Gaussian (RBF) Kernel: 

k(x, x â€²) = exp 



âˆ’ âˆ¥x âˆ’ xâ€²âˆ¥2

2Ïƒ2



.

â€¢ MatÂ´ ern Kernels: 

k(x, x â€²) = 21âˆ’Î½

Î“( Î½)

âˆš2Î½ âˆ¥x âˆ’ xâ€²âˆ¥

â„“

!Î½

KÎ½

âˆš2Î½ âˆ¥x âˆ’ xâ€²âˆ¥

â„“

!

, Î½ > 0,

11 POP: Prior-fitted Optimizer Policies 

where KÎ½ is the modified Bessel function of the second kind. â€¢ Laplacian Kernel: 

k(x, x â€²) = exp 



âˆ’ âˆ¥x âˆ’ xâ€²âˆ¥

Ïƒ



.

The universality of these kernels is often established using characterizations like Bochnerâ€™s theorem (a kernel is universal if its Fourier transform is positive on the entire frequency domain). 

A.3. Proof of the Theorem 

Let k be a universal kernel on a compact set K, and let f âˆˆ C(K) be the target function. Let Ïµ > 0 be the desired precision. 

Step 1: Density of RKHS in C(K)

Since k is a universal kernel, its RKHS Hk is dense in C(K). Therefore, for our given Ïµ > 0, there exists a function g âˆˆ H k

such that: 

sup 

> xâˆˆK

|f (x) âˆ’ g(x)| < Ïµ

2 .

Step 2: Convergence of the Interpolant 

Now we need to show that we can choose data points Xn such that the GP posterior mean Î¼n(x) (which interpolates f on 

Xn) becomes arbitrarily close to g(x).Let g be the function from Step 1. Since g âˆˆ H k, it has a finite RKHS norm, âˆ¥gâˆ¥Hk < âˆž. Let us choose a sequence of points Xn = {x1, . . . , x n} that becomes dense in K as n â†’ âˆž .Let Î¼gn be the GP posterior mean interpolating the values of g on Xn, i.e., Î¼gn(xi) = g(xi). It is a standard result in the theory of RKHS (see Wendland, Scattered Data Approximation ) that if Xn becomes dense in K, then the sequence of interpolants Î¼gn converges uniformly to g:

lim  

> nâ†’âˆž

sup 

> xâˆˆK

|Î¼gn(x) âˆ’ g(x)| = 0 .

Therefore, we can choose n large enough such that: 

sup 

> xâˆˆK

|Î¼gn(x) âˆ’ g(x)| < Ïµ

2 .

Step 3: Combining the Approximations 

Let Î¼fn be the GP posterior mean interpolating the target function f on the same set of points Xn, i.e., Î¼fn(xi) = f (xi).The posterior mean is a linear operator on the data values. Let 

Ln(Y )( x) = k(x, X n)Kâˆ’1 

> n

Y. 

Then Î¼fn = Ln(f (Xn)) and Î¼gn = Ln(g(Xn)) .We have: 

sup 

> xâˆˆK

|Î¼fn(x) âˆ’ Î¼gn(x)| = sup 

> xâˆˆK

|Ln(f (Xn) âˆ’ g(Xn))( x)| â‰¤ âˆ¥ Lnâˆ¥âˆž max  

> i=1 ,...,n

|f (xi) âˆ’ g(xi)|.

While âˆ¥Lnâˆ¥âˆž can grow with n, we can proceed using the triangle inequality: 

sup 

> xâˆˆK

|f (x) âˆ’ Î¼fn(x)| â‰¤ sup 

> xâˆˆK

|f (x) âˆ’ g(x)| + sup 

> xâˆˆK

|g(x) âˆ’ Î¼gn(x)| + sup 

> xâˆˆK

|Î¼gn(x) âˆ’ Î¼fn(x)|.

We have already bounded the first two terms. For the third term: 

Î¼gn(x) âˆ’ Î¼fn(x) = 

> n

X

> i=1

ci k(x, x i),

12 POP: Prior-fitted Optimizer Policies 

where the coefficients satisfy 

c = Kâˆ’1

> n

 g(Xn) âˆ’ f (Xn).

Hence, 

sup 

> xâˆˆK

|Î¼gn(x) âˆ’ Î¼fn(x)| â‰¤ Cn max  

> i

|g(xi) âˆ’ f (xi)| < C n

Ïµ

2 ,

where Cn depends on Xn.A more elegant argument considers the projection of f onto the subspace spanned by {k(xi, Â·)}ni=1 . The posterior mean Î¼fn

is precisely this projection. As n â†’ âˆž and Xn becomes dense in K, the span of {k(xi, Â·)}ni=1 becomes dense in Hk.Thus, the sequence Î¼fn converges to the projection of f onto Hk, denoted PHk f . So, for large enough n:

sup 

> xâˆˆK

|Î¼fn(x) âˆ’ PHk f (x)| < Ïµ

2 .

By the property of projections, PHk f is the best approximation to f within Hk. Since Hk is dense in C(K), we can find 

g âˆˆ H k such that âˆ¥f âˆ’ gâˆ¥âˆž < Ïµ/ 2. This implies that âˆ¥f âˆ’ PHk f âˆ¥âˆž < Ïµ/ 2. Combining these gives: 

sup 

> xâˆˆK

|f (x) âˆ’ Î¼fn(x)| < Ïµ

2 + Ïµ

2 = Ïµ. 

Conclusion 

The proof establishes that for any continuous function f and any precision Ïµ, one can find a finite set of sample points from f such that the posterior mean of a GP with a universal kernel approximates f to within Ïµ over the entire compact domain. The universality of the GP is therefore inherited directly from the denseness of its associated RKHS in the space of continuous functions. This is a powerful existence result, though it does not specify the number of points n required or the computational cost. 13 POP: Prior-fitted Optimizer Policies 

B. Synthetic Prior 

This appendix summarizes the parameterization of the synthetic prior used for meta-training (Table 4) and provides random visual examples of objective functions drawn from this prior (Figure 9). Each function is generated by independently sampling all prior parameters and constructing the objective according to Section 3. The resulting objectives vary naturally in scale, smoothness, curvature, and degree of non-convexity, reflecting the variability induced by the prior rather than any manual shaping of function landscapes. 

B.1. Parameter values of our proposed Prior  

> Table 4. Ranges and parameter values of the Prior.

Symbol Distribution / Value 

D 2

M 1000 

Î²1 U(10 âˆ’8, 10 âˆ’2)

Î²2 U(âˆ’50 , 50) 

Î± U(0 .05 , 0.4) 

pconvex 0.15 

â„“ U(4 , 8) 

Ïƒ U(0 .5, 3) 

B.2. Visual Illustration of the Prior 

14 POP: Prior-fitted Optimizer Policies 40 20 0 20 40 40 

20 

0

20 

40 

4

2

0

2

4

6

Function 1 

2

0

2

4

6

40 20 0 20 40 40 

20 

0

20 

40 

2.5 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

17.5 

20.0 

Function 2 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

17.5 

40 20 0 20 40 40 

20 

0

20 

40 

2

0

2

4

6

8

Function 3 

0

2

4

6

8

40 20 0 20 40 40 

20 

0

20 

40 

1

0

1

2

3

Function 4 

1.0 

0.5 

0.0 

0.5 

1.0 

1.5 

2.0 

2.5 

3.0 

40 20 0 20 40 40 

20 

0

20 

40 

2

1

0

1

2

3

4

5

Function 5 

1

0

1

2

3

4

40 20 0 20 40 40 

20 

0

20 

40 

2

0

2

4

6

8

Function 6 

0

2

4

6

8

40 20 0 20 40 40 

20 

0

20 

40 

0

2

4

6

8

10 

12 

Function 7 

2

4

6

8

10 

40 20 0 20 40 40 

20 

0

20 

40 

6

4

2

0

2

4

6

8

Function 8 

4

2

0

2

4

6

8

40 20 0 20 40 40 

20 

0

20 

40 

0

5

10 

15 

20 

Function 9 

2.5 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

17.5 

40 20 0 20 40 40 

20 

0

20 

40 

0

5

10 

15 

20 

25 

30 

Function 10 

5

10 

15 

20 

25 

40 20 0 20 40 40 

20 

0

20 

40 

2

0

2

4

6

8

10 

12 

Function 11 

0

2

4

6

8

10 

12 

40 20 0 20 40 40 

20 

0

20 

40 

2

0

2

4

6

8

10 

Function 12 

2

0

2

4

6

8

40 20 0 20 40 40 

20 

0

20 

40 

4

2

0

2

4

6

Function 13 

3

2

1

0

1

2

3

4

5

40 20 0 20 40 40 

20 

0

20 

40 

2.5 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

Function 14 

0

2

4

6

8

10 

12 

14 

40 20 0 20 40 40 

20 

0

20 

40 

2.5 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

Function 15 

2

0

2

4

6

8

10 

12 

40 20 0 20 40 40 

20 

0

20 

40 

2.5 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

17.5 

Function 16 

2.5 

0.0 

2.5 

5.0 

7.5 

10.0 

12.5 

15.0 

40 20 0 20 40 40 

20 

0

20 

40 

4

2

0

2

4

6

8

Function 17 

2

0

2

4

6

8

40 20 0 20 40 40 

20 

0

20 

40 

2

1

0

1

2

3

4

5

6

Function 18 

1

0

1

2

3

4

5

40 20 0 20 40 40 

20 

0

20 

40 

1

0

1

2

3

Function 19 

1

0

1

2

3

40 20 0 20 40 40 

20 

0

20 

40 

0

10 

20 

30 

40 

Function 20 

5

10 

15 

20 

25 

30 

35 

40 

Figure 9. Exemplary functions sampled from the prior distribution (cf 3) 

15 POP: Prior-fitted Optimizer Policies 

C. Architecture and Training Details 

This appendix reports the architectural and training hyperparameters used in all experiments. Unless stated otherwise, a single configuration is used across ablations and evaluations to isolate the effect of the proposed optimizer design from implementation-specific tuning. Table 5 summarizes the transformer-based architecture shared between actor and critic, while Table 6 details the PPO training setup and optimization hyperparameters. We set the boundary target scale as 

Vx = Vy = 3 . Together, these settings fully specify the model and training procedure required to reproduce the reported results. 

Table 5. Key architecture hyperparameters for the shared Transformer backbone and actor/critic heads. 

Component Value 

Shared backbone (Transformer) 

Embedding dimension 64 # Transformer blocks 4# Attention heads 4 ( dhead = 16 )Dropout 0.05 Pooling Last-token pooling Shared output dim 32 

Actor / Critic heads 

Actor head (MLP) 32 â†’ 16 â†’ 1

Critic head (MLP) 32 â†’ 16 â†’ 1

Table 6. PPO training hyperparameters and optimizer settings. 

Component Value 

Actor lr 1 Ã— 10 âˆ’4

Critic lr 1 Ã— 10 âˆ’4

Optimizer AdamWScheduleFree (Defazio et al., 2024) 

Actor weight decay 1 Ã— 10 âˆ’4

Critic weight decay 1 Ã— 10 âˆ’4

Actor warmup 300 Critic warmup 10 PPO clip ratio 0.1 GAE Î³ 1.0 GAE Î» 1.0 Epochs over collected trajectories 4PPO Minibatch size 4096 Gradient clip norm 0.5 KL early stopping 0.01 Adaptive KL penalty yes (initial coef 0.1, target KL 0.01) Return normalization Online Welford estimator 

16 POP: Prior-fitted Optimizer Policies 

D. Baseline Hyperparameter Optimization and Qualitative Analysis 

This appendix contains additional experimental details that support the evaluation in Section 6. It is divided into two parts. First, we report the hyperparameter optimization (HPO) procedure used for baseline methods, including the learning-rate sweeps and selected configurations. These results ensure that all baselines are evaluated under competitive and well-tuned settings. We sweep the initial learning rate for GD, Adam, and L-BFGS using a grid search over 

Î· âˆˆ { 0.01 , 0.02 , 0.03 , 0.05 , 0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 3.0, 5.0, 10 .0, 20 .0, 30 .0, 50 .0, 100 .0}.

All other hyperparameters are kept at their default values. For L-BFGS, this implies that the optimizer is allowed to perform more function evaluations per optimization iteration than the other baselines. Figure 10, Figure 11, and Figure 12 show the learning-rate sweeps for GD, Adam, and L-BFGS on the validation set, respectively. 10 2 10 1 10 0 10 1 10 2

> Learning Rate
> 0.00
> 0.02
> 0.04
> 0.06
> 0.08
> 0.10
> 0.12
> 0.14
> Normalized Improvement
> GD HPO: Learning Rate vs Normalized Improvement
> Mean Normalized Improvement
> 95% CI

Figure 10. Learning rate sweep on the in-distribution validation set. Performance for GD peaks at a learning rate of 50. 10 2 10 1 10 0 10 1 10 2

> Learning Rate
> 0.000
> 0.025
> 0.050
> 0.075
> 0.100
> 0.125
> 0.150
> 0.175
> Normalized Improvement
> Adam HPO: Learning Rate vs Normalized Improvement
> Mean Normalized Improvement
> 95% CI

Figure 11. Learning rate sweep on the in-distribution validation set. Performance for Adam peaks at a learning rate of 10. 

17 POP: Prior-fitted Optimizer Policies 10 2 10 1 10 0 10 1 10 2

Learning Rate 

> 0.000
> 0.025
> 0.050
> 0.075
> 0.100
> 0.125
> 0.150
> 0.175
> Normalized Improvement

BFGS (LBFGS) HPO: Learning Rate vs Normalized Improvement 

> Mean Normalized Improvement
> 95% CI

Figure 12. Learning rate sweep on the in-distribution validation set. Performance for L-BFGS peaks at a learning rate of 2. For learning rates > 2 , the optimization problem fails and the normalized improvement is undefined (NaN) 

18 POP: Prior-fitted Optimizer Policies 

D.1. In-Distribution Performance 

In the second part of this appendix, we provide qualitative visualizations of optimization trajectories produced by POP on functions sampled from the synthetic prior. These figures complement the quantitative results reported in Section 6 by illustrating typical optimization behaviors observed in practice. They highlight how POP adapts its step sizes and exploration behavior in response to the local geometry of the objective function. In particular, the visualizations illustrate rapid convergence on convex objectives as well as adaptive exploration and escape from local minima in non-convex settings. Figure 13 and 14 show representative trajectories on convex and non-convex functions 20                              

> 0
> 20
> 40
> x140
> 20
> 0
> 20
> 40
> x2
> 0
> 10
> 20
> 30
> 40
> f(x)
> 20 10 010 20 30 40
> x1
> 40
> 20
> 0
> 20
> x2
> POP Trajectory
> 10 10
> 10 6
> 10 2
> f(x) POP
> 10 5
> 10 3
> 10 1
> f(x)
> 010 20 30 40
> t
> 10 1
> 1
> 2
> 0
> 6
> 12
> 18
> 24
> 30
> 36
> 42 40
> 20
> 0
> 20
> 40
> x140
> 20
> 0
> 20
> 40
> x2
> 0
> 2
> 4
> 6
> 8
> 10
> f(x)
> 40 20 020 40
> x1
> 40
> 20
> 0
> 20
> 40
> x2
> POP Trajectory
> 0.0
> 0.1
> f(x) POP
> 0.1
> 0.2
> f(x)
> 010 20 30 40
> t
> 10 2
> 10 1
> 10 0
> 1
> 2
> 0.0
> 1.2
> 2.4
> 3.6
> 4.8
> 6.0
> 7.2
> 8.4
> 9.6

Figure 13. Trajectory of POPâ€™s on two convex functions. The algorithm progresses from the initial state (white cross) terminating at the final state (square). 

19 POP: Prior-fitted Optimizer Policies 40 

20 

0

20 

40 

x1 75 

50 

25 

0

25 

50 

> x2

4

2

0

2

4

6

8

> f(x)

20 0 20 40 

x1

50 

25 

0

25 

> x2

POP Trajectory 

2.5 

0.0 

> f(x) POP

10 2

10 1

10 0

> f(x)

0 10 20 30 40 

t

10 0 1

> 24.5
> 3.0
> 1.5
> 0.0
> 1.5
> 3.0
> 4.5
> 6.0
> 7.5

40 

20 

0

20 

40 

x1

50 

0

50 

> x2

5

0

5

10 

15 

> f(x)

40 20 0 20 

x1

50 

0

50 

> x2

POP Trajectory 

2.5 

0.0 

2.5 

> f(x) POP

10 2

10 1

10 0

> f(x)

0 10 20 30 40 

t

10 1

10 0

> 1
> 2
> 4.8
> 2.4
> 0.0
> 2.4
> 4.8
> 7.2
> 9.6
> 12.0
> 14.4

Figure 14. Trajectory of POPâ€™s on two non-convex functions. The algorithm progresses from the initial state (white cross) terminating at the final state (square). 

20 POP: Prior-fitted Optimizer Policies 

E. Virtual Library of Simulation Experiments benchmark 

This appendix provides detailed results for the Virtual Library of Simulation Experiments (VLSE) benchmark used in Section 6. We report per-function normalized regret curves for all methods across the full optimization horizon. Each plot corresponds to a single benchmark function evaluated at its native dimensionality. All methods are run under identical evaluation budgets using the same performance metric, and no additional tuning or task-specific adjustments are performed. 0.50                 

> 1.00 Ackley (2D)
> 0.00
> 0.50
> Bukin N.6 (2D)
> 0.00
> 0.20
> 0.40
> Cross-in-Tray (2D)
> 0.50
> 1.00
> Drop-Wave (2D)
> 0.20
> 0.40
> Eggholder (2D)
> 0.00
> 0.20
> Gramacy & Lee (2012) (2D)
> 0.00
> 0.20
> 0.40
> Griewank (2D)
> 0.00
> 0.50
> 1.00
> Holder Table (2D)
> 0.00
> 0.20
> 0.40
> Langermann (2D)
> 0.00
> 0.10
> 0.20
> Levy (2D)
> 0.00
> 0.50
> Levy N.13 (2D)
> 0.00
> 0.50
> Rastrigin (2D)
> 0.20
> 0.40
> Schaffer N.2 (2D)
> 0.10
> 0.20
> 0.30
> Schaffer N.4 (2D)
> 0.00
> 0.25
> 0.50
> Schwefel (2D)
> 020 40 60 80 100
> 0.00
> 0.20
> 0.40
> Shubert (2D)
> 020 40 60 80 100
> 0.00
> 0.25
> 0.50
> Bohachevsky (variant=1|2|3) (2D)
> 020 40 60 80 100
> 0.00
> 0.10
> 0.20
> Perm 0,d,beta (b) (2D)

Step   

> Normalized Regret
> POP
> Random Search
> Adam
> GD
> L-BFGS
> Differential Evolution
> Genetic Algorithm TPE VELO

Figure 15. The normalized regret for all methods and functions 1 to 18. Solid lines represent the mean value. 

21 POP: Prior-fitted Optimizer Policies 0.00 

0.05 

0.10 

Rotated Hyper-Ellipsoid (2D) 

0.00 

0.25 

0.50 

Sphere (2D) 

0.00 

0.20 

0.40 

Sum of Different Powers (2D) 

0.00 

0.25 

0.50 

Sum Squares (2D) 

0.00 

0.25 

0.50 

Trid (2D) 

0.00 

0.20 

Booth (2D) 

0.00 

0.25 

0.50 

Matyas (2D) 

0.00 

0.20 

McCormick (2D) 

0.00 

0.01 

Power Sum (2D) 

0.00 

0.20 

Zakharov (2D) 

0.00 

0.01 

0.01 

Three-Hump Camel (2D) 

0.00 

0.10 

0.20 

Six-Hump Camel (2D) 

0.00 

0.25 

0.50 

Dixon-Price (2D) 

0.00 

0.05 

0.10 

Rosenbrock (2D) 

0.00 

0.50 

1.00 

De Jong N.5 (2D) 

0 20 40 60 80 100 

1.00 

1.00 

1.00 

Easom (2D) 

0 20 40 60 80 100 

0.00 

0.50 

1.00 

Michalewicz (2D) 

0 20 40 60 80 100 

0.00 

0.03 

0.05 

Beale (2D) 

Step 

> Normalized Regret

POP 

Random Search 

Adam 

GD 

L-BFGS 

Differential Evolution 

Genetic Algorithm TPE VELO 

Figure 16. The normalized regret for all methods and functions 19 to 36. Solid lines represent the mean value. 

22 POP: Prior-fitted Optimizer Policies 0.00 

0.10 

0.20 

0.30 

Branin (2D) 

0.00 

0.25 

0.50 

0.75 

1.00 

Colville (4D) 

0.00 

0.10 

0.20 

Forrester et al. (2008) (1D) 

0.00 

0.02 

0.04 

0.06 

0.08 

Goldstein-Price (2D) 

0.00 

0.20 

0.40 

0.60 

0.80 

Hartmann 3D (3D) 

0.00 

0.20 

0.40 

0.60 

0.80 

Hartmann 4D (SFU standardized) (4D) 

0.00 

0.20 

0.40 

0.60 

0.80 

Hartmann 6D (6D) 

0.00 

0.10 

0.20 

Perm d,beta (b) (2D) 

0.00 

0.05 

0.10 

0.15 

Powell (2D) 

0 20 40 60 80 100 

0.40 

0.60 

0.80 

1.00 

Shekel (4D) 

0 20 40 60 80 100 

0.00 

0.10 

0.20 

0.30 

0.40 

Styblinski-Tang (2D) 

Step 

> Normalized Regret

POP 

Random Search 

Adam 

GD 

L-BFGS 

Differential Evolution 

Genetic Algorithm TPE VELO 

Figure 17. The normalized regret for all methods and functions 36 to 47. Solid lines represent the mean value. 

23