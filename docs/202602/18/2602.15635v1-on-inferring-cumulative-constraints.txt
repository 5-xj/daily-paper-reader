Title: On inferring cumulative constraints

URL Source: https://arxiv.org/pdf/2602.15635v1

Published Time: Wed, 18 Feb 2026 02:03:59 GMT

Number of Pages: 17

Markdown Content:
# ON INFERRING CUMULATIVE CONSTRAINTS 

A P REPRINT 

Konstantin Sidorov 

Faculty of Electrical Engineering, Mathematics & Computer Science Delft University of Technology Delft, The Netherlands 

k.sidorov@tudelft.nl 

February 18, 2026 

# ABSTRACT 

Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints. 

Keywords Constraint programming · Scheduling · Cumulative · Lifting · Valid inequalities 

# 1 Introduction 

Cumulative constraints and their use in scheduling problems are one of the longstanding success stories of constraint programming (CP), serving as a central abstraction and a source of powerful inferences about resource-constrained scheduling problems. A large body of work has demonstrated how powerful propagation on a single cumulative resource can be, including effective detection of overload situations, pruning based on resource envelopes, and increasingly sophisticated explanations and propagator combinations. All these developments have contributed to the state-of-the-art performance of CP solvers on industrial scheduling benchmarks. Nevertheless, most existing techniques in this space reason about one cumulative constraint at a time. Even when a model contains several resources, propagation is typically performed independently per resource, with indirect interactions only mediated through domain narrowing of start-time variables. However, the recent work by Sidorov et al. [2025a] has made a first step to expose the limitations of this “single-resource viewpoint,” including the 3n problem 

structure, a pathological case when an instance with three cumulative constraints is equivalent to a trivially infeasible single-machine scheduling problem but is intractable for a conventional lazy clause-generating solver, a paradigm subsuming many of the state-of-the-art CP codes. While Sidorov et al. have addressed a special (disjunctive) case of this problem with a Unite and Lead (further referred to as U NL) approach and managed to discover large hidden structures in several known research benchmarks, their technique has two fundamental limitations. First, while it succeeds at discovering disjunctive structures (that is, the collections of tasks that cannot be run in parallel), it does not support any extensibility for deriving more general multi-resource interactions. Second, U NL has to explicitly probe for disjunctive cliques throughout the search, incurring significant overhead even in instances where no such structure exists. Since the clique search overhead is not offset by 

> arXiv:2602.15635v1 [cs.AI] 17 Feb 2026

On inferring cumulative constraints A P REPRINT 

stronger reasoning on instances without a disjunctive structure, the solver performance deteriorates sharply on such problems. As a result, Sidorov et al. show that, when U NL does not improve the search, it commonly slows down the solver by more than an order of magnitude. In this paper, I address these limitations by proposing a new algorithmic framework for inferring cumulative constraints at the root search node. Instead of restricting attention to disjunctive reasoning, this approach operates over cumulative constraints as linear inequalities on a special representation of start time variables (occupancy vectors), and reasoning across multiple resources reduces to generating valid cutting planes in the same sense as it is understood in integer programming. This viewpoint subsumes a wide range of multi-constraint reasoning techniques from earlier work, including the version of U NL restricted to the root node and classical lower-bound techniques for RCPSPs. Next, I leverage this connection by describing a workflow for inferring cumulative constraints based on the idea of 

lifting [Padberg, 1975]. Lifting is a general-purpose technique from mixed-integer programming for tightening the generated cutting planes aT x ≤ b by picking a variable y not mentioned in the inequality and finding the largest β

such that aT x + βy ≤ b is still valid. I apply this idea by (i) discovering sets of tasks C that cannot be run in parallel, (ii) lifting the constraint of the form “tasks in C cannot consume more than (# C − 1) units of resource,” and (iii) introducing the lifted constraints as new cumulative constraints into the model. I evaluate the method as a preprocessing step for two state-of-the-art CP solvers on a suite of standard RCPSP benchmarks. The results suggest that the approach described in this paper successfully retains most of the positive traits of U NL (substantial gains on instances with hidden structure), but does so without inheriting its instability. This is helpful for advancing the best-known bounds for the benchmark instances: with the presented approach, I report the improved schedules for five benchmarks and tighten the lower bounds for 25 instances, with eight of them not requiring any search to verify the correctness of the new bound. The remainder of the paper is organized as follows. Section 2 introduces the notions of constraint programming and polyhedral geometry that are necessary for the rest of the presentation. Section 3 establishes the connection between CP models with cumulative constraints and the notion of valid inequality. Section 4 presents the lifting-based approach to inferring cumulative constraints. I evaluate this approach in Section 5 from two angles: (a) as a preprocessing technique for adding cumulative constraints to the input instance before running a CP solver, and (b) as a technique for inferring cumulative constraints with unit capacity, 1 by comparing it with the U NL approach. Section 6 discusses earlier methods for joint reasoning on cumulative constraints and their connection with the lifting approach. Finally, Section 7 concludes the narrative and discusses future research directions. 

# 2 Preliminaries 

2.1 Constraint programming 

A constraint satisfaction problem (CSP) consists of a tuple (X , C, D) where X is the set of variables , C is the set of 

constraints which specify the relations between variables, and D is the domain which specifies for each variable which values it can take [Rossi et al., 2006]. A solution I is a mapping that maps each variable in X to a single value in the domain of that variable in D which satisfies all of the constraints in C.Constraint programming (CP) is a paradigm for solving CSPs; CP solvers enforce constraints through propagators ,each represented with a subroutine that removes values from D infeasible under the constraints in C. After applying the propagators, the solver makes a decision that creates several subproblems by splitting the domain of a variable into two or more parts. This process of applying propagators and making decisions is performed until either a solution I is found, the problem is found to be unsatisfiable, or a termination criterion is met. 

Cumulative is a constraint useful for many scheduling problems to model limited renewable resources, such as available work-hours. In Definition 1, the variable si encodes the start time of task i, and (si + di) encodes its finish time. Thus, we implicitly associate each task i ∈ T with an interval [si, s i + di).

Definition 1. Let T be a set of tasks , and let a task i ∈ T be defined by its start variable si, resource usage ri ∈ Z≥0,and duration di ∈ Z≥0. Finally, let C ∈ Z≥0 be the capacity of the resource. Then the Cumulative (s, r, d, C )

constraint is the condition that at any time point τ the cumulative resource usage of intervals [si, s i + di) covering τ

does not exceed the capacity: 

∀τ ∈ Z : X  

> i∈T:si≤τ <s i+di

ri ≤ C. (1) 

> 1Also known as disjunctive or no-overlap constraints.

2On inferring cumulative constraints A P REPRINT 

An important special case of Cumulative is where the resource has unit capacity C = 1 and each task has resource usage ri ∈ { 0, 1}, known as Disjunctive . This special case is significant because many inference procedures that are intractable for Cumulative constraints can be efficiently executed for Disjunctive constraints. For example, determining satisfiability of a single Cumulative constraint is NP-complete [Baptiste et al., 1999], but determining satisfiability of a single Disjunctive constraint can be done in polynomial time [Vilím, 2004, Section 3]. 

2.2 Polyhedral geometry 

In this work, we consider sets of the form P(A, b) := {x ∈ { 0, 1}n : Ax ≤ b}; unless stated otherwise, we assume that all coefficients are nonnegative integers , that is, A ∈ Zm×n 

> ≥0

and b ∈ Zm

> ≥0

. As known from the integer programming theory [Ziegler, 1995], those sets can also be seen as vertices of some polyhedron {x : A∗x ≤ b∗, 0 ≤

x ≤ 1}, albeit with a different set of constraints given by A∗ and b∗. This motivates the following definition: 

Definition 2. An inequality πT x ≤ π0, also written as (π, π 0) ∈ Rn+1  

> ≥0

, is called a valid inequality for P(A, b) if it holds for any point in P(A, b).

Discovering valid inequalities separating a non-integer vertex x∗ of the continuous relaxation of P(A, b) from P(A, b)

itself is a theoretically rich topic with major practical implications for integer programming solvers. For the purposes of this work, we only need a few core notions; an interested reader is encouraged to survey the literature on knapsack polyhedra [Hojny et al., 2020] for a more systematic exposition. Consider first a simple family of valid inequalities: 

Definition 3. Given a polyhedron P(aT , b ) with a single inequality constraint aT x ≤ b, we say that C ⊆ { 1, . . . , n }

is its cover if a(C) := P 

> i∈C

ai > b .

Proposition 4. A cover inequality x(C) ≤ #C − 1 is a valid inequality for P(aT , b ) if C is its cover. 

Next, we need a general-purpose procedure for increasing the left-hand side of a valid inequality πT x ≤ π0—which may or may not be a cover inequality—known as lifting [Padberg, 1975]. This procedure maintains the set C of variables that have already been considered for adding to the inequality, starting with C ← { i : πi̸ = 0 }, and repeating the following steps until C = {1, . . . , n }:1. Choose an unused variable i ∈ { 1, . . . , n } \ C or terminate if none exist. 2. Solve an auxiliary subproblem over the variables in C

X

> c∈C

πcxc → max 

X

> c∈C

aj,c xc ≤ bj − aj,i ∀j (2) 

xc ∈ { 0, 1} ∀ c ∈ C

that corresponds to maximizing the left-hand side of the current inequality πT x when the variable xi under consideration is set to 1. 3. Set C ← C ∪ { i} and πi ← π0 − v∗(π, π 0, C ; A, b), where v∗(π, π 0, C, A, b) is the maximum value of the objective of the problem given by Equation (2). To illustrate the concepts of cover inequality and lifting, we apply all of them in the following example, which is also used later in the paper to illustrate the proposed approach: 

Example 5. Consider the following knapsack problem: 

x1 + x2 + x3 + x4 → max 5x1 + 3 x2 + 2 x3 + 4 x4 ≤ 7 (3) 

x ∈ { 0, 1}4.

The upper bound on the objective derived by relaxing the integrality constraints 2 is equal to 2 12 and is achieved on the solution x(1) =  0, 1, 1, 12

. The support set C = {2, 3, 4} of this solution is the cover for Equation (3) (3 + 2 + 4 = 9 > 7), and it corresponds to the cover inequality x2 + x3 + x4 ≤ 2 forbidding the simultaneous choice of those three variables. 

> 2Also known as the linear programming relaxation.

3On inferring cumulative constraints A P REPRINT 

Since this is a valid inequality, it is possible to add it to the optimization model alongside Equation (3) and solve the resulting linear programming relaxation; this yields an upper bound of 2 25 and is achieved on the solution x(2) =  25 , 1, 1, 0. We can improve this bound by applying the lifting procedure first; in this example, we lift the only remaining variable x1 with the following reasoning. Consider a constraint αx 1 + x2 + x3 + x4 ≤ 2 for arbitrary α ≥ 0 and observe that it holds for any solution satisfying Equation (3) with x1 = 0 regardless of α. Suppose that x1 = 1 , and rewrite the lifted constraint and Equation (3) as follows: 

3x2 + 2 x3 + 4 x4 ≤ 2

| {z }

> Equation (3)

=⇒ α ≤ 2 − (x2 + x3 + x4)

| {z } 

> lifted constraint for x1=1

. (4) Since we look for the tightest valid constraint, we can achieve this by choosing the largest α such that Equation (4) is still true for any x2, x 3, x 4 ∈ { 0, 1}. Specifically, we achieve this by setting 

α ← 2 − max {x2 + x3 + x4|3x2 + 2 x3 + 4 x4 ≤ 2, x 2, x 3, x 4 ∈ { 0, 1}} , (5) which also happens to be the problem written in Equation (2) for the values of (A, b, C, π, π 0) used in this example. To solve the lifting subproblem in Equation (5), observe that x3 = 1 , x 2 = x4 = 0 is a feasible solution, but setting any two variables to one is not possible under the 3x2 + 2 x3 + 4 x4 ≤ 2 constraint. Therefore, setting α ← 2 − 1 = 1 

retains the validity of the inequality, and we obtain the lifted inequality x1 + x2 + x3 + x4 ≤ 2. To conclude the example, observe that adding it alongside Equation (3) and solving the linear programming relaxation yields an upper bound of 2, achieved on, among others, x∗ = (0 , 1, 0, 1) , which is enough to conclude optimality. 

# 3 From Cumulative to linear inequalities 

We start by introducing a representation of the start-time variables that we need in order to reason about linear inequalities: 

Definition 6. Given a variable x of a CSP and an integer d, further referred to as time and duration , we associate the variable assignments with occupancy vectors ⟨x | d⟩ ∈ { 0, 1}Z as follows: 

⟨x | d⟩(t) := 

1 if x ≤ t < x + d

0 otherwise .

The key point behind this definition is that a Cumulative constraint can be rewritten as a linear inequality 3 over the occupancy vectors: 

Cumulative (x, r, d, C ) ⇐⇒ X

> i∈T

ri⟨xi | di⟩ ≤ C. (6) This reformulation is similar to the time-resource decomposition [Schutt et al., 2009] in the sense that it linearizes a 

Cumulative constraint, but the main difference is that Equation (6) introduces one vector inequality, which is possible through the use of occupancy vectors. Conversely, the time-difference decomposition introduces one constraint per time point, as well as a collection of constraints that enforce Definition 6. While using Equation (6) directly does not provide any advantage for solving CSPs with Cumulative constraints, it provides a new viewpoint for such models: they can be seen as models with polyhedral constraints over occupancy vectors, with the coefficients of inequalities mapping one-to-one to the arguments of Cumulative constraints. Given that, it is natural to assume that if a part of the CP model can be restated in a polyhedral form, then the cutting-plane approaches could be used to derive new linear inequalities—or, in CP terms, new Cumulative constraints—with potentially better inference through aggregation. The next result formalizes this insight and sets the stage for a more practical discussion of inferring Cumulative constraints: 

Lemma 7. Consider a CSP (X , D, C) with X = ( x1, . . . , x n) such that all satisfying assignments also satisfy the constraints Pnj=1 aij ⟨xj | dj ⟩ ≤ bi, 1 ≤ i ≤ m for some A ∈ Zm×n 

> ≥0

and b ∈ Zm

> ≥0

. Let (π, π 0) be a valid inequality for the polyhedron P(A, b). Then any assignment satisfying the CSP also satisfies Cumulative (X , π, d, π 0).Note 8. We assume in this lemma that all linear inequalities on occupancy vectors use the same duration variables for any given time variable. This is a natural assumption, since di commonly refers to the duration of i-th task, which does not depend on the considered resource.        

> 3For simplicity, an inequality between a scalar and a vector x≤bis understood as the coordinate-wise inequality x≤(b, . . . , b ).

4On inferring cumulative constraints A P REPRINT 

Proof. Suppose that this is not the case; then, by Equation (6), there is a satisfying assignment θ of the CSP such that Pnj=1 aij ⟨xj | dj ⟩ ≤ bi, 1 ≤ i ≤ m are true but Pnj=1 πj ⟨xj | dj ⟩ ≤ π0 is not. Let t be the time point where 

Pnj=1 πj ⟨xj | dj ⟩ ≤ π0 does not hold, that is, Pnj=1 πj yj > π 0 for yj = ⟨xj | dj ⟩(t).We thus have constructed a vector y ∈ { 0, 1}n such that Ay ≤ b and πT y > π 0. However, that contradicts the definition of the valid inequality, since y is a point in the polyhedron P(A, b) for which the valid inequality πT y ≤ π0

does not hold. To see the utility of Lemma 7, consider the following example: 

Example 9. Consider a CSP on variables x = ( x1, x 2, x 3, x 4) with a constraint 

Cumulative (x, (5 , 3, 2, 4) , d, 7) ⇐⇒ 5⟨x1 | d1⟩ + 3 ⟨x2 | d2⟩ + 2 ⟨x3 | d3⟩ + 4 ⟨x4 | d4⟩ ≤ 7

for some vector of durations d. We know from Example 5 that (π; π0) = (1 , 1, 1, 1; 2) is a valid inequality for the 

P(A, b) polyhedron formed from the Cumulative constraint. But, by Lemma 7, that means that we can introduce the following constraint to the CSP: 

Cumulative (x, (1 , 1, 1, 1) , d, 2) ⇐⇒ ⟨ x1 | d1⟩ + ⟨x2 | d2⟩ + ⟨x3 | d3⟩ + ⟨x4 | d4⟩ ≤ 2.

In other words, we have discovered that at most two tasks can run in parallel in any feasible assignment. More generally, Lemma 7 suggests that inferring Cumulative constraints can be done by writing out the model constraints, where possible, in the occupancy-vector form, discovering a valid inequality for those constraints, and substituting its coefficients as usages and capacity for a new “resource.” The discovery step, however, has a subtle difference from the integer programming case: the goal there is usually not simply to produce new inequalities, but to 

separate a non-integer solution x∗ from the linear programming relaxation by a valid inequality [Kaparis and Letchford, 2010]; this, in turn, suggests starting with a cover violated by x∗. In the absence of such guidance, some other strategy for enumerating covers is required. The next section describes these steps—cover generation, lifting, and constraint introduction—in more detail. 

# 4 Inferring Cumulative constraints by lifting 

This section introduces an approach for inferring auxiliary Cumulative constraints from m constraints Pnj=1 aij ⟨xj |

dj ⟩ ≤ bi for 1 ≤ i ≤ m. To employ the lifting in the absence of a guiding “non-solution,” I propose instead to: 1. Find a collection C of at most Ncover covers for the constraints Ax ≤ b, x ∈ { 0, 1}n.2. For each cover C ∈ C , lift the cover inequality x(C) ≤ #C − 1 with respect to the polyhedron P(A, b) as described in Section 2. 3. Among the lifted Cumulative constraints, add a limited number Nout of them, giving preference to the most restrictive constraints. Since this procedure is bound to produce many valid inequalities, the next definition introduces a quality metric of a 

Cumulative constraint. I use it directly in Step 3 by giving preferences to the constraints with the larger score, as well as to choose which of the (many) covers in C will eventually be lifted. 

Definition 10. Given a constraint Cumulative (x, r, d, C ), we call its capacity bound the ratio of total resource usage over the time horizon to its capacity per time unit: L(r, d, C ) :=   

> P
> idiri
> C

.

The relevance of this quantity, as pointed out in early RCPSP work [Stinson et al., 1978, p. 255] [Klein and Scholl, 1999, Section 2], is that it gives a lower bound on the duration spanned by the tasks as formalized by the following statement, the proof of which is available in the aforementioned RCPSP work [Stinson et al., 1978]. 

Lemma 11. For any satisfying assignment θ of a constraint Cumulative (x, r, d, C ), let s := min i xi and f := max i(xi + di).4 Then f − s ≥ L (r, d, C ).

Step 1 enumerates seed covers independently for each of the m Cumulative constraints. First, this procedure collects the “short” covers of cardinality two and three and keeps the best Ncover covers according to the capacity bound. Given one such constraint P 

> i

ai⟨xi | di⟩ ≤ b, the “short” covers are constructed as follows:           

> 4In the scheduling interpretation, sis the earliest starting time among the constrained tasks, fis the latest finishing time among these tasks, and their difference (f−s)is the makespan of the plan.

5On inferring cumulative constraints A P REPRINT 

• Collect all pairs {i, j } such that ai + aj > b , which is done with time complexity O(n2).• For any non-covering pair {i, j }, consider the set K(i, j ) = {k : ak > b − (ai + aj )}. If it is empty, do nothing; otherwise, add a cover {i, j, k (i, j )} with k(i, j ) = arg max k∈K(i,j ) dk, that is, complete the cover by the longest eligible task. This step is done with time complexity O(bn 2).Then, this set is extended by adding “long” covers (with no a priori limit on the resulting cardinality). Given a constraint P 

> i

ai⟨xi | di⟩ ≤ b, long covers are generated by grouping tasks by their resource consumption values v. For each group Bv := {i : ai = v}, pick the smallest k such that kv > b , and add the two uniform seed covers consisting of (i) the k longest tasks in Bv and (ii) the k shortest tasks in Bv ; this is done in O(bn log n).The complete procedure can be seen as a consecutive application of Algorithm 1 on the original CSP and Algorithm 2 on the CSP and the covers constructed in the earlier phase; in both algorithms, TopK (C, k ) is a procedure that sorts 

C by descending value of L(·, d, ·) and returns the first k elements. Aside from the steps discussed in this section, the implementation employs two additional optimizations: First, it discards any valid inequalities (π, π 0) that are dominated by the model constraints in the sense that π ≤ ai and π0 ≥ b. Second, after lifting a cover C to a constraint of the form P 

> k∈C′

xk + · · · ≤ #C − 1, all subsets of C′ with cardinality #C are marked as covers which are not lifted later ; to see why this is helpful, consider the following example. 

Example 12. Suppose that Algorithm 2 has lifted a cover C0 = {1, 2} to a constraint 

⟨x1 | d1⟩ + ⟨x2 | d2⟩

| {z }

> cover inequality

+ ⟨x3 | d3⟩ + · · · + ⟨x100 | d100 ⟩

| {z }

> lifted terms

≤ 1. (7) In other words, lifting has discovered that no two tasks among the set C′ = {1, 2, . . . , 100 } can run in parallel, which translates to an update Q ← Q ∪ (C′, 2) .Now, consider later iterations of the loop over covers C that visit other binary covers within C′; since any two tasks form a cover, this happens for  100 2

 − 1 = 4949 iterations unless some task pairs were discarded before the lifting. Observe that the “skipping” condition holds for any two tasks in C′ since (C′, 2) ∈ Q, C ′ ⊇ C, and #C ≥ 2; that means that none of those iterations invokes the v∗ function, and skipping ≈ 5000 redundant calls to v∗ is a performance gain, because (a) solving lifting subproblems is the bottleneck of Algorithm 2, and (b) lifting on C′ is likely to re-discover Equation (7) instead of a new constraint; for example, it is guaranteed to do so if it lifts the variables in C′ before others. 

# 5 Experimental evaluation 

In this section, I evaluate the performance of the presented approach on a collection of research benchmarks. I also compare the resulting bounds with those reported in the literature and show that the approach discovers new bounds for several benchmark instances (25 lower and five upper bounds), with eight lower bounds directly corresponding to the capacity bounds of newly introduced constraints. The implementation of the approach, together with the infrastructure for running the experiments and processing their results, is available in the supplementary material; the modeling part is based on the CPMpy library [Guns, 2019]. All lifting subproblems (Equation (2)) are solved with Gurobi 12 [Gur, 2024]. I ran the experiments on DelftBlue [Delft High Performance Computing Centre, 2024], with each run of an instance being allocated a single core of an Intel Xeon E5-6248R 24C 3.0GHz processor and 4000 MB of RAM with a time limit of one hour. I use the benchmark collection previously used by Sidorov et al. [2025b] to evaluate the U NL approach for the same two problem models (RCPSP and RCPSP/max) with Cumulative constraints and difference constraints of the form xj − xi ≥ γij for RCPSP/max and xj − xi ≥ di for RCPSP, both having the latest completion time of all tasks (makespan) as the minimization objective. More specifically, I use 736 RCPSP instances from MiniZinc benchmarks [Stuckey et al., 2014, Olaguíbel and Goerlich, 1989, Baptiste and Pape, 2000, Carlier and Néron, 2003, Koné et al., 2011, Kolisch and Sprecher, 1997] and 349 RCPSP/max benchmarks distributed by PSPLIB [Kolisch and Sprecher, 1997] through C, D, UBO, and SM suites. All data files are available in the supplementary materials. Given a minimization objective O, each of the solvers is run with one of the two search directions. In the primal search ,5 the solver generates a series of problems with an extra assumption JO ≤ onK for decreasing values of on. In the dual search ,6 the solver generates a series of problems with an extra assumption JO ≤ onK for increasing values of 

on.

> 5Also known as the linear SAT-UNSAT search.
> 6Also known as the linear UNSAT-SAT search or destructive lower bound.

6On inferring cumulative constraints A P REPRINT 

Data: A CSP (X , D, C).

Data: A polyhedron P(A, b) and durations d satisfying the assumptions of Lemma 7. 

Data: The maximum number of considered short covers Ncover .

Result: A set of valid covers. 

C ← ∅ ; /* The set of all discovered covers */ // Generate “short” covers of cardinalities two and three 

for r ← 1 to m do for i ← 1 to n, j ← i + 1 to n do if ar,i + ar,j > b r then 

C ← C ∪ {{ i, j }} ; /* add all binary covers */ 

else if K(i, j )̸ = ∅ then 

C ← C ∪ {{ i, j, k (i, j )}} ; /* complete the ternary cover */ // Retain the Ncover covers with the highest capacity bound 

C ← TopN (C, N cover );

// Generate “long” covers of unbounded cardinality 

for r ← 1 to m do 

// Group tasks by consumption value v

foreach v with {i : ar,i = v}̸ = ∅ do 

// Pick the smallest k such that k · v > b r

k ← min {t ∈ Z : t · v > b r };

Cmax ← the k indices in Bv with the largest durations di;

Cmin ← the k indices in Bv with the smallest durations di;

// Add the longest and shortest k tasks as covers 

C ← C ∪ { Cmax , C min };

return C;

Algorithm 1: Enumeration of covers. Each of the runs involves one of the following CP solvers: Pumpkin [Flippo et al., 2024] and CP-SAT [Perron and Didier, 2024]. The choice of the solvers is dictated by two factors: (a) both of the mentioned codes are advanced, state-of-the-art solvers performing well at the recent editions of MiniZinc Challenge, and (b) both of them support both the primal and the dual search directions. In the baseline runs, the solver is run on the input CSP as is, whereas the runs with the lifting first execute Algorithm 1 with Ncover = 100 , then Algorithm 2 with Nout = 5 , and proceed with the solver execution on the augmented CSP in the remaining time. I budget the preprocessing by limiting the number of candidate covers considered ( Ncover ) and the number of added inferred constraints ( Nout ), rather than by imposing a wall-time cutoff, since wall-time on shared HPC systems is not deterministic. All solvers have been invoked with the free search. To evaluate the solver performance when it did not conclude optimality, I measure the rate of progress of a solver towards the best-known bound with the following metrics. First, if M (t) is the lowest makespan discovered at time 

t ∈ [0 , T ], and M ∗ is the lowest discovered makespan for this problem, then the primal integral is R T    

> 0
> M(t)−M∗
> M(t)

and measures how fast the solver progresses towards good solutions [Berthold, 2013]. Conversely, if B(t) is the highest lower bound discovered at time t ∈ [0 , T ], and B∗ is the highest discovered lower bound for this problem, the dual integral is defined as R T 

> 0
> B∗−B(t)
> B∗

.All source code necessary to reproduce the experiments, including the implementation of the lifting approach, as well as output data files, can be accessed in Zenodo [Sidorov, 2026a,b]. 

5.1 Evaluation against direct CP solver application 

First, I report the evaluation of the lifting approach for the instances where both the run performing lifting and its counterpart that does not lift (while having the same settings otherwise) have proven optimality. As shown on Figure 1, lifting is consistently able to cut down the solving time by large factors, sometimes from taking dozens of minutes to sub-second times. On the other hand, most of the instances with large (tenfold or more) slowdowns correspond to the cases where the lifting time dominates the solving time, which in practice can be avoided with an appropriate termination criterion for the lifting procedure. 7On inferring cumulative constraints A P REPRINT 

Data: A CSP (X , D, C).

Data: A polyhedron P(A, b) and durations d satisfying the assumptions of Lemma 7. 

Data: A set of covers C.

Data: The maximum number of added constraints Nout .

Result: A set of at most Nout valid Cumulative constraints. 

Q ← ∅ ; /* (C, k ) ∈ Q =⇒ any subset of C of size k is a cover */ 

O ← ∅ ; /* The set of all discovered Cumulative constraints */ 

for C ∈ C do 

/* Skip this cover if already mentioned in an earlier constraint */ 

if ∃(C′, k ) : C′ ⊇ C ∧ k ≤ #C then continue ;

π ← 0, π(C) ← 1, π 0 ← #C − 1; /* Invariant: (π, π 0) is valid */ 

I ← { 1, . . . , n } \ C; /* Invariant: i ∈ I =⇒ πi = 0 */ 

while I̸ = ∅ do 

i ← arg min i∈C di; /* Choose the shortest remaining task */ 

πi ← π0 − v∗(π, π 0, I, A, b); /* v∗ is obtained from Equation (2) */ 

I ← I \ { i}; /* Mark this variable as lifted */ /* Mark all elements with unit coefficients as visited */ 

Q ← Q ∪ ({i : πi = 1 }, #C);

/* Add the discovered Cumulative unless it is dominated */ 

if ∃r : ar ≥ π ∧ br ≤ π0 then continue ;

O ← O ∪ Cumulative (X , π, d, π 0) ;

// Retain the Nout best constraints with the highest capacity bound 

O ← TopN (O, N out );

return O;

Algorithm 2: Lifting-based procedure for inferring auxiliary Cumulative constraints. 

10 010 110 210 3

10 -1 10 0 10 1 10 2 10 3Time to optimality without lifting Time to optimality with lifting 

Fraction of lifting time in total runtime 

25% 50% 75% Figure 1: Impact of using lifting on time to optimality. Every point corresponds to a pair of runs on the same instances, using the same underlying solver with the same search direction. Diagonal lines are evenly spaced and correspond to a tenfold relative change between the durations. However, most of the instances were not solved to optimality with either approach, which means that making progress with further comparisons is more viable with primal and dual integrals as metrics. I proceed with examining this comparison on Figure 2b, which supports the thesis that lifting performs as a helpful preprocessing routine: it substantially reduces the search effort on many instances, while not incurring a lot of search overhead if it backfires. On the flip side (Figure 2a), this is not true for primal integrals: in other words, adding new constraints does not help discover good solutions as consistently, but can hinder this process much worse. These results already show an improvement over the U NL, since Sidorov et al. mention a much larger variance for dual integrals, especially on the degradation side. For example, they report that their approach degrades the search 8On inferring cumulative constraints A P REPRINT      

> 10 -2 10 010 2
> 10 -2 10 010 2Primal integral without lifting Primal integral with lifting
> (a) Primal integrals.
> 10 -2 10 010 2
> 10 -3 10 -2 10 -1 10 010 110 210 3Dual integral without lifting Dual integral with lifting
> (b) Dual integrals.

Figure 2: Comparison of integrals between the baseline and lifting approach. by at least an order of magnitude on many instances, whereas in the current evaluation, this is an exceptionally rare event happening only in 19 pairs of runs. I investigate this in more detail in Table 1, which shows that even the “2× degradation” event is around the 95th percentile of observed degradations, whereas the opposite 5th percentile corresponds to 10× improvements for RCPSP benchmarks and to 50× improvements for RCPSP/max benchmarks. Table 1: Percentiles of the observed distribution of the ratio of dual integrals between baseline runs and the corresponding runs with lifting across problem types. 

Problem type 2.5% 5% 25% 50% 75% 95% 97.5% RCPSP 2.1 × 10 −3 0.09 0.96 1.00 1.17 2.07 3.05 

RCPSP/max 3.4 × 10 −3 0.02 0.71 1.03 1.30 1.98 2.42 I also observe that in many “high-performing” runs of lifting approach, the underlying constraints (or, at least, the one yielding the highest bound) commonly happened to be a Disjunctive constraint, that is, to have the right-hand side equal to one. In a similar vein, I investigate how the influence of lifting on the search depends on the best constraint, which is summarized in Figure 3. Indeed, the lifting runs that ended up on a Disjunctive constraint as the best one are responsible for the biggest relative gains; however, the runs that landed on a Cumulative constraint still routinely produce large relative improvements, even if more modest. That said, discovering general Cumulative constraints is only helpful when the capacity of the discovered constraint is not large; conversely, in the runs where the best constraint inferred through lifting had a capacity larger than eight units, the CP solvers have not been able to convert it into a significantly faster search. I present the runtime distribution of the preprocessing phase of the lifting approach (that is, Algorithm 1 and Algorithm 2 but not the CP solver) in Figure 4. As can be seen on the histogram, under the budget imposed on lifting in this evaluation, all the lifting-specific work takes a few seconds in the majority of instances; on the other hand, the runs where lifting took more than a minute are covered exclusively by two collections with more than 500 tasks per instance, that is, UBO500 and UBO1000. Last, I report the updated state-of-the-art bounds for these benchmarks discovered with the lifting approach in Appendix A, together with the criterion I used to choose which bounds to report. While many of the new bounds have been discovered during search, some are computed by taking the largest capacity bound among lifted constraints, thereby making them verifiable by inspecting the constraint (rather than retracing the reasoning made by a solver). While many such “search-less” bounds are encoded by Disjunctive constraints (and therefore could, in principle, have also been proven by U NL without any search), there is a bound for instance #6 from the UBO1000 collection that improves upon the previously best known bound and is produced by a Cumulative with the capacity of three units. I 9On inferring cumulative constraints A P REPRINT 

10 -6 10 -4 10 -2 10 010 2

1 2—8 > 8 Right-hand side of the best inferred constraint Ratio of dual integrals Figure 3: Ratios of dual integrals between baseline runs and the corresponding runs with lifting across the right-hand side values of the best inferred constraint. 

050 100 150 

10 0 10 1 10 2 10 3Time spent on constraint discovery Number of samples 

500 tasks or more? 

FALSE TRUE Figure 4: Distribution of time required to run the inference of Cumulative constraints. also note that no less than twelve of the instances in Pack and Pack-d collections [Carlier and Néron, 2003, Koné et al., 2011] can be closed immediately by using one of the lifted cumulative constraints, with the capacities varying between one and three. 

5.2 Evaluation against U NL

I now proceed with the direct comparison with U NL; to this end, I run the following two solver configurations on each of the benchmark instances: • U NL with dual search in the same configuration as in Sidorov et al. • Pumpkin with dual search, prefaced with lifting in the same configuration as in the previous subsection, but only considering covers with two elements . This additional restriction is introduced to ensure that the lifting does not gain any advantage from discovering non-Disjunctive constraints. As can be seen from Figure 5, the proposed approach consistently outperforms U NL with respect to time to optimality for the applicable instances (Figure 5a) and has varying performance and with respect to the dual integral (Figure 5b). This is explained by the fact that the lifting approach explicitly introduces Disjunctive constraints into the model, which are then used in specialized—and highly optimized—propagation algorithms. In contrast, U NL maintains all the facts around the disjointness of task pairs in a single graph and applies pruning for some of the cliques of that graph. These plots also suggest that the lifting approach does not fully subsume U NL, as for some instances, lifting yields a notably worse performance. Interestingly, while many instances favorable for U NL are adequately explained by reasons unrelated to the methodology of this paper (such as the implementation of the preprocessing stage or discrepancies between the underlying solver versions), this is not true in all cases. For example, instance #40 from the UBO50 collection is solved to optimality for ≈ 30 seconds with U NL and for ≈ 8 minutes with lifting, with U NL also doing 90× fewer constraint propagations until optimality. The reason for this is that the lifting approach chooses Nout 

10 On inferring cumulative constraints A P REPRINT 

constraints with high capacity bounds (which in this case is the sum of durations) that also happen to share many variables, which translates into an insufficient extraction of the disjointness information; this reasoning is made more specific in Appendix B.          

> 10 -3 10 -2 10 -1 10 010 110 210 3
> 10 -3 10 -2 10 -1 10 010 110 210 3Time to optimality with UnL Time to optimality with lifting
> (a) Time to optimality for the instances solved to optimality with both approaches.
> 10 -2 10 010 2
> 10 -4 10 -2 10 010 2Dual integral of the UnL run Dual integral of the lifting run
> (b) Dual integrals for all evaluated instances.

Figure 5: Comparison between the lifting approach and U NL. 

# 6 Related work 

The need to reason simultaneously about multiple resources has been recognized across optimization communities throughout the history of earlier developments. For instance, early exact approaches for RCPSP consistently employed global lower-bounding techniques that used the entire problem instance structure, such as the lower bounds mentioned by Klein and Scholl [1999] based on solving relaxations to node packing (LB4) and parallel machine scheduling (LB5) problems. The presented approach can be seen as a generalization of these techniques in the following sense: Algorithm 2 with an appropriate choice of the input covers and an ordering of lifted variables can yield the same bounds as LB4 or LB5 implementations by letting a CP solver propagate the lifted Cumulative constraints. However, unlike using LB4 or LB5 directly, the new Cumulative remains known to a CP solver as one of the constraints rather than as an exogenous relaxation step, with any propagation algorithms for Cumulative applicable to it. Within the CP community, the limitations of the “single-resource viewpoint” were also acknowledged for a long time. Beldiceanu and Carlsson [2002] introduced the Cumulatives constraint to capture multiple resources in a single constraint, which was later improved by faster filtering algorithms [Letort et al., 2013, 2015]. While these works highlighted the need for multi-resource interaction, their focus was not on discovering new inference strategies but on running existing strategies faster, by replacing a “ping-pong” effect, where propagation for one Cumulative triggers a propagation for another, with a singular pass accounting for all Cumulative constraints. Conversely, the lifting approach can not only draw conclusions from several Cumulative constraints at once but can also draw conclusions unreachable (without an exponential blowup) by reasoning over them in isolation. Other efforts to mitigate the weaknesses of the single-resource viewpoint have focused on extracting stronger inferences from individual constraints by deriving non-dominated implied constraints. Carlier and Néron [2007] investigated the systematic derivation of such constraints for a single Cumulative by searching for one-to-one mappings from original resource consumptions to a reduced set of demands that preserve infeasibility while maximizing pruning. This logic was further refined by Baptiste and Bonifas [2018] by examining the dual linear program of a preemptive relaxation of 

Cumulative and introducing a mechanism for assigning a higher consumption level to a fixed number of tasks within a demand group. While these techniques strengthen the reasoning of a single resource, they remain fundamentally limited to intra-resource analysis. This work instead uses lifting as a tool for inter-resource reasoning; rather than refining a single Cumulative in isolation, the presented work derives global inequalities that capture the collective contention of tasks across the entire problem structure. 11 On inferring cumulative constraints A P REPRINT 

The earlier work by Sidorov et al. [2025a] implements a whole-problem reasoning scheme without merging constraints, opting instead to discover and communicate disjointness between tasks across resources. While U NL can theoretically emulate the node-packing bound (LB4) of Klein and Scholl, it lacks the expressive power to capture variable non-unit demands (LB5). Furthermore, it relies on managing auxiliary variables during search, which can be both a strength (by discovering a disjointness pattern conditional to a search state) and a hindrance (if there is no disjointness structure to be exploited). Conversely, the presented work compiles these interactions into conventional Cumulative constraints before initiating any search by a CP solver. The core mechanism that supports the contributions of this paper—lifting—originated in the integer programming community as a method for tightening cutting planes for knapsack constraints. First introduced by Padberg [1975], the technique iteratively strengthens an inequality by introducing variables not yet in the support. Later refinements, such as the ones proposed by Zemel [1989], demonstrated that all sequential lifting coefficients could be computed efficiently via dynamic programming. Currently, lifting is a standard part of the modern MILP solver practice, as exemplified by the SCIP optimization suite [Hojny et al., 2025], which extensively employs lifting procedures within its knapsack constraint handler to strengthen the linear relaxation of the problem. In the context of scheduling, polyhedral methods have been originally applied to single-machine problems [Queyranne, 1993, Dyer and Wolsey, 1990]. For the general RCPSP, early polyhedral approaches [Olaguíbel and Goerlich, 1993] focused on conflict variables (indicating a precedence within a pair of tasks) rather than start times, limiting their direct applicability in standard CP models. More recently, approaches combining CP propagators with LP relaxations have been explored, with CP-SAT [Perron and Didier, 2024], one of the state-of-the-art CP solvers, integrating scheduling cuts as special separation procedures for tightening LP relaxations. While scheduling cuts are constructed with a procedure similar to the one proposed here (choose a cover and lift it), those cuts are derived from energy 7 overflows; on the other hand, lifting as presented in this paper operates directly on the demands via occupancy vectors, allowing us to view Cumulative constraints as linear inequalities and apply standard lifting machinery directly to the start-time representation. Collectively, these works support the thesis that the “single-resource viewpoint” is insufficient for complex scheduling benchmarks. While integer programming and specialized branch-and-bound approaches have long exploited whole-problem structures, CP has historically struggled to integrate these insights without sacrificing the modularity of propagators. By establishing a formal link between Cumulative constraints and lifted linear inequalities, this work offers a pragmatic path to bring these powerful global inferences into CP solvers. 

# 7 Conclusions 

This paper presents a novel approach for aggregating Cumulative constraints by (i) reformulating them as linear constraints in the occupancy-vector representation, (ii) discovering sets of tasks that cannot be run jointly, and (iii) lifting the linear constraint blocking this group of tasks. The experimental evaluation shows that this approach is, at best, fundamentally helpful—or, at worst, not too obstructive—for proving objective bounds, although those advantages do not translate as well to discovering new high-quality solutions. One possible direction for future work is a tighter coupling with the search loop, for instance, by adding Cumulative 

constraints not before the search but during the search. The core technical problem here is that a Cumulative lifted from a conflicting assignment does not necessarily propagate after backtracking, as observed in earlier work on linear constraint learning in CP [Nieuwenhuis, 2014]. However, exploiting this procedure in a more heuristic way (e.g., during restarts) could be helpful to leverage the information learned during search. Last, the running assumption in this work is that both durations and resource consumptions are constant. While this is valid for RCPSP and RCPSP/max, this is not true for several other scheduling problem formulations studied previously, most notably multi-mode scheduling problems [Hartmann and Drexl, 1998]. However, extending this approach to multi-mode RCPSP—where task durations and demands are variable—will require handling the non-linearities resulting from the products of occupancy vectors and resource consumptions. 

# Acknowledgements 

I would like to thank Imko Marijnissen for the enlightening discussions that sparked the development of the technique presented in this paper. 

> 7The product of duration and resource consumption.

12 On inferring cumulative constraints A P REPRINT 

# Funding 

Konstantin Sidorov is supported by the TU Delft AI Labs program as part of the XAIT lab. 

# References 

Konstantin Sidorov, Imko Marijnissen, and Emir Demirovi´ c. Unite and lead: Finding disjunctive cliques for scheduling problems. In Maria Garcia de la Banda, editor, 31st international conference on principles and practice of constraint programming (CP 2025) , volume 340 of Leibniz International Proceedings in Informatics (LIPIcs) , pages 35:1–35:24, Dagstuhl, Germany, 2025a. Schloss Dagstuhl – Leibniz-Zentrum für Informatik. doi:10.4230/LIPIcs.CP.2025.35. Manfred W Padberg. Technical note–a note on zero-one programming. Operations research , 23(4):833–837, 1 August 1975. doi:10.1287/opre.23.4.833. Francesca Rossi, Peter van Beek, and Toby Walsh, editors. Handbook of constraint programming . Foundations of artificial intelligence. Elsevier Science, London, England, 18 August 2006. doi:10.1016/s1574-6526(06)x8001-x. Philippe Baptiste, Claude Le Pape, and Wim Nuijten. Satisfiability tests and time-bound adjustments for cumulative scheduling problems. Annals of operations research , 92:305–333, 1999. doi:10.1023/a:1018995000688. Petr Vilím. O(n log n) filtering algorithms for unary resource constraint. In Integration of AI and OR techniques in constraint programming for combinatorial optimization problems , volume 3011 of Lecture Notes in Computer Science , pages 335–347, Berlin, Heidelberg, 2004. Springer Berlin Heidelberg. doi:10.1007/978-3-540-24664-0_23. Günter M Ziegler. Faces of polytopes. In Lectures on polytopes: Updated seventh printing of the first edition , pages 51–76. Springer New York, New York, NY, 1995. doi:10.1007/978-1-4613-8431-1_2. Christopher Hojny, Tristan Gally, Oliver Habeck, Hendrik Lüthen, Frederic Matter, Marc E Pfetsch, and Andreas Schmitt. Knapsack polytopes: a survey. Annals of operations research , 292(1):469–517, 1 September 2020. doi:10.1007/s10479-019-03380-2. Andreas Schutt, Thibaut Feydy, Peter J Stuckey, and Mark G Wallace. Why cumulative decomposition is not as bad as it sounds. In Principles and practice of constraint programming - CP 2009 , Lecture Notes in Computer Science, pages 746–761, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg. doi:10.1007/978-3-642-04244-7_58. Konstantinos Kaparis and Adam N Letchford. Separation algorithms for 0-1 knapsack polytopes. Mathematical programming , 124(1-2):69–91, July 2010. doi:10.1007/s10107-010-0359-5. Joel P Stinson, Edward W Davis, and Basheer M Khumawala. Multiple resource–constrained scheduling using branch and bound. AIIE Transactions , 10(3):252–259, 1 September 1978. doi:10.1080/05695557808975212. Robert Klein and Armin Scholl. Computing lower bounds by destructive improvement: An application to resource-constrained project scheduling. European journal of operational research , 112(2):322–346, January 1999. doi:10.1016/s0377-2217(97)00442-6. Tias Guns. Increasing modeling language convenience with a universal n-dimensional array, CPpy as python-embedded example. In Proceedings of the 18th workshop on constraint modelling and reformulation at CP (ModRef 2019) ,volume 19, 2019. 

Gurobi optimizer reference manual . Gurobi Optimization, LLC, November 2024. Delft High Performance Computing Centre. DelftBlue, 2024. Konstantin Sidorov, Imko Marijnissen, and Emir Demirovi´ c. Data collection for “Unite and lead: Finding disjunctive cliques for scheduling problems”, June 2025b. URL https://doi.org/10.5281/zenodo.15624416 .Peter J Stuckey, Thibaut Feydy, Andreas Schutt, Guido Tack, and Julien Fischer. The MiniZinc Challenge 2008–2013. 

AI magazine , 35(2):55–60, 1 June 2014. doi:10.1609/aimag.v35i2.2539. Ramón Alvarez-Valdés Olaguíbel and José Manuel Tamarit Goerlich. Heuristic algorithms for resource-constrained project scheduling: A review and an empirical analysis. In Advances in project scheduling , volume 9 of Studies in Production and Engineering Economics , chapter 5, pages 113–134. Elsevier, North Holland, 1989. doi:10.1016/b978-0-444-87358-3.50009-2. Philippe Baptiste and Claude Le Pape. Constraint propagation and decomposition techniques for highly disjunctive and highly cumulative project scheduling problems. Constraints: an international journal , 5(1/2):119–139, January 2000. doi:10.1023/a:1009822502231. Jacques Carlier and Emmanuel Néron. On linear lower bounds for the resource constrained project scheduling problem. 

European journal of operational research , 149(2):314–324, 1 September 2003. doi:10.1016/s0377-2217(02)00763-4. 13 On inferring cumulative constraints A P REPRINT 

Oumar Koné, Christian Artigues, Pierre Lopez, and Marcel Mongeau. Event-based MILP models for resource-constrained project scheduling problems. Computers & operations research , 38(1):3–13, 1 January 2011. doi:10.1016/j.cor.2009.12.011. Rainer Kolisch and Arno Sprecher. PSPLIB - a project scheduling problem library. European journal of operational research , 96(1):205–216, January 1997. doi:10.1016/s0377-2217(96)00170-1. Maarten Flippo, Konstantin Sidorov, Imko Marijnissen, Jeff Smits, and Emir Demirovi´ c. A multi-stage proof logging framework to certify the correctness of CP solvers. In 30th international conference on principles and practice of constraint programming (CP 2024) , pages 11:1–11:20. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 29 August 2024. doi:10.4230/LIPICS.CP.2024.11. Laurent Perron and Frédéric Didier. CP-SAT, 7 May 2024. Timo Berthold. Measuring the impact of primal heuristics. Operations research letters , 41(6):611–614, 1 November 2013. doi:10.1016/j.orl.2013.08.007. Konstantin Sidorov. Source code bundle for the experiments on cumulative lifting, February 2026a. URL https: //doi.org/10.5281/zenodo.18663630 .Konstantin Sidorov. Execution logs and reports of the experiments on cumulative lifting, February 2026b. URL 

https://doi.org/10.5281/zenodo.18663551 .Nicolas Beldiceanu and Mats Carlsson. A new multi-resource cumulatives constraint with negative heights. In Principles and practice of constraint programming - CP 2002 , volume 2470 of Lecture Notes in Computer Science , pages 63–79, Berlin, Heidelberg, 2002. Springer Berlin Heidelberg. doi:10.1007/3-540-46135-3_5. Arnaud Letort, Mats Carlsson, and Nicolas Beldiceanu. A synchronized sweep algorithm for the k-dimensional cumulative constraint. In Integration of AI and OR techniques in constraint programming for combinatorial optimization problems , volume 7874 of Lecture Notes in Computer Science , pages 144–159, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. doi:10.1007/978-3-642-38171-3_10. Arnaud Letort, Mats Carlsson, and Nicolas Beldiceanu. Synchronized sweep algorithms for scalable scheduling constraints. Constraints: An International Journal , 20(2):183–234, April 2015. doi:10.1007/s10601-014-9172-8. Jacques Carlier and Emmanuel Néron. Computing redundant resources for the resource constrained project scheduling problem. European journal of operational research , 176(3):1452–1463, 1 February 2007. doi:10.1016/j.ejor.2005.09.034. Philippe Baptiste and Nicolas Bonifas. Redundant cumulative constraints to compute preemptive bounds. Discrete applied mathematics , 234:168–177, 10 January 2018. doi:10.1016/j.dam.2017.05.001. Eitan Zemel. Easily computable facets of the knapsack polytope. Mathematics of Operations Research , 14(4):760–764, November 1989. doi:10.1287/moor.14.4.760. Christopher Hojny, Mathieu Besançon, Ksenia Bestuzheva, Sander Borst, João Dionísio, Johannes Ehls, Leon Ei-fler, Mohammed Ghannam, Ambros Gleixner, Adrian Göß, Alexander Hoen, Jacob von Holly-Ponientzietz, Rolf van der Hulst, Dominik Kamp, Thorsten Koch, Kevin Kofler, Jurgen Lentz, Marco Lübbecke, Stephen J Ma-her, Paul Matti Meinhold, Gioni Mexi, Til Mohr, Erik Mühmer, Krunal Kishor Patel, Marc E Pfetsch, Sebastian Pokutta, Chantal Reinartz Groba, Felipe Serrano, Yuji Shinano, Mark Turner, Stefan Vigerske, Matthias Wal-ter, Dieter Weninger, and Liding Xu. The SCIP optimization suite 10.0. arXiv [math.OC] , 23 November 2025. doi:10.48550/arXiv.2511.18580. Maurice Queyranne. Structure of a simple scheduling polyhedron. Mathematical Programming , 58(1-3):263–285, January 1993. doi:10.1007/bf01581271. Martin E Dyer and Laurence A Wolsey. Formulating the single machine sequencing problem with release dates as a mixed integer program. Discrete Applied Mathematics , 26(2-3):255–270, 1 March 1990. doi:10.1016/0166-218x(90)90104-k. Ramón Alvarez-Valdés Olaguíbel and Josémanuel Tamarit Goerlich. The project scheduling polyhedron: Dimen-sion, facets and lifting theorems. European Journal of Operational Research , 67(2):204–220, 11 June 1993. doi:10.1016/0377-2217(93)90062-r. Robert Nieuwenhuis. The IntSat method for integer linear programming. In Principles and practice of constraint programming , volume 8656 of Lecture notes in computer science , pages 574–589, Cham, 2014. Springer International Publishing. doi:10.1007/978-3-319-10428-7_42. Sönke Hartmann and Andreas Drexl. Project scheduling with multiple modes: A comparison of exact al-gorithms. Networks. An International Journal , 32(4):283–297, December 1998. doi:10.1002/(sici)1097-0037(199812)32:4<283::aid-net5>3.0.co;2-i. 14 On inferring cumulative constraints A P REPRINT 

Petr Vilím, Philippe Laborie, and Paul Shaw. Failure-directed search for constraint-based scheduling. In Integration of AI and OR techniques in constraint programming , volume 9075 of Lecture Notes in Computer Science , pages 437–453, Barcelona, Spain, 2015. Springer Cham. doi:10.1007/978-3-319-18008-3_30. Petr Vilím. Timetable edge finding filtering algorithm for discrete cumulative resources. In Integration of AI and OR techniques in constraint programming for combinatorial optimization problems , volume 6697 of Lecture Notes in Computer Science , pages 230–245, Berlin, Heidelberg, 2011. Springer-Verlag. doi:10.1007/978-3-642-21311-3_22. Peter J Stuckey. RCPSP. https://people.eng.unimelb.edu.au/pstuckey/rcpsp/ . Accessed: 2025-3-29. 

# A Novel bounds 

I report the bounds discovered by the lifting approach if they are both better than the previously reported bounds and are not directly reproducible without lifting. More precisely, I report bounds that are simultaneously (a) tighter than the bounds reported in the previous sources known to us that used the same benchmarks [Kolisch and Sprecher, 1997, Vilím et al., 2015, Vilím, 2011, Stuckey, Sidorov et al., 2025a], and (b) either tighter than any bound derived without preprocessing or matches it but was derived at least ten times faster than with any other approach. Novel upper bounds (makespans) are reported in Table 2, and novel lower bounds are reported in Table 3; the same data is available as supplementary materials. All bounds are reported for RCPSP/max benchmarks; all reported durations are in MM:SS format. To indicate the remaining optimality gap, I also state the best known lower bound in Table 2 and the best known makespan in Table 3; in either case, that bound is the tightest among the previously reported values and the values discovered without preprocessing. Additionally, Table 4 reports the lower bounds that can be verified by an appropriate lifted constraint. Table 2: Novel upper bounds derived with lifting. 

Collection # Ref. objective New objective Time Best bound 

C 63 366 363 43:50 347 C 67 350 349 23:56 346 UBO100 8 385 383 41:56 376 UBO100 32 434 432 26:59 414 UBO200 4 893 838 29:25 605 

# B Disjointness structure of instance #40 from UBO50 

As mentioned in the main text, U NL outperforms the lifting approach on this instance; this appendix provides a more specific explanation for this. Figure 6 demonstrates the complement of the disjointness graph of that instance, with vertices corresponding to each of the fifty tasks that are scheduled in this instance, and edges connect pairs {u, v } of vertices that can be executed in parallel (that is, ai,u + ai,v ≤ bi for some i). Vertex colors are assigned as follows: red vertices correspond to tasks shared between all constraints added by lifting, blue vertices correspond to tasks that are mentioned in some (but not all) constraints added by lifting, and green vertices correspond to tasks not mentioned in any lifted constraint. The important insight in the structure of this graph is that it is (a) very sparse, with a random pair of tasks having 87% probability of being disjoint, and (b) the green vertices are loosely constrained by the rest of the graph; to justify the latter, I observe that 235 out of 262 maximal independent sets of this graph involve a green vertex. This suggests that UNL is able to exploit all of the graph structure, which opens the inferences about the tasks corresponding to the green vertices that are not available to the CP solver after the lifting procedure concludes. On the other hand, this shows the insufficiency of the selection step based solely on a quality metric of the Cumulative 

constraint. In this example, the lifting procedure chooses many independent sets (equivalently, Disjunctive con-straints) that contain all the red vertices and have a large total duration of tasks. Taken on a per-constraint basis, this is a good decision, as this set covers 40% of the problem variables; unfortunately, none of the green vertices can be added to this set. I believe that resolving the issue demonstrated by this instance would require a more elaborate selection procedure that rewards the overall coverage of the space of known conflicts, rather than the quality of each of the added constraints in isolation. 15 On inferring cumulative constraints A P REPRINT 

Table 3: Novel lower bounds derived with lifting. 

Collection # Ref. bound New bound Time Best objective 

C 63 343 348 43:08 363 C 66 340 347 49:16 368 UBO100 70 375 387 43:43 408 UBO200 2 682 731 46:25 813 UBO200 3 482 542 30:47 906 UBO200 5 499 559 57:28 767 UBO200 6 538 569 38:29 765 UBO200 8 505 583 48:53 911 UBO200 32 753 794 51:45 863 UBO200 33 784 785 56:14 834 UBO200 34 595 673 35:16 774 UBO200 65 728 747 35:22 814 UBO200 70 724 823 57:43 877 UBO500 3 1159 1260 43:30 1808 UBO500 6 1202 1350 01:05 2035 UBO500 8 1175 1208 00:56 2212 UBO500 38 1258 1398 01:12 2575 UBO1000 2 2321 2518 32:11 5479 UBO1000 6 2311 2329 04:12 3508 UBO1000 9 2274 2414 03:55 4231 UBO1000 10 2367 2441 02:30 3702 UBO1000 35 2382 2563 03:09 4737 UBO1000 68 2478 2550 03:12 4688 Table 4: Novel search-less lower bounds derived with lifting. The bounds marked with an asterisk have not been improved by a CP solver. 

Collection # Ref. bound New bound Objective Capacity 

UBO200 3 482 531 906 1UBO200 4 514 583 838 1UBO200 5 499 533 767 1UBO200 6 538 558 765 1UBO200 8 505 559 911 1UBO500 3 1159 1260* 1808 1UBO500 4 1230 1259 2774 1UBO500 6 1202 1345 2035 1UBO500 8 1175 1328* 2212 1UBO500 38 1258 1448* 2575 1UBO1000 2 2321 2515 5479 1UBO1000 6 2311 2315 3508 3UBO1000 9 2274 2418* 4231 1UBO1000 10 2367 2482* 3702 1UBO1000 35 2382 2568* 4737 1UBO1000 68 2478 2536 4688 116 On inferring cumulative constraints A P REPRINT 

Figure 6: The complement of the disjointness graph of instance #40 from UBO50; vertex color encodes the number of lifted constraints which mention the corresponding task, and the area occupied by a vertex is proportional to the task duration. 17