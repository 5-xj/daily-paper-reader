# Mining Generalizable Activation Functions
# 挖掘可泛化的激活函数

**Authors**: Alex Vitvitskyi, Michael Boratko, Matej Grcic, Razvan Pascanu, Deep Shah, Petar Veličković \\
**Date**: 2026-02-05 \\
**PDF**: https://arxiv.org/pdf/2602.05688v1 \\
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 9.0 \\
**Evidence**: evolutionary search for discovering new activation functions using LLMs \\

---

## Abstract
The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.

## 摘要
激活函数的选择是一个活跃的研究领域，不同的提议旨在改进优化的同时保持

---

## 论文详细总结（自动生成）

这是一份关于 Google DeepMind 发表的论文《Mining Generalizable Activation Functions》（挖掘可泛化的激活函数）的结构化深度总结。

---

### 1. 核心问题与整体含义
**研究动机与背景：**
激活函数是神经网络非线性能力的核心，它不仅影响优化效率，还决定了架构的**隐式归纳偏置（Implicit Inductive Bias）**。传统的激活函数（如 ReLU, GELU）多为人工设计或针对特定领域内（In-distribution）性能进行搜索。
本文的核心问题是：**如何自动化地发现能够提升模型“分布外泛化（OOD Generalization）”能力的激活函数？** 作者认为，激活函数如果能更好地尊重数据的内在结构（如周期性、对称性），就能在面对分布偏移时表现更稳健。

### 2. 方法论
论文提出了一种基于进化搜索的自动化发现框架，核心思想如下：

*   **AlphaEvolve 框架：** 利用前沿大语言模型（如 Gemini）作为“变异算子（Mutator）”。与以往限制在预定义操作符组合的搜索空间不同，该方法在**全 Python 函数空间**内搜索，仅受限于计算量（FLOPs）预算。
*   **小规模实验室协议（Small-scale Lab Approach）：** 为了解决神经架构搜索（NAS）计算昂贵的问题，作者在小型合成数据集上训练微型 MLP。这种快速迭代允许进化算法频繁评估候选函数的“适应度”。
*   **OOD 适应度函数：** 搜索的目标不是训练损失，而是**在 OOD 测试集上的验证损失**。例如，在 $[0, 0.5]$ 区间训练，在 $[0.5, 1.0]$ 区间测试，迫使算法寻找具有外推能力的函数。
*   **关键技术细节：**
    *   **LLM 引导：** LLM 不仅生成代码，还会生成设计理由（Rationale），利用其内在的科学知识库引导搜索向有意义的方向演进。
    *   **函数演化路径：** 从简单的 ReLU 开始，逐渐演化出结合了周期性信号（sin, sinc）和批次统计量（Batch Statistics）的复杂函数。

### 3. 实验设计
**数据集与场景：**
1.  **合成数据集（搜索阶段）：** 随机多项式（1D/20D）、调和函数（Harmonics）、Feynman 符号回归数据集（物理公式）。
2.  **下游验证基准（迁移阶段）：**
    *   **图像分类：** CIFAR-10, ImageNet-1K（测试通用性）。
    *   **算法推理：** CLRS-30（测试尺寸泛化能力，典型的 OOD 任务）。
    *   **分子图学习：** ogbg-molhiv（测试结构泛化）。

**对比方法（Benchmark）：**
*   基准激活函数：**ReLU**（搜索起点）和 **GELU**（工业界标准）。
*   对比了多种搜索到的新函数：如 **GELUSine**、**GELU-Sinc-Perturbation**、**GMTU**、**Turbulent** 等。

### 4. 资源与算力
*   **算力说明：** 论文强调了其方法的“高效性”，因为内部循环使用的是小型 MLP 和合成数据。
*   **具体细节：** 文中**未明确给出**具体的 GPU 型号、数量或总训练时长。但提到内部训练循环仅需 50 步（Steps），Batch Size 为 128，这表明单次评估的算力需求极低，适合大规模进化迭代。

### 5. 实验数量与充分性
*   **实验规模：** 做了 10 种不同激活函数的详细对比，涵盖了从 1D 合成数据到大规模 ImageNet 的跨度。
*   **消融实验：** 针对 `GELUSine` 函数中的超参数 $\alpha$ 进行了专门的扫参实验（见附录 C），验证了 AlphaEvolve 发现的参数确实处于最优区间。
*   **客观性：** 实验不仅展示了成功案例，也诚实地记录了某些在合成数据上表现极佳的函数（如基于 Batch 统计的函数）在下游任务中因过拟合或 OOM（显存溢