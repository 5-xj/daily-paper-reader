Title: UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents

URL Source: https://arxiv.org/pdf/2602.05832v1

Published Time: Fri, 06 Feb 2026 02:25:11 GMT

Number of Pages: 23

Markdown Content:
# UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

Han Xiao 1 * Guozhi Wang 2 * Hao Wang 2 * Shilong Liu 3 Yuxiang Chai 1 Yue Pan 2 Yufeng Zhou 2

Xiaoxin Chen 2 Yafei Wen 2 Hongsheng Li 1 4 5 † 

 Project Page: https://ui-mem.github.io/ 

Abstract 

Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. How-ever, its effectiveness is severely hindered by in-efficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these chal-lenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchi-cal Experience Memory. Unlike traditional re-play buffers, our memory accumulates structured knowledge, including high-level workflows, sub-task skills, and failure patterns. These experi-ences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sam-pling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Fur-thermore, a Self-Evolving Loop continuously ab-stracts novel strategies and errors to keep the memory aligned with the agent’s evolving policy. Experiments on online GUI benchmarks demon-strate that UI-Mem significantly outperforms tra-ditional RL baselines and static reuse strategies, with strong generalization to unseen applications. 

1. Introduction 

Recent advances in Multi-modal Large Language Models (MLLMs) (Liu et al., 2023; Bai et al., 2023; Achiam et al., 

> 1

Multimedia Laboratory (MMLab), The Chinese University of Hong Kong 2vivo AI Lab 3Princeton University 4Shenzhen Loop Area Institute 5Shanghai AI Lab. Correspondence to: Han Xiao <1155229123@link.cuhk.edu.edu >, Hongsheng Li 

<hsli@ee.cuhk.edu.hk >.

Preprint. February 6, 2026. (a) Standard Online RL      

> Agent
> Trajectory
> Rare Success
> (b) Experience Replay
> Cross -Task
> ×
> ✓
> Buffer
> Old Trajectory
> Online
> (c) Dense Reward
> Cross -Task
> …
> ✓✓×
> Step -level Reward
> (d) Evolving Memory (Ours)
> Guidance -aware Reward
> Guided Trajectory
> Experience Memory
> Guidance
> Successful
> Trajectory
> Failed
> Trajectory
> Cross -Task
> Agent
> Trajectory
> Outcome Reward
> Agent
> Trajectory
> Agent
> ×✓
> Success
> Plan
> Failure
> Pattern
> Outcome Reward
> Dense Reward

Figure 1. Comparison of RL paradigms for GUI agents. (a) Standard Online RL suffers from sparse rewards. (b) Experience Replay and (c) Dense Reward address sample efficiency and credit assignment respectively, but both lack mechanisms for Cross-Task Transfer. (d) Our Framework introduces an Evolving Memory that provides hierarchical guidance for exploration and continuously updates itself by abstracting successful plans and failure patterns from new trajectories, enabling cross-task knowledge transfer. 

2023) have enabled their application as autonomous GUI agents (Deng et al., 2023; Zhang et al., 2025a; Qin et al., 2025), capable of interpreting screenshots and generating sequential actions to accomplish user goals. To further enhance the decision-making capabilities of these agents, Reinforcement Learning (RL) (Liu et al., 2025a; Lu et al., 2025b) has emerged as a promising paradigm. In particular, Online RL methods (Bai et al., 2024; Xu et al., 2025) en-able agents to learn optimal policies through direct environ-ment interaction, generating diverse rollouts and receiving outcome-based feedback via algorithms like Group Relative Policy Optimization (GRPO) (Shao et al., 2024). However, the effectiveness of standard online RL in GUI environments is severely impeded by the combination of long-horizon tasks and extremely sparse reward signals. In 1

> arXiv:2602.05832v1 [cs.CV] 5 Feb 2026 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents

such settings, agents are often trapped in a cycle of blind trial-and-error, facing two primary bottlenecks: (1) Inef-ficient credit assignment : even if a trajectory contains several correct intermediate steps, such as navigating to the correct page, a single final error results in negative feedback, preventing the agent from reinforcing its partial successes. 

(2) Repetitive errors across tasks : the agent often encoun-ters similar failure modes (e.g., handling a confirmation popup) across different high-level tasks. Lacking a mech-anism to store and transfer this experience, the agent must rediscover errors from scratch in every new task. Existing approaches only partially address these bottlenecks. Data reuse methods, such as Experience Replay (Xu et al., 2025; Lu et al., 2025a), stabilize training by injecting past successful trajectories when the on-policy batch contains mostly failures. However, they fail to generate meaningful trajectories when facing novel tasks, especially early in train-ing when success is rare. Reward refinement methods (Wang et al., 2023; Feng et al., 2025) introduce step-level dense rewards to provide fine-grained supervision. While they identify which step was correct within the current rollout, they do not enable transfer of reusable experience across different tasks and applications. Our key insight is that RL for GUI agents should not merely learn from raw trajectories or rely on dense supervision; it should accumulate, reuse, and evolve experience that transfers across tasks. Motivated by this, we propose UI-Mem (Figure 1), a novel framework that enhances GUI online RL with a Hierarchical Experience Memory . Un-like traditional replay buffers that store raw trajectories, our memory accumulates structured knowledge: high-level workflows for planning, subtask skills for execution, and failure patterns to prevent repetitive errors. To efficiently utilize the memory, we store this structured knowledge as parameterized templates. When given a novel task, the agent retrieves relevant abstract templates (e.g., “Send email to 

{{ recipient }} ”) and instantiates them with current task details to form a concrete plan. This design addresses both bottlenecks: hierarchical decomposition allows the agent to receive credit for completing intermediate subtasks even when the full task fails, while template abstraction enables skill reuse across different applications. However, integrating such memory into online RL intro-duces new challenges. Simply prepending retrieved plans to every rollout prompt risks the agent learning to follow exter-nal instructions rather than internalizing skills. Moreover, if strong guidance causes all trajectories in a rollout group to succeed, the advantage variance drops to zero, eliminating the learning signal. Therefore, we introduce a Stratified Group Sampling mechanism during the memory-guided exploration process. Instead of uniform prompting, we con-struct a diverse batch for GRPO by injecting varying levels of guidance into the prompts of different trajectories within the same group. Some trajectories receive full hierarchical plans to improve success rates, while others receive partial or no guidance to encourage independent exploration. This creates within-group outcome diversity essential for advan-tage estimation, driving the unguided policy to bridge the performance gap towards the guided trajectories. We em-ploy a dynamic curriculum to progressively reduce guidance as task success rates improve. Finally, as the agent’s policy improves, it discovers novel strategies and encounters new failure modes absent from the existing memory. To ensure that the memory accommodates evolving task distributions, we introduce a Self-Evolving Loop . We abstract success-ful trajectories into new plans and extract failure patterns from errors, dynamically updating the memory pool to co-evolve with the agent’s policy. Extensive evaluations on online GUI benchmarks demonstrate that our method signif-icantly outperforms traditional RL baselines and static data reuse strategies across varying model scales. Furthermore, the generalizability to unseen applications highlights the effectiveness of our abstraction and retrieval mechanisms. 

2. Related Work 

Multi-modal GUI Agents. Recent advances in Large Lan-guage Models (LLMs) (OpenAI, 2023a;b) and Multi-modal LLMs (MLLMs) (Liu et al., 2023; Bai et al., 2023) have significantly empowered agents to perceive and interact with Graphical User Interfaces (GUIs). Early efforts (Wang et al., 2024; Zhang et al., 2025a) build agent frameworks upon commercial MLLMs and demonstrate promising re-sults in mobile and desktop environments. Recent research has shifted towards fine-tuning open-source MLLMs. For instance, OS-Atlas (Wu et al., 2024) and Aguvis (Xu et al., 2024b) focus on enhancing UI grounding and propose a unified action space for cross-platform generalization. UI-TARS (Qin et al., 2025) further scales up instruction tuning with massive datasets covering element-wise grounding and System-2 reasoning. UI-Genie (Xiao et al., 2025) introduces a self-evolving framework for synthesizing high-quality trajectories. Despite these successes in Supervised Fine-Tuning (SFT), these agents often struggle with multi-step reasoning and generalization to novel tasks outside their training distribution, which requires further optimization through environment interaction. 

Reinforcement Learning for Language and Agents. 

Reinforcement Learning has emerged as a powerful paradigm for aligning language models with human pref-erences (Christiano et al., 2017) and improving reasoning capabilities (Schulman et al., 2017; Rafailov et al., 2023) beyond SFT. Recent advancements such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024) offer ef-ficient optimization by leveraging group-based advantage 2UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents Semantic 

> Retrieval

Hierarchical Experience 

Memory         

> Workflow
> BookFlight
> 1. Open {{ App_Name }}
> 2. Search flight: {{Origin}} to {{Dest}}
> 3. Select Date: {{Date}}
> 4. Click Search & Select Flight
> Subtask Skill
> Input Flight Details
> 1. Tap "From" bar and Type {{Origin}}
> 2. Tap "To" bar and Type {{Dest}}
> 3. Tap "Departure Date" and Scroll to
> {{Date}}
> Failure Pattern
> Fix: Ensure date field is not empty.
> Pattern : Do not click {{ Search_Btn }}
> before {{Date}} is selected.
> Book a flight from San Francisco to
> NYC on March 15.

Stratified Group Sampling        

> Prompt: Query + Retrieved
> Workflow+ Subtask + Failure Pattern
> Strong
> Guidance
> Prompt :Query + Retrieved Workflow
> Weak
> Guidance
> Prompt: Query
> (Pure Exploration)
> No
> Guidance
> MLLM GUI Agent
> Policy Optimization
> Generated Trajectories
> Dynamic Curriculum
> Guidance ↓ as success rate ↑
> Group size = G
> ×
> ×
> ✓R1=0.8
> R2=0.6
> R3=0.2
> Experience Abstraction

Self -Evolving Loop   

> Memory Update
> Guidance
> Guidance -aware
> Reward
> Group Advantage
> Estimation
> ✓
> ×
> Successful Trajectories
> Extract Plans & Replace "NYC" -> {{Dest}}
> Failed Trajectories
> Failure Diagnoses to avoid
> No -guidance
> bonus
> Rollout
> Policy
> Update

Figure 2. Overview of the proposed UI-Mem framework. Given a task instruction, the agent retrieves hierarchical experience including Workflows, Subtask Skills, and Failure Patterns. We employ Stratified Group Sampling to generate a group of trajectories under varying levels of guidance (Strong, Weak, and No Guidance), enabling effective advantage estimation for Policy Optimization . Finally, a 

Self-Evolving Loop extracts abstract plans from successful trajectories and diagnoses from failures to update the memory, facilitating continuous refinement and cross-task transfer. 

estimation without a critic network. In GUI domains, sev-eral works (Lu et al., 2025b; Liu et al., 2025a) have explored GRPO-based optimization with rule-based rewards in an of-fline manner. However, applying online RL to GUIs remains challenging due to extremely sparse rewards. To mitigate this, recent works improve reward design with step-level feedback (Wang et al., 2023; Feng et al., 2025), while others rely on Experience Replay to stabilize training (Bai et al., 2024; Xu et al., 2025; Lu et al., 2025a). Despite their ef-fectiveness, these approaches lack mechanisms to transfer reusable experience across tasks, failing to address the fun-damental bottleneck of generating high-quality trajectories in the first place. By contrast, our UI-Mem constructs a hi-erarchical, evolving experience memory that actively guides online sampling, significantly improving both trajectory quality and training efficiency. 

3. Method 

3.1. Overview 

We formulate the GUI interaction as an MDP and adopt GRPO (Shao et al., 2024) as our base optimizer. Formal definitions are provided in Appendix (Section A). However, standard GRPO struggles in the context of online GUI RL. Due to the vast exploration spaces and long horizons, the sampled group often consists entirely of failures. This re-sults in zero variance in the advantage term, providing no gradient for optimization. Furthermore, even when partial success is achieved in some trajectories, the agent lacks a mechanism to retain this experience, forcing it to rediscover solutions for similar tasks from scratch. To address these challenges, we propose UI-Mem, a novel framework that integrates structured experience memory into the online RL loop. As illustrated in Figure 2, UI-Mem consists of three core components: (1) Hierarchical Ex-perience Memory (Section 3.2), which constructs a struc-tured memory pool that stores reusable workflows, subtask skills, and failure patterns as parameterized templates; (2) Memory-Guided Exploration (Section 3.3), which utilizes a stratified group sampling mechanism that injects different strengths of memory guidance into the same GRPO group, facilitating effective advantage estimation while prevent-ing guidance dependency; (3) Self-Evolving Loop (Sec-tion 3.4), which continuously refines the memory by extract-ing novel experience from the newly collected trajectories, enabling progressive improvement and cross-task transfer. 

3.2. Hierarchical Experience Memory 

Traditional replay buffers store raw trajectories as sequences of state-action pairs, which suffer from high variance and poor generalization to minor UI changes, making it difficult to transfer experience across tasks or apps. To overcome this limitation, we construct a structured, hierarchical, and abstracted experience memory. This design allows the agent to retrieve relevant past experience and instantiate it to form specific plans when facing novel tasks. 3UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

Memory Representation. We draw inspiration from hu-man cognitive processes in GUI navigation, where users naturally decompose tasks into subtasks and rely on familiar interaction patterns from well-known apps. To mimic this, we formulate our memory at three distinct levels: • High-Level Workflows ( W): At the top level, we store workflow plans that capture global strategies for task completion. Each workflow is an ordered sequence of subtasks. For instance, for the task “Send an email”, the workflow might be “[Open Mail App, Select Recip-ient(s), Type Content, Send]”. This enables the agent to reuse effective planning for semantically similar tasks. • Mid-Level Subtask Skills ( Σ): At the execution level, we maintain a library of subtask skills corresponding to atomic capabilities that recur across many high-level tasks. Examples include “search for an item”, “fill a form field”, or “navigate to a specific folder”. For each subtask, we store summarized action sequences in natural language, such as “Tap the search icon, type 

{{ query }} , then select the top result”. These fine-grained skills are highly reusable across applications with similar interaction paradigms. • Failure Patterns ( F): To prevent repetitive errors, we also record the common failure modes derived from previously failed trajectories. For instance, if an agent frequently fails due to forgetting to enter the correct filename, the memory stores the pattern “Avoid click-ing the ‘Save’ icon before entering a filename”. This enables the agent to avoid known failure modes rather than rediscovering them through costly trial-and-error. 

Abstraction into Templates. A key observation is that many GUI task instructions share a similar structure but dif-fer only in concrete values such as filenames, dates, phone numbers, or UI element names. Therefore, we employ an abstraction mechanism that clusters semantically similar in-structions and subtask definitions into templates. Concrete values in the experience are replaced with semantic place-holders. For example, the specific step “Click the ‘Save’ button to confirm creating report.txt” becomes the template “Click the {{ confirm button }} button to confirm creat-ing {{ filename }} ”. This abstraction maximizes transfer-ability by allowing a single parameterized plan to generalize to novel tasks. It also minimizes storage redundancy by con-solidating semantically equivalent experiences into compact, easy-to-retrieve templates. 

Storage and Retrieval. The memory is stored in a vector database indexed by the embeddings of task and subtask de-scriptions, enabling efficient similarity search at scale. The memory retrieval process is illustrated in Figure 3. During rollout, given a new task instruction q, we first compute its embedding and perform semantic matching with stored task What's the time difference between New York and Toronto?                        

> Task
> Instruction
> [Open Clock App]
> [Add {{city1}} ]
> [Add {{city2}} ]
> [Compare Times]
> Instantiated Experience
> Level 1: HIGH -LEVEL Workflow
> Level 2: MID -LEVEL Subtask Skills
> TASK TEMPLATE:
> Determine time diff between
> {{city1}} and {{city2}}
> SUBTASK TEMPLATE:
> [Add City {{ target_city }} ]
> 1. Tap + button
> 2. Type {{ target_city }} in search bar
> 3. Select the first result matching {{ target_city }}
> EXTRACTED:
> city1: New York city2: Toronto
> Level 3: FAILRE Patterns
> SUBTASK TEMPLATE:
> [Add City {{ target_city }} ]
> If {{ target_city }} has multiple timezones , check the
> region carefully.
> Open Clock App. 1
> Subtask: Add City New York .2
> Tip:
> Tap +, Type New York, Select result.
> Avoid:
> Ensure correct New York region selected.
> Subtask: Add City Toronto .3
> Tip:
> Tap +, Type Toronto, Select result.
> Avoid:
> Ensure correct Toronto region selected.

Figure 3. Illustration of the Hierarchical Experience Retrieval process. Given a task instruction, the system performs template matching to extract specific variables (e.g., city names). The retrieved experience is instantiated with the extracted variables to form a concrete, actionable plan for the current rollout. 

templates. For matched templates, we extract instruction-specific variable bindings and instantiate the corresponding experience by substituting placeholders with the extracted values. A retrieved task template may be associated with multiple experience entries accumulated from past explo-rations. To balance exploitation and exploration, we em-ploy a scoring mechanism inspired by the Upper Confidence Bound (UCB) algorithm. For a candidate plan p, its retrieval score S(p) is: 

S(p) = Nsucc (p)

P

p′∈P Nsucc (p′) + λucb 

s

ln( P

p′∈P Nused (p′)) 

Nused (p) + 1 

(1) where Nsucc and Nused denote the success count and us-age count respectively, and λucb is the exploration weight. The first term encourages the selection of plans with high historical success rates, while the second term provides an exploration bonus for plans that have been tried fewer times. For failure patterns, we use a recency bias to prioritize re-cent error diagnoses, ensuring that the guidance reflects the agent’s latest policy behavior. This adaptive retrieval mecha-nism ensures that the guidance provided to the agent evolves dynamically as the memory is refined through training. 

3.3. Memory-Guided Exploration 

Having established the hierarchical memory structure, we now address how to leverage this memory to enhance on-line RL training. A naive approach might simply prepend retrieved experience to every rollout prompt, treating the memory as a fixed reference. However, this strategy suffers from the risk of dependency overfitting, where the agent 4UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

learns to follow external instructions rather than internal-izing transferable skills. Furthermore, if strong guidance makes all trajectories in a group successful, the advantage variance becomes zero, hindering optimization in GRPO. To address these challenges, we propose a stratified group sampling mechanism that injects memory guidance at vary-ing strengths within each rollout batch, combined with a dynamic dropout curriculum that progressively reduces guid-ance as the agent’s ability improves. 

Stratified Group Sampling. The core insight behind our sampling strategy is that effective GRPO training requires within-group diversity in trajectory outcomes. Rather than applying uniform guidance, we partition each group of G

trajectories into three subgroups receiving strong, weak, and no guidance, with respective proportions λstrong , λweak , and 

λnone , where λstrong + λweak + λnone = 1 . Specifically, for a task q, we retrieve the best workflow Wq , skills Σq , and failure patterns Fq . The subgroups are composed as follows: • Strong-Guidance (λstrong · G trajectories): Condi-tioned on full guidance ( q, Wq , Σq , Fq ). These tra-jectories exploit the valuable guidance to seek high-quality solutions, thereby providing potential positive anchors to stabilize the learning process. • Weak-Guidance (λweak · G trajectories): Conditioned only on the high-level workflow ( q, Wq ). This forces the agent to formulate the low-level execution details, bridging the gap between planning and acting. • No-Guidance (λnone · G trajectories): Sampled with no external memory, encouraging pure exploration and providing an unbiased estimate of the agent’s internal-ized policy. This stratified design increases the diversity of outcomes within each group, which is essential for estimating advan-tages in GRPO. Through this mechanism, the unguided policy is trained to match the performance of guided sub-groups, thus gradually transferring the knowledge encoded in the external memory into the agent’s internal parameters. 

Dynamic Dropout Curriculum. While stratified group sampling improves within-batch diversity, using fixed guid-ance proportions throughout training is suboptimal. In the early stages of training, strong guidance is crucial for gen-erating positive rewards. As training progresses, continued reliance can hinder generalization. To ensure that the agent eventually internalizes experience, we introduce a dynamic curriculum that gradually withdraws the guidance. Specifi-cally, we track the Exponential Moving Average (EMA) of the success rate St ∈ [0 , 1] for each task. We define two thresholds: θstart , below which the agent is considered to struggle with the task, and θend , above which the agent has achieved reliable competence. The sampling probabilities 

λstrong and λnone are modeled as linear functions of St:

λstrong (St) = clip  λmax  

> strong

− ϕ(St)∆ λstrong , λ min 

> strong

, λ max 

> strong



(2) 

λnone (St) = clip  λmin  

> none

+ ϕ(St)∆ λnone , λ min 

> none

, λ max 

> none

 (3) where ϕ(St) = St−θstart  

> θend −θstart

represents the normalized progress, and ∆λstrong , ∆λnone represent the respective ranges. As the agent’s capabilities improve ( St ↑), λstrong decreases and λnone increases, shifting the focus from imitation to independent task solving. 

Guidance-Aware Reward Shaping. Standard outcome-based rewards treat all successful trajectories equally, re-gardless of how the success was achieved. However, in our stratified sampling framework, a success obtained through comprehensive step-by-step guidance is qualitatively less valuable than a success achieved through pure exploration. To align the reward signal with our optimization objective, we introduce guidance-aware reward shaping that augments the base outcome reward with two components. First, we calculate a progress reward rprogress based on the comple-tion of subtasks defined in the reference workflow W:

rprogress = |S completed ||S total | (4) Second, we add a bonus for unguided success to encourage internalization of skills. The shaped reward R(τ ) for a trajectory τ is computed as: 

R(τ ) = λo · routcome + λp · rprogress 

+ α · I(guidance (τ ) = None ) · routcome 

(5) where λo and λp are weights for outcome and progress rewards respectively, I(·) is the indicator function and α is a bonus coefficient. This shaping mechanism provides denser subtask-level learning signals while encouraging the agent to solve tasks without depending on guidance tokens. 

3.4. Self-Evolving Loop 

To ensure that our memory adapts to the evolving policy and captures novel strategies discovered during training, we introduce a self-evolving loop (Figure 4) that continuously extracts, abstracts, and integrates new experience from the trajectories generated during online RL. 

Experience Extraction and Abstraction. At the end of each training iteration, we first employ a reward model to evaluate each trajectory, including determining the global success outcome and identifying the completed subtasks. Based on these judgments, we invoke an LLM-based experi-ence extraction module to derive structured experience. For successful trajectories ( R(τ ) = 1 ), the module extracts the high-level workflow W and an execution plan Σ for each 5UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents ope n_a pp [settings]”                     

> Task: Open the ‘To -Do' file and change
> the font size to 17 in File Settings
> Failed Action: Target '17' was off -screen.
> Failed to scroll down to reveal the option.
> Raw Trajectory
> Task: Disable Contacts' APP notifications
> TEMPLATE ABSTRACTION:
> Contacts ->{{ app_name }} Toggle Off ->{{ action_state }}
> Experience
> Extraction
> TEMPLATE ABSTRACTION:
> 17 ->{{ Target_value }} Failure Pattern: Target Off -screen
> Workflow & Skills Failure Patterns
> Hierarchical Experience Memory
> +New Plans +New Rules
> Cross -Task
> Transfer
> Scenario 1: Workflow Reuse
> New Task: Turn off notifications for
> Youtube .
> Apply Template: {{ app_name }} ->Youtube
> Scenario 2: Failure Avoidance
> New Task: Select ‘Dark Mode’ in system
> settings.
> Corrective Action: Scroll down first , then
> select ‘Dark Mode’.

Figure 4. The Self-Evolving Loop. Successful plans and failure causes are extracted from new trajectories to continually refine the memory and guide next rollouts. 

completed subtask. For failed trajectories ( R(τ ) = 0 ), the module extracts valid plans for successfully completed sub-tasks and generates a failure diagnosis F specifically for the first failed subtask. To maximize the transferability of ex-tracted experience, we transform these raw extractions into generalized ones by using the abstraction mechanism de-scribed in Section 3.2. The extracted workflows, skills, and failure patterns are parameterized by replacing concrete val-ues with semantic placeholders (e.g., replacing “report.pdf” with {{ filename }} ). 

Memory Update. Finally, we integrate the abstracted ex-perience into the hierarchical memory {W , Σ, F} . We com-pute semantic embeddings for the newly extracted entries and query the vector database for existing counterparts with high cosine similarity. If a similar entry is found, we merge the new experience by updating the associated statistics: increasing the success count Nsucc for successful plans or updating the timestamp for failure patterns. If no similar entry exists, we create a new entry with initialized statistics (Nsucc = 1 , Nused = 0 ). Through this continual updating process, the memory pool co-evolves with the policy, pro-gressively improving the experience quality and enabling cross-task generalization throughout online training. 

4. Experiments 

4.1. Implementation Details Training Settings. To demonstrate the scalability of UI-Mem, we integrate our framework with the Qwen3-VL fam-ily (Bai et al., 2025a), specifically the 4B and 8B parameter variants. We implement a parallelized online Reinforce-ment Learning framework. Specifically, we deploy dis-tributed Android emulator instances across CPU clusters to perform asynchronous rollouts. During the sampling phase, the agent interacts with the environment guided by adaptively retrieved experiences to generate trajectories. We employ the Group Relative Policy Optimization algorithm based on our stratified group sampling. For each query, we sample G = 4 outputs. We train both models for five epochs with a learning rate of 1 × 10 −6.

Dataset Construction. To construct the training dataset, we collect task instructions from AMEX (Chai et al., 2024), AndroidLab (Xu et al., 2024a), and UI-Genie (Xiao et al., 2025), filtering for tasks compatible with core apps in our emulator environment. To increase task diversity, we em-ploy GPT-4o to augment these seed instructions, resulting in a total of Ntrain = 256 training queries. Notably, the original datasets provide only high-level task instructions without explicit subtask definitions. To enable our hierarchi-cal memory structure, we utilize the annotated trajectories to generate subtask definitions, which are subsequently refined through the self-evolving loop during training. 

Reward Evaluation and Experience Extraction. A key limitation of prior MLLM-as-a-Judge approaches is that directly processing screenshot sequences often induces se-vere visual hallucinations. To address this, we adopt a two-stage text-based verification pipeline: we first use Qwen2.5-VL-72B-Instruct (Bai et al., 2025b) to convert each screen state and action into textual descriptions, and then employ DeepSeek-V3 (Liu et al., 2024) to perform rule-based state verification on the resulting text history. This decoupling of visual grounding from logical reasoning significantly im-proves evaluation reliability (Accuracy: 0.900 vs. 0.724 for direct MLLM scoring). Then we utilize an experience extraction module (Seed1.8 (Seed, 2025a)) to obtain suc-cessful plans and failure diagnoses. Detailed designs and validation results are provided in Appendices C and D. 

4.2. Evaluation Benchmarks 

We evaluate UI-Mem on two challenging online benchmarks that require dynamic interaction: 

AndroidWorld (Rawles et al., 2024) evaluates agents on 116 programmatic tasks across 20 real-world apps. It is characterized by long-horizon dependencies (often > 10 

steps) and dynamic task parameterization, rigorously test-ing the agent’s reasoning and generalization capabilities. Rewards are derived directly from system states, ensuring robust evaluation. 

AndroidLab (Xu et al., 2024a) focuses on nine commonly 6UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents                                            

> Table 1. Performance comparison on the AndroidWorld bench-mark. The table presents closed-source API models, open-source foundation models and our approach. ⋆denotes inference-time memory retrieval. Best results are in bold , and the second-best results are underlined.
> Model Params. Success Rate
> Seed1.5-VL (Guo et al., 2025) -62.1 UI-Tars-1.5 (Seed, 2025b) -64.2 Gemini-2.5-Pro (Comanici et al., 2025) -69.7 Seed1.8 (Seed, 2025a) -70.7
> Qwen3-VL-2B (Bai et al., 2025a) 2B 36.4 MAI-UI-2B (Zhou et al., 2025) 2B 49.1 ScaleCUA-3B (Liu et al., 2025b) 3B 23.7 Ferret-UI Lite-3B (Yang et al., 2025) 3B 28.0 Qwen3-VL-4B (Bai et al., 2025a) 4B 45.3 UI-Mem-4B 4B 58.2 UI-Mem-4B ⋆4B 62.5
> UI-Venus-7B (Gu et al., 2025) 7B 49.1 GUI-Owl-7B (Ye et al., 2025) 7B 66.4 Step-GUI-8B (Yan et al., 2025) 8B 67.7 UI-Tars-1.5-7B (Seed, 2025b) 7B 30.0 Qwen3-VL-8B (Bai et al., 2025a) 8B 47.6 UI-Mem-8B 8B 66.8 UI-Mem-8B ⋆8B 71.1

used offline applications. It provides fine-grained metrics beyond binary success, making it ideal for analyzing subtask completion and operation efficiency. The benchmark com-prises 138 tasks simulating daily user interactions such as managing events, editing notes, and retrieving information. 

4.3. Main Results Performance on AndroidWorld. Table 1 presents the suc-cess rates on AndroidWorld. UI-Mem demonstrates consis-tent improvements across model scales. The UI-Mem-4B model achieves a success rate of 58.2%, significantly out-performing the vanilla Qwen3-VL-4B (45.3%) and even surpassing the larger UI-Venus-7B (49.1%). This result val-idates that our memory-guided training enables the agent to learn more efficient policies than standard supervised fine-tuning or vanilla RL. When equipped with the expe-rience memory at inference time (denoted as UI-Mem ⋆), the performance further improves to 62.5%. This indicates that UI-Mem also learns to effectively retrieve and utilize external memory to solve novel tasks. On the 8B scale, UI-Mem-8B with memory retrieval achieves a state-of-the-art success rate of 71.1%, outperforming closed-source com-mercial APIs such as Gemini-2.5-Pro and Seed1.8. This suggests that our hierarchical memory mechanism scales ef-fectively with model capacity, unlocking stronger reasoning capabilities in larger foundation models. 

Performance on AndroidLab. We utilize AndroidLab to analyze fine-grained capabilities using Sub-Goal Success Rate (Sub-SR), Reversed Redundancy Ratio (RRR), Reason-able Operation Ratio (ROR), and overall Success Rate (SR). Results are shown in Table 2. UI-Mem shows a substantial improvement in Sub-SR compared to the vanilla baseline, demonstrating the effectiveness of our hierarchical mem-ory. By explicitly retrieving workflows and subtask plans, the agent can decompose complex long-horizon tasks into manageable segments, thereby reducing error propagation. Notably, UI-Mem-8B outperforms MobileRL (42.5%), a strong baseline that utilizes static experience replay. While MobileRL relies on the reuse of successful trajectories, UI-Mem employs active memory to guide the generation of high-quality new trajectories with diverse progress rewards. This allows our agent to explore the state space more effi-ciently and adapt to dynamic UI changes, rather than merely memorizing past paths. 

4.4. Ablation Study Comparison with RL Training Paradigms. We com-pare our method with other RL methods to demonstrate the advantage of our memory-guided approach. As shown in Table 3, we compare UI-Mem with: (1) Vanilla GRPO, which adopts GRPO training from scratch without external memory; (2) GRPO + Progress Reward, which augments GRPO with subtask-level dense rewards; (3) Inference-time Prompting, which retrieves experience from our memory as context during inference only; (4) GRPO + Experience Replay, which mixes top-k past successful trajectories into the training batch; and (5) GRPO + Inference-time Prompt-ing, where we apply memory retrieval to the GRPO-trained model. Results indicate that vanilla GRPO struggles with ex-ploration in the vast GUI state space. While dense rewards improve the performance (57.3%), they cannot transfer past experience to novel tasks. Experience Replay improves training stability (56.5%), but remains limited to replaying static historical data and fails to actively guide exploration toward new solutions. Notably, Inference-time Prompting yields limited gains when applied to the baseline and GRPO-trained model, suggesting that simply providing context is insufficient if the model has not internalized how to utilize it. In contrast, UI-Mem achieves the highest success rate, demonstrating that our approach, which combines memory-guided exploration during training with adaptive retrieval, substantially outperforms other methods. 

Component Analysis. We further evaluate the contribu-tion of different components in our framework. As illus-trated in Figure 5, we compare the full UI-Mem framework against variants where specific components are removed or revised. We first analyze the effectiveness of our hierar-chical memory structure. Removing the hierarchy to rely solely on specific components leads to significant perfor-mance drops. Using only high-level workflows, low-level subtask skills or failure patterns each underperforms the full method by a significant margin. This demonstrates that the three experience types of our memory system cap-ture complementary knowledge. Moreover, replacing our 7UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

Table 2. Performance comparison on the AndroidLab benchmark. The table presents closed-source API models, open-source foundation models and our approach. ⋆ denotes inference-time memory retrieval. The best results are in bold , and the second-best are underlined. Model Params Sub-Goal Success Rate Reversed Redun-dancy Ratio Reasonable Operation Ratio Success Rate Gemini-1.0 – 12.6 72.5 76.7 10.9 Claude-3-Opus – 15.1 81.4 83.9 13.0 Gemini-1.5-Pro – 18.5 106.0 91.5 16.7 Claude-3.5-Sonnet – 32.7 113.4 81.2 29.0 GPT-4o – 35.0 87.3 85.4 31.2 AutoGLM – – – – 36.2 

UI-Genie-Agent 3B 35.4 91.3 90.6 28.8 Qwen3-VL-4B 4B 48.2 93.0 90.5 37.0 UI-Mem-4B 4B 49.5 93.0 93.5 37.7 UI-Mem-4B ⋆ 4B 51.9 89.2 94.6 39.9 

LLaMA3.2-11B-Vision 11B 13.0 61.7 87.9 10.1 CogVLM2-ft 19B 16.1 57.4 85.6 11.6 Qwen2.5-VL 7B 18.7 70.6 76.8 14.9 UI-Genie-Agent 7B 46.3 92.8 91.4 38.7 Qwen3-VL-8B 8B 45.3 97.7 91.8 34.8 UI-TARS-1.5-7B 7B 49.4 84.2 92.5 40.6 MobileRL 7B - - - 42.5 UI-Mem-8B 8B 52.7 93.9 90.9 43.5 UI-Mem-8B ⋆ 8B 56.0 89.8 94.9 44.9 

Table 3. Comparison of different RL training paradigms on AndroidWorld. 

Method Mechanism SR (%) 

Qwen3-VL-8B Baseline 47.6 Vanilla GRPO Online RL 53.0 GRPO + Progress Reward Online RL 57.3 Inference-time Prompting RAG (No Training) 52.6 GRPO + Experience Replay Data Reuse 56.5 GRPO + Inference-time Prompting Online RL+RAG 55.2 

UI-Mem (Ours) Memory-Guided RL 66.8 

abstracted memory with raw experience results in a signif-icant performance drop (58.2%). This confirms that raw data are verbose, task-specific, and difficult to generalize across different tasks. Furthermore, disabling the mem-ory update reduces performance to 62.9%, highlighting the importance of continuously refining the memory pool dur-ing the training progress. Providing full memory guidance without stratified sampling significantly decreases the per-formance. This result reveals that excessive guidance can lead to over-reliance on memory, undermining the agent’s ability to develop autonomous reasoning. Our stratified sampling strategy effectively balances guidance and explo-ration, enabling the agent to benefit from memory without sacrificing its capacity for independent problem-solving. 

Cross-Application Generalization. While our main benchmarks (AndroidWorld and AndroidLab) already evalu-ate out-of-domain tasks, we conduct a stricter generalization test by evaluating on applications entirely unseen during training. Specifically, we select five held-out apps from 30 35 40 45 50 55 60 65 70   

> Success Rate (%)
> Ours
> (Full Method)
> w/o Hierarchy
> (Only Workflows)
> w/o Hierarchy
> (Only Subtask Skills)
> w/o Hierarchy
> (Only Failure Patterns)
> w/o Abstraction
> (Raw Experience)
> w/o Self-Evolution
> (Static Memory)
> w/o Stratified Sampling
> (Full guidance)
> w/o Training
> (Inference-time Prompting)
> Vanilla GRPO
> 66.8%
> 59.5%
> 61.6%
> 58.6%
> 58.2%
> 62.9%
> 48.3%
> 52.6%
> 53.0%
> Component Analysis
> Proposed Method Ablations Baselines

Figure 5. Component analysis of UI-Mem. We evaluate the impact of removing different components in our framework. 

AndroidLab that are completely absent from our training set: Bluecoins, Cantook, Maps.me, Pi-Music, and Zoom. Figure 6 compares the success rate of the baseline model, our method applied in a zero-shot manner (without memory retrieval), and our full method with retrieved memory. As shown, our zero-shot model already outperforms or matches the baseline across all five apps, demonstrating that our train-ing method improves generalization even without explicit memory support. More notably, incorporating retrieved memory at inference time provides consistent additional gains, with particularly substantial improvements on Blue-coins and Maps.me. This indicates that experiences learned from training apps can effectively transfer to novel applica-tions through our memory retrieval mechanism. 8UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents Bluecoins Cantook Maps.me Pi-Music Zoom  

> Application
> 0
> 20
> 40
> 60
> 80
> 100
> Success Rate (%)
> 33.3
> 41.7
> 13.3
> 16.7
> 40.0
> 33.3
> 50.0
> 33.3
> 25.0
> 40.0
> 53.3
> 66.7
> 33.3 33.3
> 80.0
> Cross-Task Transfer Performance
> Baseline (Qwen3-VL)
> Ours (Zero-shot)
> Ours (Retrieve Memory)

Figure 6. Cross-task generalization performance on five held-out applications unseen during training. 0 20 40 60 80     

> Training Steps
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Success Rate
> (a) Training Success Rate
> UI-Mem (Ours)
> Standard GRPO
> 020 40 60 80
> Training Steps
> 0.05
> 0.00
> 0.05
> 0.10
> 0.15
> 0.20
> 0.25
> Reward Variance
> (b) Intra-Group Reward Variance
> UI-Mem (Ours)
> Standard GRPO

Figure 7. Training dynamics comparison between UI-Mem and standard GRPO. (a) Success rate over training steps. (b) Mean intra-group reward variance, which directly impacts the effectiveness of advantage estimation. 

Impact of Memory-Guided Exploration. To validate the effectiveness of our framework, we analyze the training dynamics in Figure 7. As shown in Figure 7(a), standard GRPO exhibits highly unstable training dynamics, with suc-cess rate fluctuating dramatically throughout the training process without showing a clear upward trend. In contrast, UI-Mem achieves stable improvement and faster conver-gence. This performance gap is mechanistically explained by Figure 7(b), which plots the intra-group reward vari-ance—a critical factor for advantage estimation in GRPO. The baseline frequently collapses to near-zero when sam-pled groups consist entirely of failures, resulting in vanish-ing gradient. The baseline exhibits negligible variance (near zero) as most sampled groups consist entirely of failures, resulting in vanishing gradients. Conversely, our Stratified Group Sampling ensures a mixture of successful (guided) and exploratory trajectories within each group, maintaining a healthy variance level. This continuous supply of infor-mative gradients facilitates effective policy optimization and progressive internalization of external memory into the agent’s parameters. 

5. Conclusion 

In this work, we introduced UI-Mem, a novel framework de-signed to overcome the fundamental inefficiencies of online Reinforcement Learning in GUI environments. UI-Mem constructs a hierarchical, self-evolving memory that decom-poses raw experiences into reusable workflows, subtask skills, and failure patterns. We utilized this memory through a stratified group sampling mechanism tailored for GRPO, which balances memory-guided exploitation with necessary exploration to facilitate effective advantage estimation. We also introduced a self-evolving loop to continually refine the memory pool to adapt to the current policy. Experiments on challenging GUI benchmarks demonstrate that UI-Mem sig-nificantly outperforms baselines in both sample efficiency and success rate, with strong cross-task generalization en-abled by reusable experience transfer. 

Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning, specifically focusing on the efficiency and generalization of autonomous GUI agents. This work can assist users with disabilities or limited technical profi-ciency in navigating complex mobile interfaces with natural language commands. However, we acknowledge several po-tential risks associated with this research. The deployment of Online RL agents involves direct interaction with live environments. During the exploration phase, an agent might perform unintended actions (e.g., deleting data or financial transactions), necessitating the development of robust safe exploration protocols before real-world deployment. More-over, while our memory abstraction mechanism replaces sensitive text with placeholders like {{ password }} , pro-cessing screenshots with personal data still raises privacy concerns that demand strict handling procedures. 

References 

Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. Bai, H., Zhou, Y., Pan, J., Cemri, M., Suhr, A., Levine, S., and Kumar, A. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. 

Advances in Neural Information Processing Systems , 37: 12461–12495, 2024. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A frontier large vision-language model with versatile abilities. ArXiv ,abs/2308.12966, 2023. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, 9UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. 

arXiv preprint arXiv:2511.21631 , 2025a. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 ,2025b. Chai, Y., Huang, S., Niu, Y., Xiao, H., Liu, L., Zhang, D., Gao, P., Ren, S., and Li, H. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490 , 2024. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information pro-cessing systems , 30, 2017. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 , 2025. Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: Towards a general-ist agent for the web. Advances in Neural Information Processing Systems , 36:28091–28114, 2023. Feng, L., Xue, Z., Liu, T., and An, B. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978 , 2025. Gu, Z., Zeng, Z., Xu, Z., Zhou, X., Shen, S., Liu, Y., Zhou, B., Meng, C., Xia, T., Chen, W., et al. Ui-venus techni-cal report: Building high-performance ui agents with rft. 

arXiv preprint arXiv:2508.10833 , 2025. Guo, D., Wu, F., Zhu, F., Leng, F., Shi, G., Chen, H., Fan, H., Wang, J., Jiang, J., Wang, J., et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062 , 2025. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 , 2024. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485 , 2023. Liu, Y., Li, P., Xie, C., Hu, X., Han, X., Zhang, S., Yang, H., and Wu, F. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239 , 2025a. Liu, Z., Xie, J., Ding, Z., Li, Z., Yang, B., Wu, Z., Wang, X., Sun, Q., Liu, S., Wang, W., et al. Scalecua: Scaling open-source computer use agents with cross-platform data. arXiv preprint arXiv:2509.15221 , 2025b. Lu, F., Zhong, Z., Liu, S., Fu, C.-W., and Jia, J. Arpo: End-to-end policy optimization for gui agents with experience replay. arXiv preprint arXiv:2505.16282 , 2025a. Lu, Z., Chai, Y., Guo, Y., Yin, X., Liu, L., Wang, H., Xiong, G., and Li, H. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620 , 2025b. OpenAI. Chatgpt. https://chat.openai.com ,2023a. OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023b. Qin, Y., Ye, Y., Fang, J., Wang, H., Liang, S., Tian, S., Zhang, J., Li, J., Li, Y., Huang, S., et al. Ui-tars: Pioneer-ing automated gui interaction with native agents. arXiv preprint arXiv:2501.12326 , 2025. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimiza-tion: Your language model is secretly a reward model. 

Advances in neural information processing systems , 36: 53728–53741, 2023. Rawles, C., Clinckemaillie, S., Chang, Y., Waltz, J., Lau, G., Fair, M., Li, A., Bishop, W., Li, W., Campbell-Ajala, F., et al. Androidworld: A dynamic benchmark-ing environment for autonomous agents. arXiv preprint arXiv:2405.14573 , 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. 

arXiv preprint arXiv:1707.06347 , 2017. Seed, B. Seed1. 8 model card: Towards generalized real-world agency. 2025a. Seed, B. Ui-tars-1.5. https://seed-tars.com/1.5 ,2025b. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Push-ing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024. Wang, J., Xu, H., Jia, H., Zhang, X., Yan, M., Shen, W., Zhang, J., Huang, F., and Sang, J. Mobile-agent-v2: Mobile device operation assistant with effective nav-igation via multi-agent collaboration. arXiv preprint arXiv:2406.01014 , 2024. 10 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. 

arXiv preprint arXiv:2312.08935 , 2023. Wu, Z., Wu, Z., Xu, F., Wang, Y., Sun, Q., Jia, C., Cheng, K., Ding, Z., Chen, L., Liang, P. P., et al. Os-atlas: A foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218 , 2024. Xiao, H., Wang, G., Chai, Y., Lu, Z., Lin, W., He, H., Fan, L., Bian, L., Hu, R., Liu, L., et al. Ui-genie: A self-improving approach for iteratively boosting mllm-based mobile gui agents. arXiv preprint arXiv:2505.21496 ,2025. Xu, Y., Liu, X., Sun, X., Cheng, S., Yu, H., Lai, H., Zhang, S., Zhang, D., Tang, J., and Dong, Y. Androidlab: Train-ing and systematic benchmarking of android autonomous agents. arXiv preprint arXiv:2410.24024 , 2024a. Xu, Y., Wang, Z., Wang, J., Lu, D., Xie, T., Saha, A., Sahoo, D., Yu, T., and Xiong, C. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454 , 2024b. Xu, Y., Liu, X., Liu, X., Fu, J., Zhang, H., Jing, B., Zhang, S., Wang, Y., Zhao, W., and Dong, Y. Mobilerl: On-line agentic reinforcement learning for mobile gui agents. 

arXiv preprint arXiv:2509.18119 , 2025. Yan, H., Wang, J., Huang, X., Shen, Y., Meng, Z., Fan, Z., Tan, K., Gao, J., Shi, L., Yang, M., et al. Step-gui technical report. arXiv preprint arXiv:2512.15431 , 2025. Yang, Z., Dou, Z.-Y., Feng, D., Huang, F., Nguyen, A., You, K., Attia, O., Yang, Y., Feng, M., Zhang, H., et al. Ferret-ui lite: Lessons from building small on-device gui agents. 

arXiv preprint arXiv:2509.26539 , 2025. Ye, J., Zhang, X., Xu, H., Liu, H., Wang, J., Zhu, Z., Zheng, Z., Gao, F., Cao, J., Lu, Z., et al. Mobile-agent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144 , 2025. Zhang, C., Yang, Z., Liu, J., Li, Y., Han, Y., Chen, X., Huang, Z., Fu, B., and Yu, G. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Sys-tems , pp. 1–20, 2025a. Zhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie, P., Yang, A., Liu, D., Lin, J., Huang, F., and Zhou, J. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176 , 2025b. Zhou, H., Zhang, X., Tong, P., Zhang, J., Chen, L., Kong, Q., Cai, C., Liu, C., Wang, Y., Zhou, J., et al. Mai-ui technical report: Real-world centric foundation gui agents. arXiv preprint arXiv:2512.22047 , 2025. 11 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

Appendix A. Preliminaries 

GUI Task Formulation. We formalize the interaction between the MLLM agent and the Graphical User Interface (GUI) as a finite-horizon Markov Decision Process (MDP), represented by the tuple M = ⟨S , A, P, R, H ⟩. The state space S

captures the multi-modal context; a state st ∈ S comprises the pixel-level screenshot It and the high-level user instruction q.The action space A consists of atomic GUI operations (e.g., Click , Swipe , Type , Home ). At timestep t, the agent policy 

πθ (at|st, q ) generates an action at, transitioning the environment to st+1 according to the system dynamics P(st+1 |st, a t).Reflecting the sparse nature of real-world GUI tasks, the reward function is defined as R(τ ) = routcome ∈ { 0, 1}, provided only at the termination of the trajectory τ = ( s0, a 0, . . . , s T , a T ), where T ≤ H.

Group Relative Policy Optimization (GRPO). We adopt GRPO as our base optimization algorithm, as it eliminates the need for a separate critic model. GRPO samples a group of G trajectories {τi}Gi=1 for a task instruction q from the current policy πθold . The advantage Ai for each trajectory is computed by normalizing rewards within the group: 

Ai = R(τi) − mean ({R (τj )}Gj=1 )

std ({R (τj )}Gj=1 ) + ϵ (6) where ϵ is a small constant for numerical stability. The policy is updated by maximizing the following objective: 

JGRP O (θ) = Ec∼D ,{τi}∼ πθold 

"

1

G

> G

X

> i=1

min 

 πθ (τi)

πθold (τi) Ai,

clip 

 πθ (τi)

πθold (τi) , 1 − δ, 1 + δ



Ai



− βDKL (πθ || πref )

# (7) where δ is the clipping hyperparameter and β controls the KL-divergence penalty from the reference model πref to prevent policy collapse. 

B. Example of Hierarchical Memory Structure 

We provide a detailed example of our hierarchical memory representation structure in Figure 8. Beyond standard task instructions, we introduce an essential states template , which defines the key intermediate states that characterize the valid completion of the overall task, thereby facilitating our reward evaluation based on rule-based state verification. 

C. Reward Model Details 

Reward Model Design. We identify a significant limitation in previous MLLM-as-a-Judge approaches: directly feeding the entire history of screenshots often leads to severe hallucination issues. To address this, we propose a text-based verification approach. Our method operates in two stages. First, we utilize Qwen2.5-VL-72B-Instruct to generate textual descriptions for each screen state and the corresponding executed actions in the trajectory. As illustrated in Figure 9, by analyzing paired screenshots captured before and after an action, the model provides precise descriptions of the UI elements relevant to the task instruction, as well as the specific operation performed. These descriptions are then concatenated to form a textual history of the agent’s actual actions. After that, we feed this textual history into an open-source LLM, DeepSeek-V3, which is prompted to identify which essential states are completed based on the UI descriptions and action descriptions. This effectively converts the previous visual-based evaluation into a robust rule-based verification process, which we have found to be significantly more accurate than direct MLLM evaluation. Based on the model response, we calculate a precise progress reward Rprogress = |S completed |/|S total |, offering a dense signal for optimization. By grounding reasoning in verified state changes rather than raw pixels, this approach significantly reduces the rate of hallucination common in pure MLLM-based evaluators. We provide our prompt for the LLM verification model in Figure 10. 12 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents                                                                               

> [{"task_id": "AutoGenerated_Task_75", "package_name": ["net.gsantner.markor"], "task_template": {"content": "Rename file {{current_filename}} to {{new_filename}} in Markor.", "parameter_config": {"fixed_parameters": {"app_package": "net.gsantner.markor", "rename_icon_name": "A", "confirm_button_name": "OK" }, "variable_parameters": ["current_filename", "new_filename"] }}, "essential_states_template": {"S1": { "content": "File ’{{current_filename}}’ is selected with context options visible.", "variable_mapping": {"current_filename": "current_filename" }}, "S2": { "content": "Rename dialog is active." }, "S3": { "content": "File is renamed to ’{{new_filename}}’.", "variable_mapping": {"new_filename": "new_filename" }}}, "subtask_template": {"T1": { "subtask_label": "Select Note via Long Press", "content": "Long press the file named ’{{current_filename}}’." }, "T2": { "subtask_label": "Tap Rename Icon", "content": "Tap the ’{{icon_name}}’ icon to open rename options." }, "T3": { "subtask_label": "Enter Text and Confirm", "content": "Enter ’{{new_filename}}’ and tap ’{{button_name}}’ to confirm." }}}]

Figure 8. Example of Hierarchical Data Structure. The essential states template defines the critical milestones of the task, providing a mechanism for state-based verification rather than directly judging subtask or task completion. 

Reward Model Evaluation. We quantitatively assess the accuracy of our reward computation method and compare it against previous MLLM-as-a-Judge methods. Table 4 reports the prediction accuracy, Precision, Recall, and F1 Score across different backbone models. The baseline MLLM-based approaches often struggle with visual hallucinations, resulting in suboptimal F1 scores (e.g., 0.837 for Gemini 2.5 Pro). In contrast, our proposed method in UI-Mem framework, which employs a two-stage text-based verification pipeline, consistently achieves superior performance. Notably, the Qwen2.5-VL-72B-Instruct+DeepSeek-V3 settings reaches an F1 score of 0.902, significantly outperforming the MLLM-as-a-Judge method of using closed-source API model Gemini 2.5 Pro. This demonstrates that decoupling visual grounding from logical verification effectively mitigates hallucinations, providing a robust and reliable reward signal for reinforcement learning. We adopt this settings for our reward evaluation during online training. 

D. Experience Extraction Details 

We employ an experience extraction model, specifically Seed1.8, due to its strong reasoning abilities, to derive both success plans and failure patterns from newly generated trajectories. Based on the evaluation results provided by our reward model, this extraction process is divided into two distinct phases: Subtask-Level Experience Extraction and High-level Workflow Extraction. 

Subtask-Level Experience Extraction. The model first generates experience for each subtask by analyzing the trajectory. For each completed subtask, the model abstracts detailed success plans. A critical constraint imposed is Entity Preservation :specific details (e.g., “call 1234”) are preserved to ensure subsequent experience abstraction can accurately map them to template variables. For failed subtasks (derived from the first uncompleted subtask id), the model gives analysis about the 13 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

(a) Text Input Operation  (b) Button Click Operation 

Figure 9. Textual Description Examples of UI states and executed actions. By comparing Before Action and After Action screens, the MLLM provides (1) ui description , capturing the visual state and element visibility relevant to the current task, and (2) 

action description , specifying the actual operation executed (e.g., typing text, clicking buttons). (a) shows a Text Input scenario where the user types a phone number, capturing the input field state change. (b) illustrates a Button Click operation (saving a contact), identifying the interactable element. These structured descriptions serve as grounded inputs for the Reward Computation. 

Table 4. Reward Model Accuracy Comparison. UI-Mem Text-based Verification based on open-source models (Qwen2.5-VL-72B-Instruct + DeepSeek-V3) achieves competitive performance compared with strong closed-source API models across all metrics. 

Model Accuracy Precision Recall F1 Score 

MLLM-as-a-Judge Gemini 2.5 Pro 0.776 0.845 0.908 0.837 Seed1.5-VL 0.639 0.879 0.770 0.775 Qwen2.5-VL-72B-Instruct 0.724 0.841 0.845 0.811 UI-Mem (Text-based) Gemini 2.5 Pro + DeepSeek-V3 0.922 0.912 0.967 0.916 

Seed1.5-VL + DeepSeek-V3 0.880 0.907 0.921 0.887 Qwen2.5-VL-72B-Instruct + DeepSeek-V3 0.900 0.909 0.943 0.902 

error root cause and targeted correction guidelines. We provide the used prompt in Figure 11. 

High-level Workflow Extraction. For successful trajectories, we additionally extract the high-level workflow which captures global planning. This process maps the raw agent trajectory to a sequence of subtasks. We provide the used prompt in Figure 12. 

E. Experience Abstraction Details 

Experience Parameterization. To transition from task-specific experience to generalizable knowledge, we implement an Experience Parameterization module using DeepSeek-V3 . This module transforms obtained concrete experiences into abstracted experiences containing variables, enabling the transfer of learned strategies to new tasks. The input prompt is presented in Figure 13. 

Experience Ranking Mechanism. To facilitate the selection of the most effective abstract knowledge, we implement a scoring strategy. For workflows or subtask skills, we employ an Upper Confidence Bound (UCB) algorithm: Score = 

> Nsuccess
> Ntotal

+ c

q ln Nglobal  

> Nused +1

. This balances the exploitation of high-success strategies with the exploration of newer, less-tested plans. For Failure Diagnoses , we employ a time-decay function R(t) = 11+ λ∆t derived from its last updated time. This ensures the agent prioritizes recent error, which is essential for adapting to dynamic UI updates and policy evolving. 

F. The Memory Retrieval Algorithm 

As presented in Algorithm 1 , our memory retrieval procedure consists of serveral hierarchical phases: 14 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

Algorithm 1 Hierarchical Experience Retrieval & Instantiation 

Require: Current instruction I, Task Meta-info Ttask , Subtask Meta-info Tsub , High-level Memory Mhigh , Mid-level Memory Mmid , Embedding Model E(·).

Ensure: Structured Guidance G (Plan + Tips/Warnings). 

PHASE 1: T ASK MATCHING & V ARIABLE EXTRACTION 

1: eI ← E(I)

2: Retrieve top-k candidates C ⊂ T task via C OSINE SIM (eI , E (τ )) for τ ∈ T task 

3: match, τ ∗, V ← LLM D ECIDE MATCH (I, C) ▷ Return matched template τ ∗ & variables V

4: if not match then 

5: τ ∗ ← arg max τ ∈C SCORE (τ ) ▷ Fallback to best analogy 6: V ← EXTRACT VARS (I, τ ∗)

7: end if PHASE 2: W ORKFLOW RETRIEVAL 

8: Praw ← GET BEST PLAN (Mhigh , τ ∗.id ) ▷ Retrieve execution plan with highest success rate 9: Pf illed ← ∅ 

10: for each subtask step sraw ∈ Praw do 

11: sf illed ← INSTANTIATE (sraw , V ) ▷ Fill placeholders (e.g., {{ app }} → “Gmail”) 12: Pf illed .append (sf illed )

13: end for PHASE 3: S UBTASK -L EVEL EXPERIENCE RETRIEVAL 

14: G ← InitializeGuidance (Pf illed )

15: for each subtask s ∈ Pf illed do 

16: if s.label ∈ M mid then 

17: E ← M mid [s.label ]

18: else 

19: E ← best match via C OS SIM (E(s.content ), Mmid .keys )

20: end if 

21: Etips ← UCB R ANK (E. plan summary ) ▷ Prioritize using success rate + exploration 22: Ewarn ← TIME DECAY RANK (E. failure diagnosis ) ▷ Prioritize recent failures 23: G. append ({s, E tips , E warn })

24: end for 

25: return G

Task Matching and Variable Extraction. We first encode the raw instruction into semantic embeddings, using an embedding model Qwen3-Embedding-8B (Zhang et al., 2025b). We use the embedding model to select top-k similar task templates, which are further processed by an LLM (DeepSeek-V3) to determine the best match template and extract the task-specific variables V.

High-Level Workflow Retrieval and Instantiation. Upon identifying the template, the system retrieves a corresponding abstract workflow from High-level Memory. Since the raw experience is parameterized, the system instantiates it by injecting the variable set V into predefined placeholders, transforming the workflow into a context-specific, executable plan. 

Mid-Level Experience Retrieval. After that, the instantiated plan is enriched with subtask-level guidance and failure correction guidelines. For each pending subtask, the retrieval module queries the memory pool using semantic similarity, similar to process of high-level task templates match. This step aggregates historical success plans and failure diagnoses into the final structured guidance. 

G. The Memory Update Algorithm 

The memory update mechanism (Figure 4) is the core of our self-evolving loop, deriving structured, long-term knowledge from raw trajectories. As demonstrated in Algorithm 2 , this process ensures that the agent learns effectively from past successful and failed trajectories. The procedure consists of the following phases: 

Global Statistic Update. The system updates task difficulty metrics via an Exponential Moving Average (EMA) of task success rate. This mechanism dynamically tracks the agent’s competence for each task. 

Mid-Level Experience Abstraction and De-duplication. The system abstracts subtask feedback by replacing specific parameters with generic placeholders, followed by semantic deduplication. This prevents overfitting to instance-specific values (e.g., filenames) and minimizes the memory redundancy, ensuring a compact and high-quality knowledge base. 15 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

Algorithm 2 Self-Evolving Experience Abstraction & Update 

Require: Instruction I, Trace T = {(Si, bindings i)}, Feedback F (Success/Correction), Global Rate SR .1: Input: Executed trace with instantiated templates and variable bindings. 

PHASE 1: G LOBAL STATISTIC UPDATE (EMA) 

2: SR (I) ← γ · SR (I) + (1 − γ) · Score (T ) ▷ Update moving avg success rate 

PHASE 2: M ID -L EVEL MEMORY UPDATE (S UBTASK EXPERIENCE )

3: for each subtask Si ∈ T containing feedback in F do 

4: key ← (Si.pkg , S i.label )

5: exp raw ← F .get content (Si) ▷ Get summary if success, diagnosis if fail 6: exp abs ← LLM (exp raw , S i.bindings ) ▷ Parameterize: replace values with slots 7: T arget ← M mid [key ].select list (Si.status ) ▷ Select PlanSummary or FailureDiagnosis list 8: sim ← max e∈T arget COS SIM (exp abs , e ) ▷ Check for semantic redundancy 9: if sim ≥ δdup then 

10: Update counts ( Nsuccess , N used ) ▷ Update metrics for UCB/Decay 11: if new experience is more detailed then 

12: Update content ▷ Keep higher quality experience 13: end if 

14: else 

15: T arget. append (exp abs ) ▷ Register novel subtask experience 16: end if 

17: end for PHASE 3: H IGH -L EVEL MEMORY UPDATE (T ASK PLANNING )

18: if Task is Success then 

19: Seq ← [S1.id, . . . , S T .id ] ▷ Extract abstract execution flow 20: P ← M high [I].plans 21: if Seq ∈ P then ▷ Exact workflow matching 22: Update success count & avg steps for Seq 

23: Update rationale if new one is better 24: else 

25: P.add ({Seq, count : 1 }) ▷ Record new successful workflow 26: end if 

27: Sort P by UCB algorithm. 28: end if 

High-Level Workflow Consolidation. Successful trajectories are compressed into workflows and merged into the high-level workflow library. The statistical information like success counts are also updated, ensuring that the system effectively prioritizes robust workflows and discards unstable strategies over time. 

H. Qualitative Examples 

To qualitatively evaluate the effectiveness of the proposed UI-Mem framework, we present visualized examples illustrating successful long-horizon planning, error correction via failure diagnosis, and remaining challenges in visual grounding. 

Impact of Memory Guidance. We demonstrates how different strengths of guidance contribute to the progress of task completion. We provide visualizations of the agent’s rollout trajectories for the specific task instruction “Create a new contact for Chen Yu, with the organization listed as Xiaomi.” As shown in the top row of Figure 14, full memory guidance leads to High Task Progress (Reward 1.0) , where the agent executes a flawless trajectory, mapping “Chen Yu” to the Name field and “Xiaomi” to the Company field before saving. In contrast, the middle row illustrates Partial Completion (Reward 0.6) under weak guidance (only workflow). Here, the agent exhibits incomplete task execution: while it correctly inputs the organization “Xiaomi,” it fails to input the name “Chen Yu” and deviates into irrelevant actions (e.g., adding a photo), resulting in a contact saved with missing information. Finally, without any prior guidance, the agent fails to achieve any progress in completing the task, repeatedly toggling between the search bar and the empty list view. This comparison validates our design that injecting different levels of guidance increases the outcome diversity within a rollout group given the same task instruction. 

Error Correction via Diagnosis. We demonstrate the effectiveness of integrating failure pattern into our memory using another Audio Recorder task: “Enter the list and sort the files by name (A-Z).” Initially, as shown in the top of Figure 15 (First Rollout), the agent fails by misinterpreting the welcome screen’s navigation, erroneously exiting the app to an unrelated 16 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents 

“Files” application. After this rollout, our memory incorporates the Failure Diagnosis identifying the root cause: “The user opened the Audio Recorder but navigated back... instead of proceeding to the main recording list.” Equipped with this insight, during the second attempt ( w/ Failure Diagnosis ), the agent retrieves a Correction Guideline instructing it to “Tap the ‘Get started’ button... avoid switching to other apps.” Given this guidance, the agent correctly enters the main list view and successfully performs the sorting operation. 

Plan-Execution Gap in Failure Cases. Despite the overall effectiveness of our memory guidance, there are remaining failure modes due to unexpected grounding errors. In Figure 16, the agent is required to complete the task “Create a new incognito tab and visit www.baidu.com.” The agent retrieves a fine-grained guidance to handle the distinctive “Welcome” popup and navigates the Chrome menu to select “New Incognito tab.” However, the rollout fails at the final step due to a visual grounding error. Although the planner intends to visit the URL, the agent fails to correctly select the address bar (Omnibox) to type “www.baidu.com.” 17 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents You are an expert in evaluating mobile UI automation tasks. Your goal is to determine if a user's task was successfully completed based on a trajectory of actions and a list of expected essential states. 

## Evaluation Guidelines 

1.  Primary Goal - Task Success : Your most important judgment is whether the final goal described in  Task Description  was achieved.  If the final outcome in the trajectory shows the task is done, is_task_completed MUST be true, even if some intermediate essential states were skipped or not explicitly captured. 

2.  Essential States Analysis : Identify which  essential_states  were visited. Note that a user might take shortcuts or the trajectory might miss capturing a specific intermediate state. This is acceptable as long as the final result is correct. 

3.  Evidence Based : Base your decision on the provided UI description and action summary. 

4.  Output Logic :

- If the task goal is achieved  is_task_completed: true (regardless of whether all essential states list is full). 

- If the task goal is NOT achieved  is_task_completed: false. 

System instruction 

Task Description: {task_description} 

The expected essential states (standard path) are: {essential_states}. 

Total Steps in Full Trajectory: {total_steps} 

Here is a summary of all the {total_steps} actions taken: 

{formatted_steps_str} 

Your output MUST be a strict JSON object containing the following keys: 

- completed_essential_state: A list of keys representing the essential states that  were actually observed  in the trajectory. 

- is_task_completed: A boolean (true/false). Set this to true if the Task Description implies the user's goal is met at the end of the trajectory, even if the list of completed states is incomplete. 

- reasoning: Explain your judgment. If is_task_completed is true but some states are missing, explain that the goal was achieved via an alternative path or the state was skipped. 

### Example Output format 

```json 

{{ 

"completed_essential_state": ["S1", "S5"], 

"is_task_completed": true, 

"reasoning": "Although intermediate states S2-S4 were not explicitly observed, the final screenshot shows the settings were successfully changed as requested, so the task is completed." 

}} 

``` 

User prompt  

> Figure 10. Prompt design for the LLM verification Model.

18 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents You are an expert in analyzing mobile UI automation agents. 

Your goal is to extract 'Rules of Experience' based on an execution trajectory. 

You will be given the results of a trajectory analysis, indicating which subtasks succeeded and which failed. 

System instruction 

### Task Context 

Goal: '{task_description}' 

Subtasks: {all_subtasks} 

### Trajectory Summary 

{formatted_steps_str} 

### Analysis Results (Pre-computed) 

Completed subtasks: {completed_subtasks} 

Failed subtasks: {failed_subtasks} 

### Your Task 

Generate 'Experience' for the successful subtasks and the failed subtasks. 

1.  Success Experience : For every completed subtask, extract a clear, step-by-step execution plan. 

- CRITICAL : If the step involves interacting with a specific name, number, or text found in the 'Goal', you MUST include that exact value in your summary (e.g., 'Click on contact Alice' instead of 'Click on the contact'). 

2.  Failure Analysis : For the *first* failed subtask encountered (the bottleneck), explain WHY it failed and HOW to avoid it. 

### Output Format 

Return a JSON object: 

```json 

{

"success_experiences": [ 

{ "subtask_id": "T1", "plan_summary": "Locate the conversation entry corresponding to '+123456' on the main screen and tap it..." }, 

{ "subtask_id": "T2", "plan_summary": "Type 'Hello World' into the text box..." } 

], 

"failure_diagnosis": { 

"first_failed_subtask_id": "S3", 

"root_cause": "...", 

"correction_guideline": "..." 

}

}

``` 

User prompt     

> Figure 11. Prompt design for Experience Extraction for subtask-level skills and failure analysis. This module identifies the critical
> success experiences for completing a subtask or analyzes the first failure subtask id to generate targeted error diag-nosis.

19 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents You are an expert GUI agent analyst. Your goal is to reverse-engineer the 

High-Level Execution Plan from a successful agent trajectory. 

## The Concept 

An 'Execution Plan' is strictly a sequence of Essential State IDs (e.g., ['S1', 'S3', 'S2']) 

that represents the logical order in which key milestones were achieved. 

## Inputs provided to you 

1.  Task Instruction : What the agent was trying to do. 

2.  Essential State Definitions : A dictionary mapping IDs (S1, S2...) to state descriptions. 

3.  Agent Trajectory : The actual actions taken. 

4.  Verified States : Which states the Reward Model confirms were triggered. 

## Your Task 

Analyze the connection between the Trajectory and the Essential State Definitions. 

Determine the exact order in which the Essential States were achieved. 

## Guidelines 

- Strict Vocabulary : You must ONLY use the provided State IDs (S1, S2, etc.). Do not invent new steps. 

- Order Matters : The list must reflect the chronological order of achievement. 

- Noise Filtering : If the agent performed useless actions between S1 and S2, ignore them. Only capture the state transitions. 

- Missing States : If a state was defined in the template but NOT achieved in the trajectory, do NOT include it. 

- Rationale : Provide a brief explanation of why this order was chosen (e.g., 'S3 had to be done before S2 because...'). 

## Output Format 

Respond with a JSON object: 

```json 

{

"execution_plan": ["S1", "S3", "S2"], 

"rationale": "The agent first opened the dialog (S1), then immediately solved the puzzle (S3) within the dialog, and finally confirmed the app selection (S2). 

}

``` 

System instruction 

## Task Instruction 

'{instruction}' 

## Essential State Definitions (Vocabulary) 

{states_desc_str} 

## Verified States (from Reward Model) 

{completed_states_sequence} 

## Agent Trajectory 

{history_block} 

## Perform Analysis 

Based on the trajectory and verified states, reconstruct the sequence of Essential States. 

User prompt 

Figure 12. Prompt design for High-Level Workflow Extraction. The system extracts a concrete execution plan from successful traces. 

20 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents You represent a system that generalizes specific automation rules into template rules. 

Your task is to replace specific values with their corresponding variable placeholders based on the provided bindings. 

You must output valid JSON. 

System instruction 

### Input Data 

1.  Specific Plan : "{extracted_plan_summary}" 

2.  Variable Bindings : {json.dumps(slot_bindings)} 

### Instruction 

Rewrite the 'Specific Plan' by replacing any occurrence of the values found in 'Variable Bindings' with their dictionary keys wrapped in double curly braces (e.g., {{{{key}}}}). 

Do not change the structure of the sentence, only swap the values for variables. 

### Output Format 

Return a single JSON object with the key \"generalized_plan\". Do not include markdown formatting (like ```json). 

### Example 

Input Plan : "Search for 'Starbucks' in the map bar and select the one on '5th Avenue'. 

Bindings : {{ "search_term": "Starbucks", "target_street": "5th Avenue" }} 

Output : {{ "generalized_plan": "Search for {{{{search_term}}}} in the map bar and select the one on {{{{target_street}}}}." }} 

### Your Output 

User prompt  

> Figure 13. Prompt design for Experience Abstraction. This prompt aims to map concrete entities (e.g., specific cities) to template slots without modifying the original semantic logic.

21 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents Reward:  1.0 

Reward:  0.6 

Reward:  0

> open_app [settings]
> Type text: Chen Yu
> Type text: Xiaomi
> open_app [settings]
> Type text: Xiaomi
> open_app [settings]

1.Open Contacts, tap 'Add new contact'.  (Guidance: 

Tip: 1. Open app. 2. Click orange icon if shown. 3. 

Tap the floating '+' button. Avoid: Don't use the 

'Select contacts' add button. Use 'Add your first 

contact' if list is empty.) 

2.Enter 'Chen Yu' (Name) and 'Xiaomi' (Company). 

(Guidance: Tip: 1. Type 'Chen Yu' in First Name field. 

2. Repeat for Last Name field. 3. Type 'Xiaomi' in 

Company field. Avoid: Check for exact values in 

correct fields.) 

3.Tap 'Save'. 

Strong 

Guidance 

1.Open Contacts and tap 'Add new contact’. 

2.Enter 'Chen Yu' as name and 'Xiaomi' as company. 

3.Tap 'Save' to finish. 

Weak 

Guidance 

No 

Guidance 

Livelock 

Figure 14. Trajectory analysis on the task “Create a new contact for Chen Yu, with the organization listed as Xiaomi.” Top (Reward 1.0): Full memory guidance yields a perfect execution. Middle (Reward 0.6): Incomplete execution where the agent correctly fills the organization but misses the name. Bottom (Reward 0): Lack of memory results in a total failure. First Rollout 

w/ Failure Diagnosis 

When encountering the Audio Recorder app's 

welcome screen with the blue 'Get started' 

button, tap the 'Get started' button to enter the 

main recording list instead of navigating back. 

Avoid switching to other apps like Files, as the 

task requires interacting with the Audio 

Recorder's own file list. 

Correction 

Guideline 

The user opened the Audio Recorder app but 

navigated back from its welcome screen instead 

of proceeding to the main recording list, then 

switched to the unrelated Files app, failing to 

complete the required navigation to the Audio 

Recorder's main recording list. 

Root 

Cause    

> open_app [Audio Recoder ]Navigate Back
> open_app [Audio Recoder ]

Figure 15. Visualizing the Failure Diagnosis mechanism on the task “Enter the file list and sort the files by name (A-Z).” The system identifies the navigation error in the first rollout and generates a specific Correction Guideline , enabling success in the second-round attempt. 

22 UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents w/ Guidance  

> open_app [Chrome] Wait for 2 seconds
> Type text: www.baidu.com

1.Handle setup: 'No thanks' or 'Use without an 

account'.  (Guidance: Tip: 1. Click 'Accept & 

continue'. 2. Click 'No thanks' on Sync prompt. 

Avoid: Priority is 'Use without an account' if visible. 

Ensure setup completes.) 

2.Tap 'three -dot menu' (top right).  (Guidance: Tip: 

On home screen, locate and click menu icon. Avoid: 

Confirm main interface is loaded before tapping.) 

3.Select 'New Incognito tab'.  (Guidance: Tip: Select 

this option from the expanded menu list.) 

4.Enter 'www.baidu.com' and Go.  (Guidance: Avoid: 

1. Press Enter or 'Go' icon after typing. 2. Verify 

internet connection if navigation fails.) 

Strong 

Guidance 

Figure 16. A failure case in the Browser task. Despite fine-grained guidance (opening Incognito), the agent fails to visit the URL due to a specific visual grounding error during the final action step. 

23