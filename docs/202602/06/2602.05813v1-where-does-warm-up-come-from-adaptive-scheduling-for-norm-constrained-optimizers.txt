Title: Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers

URL Source: https://arxiv.org/pdf/2602.05813v1

Published Time: Fri, 06 Feb 2026 02:31:48 GMT

Number of Pages: 26

Markdown Content:
## BRAIn Lab 

# Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers 

Artem Riabinin 1, Andrey Veprikov 1, 2 , Arman Bolatov 3, Martin Tak´ aˇ c3, Aleksandr Beznosikov 1, 2, 4 

> 1

Basic Research of Artificial Intelligence Laboratory (BRAIn Lab) 

> 2

Federated Learning Problems Laboratory 

> 3

Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) 

> 4

Innopolis University We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically. Building on this theory, we develop a practical learning rate scheduler that relies only on standard hy-perparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at 

https://github.com/brain-lab-research/llm-baselines/tree/warmup .

1 Introduction 

In this paper, we consider the problem of training large language models (LLMs), which can be formulated as the following optimization problem: 

f ⋆ := min  

> x∈X

f (x), (1) where f : X → R denotes the loss of the model x with a parameter space X := {(W1, . . . , W L) | Wi ∈ Rmi×ni }

representing the collection of L model’s layers. Nowadays, classical method is to solve (1) using norm-constrained optimizers, where the update direction is given by a Linear Minimization Oracle (LMO) over a unit ball. This framework has emerged as a powerful family of methods for training deep networks, with recent successes including Kimi K2 [Team et al., 2025] and Moonlight [Liu et al., 2025a]. It unifies several modern optimizers, including normSGD [Hazan et al., 2015a], signSGD [Bernstein et al., 2018], Lion [Chen et al., 2023], and Muon [Jordan et al., 2024]. Specifically, the LMO-based update rule is: 

xt+1 = xt + ηt LMO( gt), LMO( gt) := arg min q∈X :∥q∥=1 ⟨gt, q ⟩, (2) where t is the optimization step, ηt > 0 is the learning rate, gt is a gradient approximation (e.g., momentum), and ∥ · ∥ refers to an arbitrary, possibly non-Euclidean norm. This formulation arises naturally from minimizing a quadratic approximation of the loss function around the point xt:

f (xt + ∆ xt) ≈ f (xt) + ⟨∇ f (xt), ∆xt⟩ + λ

2 ∥∆xt∥2. (3) The update ∆xt = xt+1 − xt from (2) is the arg min of (3) with respect to ∆xt ∈ X up to multiplication factors. Different choices of norms ∥ · ∥ yield different optimizers: the Euclidean norm recovers normSGD, the ℓ1 norm gives signSGD, and the spectral norm leads to Muon. For a detailed derivation of the resulting updates see Appendix C. The success of LMO-based methods depends not only on the appropriate choice of the norm in (2) , but also on the proper selection of the learning rate ηt [Goyal et al., 2017]. In practice, empirically designed schedules are commonly used, such as linear warm-up followed by cosine decay [Loshchilov and Hutter, 2017]. In this work, we focus on the 1

> arXiv:2602.05813v1 [cs.LG] 5 Feb 2026

warm-up phase: starting with small learning rates and gradually increasing them before decay. Although warm-up has become nearly ubiquitous in practice [Goyal et al., 2017; Vaswani et al., 2017; Loshchilov and Hutter, 2017], its theoretical necessity has not been fully understood. Therefore, in this paper we address the following research questions: 

(i) Can learning rate warm-up be theoretically justified for LMO-based optimizers, rather than being treated as a purely empirical heuristic? (ii) Can the warm-up duration be determined adaptively during training, eliminating the need for manual tuning? 

Guided by this research questions, we make the following contributions: 

• We introduce a new generalized smoothness assumption where local curvature decreases with the suboptimality gap, and empirically verify that this behavior holds along optimization trajectories. 

• We provide a theoretical analysis establishing convergence guarantees for LMO-based optimizers under this assumption, where warm-up followed by decay emerges naturally from the proof rather than being imposed heuristically. 

• Based on the theory, we develop a practical learning rate scheduler with adaptive warm-up that relies only on standard hyperparameters and automatically determines the warm-up duration at the beginning of training. 

• We validate our approach on language model pretraining with LLaMA architectures, showing that the proposed adaptive warm-up matches or outperforms hand-tuned schedules across Muon, Lion, and normalized SGD without hyperparameter search. 

2 Related Work 

Norm-constrained optimizers. Norm-constrained optimizers have recently attracted significant attention in deep learning. One of the most prominent examples of such optimizers is Muon [Jordan et al., 2024], which demonstrates strong performance for training deep neural networks [Liu et al., 2025a]. Numerous studies have developed practical variants of Muon and related LMO-based algorithms for large-scale models, analyzing their empirical behavior under spectral or orthogonality constraints [Pethick et al., 2025; Riabinin et al., 2025; Amsel et al., 2025; Liu et al., 2025b; Huang et al., 2025; Kovalev, 2025; He et al., 2025]. 

Adaptive and parameter-free optimizers. A related line of work focuses on designing optimizers that require minimal or no hyperparameter tuning. In this domain, Adam [Kingma and Ba, 2015] and its variant AdamW [Loshchilov and Hutter, 2019] are adaptive coordinate-wise optimizers that have long been considered the default choice in deep learning. More recent advancements include D-Adaptation [Defazio and Mishchenko, 2023], which automatically estimates the distance to the solution to set learning rates, and Prodigy [Mishchenko and Defazio, 2024], which improves on this with tighter distance estimates and faster adaptation. Furthermore, the recently introduced Schedule-Free methods [Defazio et al., 2024] eliminate the need for learning rate schedules entirely by maintaining iterate averages that converge without explicit decay. However, despite these advances, all discussed methods, even the Schedule-Free approaches, still rely on heuristically defined learning rate warm-up phases. 

Learning rate warm-up. The learning rate warm-up is a widely used heuristic to train deep neural networks, dating back at least to He et al. [2016], which used a small constant learning rate during the initial training phase. The linear warm-up strategy, introduced by Goyal et al. [2017], has since become standard for training ResNets [He et al., 2016] and transformers [Vaswani et al., 2017]. Empirical studies have shown that warm-up enhances training stability to allow larger learning rates, reduces gradient variance, and improves model performance [Gotmare et al., 2019; Liu et al., 2019; Kosson et al., 2024]. From a geometric perspective, Gilmer et al. [2021] and Kalra and Barkeshli [2024] observed that warm-up induces a sharpness reduction phase where the largest Hessian eigenvalue decreases, enabling larger learning rates in subsequent training. 

Generalized smoothness and warm-up theory. The standard L-smoothness assumption ∥∇ f (x) − ∇ f (y)∥ ≤ 

L∥x − y∥ is insufficient to explain the necessity of warm-up, as it implies a uniform bound on curvature throughout the training landscape. Similarly, the (L0, L 1)-smoothness condition introduced by Zhang et al. [2020], which bounds the Hessian norm by L0 + L1∥∇ f (x)∥, has not, to the best of our knowledge, yielded theoretical justifications for warm-up strategies. Recent works address this limitation by linking smoothness to the suboptimality gap, bounding 2the Hessian by K0 + K1(f (x) − f ⋆) [Alimisis et al., 2025] or K0 + Kρ(f (x) − f ⋆)ρ [Liu et al., 2025c], where f ⋆ is a target (perfect) loss for the problem (1) . However, the theoretical results in these studies are limited to GD and SGD optimizers and derive schedules consisting solely of a warm-up phase without any decay. In contrast, our Assumption 2 adopts this suboptimality-dependent framework to naturally derive warm-up and subsequent decay phases specifically for LMO-based methods. 

Target loss estimation. As was mentioned in the previous paragraph, all the theoretical frameworks about warmup rely on knowledge of the target value f ⋆. It commonly appears in adaptive stepsize methods, including Polyak-type step sizes [Polyak, 1963; Orabona and D’Orazio, 2025], its stochastic variant [Loizou et al., 2021] and Polyak step method for mirror descent [You et al., 2022]. A range of techniques has been proposed to estimate or adapt such target values in practice, including adaptive running lower bounds constructed from past function values [Hazan and Kakade, 2019] and level-type schemes that replace f ⋆ by evolving target levels in Polyak-type step sizes [You and Li, 2022]. In this work, we do not employ these mechanisms and instead fix a reasonable estimate of 

f ⋆ for each setup. In Section 6.3, we perform an ablation over f ⋆ and show that choosing f ⋆ within a reasonable neighborhood of the optimal loss yields stable and robust behavior, consistently outperforming the best manually tuned warm-up schedules. 

3 Theoretical Analysis 3.1 Assumptions and Practical Motivation 

We begin our analysis with the set of assumptions required to study LMO-based optimizers for problem (1). 

Assumption 1 

The function f : X → R is star-convex, i.e., the following inequality holds: 

f (βx ⋆ + (1 − β)x) ≤ βf (x⋆) + (1 − β)f (x),

for all x ∈ X and β ∈ [0 , 1] , where x⋆ is a global minimizer of f , i.e. f (x⋆) = f ⋆.Unlike classical convexity, which requires the inequality to hold for every pair of points in X , star-convexity only enforces convexity with respect to the optimum. This reflects a setting in which the loss landscape may be highly nonconvex globally, yet becomes progressively well behaved along trajectories that approach x⋆. Such star-shaped conditions are now standard in modern theoretical analyses of deep neural networks [Zhou et al., 2019]. In particular, recent works analyzing LMO-based optimizers show that star-convexity, together with classical Lipschitz smoothness, is sufficient to guarantee convergence with a constant learning rate [Pethick et al., 2025; Kovalev, 2025]. However, since these works rely on the standard smoothness assumption, they fail to explain the emergence of learning rate warm-up. This motivates Assumption 2, which we introduce next. 

Assumption 2 

The function f : X → R is (ρ, K 0, K 1, K ρ)-smooth, i.e., there exist K0, K 1, K ρ ≥ 0 and ρ > 0 such that for all x, y ∈ X it holds that: 

∥∇ f (x) − ∇ f (y)∥⋆ ≤ K (x)∥x − y∥,

K(x) := K0 + K1 (f (x) − f ⋆) + Kρ (f (x) − f ⋆)ρ ,

where ∥ · ∥ ⋆ is the conjugate norm for ∥ · ∥ used in the update rule (2). This condition strengthens the classical smoothness model in a manner that is aligned with the geometry of deep learning. It recovers the (ρ, L 0, L ρ)-smoothness (see Section E for details). When Kρ = 0 and ∥ · ∥ := ∥ · ∥ 2,Assumption 2 reduces to the version of assumption from [Alimisis et al., 2025], which corresponds to a bound of the type: 

∥∇ f (x) − ∇ f (y)∥ ≤ (K0 + K1(f (x) − f ⋆)) ∥x − y∥.

3However, our findings (both theoretical and empirical) indicate that the regime Kρ > 0 is not merely an analytical convenience but necessary for accurately modeling the behavior of norm-constrained optimizers in deep learning. To demonstrate this, we evaluate the empirical stability ratio 

Kt = ∥∇ f (xt+1 ) − ∇ f (xt)∥⋆

∥xt+1 − xt∥

as a function of ∆t := f (xt) − f ⋆ along optimization trajectories of several LMO-based methods: Muon, normSGD, and Lion. The experiments are carried out in a large-scale pretraining regime (see Section 6 for details). Figure 1 reports the observed dependence for Lion. Analogous curves for the remaining methods are provided in Appendix B. 1 2 3 4 5        

> t= loss( xt)loss
> 0
> 1
> 2
> 3
> 4
> 5
> ||  loss( xt + 1 ) loss( xt)||  * /|| xt + 1  xt||
> 1e8
> Fitted line: K0+K1+K22
> R2 score = 0.973
> 1000
> 2000
> 3000
> 4000
> 5000
> 6000
> 7000
> 8000
> Training Iteration  t

Figure 1: Empirical smoothness ratio Kt versus suboptimality gap ∆t for Lion on large-scale pretraining. The trajectory is well-fitted by a quadratic dependence, indicating Kρ > 0. All hyperparameter setup is provided in Appendix G.2. In all cases we observe a clear parabolic trend linking Kt and ∆t, consistent with a dominant Kρ(f (x) − f ⋆)ρ term. This behavior persists across optimizers, and cannot be captured by models with Kρ = 0 .We note that earlier empirical studies generated smoothness-versus-progress plots and fitted them using linear functions [Alimisis et al., 2025]. However, those investigations examined only the earliest portion of training, under extremely small learning rates, where the suboptimality gap remains large (e.g., ∆ ≥ 6). In that narrow range, a linear fit indeed appears adequate. Yet, over the full training trajectory, where ∆ decreases by orders of magnitude, a purely linear model breaks down, and the quadratic component becomes essential. In recent warm-up studies, Liu et al. [2025c] also observed that linear functions may not fully capture these empirical trends. While their analysis effectively identifies this non-linearity, it primarily focuses on diagnosing the limitations of linear models. Our work builds upon these observations by showing that a quadratic model provides a more precise functional form to describe these dynamics across the entire trajectory, further supporting Assumption 2 as a representative model for deep learning. Finally, we require the generated sequence of iterates to remain within a bounded domain. 

Assumption 3 

The iterates xt generated by (2) are bounded, i.e., there exists D > 0 such that for all t ≥ 0 it holds that 

∥xt − x⋆∥ ≤ D. 

Assumption 3 is standard in LMO-type methods [Kovalev, 2025], ensuring that the smoothness constants in Assumption 2 remain meaningful along the trajectory. Importantly, this assumption is not required in two common cases: (i) when weight decay is used (Section 3.3), (ii) for the Euclidean norm in the step (2) , boundedness follows automatically (see Lemma 4 in Appendix). 43.2 Deterministic Case without Weight Decay 

The next theorem provides, to the best of our knowledge, the first convergence analysis under Assumption 2 to theoretically predict a learning rate schedule consisting of a warm-up phase followed by decay. 

Theorem 1 

Suppose Assumptions 1, 2, and 3 hold and the iterates xt are generated by (2) with gt = ∇f (xt).If we use the learning rate scheduler (with warm-up followed by decay for ρ > 1)

ηt = ∆t

D · K (xt) = ∆t

D · (K0 + K1∆t + Kρ(∆ t)ρ) ,

then ∆t+1 ≤ ∆t and K(xt+1 ) ≤ K (xt) for all t, and 

∆T ≤ 2D2 PT −1 

> t=0

K(xt)

T 2 = O

 1

T



,

where ∆t := f (xt) − f ⋆.

Discussion of Theorem 1. 

• Explanation of warm-up behavior. Since by Theorem 1 ∆t decreases monotonically, the learning rate ηt

exhibits a warm-up phase followed by a decay for ρ > 1. The transition occurs at the point ∆′ obtained by maximizing η(∆ t):

∆′ := 

 K0

Kρ(ρ − 1) 

1/ρ 

.

Consequently, ηt increases while ∆t > ∆′ and decreases thereafter. If we assume sublinear convergence of the objective function, specifically ∆t ∼ 1/t , the asymptotic behavior of the learning rate is illustrated in Figure 2. 0 2500 5000 7500 10000 12500 15000 17500 20000              

> Training Step
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 1.2
> 1.4
> 1.6
> Learning Rate
> Start: 0
> Warmup:
> (t) = t
> K0+K1t+K2(t)2
> Peak: '
> Decay:
> t0as t
> Warmup Phase
> Decay Phase

Figure 2: Asymptotic behavior of the learning rate ηt for parameters D = 1 , ρ = 2 , K0 = 10 −4, K1 = 0 and 

Kρ = 10 3 with ∆t = 1 /t . The trajectory explicitly shows the theoretical warm-up phase followed by decay. 

• Comparison with decay-only schedule. Consider a simplified scheduler where the adaptive term K(xt) is replaced by its initial constant value K0 := K0 + K1∆0 + Kρ(∆ 0)ρ. In this scenario, the learning rate becomes 

ηt = ∆t 

> D·K 0

and the convergence bound simplifies to 

∆T ≤ 2D2T · K 0

T 2 = 2D2K0

T = O

 1

T



.

5Although the asymptotic rate O(1 /T ) remains unchanged, the adaptive schedule in Theorem 1 provides a tighter bound. To quantify the improvement, observe that if ∆t ∼ 1/t , then K(xt) ∼ K0 + K1/t + Kρ/t ρ.For ρ > 1, the dominant term is K0, yielding PTt=1 K(xt) ∼ K0T . Thus, the adaptive bound ∆T ≲ D2K0/T 

matches the decay-only bound asymptotically. However, during the transient phase (small t), the higher-order terms K1/t and Kρ/t ρ are significant, and the adaptive schedule benefits from using larger learning rates. 

3.3 Deterministic Case with Weight Decay 

We now extend our framework by incorporating weight decay regularization with the coefficient λ > 0 into (2) ,resulting in the following update rule: 

xt+1 = (1 − λη t)xt + ηt LMO( gt). (4) This form of weight decay is a standard technique for improving generalization in large-scale neural networks [Loshchilov and Hutter, 2019]. The following Theorem 2 provides the convergence guarantees for the step (4). 

Theorem 2 

Suppose Assumptions 1 and 2 hold with ρ > 1 and the iterates xt are generated by (4) with gt = ∇f (xt),

λ ∈  0, 1max( ∥x0∥,∥x⋆∥,1/λ max )

, where 

λmax =

"

8 ρ

 K0

ρ − 1

 ρ−1

> ρ

K 

> 1
> ρ
> ρ

+ K1

!# 1/2

.

If we use the learning rate scheduler (with warm-up followed by decay) 

ηt = λ∆t

8K(xt) = λ∆t

8 ( K0 + K1∆t + Kρ(∆ t)ρ) ,

then ∆t+1 ≤ ∆t and K(xt+1 ) ≤ K (xt) for all t, and 

∆T ≤ 16 PT −1 

> t=0

K(xt)

λ2T 2 = O

 1

T



.

Discussion of Theorem 2. 

• Relaxation of Boundedness Assumption. Unlike Theorem 1, we do not require Assumption 3 (boundedness of the iterates), as the regularization term −λη txt in the update rule (4) implicitly ensures that the iterates remain bounded. 

• Comparison with Theorem 1. Note that all implications of Theorem 1 are satisfied in the context of Theorem 2 by setting D ∼ 1/λ .The deterministic analysis provides the foundation for understanding optimal learning rate schedules. However, as deep learning relies on stochastic gradient estimates in practice, we now extend our framework to address this setting. 

4 Stochastic Extensions 

Let us now consider the following stochastic optimization problem: 

min  

> x∈X

{f (x) := Eξ∼D [fξ(x)] } , (5) where fξ : X → R represents the loss of the model parameterized by x, associated with a training data point ξ

sampled from the probability distribution D.6We consider a variant of the update rule (2) for minimizing (5) , by setting gt = ∇fξt (xt) and choosing the Euclidean norm ∥ · ∥ := ∥ · ∥ 2 in the LMO step: 

xt+1 = xt − ηt ∇fξt (xt)

∥∇ fξt (xt)∥ . (6) For the stochastic setting, we can use a weaker variant of Assumption 2 that only requires smoothness relative to the optimum x⋆.

Assumption 4 

The function f : X → R is (ρ, K 0, K 1, K ρ)-smooth at the global minimizer x⋆ of f , i.e., there exist 

K0, K 1, K ρ ≥ 0, ρ > 0 such that 

∥∇ f (x)∥⋆ ≤ K (x)∥x − x⋆∥,

K(x) := K0 + K1 (f (x) − f ⋆) + Kρ (f (x) − f ⋆)ρ ,

for all x ∈ X .Additionally, to study convergence in the stochastic setting similarly to Alimisis et al. [2025], we require an interpolation condition, which is typically satisfied for over-parameterized networks [Ma et al., 2018]. 

Assumption 5 

The optimization problem (5) satisfies the interpolation regime, i.e., for the global minimizer x⋆ of f , the following holds almost surely for ξ ∼ D 

fξ(x⋆) = f ⋆ξ ,

where f ⋆ξ := inf x∈X fξ(x) > −∞ .Under the established assumptions, we characterize the convergence of (6) in Theorem 3. 

Theorem 3 

Suppose Assumptions 1, 4, 5 holds with ∥ · ∥ := ∥ · ∥ 2. Consider the iterates xt generated by (6) with learning rate scheduler 

ηt = ∆tξ

D · K ξ(xt) ,

where D := ∥x0 − x⋆∥, ∆tξ := fξt (xt) − f ⋆ξt , and Kξ(xt) := K0 + K1∆tξ + Kρ(∆ tξ)ρ. Then 

1

T 

> T−1

X

> t=0

E[∆ tξ] ≤ D2

T

vuutE

"T −1X

> t=0

(Kξ(xt)) 2

#

.

If there exists M > 0 such that ∆tξ ≤ M almost surely for all t and ξ ∼ D . Then 

1

T 

> T−1

X

> t=0

E[∆ tξ] ≤ D2 ¯K√T = O

 1

√T



,

where ¯K := K0 + K1M + KρM ρ.75 Practical Scheduler with Adaptive Warmup 

In this section, we demonstrate how to apply the optimal learning rate scheduler in practice. Fixing ρ = 2 and removing constant scaling factors for clarity, the scheduler form Theorems 1, 2 and 3 takes the form 

η(∆) = ∆

K0 + K1∆ + K2∆2 . (7) Since the coefficients K0, K 1, K 2 are unknown and cannot be measured during training, we determine them through three (by the number of the independent parameters) practical constraints. 

(1) Peak learning rate. The function η(∆) has a unique local maximum at some ∆′ ∈ [0; ∆ 0 = f (x0) − f ⋆], and we enforce 

η(∆ ′) = lr , (8) where lr denotes the peak learning rate. This quantity is a standard user-facing parameter in all PyTorch learning rate schedulers [Paszke et al., 2019]. 

(2) Warm-up floor. We match the initial step size by requiring 

η(∆ 0) = lr div , (9) where div ≥ 1 is a user-specified divisor. Typical implementations use div = 100 , again mirroring standard PyTorch practice [Paszke et al., 2019]. We solve constraints (8) and (9) analytically to express K0, K 1 and K2 in terms of ∆′ (see Appendix D.2 for closed-form expressions). Now we need to define ∆′.

(3) Target-shape matching. The third condition determines ∆′ and aligns the profile of (7) with a classical warmup+decay schedule. We first introduce the target learning rate ηtrgt (∆) that we aim to approximate: 

ηtrgt (∆) = 



lr div + (lr − lr div )(∆ 0 − ∆) ∆0 − ∆′ , ∆ ∈ [∆ ′, ∆0],

12 lr 



1 − cos  π∆/∆′

, ∆ ∈ [0 , ∆′].

The definition of ηtrgt is provided as the most commonly used choice in practice: linear warm-up and cosine decay [Paszke et al., 2019; Loshchilov and Hutter, 2017]. The functions on the intervals [0 , ∆′] and [∆ ′, ∆0] may be replaced with any schedulers without affecting the construction. Rather than matching η(∆) to ηtrgt (∆) over the entire interval [0 , ∆0], we focus on a neighborhood around ∆′,since, this is the switching point between warmup and decay. For large deviations away from ∆′, the theoretical schedule (7) significantly differs from ηtrgt (∆) , whereas matching in a neighborhood of ∆′ controls the behavior of the scheduler exactly where the transition happens. Moreover, by constraint (8) we have η(∆ ′) = ηtrgt (∆ ′),therefore the approximation error vanishes at the center. The neighborhood around ∆′ also corresponds to the maximal learning rate, and therefore to the strongest influence on training. To formalize this localized matching, we introduce an exponential weight exp −(∆ − ∆′)2/σ 2 that is maximized at 

∆′ and decays as we move away from it. The parameter σ controls the width of the neighborhood and must be chosen appropriately. We determine σ by considering the effective step size measured in Frobenius norm. Different LMO geometries produce updates of different magnitudes: for signSGD ( ℓ1 norm), the output vector of dimensionality d

consists of ±1 entries and thus has squared Frobenius norm d, requiring a smaller learning rate than normSGD (Euclidean norm), whose output has unit Frobenius norm. We formalize this idea as follows: for a given ∆ and gradient gt, the squared step size xt+1 − xt satisfies 

η(∆)LMO( gt) 2 

> F

= η(∆) 2 LMO( gt) 2 

> F

≤ κη (∆) 2,

where κ := sup ∥u∥=1 ∥u∥2 

> F

. captures the dependence on the norm used inside the LMO step (2) . Intuitively, κ is the worst-case factor between the unit ball of the optimization norm ∥ · ∥ and the Frobenius norm ( κ = d for signSGD). 8It depends on the structure of the model: number of layers and the shapes of their parameter tensors and can be computed analytically for each geometry. In Appendix D.1 we provide closed-form expressions together with the corresponding derivations for κ for all optimizers considered in our experiments. Thus, if we think of the effective step √κη (∆) as having a fixed Frobenius-scale variance σ2 

> F

, then the corresponding variance in η(∆) itself must be σ2 

> F

/κ . This leads to setting σ2 = σ2 

> F

/κ in the exponential weight, which normalizes the neighborhood around ∆′ in a way that is consistent across different LMO geometries. Combining these considerations, we formulate the objective function that we wish to minimize with respect to ∆′:

Z ∆0

> 0

exp 



− (∆ − ∆′)2κσ2

> F

  η(∆) − ηtrgt (∆) 2d∆, (10) where σF is a fixed constant. This condition is then enforced numerically: we sample 1000 candidate values of ∆′,evaluate the objective (10) for each candidate, and select the minimizer. Since this procedure is executed only once at the beginning of training, it contributes no observable overhead to the overall runtime. Once K0, K1, K2 and ∆′

are determined from constraints (8) –(10) , the scheduler is fully specified. Algorithm 1 summarizes the complete scheduler in a practical, ready-to-implement form. 

Algorithm 1: Adaptive warmup scheduler  

> 1:

Input: total steps T , target loss f ⋆, peak lr , divisor div , warmup and decay schedulers = ( linear , cos ),optimizer ∈ { Muon , Lion , normSGD }. 

> 2:

Initialize: is_init ← False , is_decay ← False , warmup_steps ← 0. 

> 3:

function get_lr (loss ) 

> 4:

∆ ← loss − f ⋆ 

> 5:

if not is_init then  

> 6:

is_init ← True  

> 7:

determine K0, K 1, K 2, ∆′ via constraints (8)–(10)  

> 8:

end if  

> 9:

if ∆ ≥ ∆′ and not is_decay then  

> 10:

warmup_steps ← warmup_steps + 1  

> 11:

return ∆

K0 + K1∆ + K2∆2 

> 12:

else  

> 13:

if not is_decay then  

> 14:

is_decay ← True  

> 15:

initialize decay scheduler with Tdecay ← T − warmup_steps and starting lr = lr  

> 16:

end if  

> 17:

return decay_scheduler.get_lr ()  

> 18:

end if  

> 19:

end function 

Line 7 of Algorithm 1 uses the initialization procedure discussed in this section. Lines 9-11 apply (7) to compute the adaptive learning rate. When ∆ < ∆′, we switch from the adaptive warmup (7) to a classical decay scheduler (lines 12-17). This choice is motivated by two factors: (i) the theoretical form in (7) produces tiny learning rates at small 

∆, and (ii) the parabolic approximation of the empirical smoothness ratio disappears in that regime (see Figure 1). 

6 Experiments 

We evaluate the proposed scheduler (Algorithm 1) in a medium- and large-scale LLM pretraining. Our setup follows the experimental protocol of [Semenov et al., 2025], where several optimizers were benchmarked on training LLaMA-style transformer models on the FineWeb dataset [Penedo et al., 2024]. For direct comparability, we adopt the same model architecture family: decoder-only transformer networks with rotary position embeddings and the standard LLaMA hyperparameter configuration [Fedus et al., 2022]. We reuse the optimizer configurations from [Semenov et al., 2025] wherever available. Specifically, for Muon and Lion we copy all hyperparameters 90 1000 2000 3000 4000 5000 6000                           

> Warmup Steps
> 3.38
> 3.40
> 3.42
> 3.44
> 3.46
> 3.48
> 3.50
> Validation Loss
> 64K iterations (1B tokens)
> 128K iterations (2.1B tokens)
> Ours - 64K iterations
> Ours - 128K iterations 02000 4000 6000 8000 10000 12000 14000 16000
> Warmup Steps
> 3.5
> 4.0
> 4.5
> 5.0
> 5.5
> 6.0
> 6.5
> Validation Loss
> 64K iterations (1B tokens)
> 128K iterations (2.1B tokens)
> Ours - 64K iterations
> Ours - 128K iterations
> 11000 13000 15000 17000
> 3.40
> 3.45
> 3.50
> 3.55
> 3.60 Zoomed View 01000 2000 3000 4000 5000
> Warmup Steps
> 4.20
> 4.25
> 4.30
> 4.35
> 4.40
> 4.45
> Validation Loss
> 64K iterations (1B tokens)
> 128K iterations (2.1B tokens)
> Ours - 64K iterations
> Ours - 128K iterations 0500 1000 1500 2000 2500 3000 3500 4000 4500
> Warmup Steps
> 3.38
> 3.40
> 3.42
> 3.44
> 3.46
> 3.48
> 3.50
> 3.52
> Validation Loss
> 8K iterations (1B tokens)
> 16K iterations (2.1B tokens)
> Ours - 8K iterations
> Ours - 16K iterations

(a) Muon 0 1000 2000 3000 4000 5000 6000 7000        

> Warmup Steps
> 3.3
> 3.8
> 4.3
> 4.8
> 5.3
> 5.8
> Validation Loss
> 8K iterations (1B tokens)
> 16K iterations (2.1B tokens)
> Ours - 8K iterations
> Ours - 16K iterations
> 2000 3000 4000 5000 6000 7000
> 3.35
> 3.40
> 3.45
> 3.50
> 3.55
> 3.60 Zoomed View

(b) Lion 0 200 400 600 800 1000 1200 1400 1600  

> Warmup Steps
> 4.4
> 4.5
> 4.6
> 4.7
> 4.8
> 4.9
> Validation Loss
> 8K iterations (1B tokens)
> 16K iterations (2.1B tokens)
> Ours - 8K iterations
> Ours - 16K iterations

(c) normSGD 

Figure 3: Validation loss on LLaMA 124M model with bs = 32 (top) and 256 (bottom) as a function of manually selected warmup length (solid) vs. adaptive warmup (dashed). Across optimizers, the adaptive method outperforms or at least matches the best manually tuned value without any grid search. (learning rate, weight decay, etc.) from the reference setup. We additionally evaluate normSGD, that was not included in [Semenov et al., 2025], therefore for this optimizer we tune lr and div independently. Across all model sizes, batches, and optimizers, we use fixed Frobenius variance σ2

F = 10 3 without any additional tuning. Full tuning ranges and configuration tables are provided in Appendix G.3. We consider models with 124M and 256M parameters, and evaluate 32 and 256 batch-size regimes. For each optimizer, we sweep over a range of manually specified warmup lengths and compare the resulting final validation loss against the value obtained from Algorithm 1. Figures 3 and 5 summarize the trends for Muon, Lion and normSGD for 124M and 256M model sizes respectfully. 

6.1 Illustration of the Adaptive Scheduler 

For illustration, Figure 4 shows the resulting learning rate profile of Algorithm 1 instantiated with the Lion optimizer. 0 10000 20000 30000 40000 50000 60000 

Training Steps 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> Learning Rate

1e 3

Start: 0

0 = lr div = 1 e 5

Warmup: 

( ) = K0 + K1 + K2 2

Peak: ′

max = lr = 1 e 3

Decay: 

Cosine Schedule 

Warmup Phase 

Decay Phase 

Figure 4: Example of learning rate schedule produced by Algorithm 1 for the Lion optimizer on FineWeb. 10 0 250 500 750 1000 1250 1500 1750 2000                    

> Warmup Steps
> 3.26
> 3.28
> 3.30
> 3.32
> 3.34
> 3.36
> 3.38
> 3.40
> Validation Loss
> 64K iterations (1B tokens)
> 128K iterations (2.1B tokens)
> Ours - 64K iterations
> Ours - 128K iterations 05000 10000 15000 20000 25000 30000
> Warmup Steps
> 4
> 5
> 6
> 7
> 8
> Validation Loss
> 64K iterations (1B tokens)
> 128K iterations (2.1B tokens)
> Ours - 64K iterations
> Ours - 128K iterations
> 11000 13000 15000 17000
> 3.40
> 3.45
> 3.50
> 3.55
> 3.60 Zoomed View 0100 200 300 400 500
> Warmup Steps
> 4.10
> 4.15
> 4.20
> 4.25
> 4.30
> Validation Loss
> 64K iterations (1B tokens)
> 128K iterations (2.1B tokens)
> Ours - 64K iterations
> Ours - 128K iterations 100 200 300 400 500
> Warmup Steps
> 3.26
> 3.28
> 3.30
> 3.32
> 3.34
> 3.36
> 3.38
> Validation Loss
> 8K iterations (1B tokens)
> 16K iterations (2.1B tokens)
> Ours - 8K iterations
> Ours - 16K iterations

(a) Muon 1000 2000 3000 4000 5000 6000        

> Warmup Steps
> 3.5
> 4.0
> 4.5
> 5.0
> 5.5
> 6.0
> Validation Loss
> 8K iterations (1B tokens)
> 16K iterations (2.1B tokens)
> Ours - 8K iterations
> Ours - 16K iterations
> 2000 3000 4000 5000 6000 7000
> 3.30
> 3.35
> 3.40
> 3.45
> 3.50 Zoomed View

(b) Lion 200 400 600 800 1000  

> Warmup Steps
> 4.20
> 4.25
> 4.30
> 4.35
> 4.40
> 4.45
> 4.50
> 4.55
> 4.60
> Validation Loss
> 8K iterations (1B tokens)
> 16K iterations (2.1B tokens)
> Ours - 8K iterations
> Ours - 16K iterations

(c) normSGD 

Figure 5: Validation loss on LLaMA 210M model with bs = 32 (top) and 256 (bottom) as a function of manually selected warmup length (solid) vs. adaptive warmup (dashed). Across optimizers, the adaptive method outperforms or at least matches the best manually tuned value without search. 

6.2 Experiment Results 

Across all optimizers, model sizes, and batch-sizes, the adaptive schedule improves or at least matches the best hand-tuned warm-up value, without requiring any grid search. This effect is visible for Muon and normalized SGD, where the dashed line corresponding to our method lies at (or below) the minimum of the manual sweep. This suggests that rather than committing to a fixed linear ramp, allowing the learning rate to follow the rational shape η(∆) yields a more stable transition into the high-curvature regime. The Lion results highlight the sensitivity of the warm-up phase: choosing a warm-up length that is too short causes optimization to diverge. Our adaptive approach navigates this regime robustly, automatically identifying an appropriate transition point ∆′ and avoiding catastrophic failures. The benefit of adaptive warm-up is most pronounced in the smaller batch-size regime ( bs = 32 ), where gradient noise is higher and the optimization dynamics are more sensitive to aggressive early steps. In this setting, careful control of the warm-up phase is critical, and the adaptive scheduler provides a clear advantage over manually specified schedules. For the larger batch-size regime ( bs = 256 ), the performance gap between different warm-up choices narrows, consistent with more stable gradients. Nevertheless, the adaptive method remains robust and competitive. Importantly, the qualitative trends are consistent across model scales: both the 124M and 210M models exhibit the same sensitivity patterns and benefit similarly from adaptive warm-up, indicating that the proposed scheduler generalizes across pretraining scales. 

6.3 Ablation Study on Target Loss 

We study the sensitivity of the proposed scheduler to the choice of the target loss f ⋆ on LLaMA 124M pretraining with batch size bs = 32 using the Muon optimizer. Figure 6 shows the final validation loss as a function of f ⋆. We observe that the scheduler is robust to choice of f ⋆: selecting f ⋆ within a reasonable neighborhood of the best achievable loss yields stable performance and consistently outperforms the optimal manually tuned warm-up. 11 0 1 2 3 4 5 

> Loss *(Target Loss)
> 3.45
> 3.50
> 3.55
> 3.60
> 3.65
> 3.70
> Validation Loss
> Adaptive (ours)
> Best tuned (grid search)

Figure 6: Ablation over the target loss f ⋆ for LLaMA 124M pretraining with bs = 32 using the Muon optimizer. The adaptive scheduler remains stable across a broad range of f ⋆ values when f ⋆ is chosen within a reasonable neighborhood of the optimal loss. 

References 

Foivos Alimisis, Rustem Islamov, and Aurelien Lucchi. Why do we need warm-up? a theoretical perspective. arXiv preprint arXiv:2510.03164 , 2025. URL https://arxiv.org/abs/2510.03164 .Noah Amsel, David Persson, Christopher Musco, and Robert M. Gower. The polar express: Optimal matrix sign methods and their application to the muon algorithm. arXiv preprint arXiv:2505.16932 , 2025. URL 

https://arxiv.org/abs/2505.16932 .Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology. In NeurIPS 2024 Workshop on Optimization for Machine Learning (OPT) , 2024. URL https://arxiv.org/abs/2409.20325 .Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signSGD: Compressed optimisation for non-convex problems. In Proceedings of the 35th International Conference on Machine Learning ,volume 80 of Proceedings of Machine Learning Research , pages 560–569. PMLR, 2018. URL https://proceedings. mlr.press/v80/bernstein18a.html .Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V Le. Symbolic discovery of optimization algorithms. Advances in Neural Information Processing Systems , 36, 2023. Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by d-adaptation. arXiv preprint arXiv:2301.07733 , 2023. URL https://arxiv.org/abs/2301.07733 .Aaron Defazio, Xingyu Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, and Ashok Cutkosky. The road less scheduled. In Advances in Neural Information Processing Systems , volume 37, 2024. URL 

https://arxiv.org/abs/2405.15682 .William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research , 23(120):1–39, 2022. Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur, David Cardoze, George Dahl, Zachary Nado, and Orhan Firat. Loss surface simplexes for mode connecting volumes and fast ensembling. In Proceedings of the 38th International Conference on Machine Learning , 2021. URL https://arxiv.org/abs/ 2102.13042 .12 Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at deep learning heuris-tics: Learning rate restarts, warmup and distillation. In International Conference on Learning Representations ,2019. URL https://arxiv.org/abs/1810.13243 .Priya Goyal, Piotr Doll´ ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. In arXiv preprint arXiv:1706.02677 , 2017. URL https://arxiv.org/abs/1706.02677 .Elad Hazan and Sham Kakade. Revisiting the polyak step size. arXiv preprint arXiv:1905.00313 , 2019. Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. Advances in Neural Information Processing Systems , 28, 2015a. Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In 

Advances in Neural Information Processing Systems , volume 28, 2015b. URL https://proceedings.neurips. cc/paper/2015/hash/934815ad542a4a7c5e8a2dfa04fea9f5-Abstract.html .Chuan He, Zhanwang Deng, and Zhaosong Lu. Low-rank orthogonalization for large-scale matrix optimization with applications to foundation model training. arXiv preprint arXiv:2509.11983 , 2025. URL https://arxiv.org/ abs/2509.11983 .Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016. URL 

https://arxiv.org/abs/1512.03385 .Feihu Huang, Yuning Luo, and Songcan Chen. Limuon: Light and fast muon optimizer for large models. arXiv preprint arXiv:2509.14562 , 2025. URL https://arxiv.org/abs/2509.14562 .Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan.github.io/posts/muon/ .Divyam S Kalra and Maissam Barkeshli. Why warmup the learning rate? underlying mechanisms and improvements. 

arXiv preprint arXiv:2406.09405 , 2024. URL https://arxiv.org/abs/2406.09405 .Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations , 2015. URL https://arxiv.org/abs/1412.6980 .Atli Kosson, Johanni Brea, and Martin Jaggi. Rotational equilibrium: How weight decay balances learning across neural networks. arXiv preprint arXiv:2305.17212 , 2024. URL https://arxiv.org/abs/2305.17212 .Dmitry Kovalev. Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization. arXiv preprint arXiv:2503.12645 , 2025. URL https://arxiv.org/abs/2503.12645 .Kfir Levy. The power of normalization: Faster evasion of saddle points. In Advances in Neural Information Processing Systems , 2016. URL https://arxiv.org/abs/1611.04831 .Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, and Zhilin Yang. Muon is scalable for llm training, 2025a. URL https: //arxiv.org/abs/2502.16982 .Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265 , 2019. URL https: //arxiv.org/abs/1908.03265 .Qiang Liu, Jonathan Li, and Lizhang Chen. Muon optimizes under spectral norm constraints. arXiv preprint arXiv:2506.15054 , 2025b. URL https://arxiv.org/abs/2506.15054 .13 Yuxing Liu, Yuze Ge, Rui Pan, An Kang, and Tong Zhang. Theoretical analysis on how learning rate warmup accelerates convergence. arXiv preprint arXiv:2509.07972 , 2025c. URL https://arxiv.org/abs/2509.07972 .Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In International Conference on Artificial Intelligence and Statistics , pages 1306–1314. PMLR, 2021. Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations , 2017. URL https://arxiv.org/abs/1608.03983 .Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations , 2019. URL https://arxiv.org/abs/1711.05101 .Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 3325–3334. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/ma18a.html .Konstantin Mishchenko and Aaron Defazio. Prodigy: An expeditiously adaptive parameter-free learner. In Proceed-ings of the 41st International Conference on Machine Learning , volume 235 of Proceedings of Machine Learning Research , pages 35887–35912. PMLR, 2024. URL https://proceedings.mlr.press/v235/mishchenko24a.html .Francesco Orabona and Ryan D’Orazio. New perspectives on the polyak stepsize: Surrogate functions and negative results. arXiv preprint arXiv:2505.20219 , 2025. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨ opf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems , volume 32, 2019. URL https://arxiv.org/abs/1912.01703 .Guilherme Penedo, Hynek Kydl´ ıˇ cek, Anton Lozhkov, Margaret Mitchell, Colin A Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems , 37:30811–30849, 2024. Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, and Volkan Cevher. Training deep learning models with norm-constrained lmos. In Proceedings of the 42nd International Conference on Machine Learning , volume 267 of Proceedings of Machine Learning Research , pages 49069–49104. PMLR, 2025. URL https://proceedings.mlr.press/v267/pethick25a.html .Boris T Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics , 3(4):864–878, 1963. Artem Riabinin, Egor Shulgin, Kaja Gruntkowska, and Peter Richt´ arik. Gluon: Making muon & scion great again! (bridging theory and practice of lmo-based optimizers for llms). arXiv preprint arXiv:2505.13416 , 2025. URL 

https://arxiv.org/abs/2505.13416 .Andrei Semenov, Matteo Pagliardini, and Martin Jaggi. Benchmarking optimizers for large language model pretraining. arXiv preprint arXiv:2509.01440 , 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, 14 Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534 .Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems , volume 30, 2017. URL https://arxiv.org/abs/1706.03762 .Jun-Kai You and Yen-Huan Li. Two polyak-type step sizes for mirror descent. arXiv preprint arXiv:2210.01532 ,2022. Jun-Kai You, Hao-Chung Cheng, and Yen-Huan Li. Minimizing quantum r´ enyi divergences via mirror descent with polyak step size. In 2022 IEEE International Symposium on Information Theory (ISIT) , pages 252–257. IEEE, 2022. Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: Atheoretical justification for adaptivity. arXiv preprint arXiv:1905.11881 , 2020. URL https://arxiv.org/abs/ 1905.11881 .Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global minimum in deep learning via star-convex path. In International Conference on Learning Representations , 2019. URL 

https://arxiv.org/abs/1901.00451 .15 Appendix 

Supplementary Materials for Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers 

A Automatic Boundedness for the Euclidean Norm 

The following lemma confirms that for the Euclidean norm, Assumption 3 is automatically satisfied by our learning rate schedule. 

Lemma 4. Suppose Assumptions 1 and 2 hold under the Euclidean norm ∥ · ∥ = ∥ · ∥ 2. Consider the iterates xt

generated by (2) with gt = ∇f (xt) and learning rate ηt = ∆t 

> D·K (xt)

, where D := ∥x0 − x⋆∥2. Then ∥xt − x⋆∥2 ≤ D

for all t ≥ 0.Proof. We proceed by induction on t. The base case t = 0 holds trivially since ∥x0 − x⋆∥2 = D.For the inductive step, assume ∥xt − x⋆∥2 ≤ D. From the update rule (2) with the Euclidean norm: 

xt+1 = xt − ηt ∇f (xt)

∥∇ f (xt)∥2

.

Using the star-convexity condition (Assumption 1): 

⟨∇ f (xt), x t − x⋆⟩ ≥ f (xt) − f ⋆ = ∆ t > 0.

We compute: 

∥xt+1 − x⋆∥22 = ∥xt − x⋆∥22 − 2ηt ⟨∇ f (xt), x t − x⋆⟩∥∇ f (xt)∥2

+ ( ηt)2

≤ ∥ xt − x⋆∥22 − 2ηt ∆t

∥∇ f (xt)∥2

+ ( ηt)2.

By Assumption 2, ∥∇ f (xt)∥2 ≤ K (xt)∥xt − x⋆∥2 ≤ K (xt)D. With ηt = ∆t 

> D·K (xt)

:

∥xt+1 − x⋆∥22 ≤ D2 − 2 · ∆t

D · K (xt) · ∆t

K(xt)D + (∆ t)2

D2K(xt)2

= D2 − 2(∆ t)2

D2K(xt)2 + (∆ t)2

D2K(xt)2

= D2 − (∆ t)2

D2K(xt)2 ≤ D2.

Thus ∥xt+1 − x⋆∥2 ≤ D, completing the induction. 

B Empirical Motivation for Assumption 2 

To complement the main-text visualization based on Lion (Figure 1), we report the same Lipschitz–style diagnostic for Muon and normalized SGD. For each optimizer, we plot the empirical ratio 

Kt = ∥∇ f (xt+1 ) − ∇ f (xt)∥⋆

∥xt+1 − xt∥ versus ∆t = f (xt) − f ⋆,

and overlay a fitted curve of the form 

K0 + K1∆ + K2∆2.

The resulting trajectories, shown in Figures 1 and 7, reveal three consistent phenomena. 16 1. Suboptimality-dependent curvature is essential. Across all optimizers, the empirical dependence of 

Kt on ∆t exhibits clear curvature, and the quadratic term dominates the linear component except near the beginning of training. This confirms that Kρ must be strictly positive in Assumption 2. 2. Different optimizers induce different curvature profiles. The magnitude and slope of Kt vary substantially across methods. Muon produces smoother trajectories with a tight parabolic fall-off, while normSGD displays a steeper and higher-variance decrease as training progresses. These differences reflect the underlying geometry of each update rule—LMO direction choice, normalization scale, and implicit conditioning—and highlight why a single global Lipschitz constant would poorly approximate training dynamics. 3. The trend is optimizer-universal. Even though the shapes differ, both optimizers exhibit the same qualitative pattern as Lion in the main text: the curvature decreases monotonically with ∆t and cannot be explained by a constant Lipschitz term alone. That is, all three optimizers behave as predicted by Assumption 2, but with optimizer-dependent constants. Overall, these additional results reinforce that Kρ > 0 is not an analytical convenience but an empirically grounded property of modern LMO optimizers, and that the precise smoothness structure is optimizer-specific—supporting our choice to estimate K0, K 1, K 2 rather than assume a fixed curvature model. 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0         

> t= loss( xt)loss
> 200
> 400
> 600
> 800
> 1000
> 1200
> 1400
> 1600
> ||  loss( xt + 1 ) loss( xt)||  * /|| xt + 1  xt||
> Fitted line: K0+K1+K22
> R2 score = 0.857
> 2000
> 4000
> 6000
> 8000
> 10000
> 12000
> 14000
> Training Iteration  t

(a) Muon 3.50 3.75 4.00 4.25 4.50 4.75 5.00 5.25          

> t= loss( xt)loss
> 10000
> 20000
> 30000
> 40000
> 50000
> 60000
> ||  loss( xt + 1 ) loss( xt)||  * /|| xt + 1  xt||  Fitted line: K0+K1+K22
> R2 score = 0.825
> 1000
> 2000
> 3000
> 4000
> 5000
> 6000
> 7000
> 8000
> 9000
> Training Iteration  t

(b) normSGD 

Figure 7: Empirical smoothness ratio Kt versus suboptimality gap ∆t for Muon and normalized SGD. The fitted curve K0 + K1∆ + K2∆2 (red) demonstrates that a quadratic term is necessary to explain the observed trajectory, and that the magnitude and curvature differ across optimizers. 

Experimental setup. All measurements are obtained using the same model, data pipeline, and optimizer hyperparameters as in Section 6, with settings inherited from [Semenov et al., 2025]. The only aspect that varies across optimizers is the number of optimization steps observed. For each iteration, we estimate ∆t = fξt (xt) − f ⋆

and compute 

Ktξt = ∥∇ fξt (xt+1 ) − ∇ fξt (xt)∥⋆

∥xt+1 − xt∥

on the same mini-batch ξt of size 64 by performing an additional forward and backward pass, ensuring that the difference is not corrupted by batch-to-batch noise. While both ∆t and Rt remain stochastic due to minibatch sampling, the batch is sufficiently large for these quantities to provide faithful estimates of the smoothness trend under the same training regime used in the main experiments. 

C Connection to Specific Optimizers: Detailed Derivations 

This general LMO-based framework in (2) unifies several popular optimizers by instantiating different norms ∥ · ∥ 

on the parameter space. Table 1 summarizes the correspondence. 17 Optimizer Norm Dual LMO( g)

normSGD ℓ2 ℓ2 −g/ ∥g∥

signSGD/Lion ℓ∞ ℓ1 −sign( g)

Muon Spectral Nuclear −U V ⊤

Layer-wise max i ∥·∥ (i)

P 

> i

∥·∥ (i),⋆ per-layer  

> Table 1: Norm choice and corresponding optimizers. The update rule (2) recovers each optimizer.

Recall that the linear minimization oracle is 

LMO( g) = arg min 

> ∥t∥=1

⟨g, t ⟩ = − g⋆

∥g⋆∥ ,

where g⋆ is the subgradient of the dual norm ∥g∥⋆ = max ∥t∥=1 ⟨g, t ⟩. Below we derive the LMO for each norm choice. 

Normalized Gradient Descent ( ℓ2 norm). For the Euclidean norm ∥ · ∥ = ∥ · ∥ 2, the dual norm is also ℓ2

(self-duality), and LMO( g) = −g/ ∥g∥2. The update (2) becomes: 

xt+1 = xt − ηt ∇f (xt)

∥∇ f (xt)∥2

,

which is exactly Normalized Gradient Descent [Hazan et al., 2015b; Levy, 2016]. This method decouples the direction from the magnitude, ensuring unit-norm steps. 

signSGD ( ℓ∞ norm). For the ℓ∞ norm ∥t∥∞ = max i |ti|, the dual norm is ℓ1: ∥g∥⋆ = ∥g∥1 = P 

> i

|gi|. The LMO has the closed form: 

LMO( g) = − sign( g),

where sign( g)i = sign( gi) ∈ {− 1, 0, +1 }. The update (2) becomes: 

xt+1 = xt − ηt sign( ∇f (xt)) ,

which is exactly signSGD [Bernstein et al., 2018]. This connection was noted by Bernstein and Newhouse [2024], who observed that signSGD is steepest descent under the ℓ∞ norm. 

Lion ( ℓ∞ norm with momentum). Lion [Chen et al., 2023] extends signSGD by incorporating momentum: 

mt+1 = β2mt + (1 − β2)∇f (xt), xt+1 = xt − ηt sign( β1mt + (1 − β1)∇f (xt)) .

Since the update direction is still given by the sign function, Lion corresponds to the ℓ∞ norm geometry with a momentum-averaged gradient. This optimizer was discovered via symbolic search and has demonstrated strong performance across vision, language, and diffusion models while being more memory-efficient than Adam. 

Muon (spectral norm on matrices). For matrix-valued parameters W ∈ Rm×n, the spectral norm is ∥W ∥op =

σmax (W ) (largest singular value). Its dual is the nuclear norm ∥G∥⋆ = P 

> i

σi(G) (sum of singular values). The LMO is the orthogonalized gradient: 

LMO( G) = −U V ⊤,

where G = U ΣV ⊤ is the SVD. The update (2) becomes: 

W t+1 = W t − ηt Ortho( ∇f (W t)) ,

where Ortho (G) = U V ⊤ is the nearest (semi-)orthogonal matrix to G. This is exactly the Muon optimizer [Jordan et al., 2024], which uses Newton-Schulz iterations to efficiently approximate the orthogonalization. Recent work has scaled Muon to large language models [Liu et al., 2025a]. 18 Layer-wise optimizers (supremum norm over layers). In practice, modern neural networks consist of L

layers with parameters (W1, . . . , W L), where each layer may have different dimensions and structure. A natural approach is to apply different norms to different layers and combine them via the supremum: 

∥(W1, . . . , W L)∥ := max  

> i=1 ,...,L

∥Wi∥(i),

where ∥ · ∥ (i) denotes the norm chosen for the i-th layer. The dual norm is then 

∥(G1, . . . , G L)∥⋆ =

> L

X

> i=1

∥Gi∥(i),⋆ ,

and the LMO decomposes layer-wise: 

LMO( G1, . . . , G L) =  LMO (i)(Gi)Li=1 ,

where LMO (i) is the LMO corresponding to the norm ∥ · ∥ (i).This framework allows combining different optimizers for different layers—for example, using Muon (spectral norm) for large attention matrices, signSGD/Lion ( ℓ∞ norm) for embedding layers, and Normalized GD ( ℓ2 norm) for small dense layers. Such hybrid strategies are commonly used in practice [Jordan et al., 2024; Liu et al., 2025a]. 

D Details of the Adaptive Scheduler from Section 5 D.1 Closed-form expressions for κ

Recall that our learning-rate matching objective uses a Gaussian weight 

˜∆ ∼ N  ∆′; σ2 

> F

/κ ,

where κ controls the scale of the effective step in Frobenius norm. Formally, 

κ := sup 

> ∥u∥=1

∥u∥2 

> F

,

where ∥ · ∥ is the norm used inside the LMO update and ∥ · ∥ F is the Frobenius norm. In our implementation, parameters are grouped by layers, and the global optimization norm is defined as the maximum over layers of the per-layer norm. Consider a model consisting of layers indexed by ℓ = 1 , . . . , L , where the ℓ-th weight matrix has dimensions mℓ × nℓ. Under the max-over-layers geometry used throughout, the closed-form expressions for κ are: 

• Muon (spectral norm). Each layer uses the spectral norm ∥ · ∥ op and its dual, the nuclear norm ∥ · ∥ ∗. For matrices with operator norm bounded by 1,

sup 

> ∥A∥op =1

∥A∥2 

> F

= rank( A) = min( mℓ, n ℓ),

and the nuclear norm satisfies ∥A∥∗ ≤ rank (A)∥A∥op . With a max-over-layers constraint, the total worst-case Frobenius contribution sums across layers: 

κMuon =

> L

X

> ℓ=1

min( mℓ, n ℓ).

• Lion / Sign-type (entrywise ℓ∞ norm). Here the layerwise optimization norm is ∥ · ∥ ∞, and the dual is 

∥ · ∥ 1. For a matrix with dℓ = mℓnℓ entries, 

sup 

> ∥A∥∞=1

∥A∥2 

> F

= dℓ, ∥A∥1 ≤ dℓ∥A∥∞.

Accumulating across layers yields 

κLion =

> L

X

> ℓ=1

mℓnℓ.

19 • Normalized SGD (Frobenius norm). In this case, the per-layer norm is already the Frobenius norm, and the global norm is 

∥W ∥ = max  

> ℓ

∥Wℓ∥F .

The corresponding dual norm aggregates Frobenius contributions over layers, giving 

sup   

> max ℓ∥Uℓ∥F=1
> L

X

> ℓ=1

∥Uℓ∥2 

> F

= L. 

Thus, 

κnormSGD = L. 

Intuitively, κ measures the worst-case amplification of the effective step when mapping from the LMO geometry to Frobenius scale. The differences above reflect the fundamental geometry of the optimizers: spectral structure for Muon, elementwise bounds for Lion, and layerwise normalization for normalized SGD. These forms of κ are used in all our experiments (see Section 6). 

D.2 Closed-form solution for K0, K 1, K 2 and selection of ∆′

We work with the practical schedule 

η(∆) = ∆

K0 + K1∆ + K2∆2 ,

with three unknown coefficients K0, K 1, K 2 and three constraints: (i) η′(∆ ′) = 0 (unique critical point at ∆′), (ii) η(∆ ′) = lr (peak learning rate), (iii) η(∆ 0) = lr /div (initial warmup value). 

Step 1: critical point. Differentiating η(∆) ,

η′(∆) = (K0 + K1∆ + K2∆2) − ∆( K1 + 2 K2∆) (K0 + K1∆ + K2∆2)2 = K0 − K2∆2

(K0 + K1∆ + K2∆2)2 .

Setting η′(∆ ′) = 0 yields 

K0 = K2(∆ ′)2. (11) 

Step 2: value constraints. Condition (ii) gives 

∆′

K0 + K1∆′ + K2(∆ ′)2 = lr .

Using (11), 

∆′

2K2(∆ ′)2 + K1∆′ = lr =⇒ 1 = lr(2 K2∆′ + K1).

Solving for K1:

K1 = 1lr − 2K2∆′. (12) Condition (iii) gives 

∆0

K0 + K1∆0 + K2(∆ 0)2 = lr div .

Substituting (11), 

∆0

K2(∆ ′)2 + K1∆0 + K2(∆ 0)2 = lr div . (13) 20 Step 3: solving for K2, K 1, K 0. Insert (12) into (13) and rearrange to obtain 

(div − 1)∆ 0 = lr K2(∆ 0 − ∆′)2.

Thus 

K2 = ∆0(div − 1) lr(∆ 0 − ∆′)2 . (14) Then 

K0 = K2(∆ ′)2 = ∆0(∆ ′)2(div − 1) lr(∆ 0 − ∆′)2 . (15) Finally use (12) and (14) to obtain 

K1 = (∆ 0)2 − 2∆ 0∆′div + (∆ ′)2

lr(∆ 0 − ∆′)2 . (16) It is immediate to verify that (11), (16), (14) jointly satisfy (i)–(iii). 

Step 4: selecting ∆′. The coefficients above hold for any ∆′ with ∆′̸ = ∆ 0, leaving ∆′ as a free scalar. We choose it by minimizing the MSE between the schedule and a target warmup+decay curve: 

∆′ = arg min 

> ∆>0

n

E ˜∆∼N (∆; σ2 

> F/κ )

(η( ˜∆) − ηtrgt ( ˜∆)) 2o

,

In practice, we evaluate 1000 candidate values of ∆′, form (K0, K 1, K 2) using (14) –(16) , enforce constraints numerically, and choose the minimizer. This is done once at initialization and incurs negligible runtime overhead. 

E Connection (ρ, K 0, K 1, K ρ)-smoothness with (ρ, L 0, L ρ)-smoothness 

The following Lemma 5 connects the standard (ρ, L 0, L ρ)-smoothness (Assumption 6) with the (ρ, K 0, K 1, K ρ)-smoothness (Assumption 2). 

Assumption 6 

The function f : X → R is (ρ, L 0, L ρ)-smooth, i.e., there exist L0, L ρ ≥ 0, ρ > 0 such that 

∥∇ f (x) − ∇ f (y)∥⋆ ≤ (L0 + Lρ∥∇ f (x)∥ρ⋆)∥x − y∥

for all x, y ∈ X .

Lemma 5. If the function f is (ρ, L 0, L ρ)-smooth with 0 < ρ < 2, then it is 

 ρ 

> 2−ρ

, K 0, K 1, K ρ



-smooth. Proof. It is easy to show that if f is (ρ, L 0, L ρ)-smooth, then for all x, y ∈ X ,

f (y) ≤ f (x) + ⟨∇ f (x), y − x⟩ + L0 + Lρ∥∇ f (x)∥ρ⋆

2 ∥y − x∥2.

By taking y = x + ∥∇ f (x)∥⋆  

> L0+Lρ∥∇ f(x)∥ρ⋆

LMO (∇f (x)) , we have f ⋆ ≤ f (y) ≤ f (x) − ∥∇ f (x)∥2  

> ⋆
> 2L0+2 Lρ∥∇ f(x)∥ρ⋆

. Denoting g := 

∥∇ f (x)∥⋆ and ∆ := f (x) − f ⋆, this gives ∆ ≥ g2 

> 2( L0+Lρgρ)

.We now upper bound gρ in terms of ∆. If gρ ≤ L0 

> Lρ

, then trivially gρ ≤ L0 

> Lρ

. Otherwise, L0 + Lρgρ ≤ 2Lρgρ, hence 

∆ ≥ g2−ρ 

> 4Lρ

. Since 0 < ρ < 2, raising both sides to the power ρ 

> 2−ρ

gives gρ ≤ (4 Lρ∆) ρ 

> 2−ρ

. Combining both cases via 

max {a, b } ≤ a + b:

gρ ≤ L0

Lρ

+ (4 Lρ∆) ρ 

> 2−ρ

, so L0 + Lρgρ ≤ 2L0 + Lρ(4 Lρ) ρ 

> 2−ρ

∆ ρ 

> 2−ρ

.

Plugging this into Assumption 6 yields Assumption 2 with exponent ρ 

> 2−ρ

, constants K0 := 2 L0, K1 := 0 , and 

Kρ := Lρ(4 Lρ) ρ 

> 2−ρ

.21 F Proofs 

Corollary 1 

If f is differentiable and star-convex (Assumption 1), then for all x ∈ X :

f (x) − f ⋆ ≤ ⟨∇ f (x), x − x⋆⟩.

Proof. From star-convexity, f (x + β(x⋆ − x)) − f (x) ≤ − β(f (x) − f ⋆) for β ∈ [0 , 1] . Dividing by β and taking 

β → 0+ yields the directional derivative ⟨∇ f (x), x ⋆ − x⟩ ≤ − (f (x) − f ⋆).

F.1 Proof of Theorem 1 

Proof. Denote Kt := K0 + K1∆t + Kρ(∆ t)ρ. By Assumption 2, 

∆t+1 ≤ ∆t + ⟨∇ f (xt), x t+1 − xt⟩ + Kt

2 ∥xt+1 − xt∥2.

Since 

⟨∇ f (xt), x t+1 − xt⟩ = ηt⟨∇ f (xt), LMO( ∇f (xt)) ⟩ = −ηt∥∇ f (xt)∥⋆,

then 

∆t+1 ≤ ∆t − ηt∥∇ f (xt)∥⋆ + Kt

2 (ηt)2.

By Corollary 1 and the boundedness of the iterates xt, we have ∆t = f (xt)−f ⋆ ≤ ⟨∇ f (xt), x t −x⋆⟩ ≤ ∥∇ f (xt)∥⋆∥xt −

x⋆∥ ≤ D∥∇ f (xt)∥⋆, where D > 0. Therefore, 

∆t+1 ≤ ∆t − ∆t

D ηt + Kt

2 (ηt)2.

The right hand side is minimized by setting ηt = ∆t 

> DKt

, thus ∆t+1 ≤ ∆t and 

∆t+1 ≤ ∆t − (∆ t)2

2D2Kt ≤ ∆t − ∆t∆t+1 

2D2Kt .

Telescoping the sum after dividing both sides by ∆t∆t+1 yields PT −1  

> t=0 12D2Kt

≤ 1∆T − 1∆0 . By the Cauchy-Schwarz inequality, PT −1  

> t=0 12D2Kt

≥ T 2   

> PT−1
> t=0 2D2Kt

, which gives T 2   

> PT−1
> t=0 2D2Kt

≤ 1∆T − 1∆0 ≤ 1∆T . Therefore, 

∆T ≤ 2D2 PT −1 

> t=0

Kt

T 2 .

It is easy to show for ρ > 1 that since ∆t is non-increasing, then the learning rate ηt = ∆t 

> DKt

is non-decreasing for 

∆t ≥

 K0

> (ρ−1) Kρ

 1

> ρ

(warm-up stage) and non-increasing thereafter (decay stage). 

F.2 Proof of Theorem 2 

Proof. Denote Kt := K0 + K1∆t + Kρ(∆ t)ρ. By Assumption 2, 

∆t+1 ≤ ∆t + ⟨∇ f (xt), x t+1 − xt⟩ + Kt

2 ∥xt+1 − xt∥2.

Note that g(u) = u 

> 8( K0+K1u+Kρuρ)

, u > 0, ρ > 1, is maximized at u =

 K0

> Kρ(ρ−1)

 1

> ρ

with a maximum value of 



8



ρ

 K0

> ρ−1

 ρ−1

> ρ

K 

> 1
> ρ
> ρ

+ K1

 −1

, so 0 < λη t ≤ 1 for 0 < λ ≤ 1max( ∥x0∥,∥x⋆∥,1/λ max ) , where λmax =



8



ρ

 K0

> ρ−1

 ρ−1

> ρ

K 

> 1
> ρ
> ρ

+ K1

 

22 Since 

∥xt+1 − x⋆∥ = ∥(1 − λη t)xt + ηt LMO( gt) − x⋆∥ = ∥(1 − λη t)( xt − x⋆) + ηt(LMO( gt) − λx ⋆)∥≤ (1 − λη t)∥xt − x⋆∥ + ηt∥ LMO( gt) − λx ⋆∥≤ (1 − λη t)∥xt − x⋆∥ +

 1

λ + ∥x⋆∥



λη t,

then 

∥xt+1 − x⋆∥ − 

 1

λ + ∥x⋆∥



≤ (1 − λη t)



∥xt − x⋆∥ − 

 1

λ + ∥x⋆∥

 

≤ · · · ≤ ∥ x0 − x⋆∥ − 

 1

λ + ∥x⋆∥



,

and ∥xt+1 − x⋆∥ ≤ ∥ x0 − x⋆∥. Similarly, ∥xt+1 ∥ ≤ ∥ x0∥.Based on these inequalities and the condition on λ, we obtain ∥xt+1 − xt∥ ≤ 2ηt and ∥xt − x∥ ≤ 2ηt, where 

x := (1 − λη t)xt + λη tx⋆.Therefore, 

∆t+1 ≤ ∆t + ⟨∇ f (xt), x t+1 − xt⟩ + 2 Kt(ηt)2.

Let us construct an upper bound for ⟨∇ f (xt), x t+1 − xt⟩:

⟨∇ f (xt), x t+1 − xt⟩ = ⟨∇ f (xt), x − xt⟩ + ηt⟨∇ f (xt), LMO( ∇f (xt)) − λx ⋆⟩≤ ⟨∇ f (xt), x − xt⟩ − ηt∥∇ f (xt)∥⋆ + ηtλ∥x⋆∥∥∇ f (xt)∥⋆

≤ ⟨∇ f (xt), x − xt⟩.

By Assumption 2, 

⟨∇ f (xt), x − xt⟩ ≤ f (x) − f (xt) + Kt

2 ∥xt − x∥2 ≤ f (x) − f (xt) + 2 Kt(ηt)2.

By Assumption 1, 

f (x) − f (xt) = f ((1 − λη t)xt + λη tx⋆) − f (xt) ≤ − λη t∆t.

Combining these: 

⟨∇ f (xt), x t+1 − xt⟩ ≤ − λη t∆t + 2 Kt(ηt)2,

and thus 

∆t+1 ≤ ∆t − λη t∆t + 4 Kt(ηt)2.

The right hand side is minimized by setting ηt = λ∆t 

> 8Kt

, thus ∆t+1 ≤ ∆t and 

∆t+1 ≤ ∆t − λ2(∆ t)2

16 Kt ≤ ∆t − λ2∆t∆t+1 

16 Kt .

The rest of the proof is straightforward. 

F.3 Proof of Theorem 3 

Proof. Denote Ktξ := K0 + K1∆tξ + Kρ(∆ tξ)ρ and ¯K := K0 + K1M + KρM ρ. By the update rule, 

∥xt+1 − x⋆∥2 = ∥xt − x⋆∥2 − 2ηt

∥∇ fξt (xt)∥ ⟨∇ fξt (xt), x t − x⋆⟩ + ( ηt)2

Using Corollary 1 and Assumptions 4, 5, we have 

∥xt+1 − x⋆∥2 ≤ ∥ xt − x⋆∥2 − 2ηt∆tξ

∥∇ fξt (xt)∥ + ( ηt)2 ≤ ∥ xt − x⋆∥2 − 2ηt∆tξ

Ktξ∥xt − x⋆∥ + ( ηt)2.

23 One can verify that ∥x1 − x⋆∥ ≤ ∥ x0 − x⋆∥ =: D for t = 0 almost surely. By induction, ∥xt − x⋆∥ ≤ D for all t ≥ 0

almost surely. Consequently, 

∥xt+1 − x⋆∥2 ≤ ∥ xt − x⋆∥2 − 2ηt∆tξ

D · K tξ

+ ( ηt)2 = ∥xt − x⋆∥2 − (∆ tξ)2

D2(Ktξ)2 .

After taking expectation and summing from t = 0 to T − 1, we obtain 

E∥xT − x⋆∥2 ≤ D2 − 1

D2 

> T−1

X

> t=0

E

" (∆ tξ)2

(Ktξ)2

#

.

Since E[∥xT − x⋆∥2] ≥ 0, we have PT −1 

> t=0

E[(∆ tξ)2/(Ktξ)2] ≤ D4.We use the inequality: for real X and positive Y with finite expectations, E[X2/Y ] ≥ (E[X]) 2/E[Y ]. By applying this with X = ∆ tξ and Y = ( Ktξ)2, we have 

E

" (∆ tξ)2

(Ktξ)2

#

≥ (E[∆ tξ]) 2

E[( Ktξ)2] .

Combining with the Cauchy-Schwarz inequality gives 

 P T −1 

> t=0

E[∆ tξ]2

EP T −1 

> t=0

(Ktξ)2 ≤ D4.

Dividing by T 2 and taking square root yields the general bound: 

1

T 

> T−1

X

> t=0

E[∆ tξ] ≤ D2

T

vuutE

hT −1X

> t=0

(Ktξ)2

i

. (17) If ∆tξ ≤ M a.s. for all t, then (Ktξ)2 ≤ ¯K2, and thus 

1

T 

> T−1

X

> t=0

E[∆ tξ] ≤ D2 ¯K√T = O(T −1/2).

24 G Experimental Details G.1 Model Architecture 

All experiments were conducted on NVIDIA H200 GPUs with 140GB memory. Table 2 describes the Llama-based architecture used in our experiments. With nlayer = 12 , this configuration yields a 124M parameter model. Increasing 

nlayer to 24 produces a 210M parameter model.  

> Table 2: Model architecture hyperparameters. The configuration yields a 124M parameter Llama model.

Parameter Value Description 

nlayer 

12 (for 124M model) Number of transformer layers 24 (for 210M model) 

nembd 768 Embedding dimension 

nhead 12 Number of attention heads Vocabulary size 50257 Size of tokenizer vocabulary 

G.2 Assumption Validation Experiments 

Table 3 lists the training hyperparameters used for validating the generalized smoothness assumption (Section 3.1). The parameters are identical across all three optimizers (signSGD, Muon, normalized SGD) except for the learning rate and the number of training iterations, which are shown in separate columns.  

> Table 3: Training hyperparameters for assumption validation experiments on FineWeb dataset for Figures 1 from Section 3.1 and Figure 7 from Appendix B.

Parameter signSGD Muon normSGD 

Learning rate ( lr ) 10 −4 10 −4 10 −3

Training iterations 10k 15k 10k Batch size 64 Sequence length 512 Gradient accumulation steps 1Warmup steps 1000 Decay scheduler Cosine Gradient clipping 0.5 Weight decay 0.1 Momentum 0.9 Divisor ( div ) 100 Target loss f ⋆ 3.2 Smoothness exponent ρ 2The common hyperparameters follow the setup from [Semenov et al., 2025]. The parameters are identical across all three optimizers (signSGD, Muon, normalized SGD) except for the learning rate and the number of training iterations. The learning rates are chosen to ensure comparable training dynamics across optimizers: signSGD use lr = 10 −4, while normalized SGD requires lr = 10 −3 due to its smaller effective step size in Frobenius norm (see Section 5 for a detailed discussion of geometry-dependent scaling). Muon uses lr = 10 −4 to carefully capture the smoothness landscape. The number of training iterations is adjusted to capture sufficient data for fitting the 25 smoothness ratio: Muon requires 15k iterations to reach lower loss values (since it uses quite low lr ), while signSGD and normalized SGD converge faster and use 10k iterations. 

G.3 Main Experiments Hyperparameters 

Table 4: Training hyperparameters for main experiments on FineWeb dataset for Figures 3 and Figures 5 from Section 6. 

Optimizer Parameter 124M, BS=256 210M, BS=256 124M, BS=32 210M, BS=32 

Muon Learning rate 2 × 10 −3 3 × 10 −3 10 −3 10 −3

Target loss f ⋆ 3.3 (1B train tokens), 3.2 (2.1 train tokens) Momentum ( β1) 0.8 Divisor ( div ) 100 Lion Learning rate 10 −3

Target loss f ⋆ 3.4 (1B train tokens), 3.3 (2.1 train tokens) Momentum ( β1) 0.9 Second momentum ( β2) 0.99 Divisor ( div ) 100 normSGD Learning rate {10 −4, 10 −3, 10 −2, 10 −1}

Target loss f ⋆ 4.3 (1B train tokens), 4.1 (2.1 train tokens) Momentum {0.8, 0.9, 0.95 , 0.99 }

Divisor ( div ) {2, 5, 10 , 100 } {2, 5, 10 , 100 }

Common hyperparameters (all optimizers) 

Sequence length 512 Gradient accumulation 1Decay scheduler Cosine Gradient clipping 0.5 Weight decay 0.1 Dropout 0.0 Frobenius variance σ2 

> F

10 3

For Muon and Lion optimizers, we use the hyperparameters from [Semenov et al., 2025] without additional tuning. For normalized SGD (normSGD), we perform a hyperparameter sweep over learning rate, momentum, and divisor. The best configurations are highlighted in bold. Interestingly, we observe that the optimal divisor depends on the batch size: div = 5 performs best for bs = 256 , while div = 2 is optimal for bs = 32 .26