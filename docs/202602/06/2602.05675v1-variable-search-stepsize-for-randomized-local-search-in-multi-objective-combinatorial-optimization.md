# Variable Search Stepsize for Randomized Local Search in Multi-Objective Combinatorial Optimization
# 多目标组合优化中随机局部搜索的可变搜索步长

**Authors**: Xuepeng Ren, Maocai Wang, Guangming Dai, Zimin Liang, Qianrong Liu, Shengxiang Yang, Miqing Li \\
**Date**: 2026-02-05 \\
**PDF**: https://arxiv.org/pdf/2602.05675v1 \\
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:LNS</span> <span class="tag-label tag-pink">keyword:EOH</span> \\
**Score**: 8.0 \\
**Evidence**: discusses randomized local search and neighborhood variation in combinatorial optimization \\

---

## Abstract
Over the past two decades, research in evolutionary multi-objective optimization has predominantly focused on continuous domains, with comparatively limited attention given to multi-objective combinatorial optimization problems (MOCOPs). Combinatorial problems differ significantly from continuous ones in terms of problem structure and landscape. Recent studies have shown that on MOCOPs multi-objective evolutionary algorithms (MOEAs) can even be outperformed by simple randomised local search. Starting with a randomly sampled solution in search space, randomised local search iteratively draws a random solution (from an archive) to perform local variation within its neighbourhood. However, in most existing methods, the local variation relies on a fixed neighbourhood, which limits exploration and makes the search easy to get trapped in local optima. In this paper, we present a simple yet effective local search method, called variable stepsize randomized local search (VS-RLS), which adjusts the stepsize during the search. VS-RLS transitions gradually from a broad, exploratory search in the early phases to a more focused, fine-grained search as the search progresses. We demonstrate the effectiveness and generalizability of VS-RLS through extensive evaluations against local search and MOEAs methods on diverse MOCOPs.

## 摘要
在过去的二十年里，演化多目标优化的

---

## 论文详细总结（自动生成）

这篇论文提出了一种名为**可变步长随机局部搜索（Variable Stepsize Randomized Local Search, VS-RLS）**的方法，旨在解决多目标组合优化问题（MOCOPs）中传统算法容易陷入局部最优的问题。

以下是对该论文的结构化深入总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **研究背景**：过去二十年，多目标优化研究多集中在连续域，而对组合优化（MOCOPs，如背包问题、旅行商问题）关注较少。
*   **核心矛盾**：虽然主流的多目标演化算法（MOEAs，如 NSGA-II）在连续问题上表现优异，但在处理离散的 MOCOPs 时，往往不如简单的随机局部搜索（RLS）。
*   **现有缺陷**：现有的随机局部搜索方法通常采用**固定邻域结构**（即固定步长）。步长过小容易陷入局部最优，步长过大则会导致搜索过于粗糙，难以实现精细化改进。
*   **研究目的**：通过引入动态调整的搜索步长，平衡算法的探索（Exploration）与利用（Exploitation）能力。

### 2. 方法论：核心思想与算法流程
VS-RLS 的核心思想是**在搜索过程中动态调整邻域扩展的范围**，其方法论包含以下关键点：

*   **核心机制（可变步长）**：在每一次迭代中，算法从步长为 1（最小邻域）开始采样。如果没有找到能改进当前解集（即不被存档集支配）的新解，则逐渐增加步长（扩大邻域范围），直到找到合适解或达到当前阶段的步长上限。
*   **两阶段策略**：
    1.  **探索阶段（Phase 1）**：在搜索初期，设置较大的步长上限（通常设为决策空间维度 $D$）。这允许算法进行大范围跳转，以跳出局部最优并探索未知的潜力区域。
    2.  **利用阶段（Phase 2）**：在搜索后期（达到预设迭代次数 $T_{vl}$ 后），将步长上限缩小为一个较小的常数 $V_C$（如 3）。此时算法专注于在已发现的潜力区域进行精细化微调。
*   **算法流程**：
    1.  随机初始化一个解并存入存档集（Archive）。
    2.  从存档集中随机选择一个父代。
    3.  **内层循环**：从步长 $N_{cb}=1$ 开始，在邻域内随机采样生成子代。
    4.  **判断与更新**：若子代不被存档集支配，则更新存档集并结束本次迭代；否则增加步长 $N_{cb}$，重复采样直至达到上限 $V_L$。

### 3. 实验设计
*   **测试问题（MOCOPs）**：
    *   多目标 0-1 背包问题（Knapsack）
    *   多目标旅行商问题（TSP）
    *   多目标二次指派问题（QAP）
    *   多目标 NK-Landscape 问题
*   **对比算法（Benchmark）**：
    *   **局部搜索类**：SEMO（固定步长的随机局部搜索）、PLS（帕累托局部搜索）。
    *   **演化算法类**：NSGA-II、MOEA/D、SMS-EMOA。
    *   **基准线**：RS（随机采样）。
*   **评价指标**：超体积指标（Hypervolume, HV），用于综合衡量解集的收敛性和多样性。

### 4. 资源与算力
*   **算力说明**：论文中**未明确说明**具体的硬件配置（如 GPU/CPU 型号）。
*   **计算开销**：实验通过控制“函数评估次数”（Function Evaluations）来公平比较不同算法。评估规模从 $1 \times 10^5$ 到 $5 \times 10^6$ 不等。
*   **代码开放**：作者提供了 GitHub 链接以供复现。

### 5. 实验数量与充分性
*   **实验规模**：
    *   针对 4 种不同类型的经典 MOCOPs 进行了测试。
    *   考虑了**扩展性**：测试了不同维度（如背包问题从 100 到 1000 维）。
    *   考虑了**计算预算**：测试了不同评估次数对算法性能的影响。
    *   **消融实验**：对关键参数 $V_C$（后期步长）和 $T_{vl}$（切换点）进行了灵敏度分析。
*   **充分性评价**：每组实验独立运行 30 次并进行了 Wilcoxon 秩和检验（显著性水平 0.05）。实验覆盖了不同景观特征的问题，数据量充足，对比维度全面，具有较高的客观性。

### 6. 主要结论与发现
*   **整体优越性**：VS-RLS 在大多数测试实例上的 HV 值显著优于对比的 MOEAs 和固定步长的局部搜索方法。
*   **多样性优势**：VS-RLS 在保持解的多样性方面表现尤为突出，尤其是在高维背包问题中。
*   **动态适应性**：在 QAP 等复杂景观中，VS-RLS 能通过扩大步长逃离停滞；在 NK-Landscape 等平滑景观中，则能通过缩小步长稳步改进。
*   **预算敏感性**：VS-RLS 在评估预算充足时表现极强，但在预算极低（如 $1 \times