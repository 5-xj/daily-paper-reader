Title: Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents

URL Source: https://arxiv.org/pdf/2602.05810v1

Published Time: Fri, 06 Feb 2026 02:10:29 GMT

Number of Pages: 39

Markdown Content:
# BIFROST : S TEERING STRATEGIC TRAJECTORIES TO BRIDGE 

# CONTEXTUAL GAPS FOR SELF -I MPROVING AGENTS 

Quan M. Tran 1, Zhuo Huang 1, Wenbin Zhang 2, Bo Han 3,4,Koji Yatani 5, Masashi Sugiyama 4,5 Tongliang Liu 1

> 1

Sydney AI Centre, The University of Sydney; 2Florida International University; 

> 3

Hong Kong Baptist University; 4RIKEN Center for Advanced Intelligence Project; 5The University of Tokyo 

ABSTRACT 

Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts. 

Keywords Self-Improving Agents · In-Context Learning · Representation Engineering 

1 Introduction 

Autonomous agents have fundamentally transformed fields like robotics [ 1, 2, 3] and autonomous driving [ 4 , 5]by harnessing the power of self-improvement mechanisms [ 6, 7 , 8 , 9, 10 , 11 ]. By integrating systemic reflection and autonomous error correction, these agents iteratively evolve their decision-making to master increasingly high-dimensional tasks [ 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 ]. To optimize this evolutionary process, emerging research focuses on the strategic reuse of successful task trajectories as in-context demonstrations [ 12 , 20 , 15 , 21 ]. This paradigm allows agents to capitalize on historical successes [ 22 , 23 , 16 , 24 ], significantly enhancing efficiency while reducing the computational burden of iterative self-reflection [25]. Despite the promise of trajectory reuse, existing self-improvement frameworks remain suboptimal when faced with contextual non-stationarity across tasks. Current approaches often treat past trajectories as context-specific artifacts [ 26 , 27 , 24 , 13 , 28 ], either discarding them to rely on computationally intensive fine-tuning [ 29 , 30 , 31 , 32 , 33 , 34 ]or attempting to repurpose them via static heuristics [ 35 , 36 , 37 , 38 , 24 , 39 ]. However, because these trajectories are inherently coupled with their original task environments, such a direct calibration leads to significant contextual misalignment [ 40 , 23 , 41 , 42 ]. This mismatch not only degrades the generalization capabilities but can also actively hinder their performance when applied to novel domains [ 38 , 43 , 44 ]. Consequently, the fundamental principle of cross-context trajectory adaptation remains a critical and largely unexplored frontier. To address this challenge, we uncover a fundamental context-trajectory correlation within the agentic representation space. We observe that shifts in task context are strikingly parallel to shifts in the corresponding successful trajectories when projected into the latent manifold. Leveraging this geometric insight, we introduce BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free framework that utilizes context differentials to precisely steer historical trajectories toward the target task requirements. By modeling the transition between past and current tasks 

> arXiv:2602.05810v1 [cs.LG] 5 Feb 2026

Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents Contextual Hidden State Representation 

> Latent concept approximated by
> previous in-context examples
> target query
> Shifted latent concept
> to the new context
> previous query
> Trajectory Level
> context-shift concept
> Agent with previous
> in-context examples
> Steered agent adapted
> to new context

Figure 1: Illustration of Bifrost: It identifies the context-shift concept between previous and target tasks, which contains essential knowledge for bridging the context gap. Further, by steering along the context shift direction, Bifrost helps find an optimal strategy that is effective under the target context. through hidden-state representations, Bifrost effectively aligns prior experiences with the target context. This allows the agent to utilize past trajectories as highly accurate in-context examples, achieving seamless adaptation without the need for parameter updates or environment-specific fine-tuning. In practice, Bifrost identifies a precise steering vector within the representation space that maps historical trajectories onto the manifold of the target task. Unlike existing heuristics that rely on untargeted or global calibration, Bifrost uses the target context as a dynamic anchor to selectively suppress context-specific artifacts while preserving high-utility, transferable knowledge. This targeted transformation ensures that the agent can exploit previously solved trajectories even across substantial domain gaps. Consequently, Bifrost significantly enhances cross-context generalization of the model, allowing agents to reap the full rewards of self-improvement without the performance degradation typically caused by environment shifts. Across diverse benchmarks, Bifrost consistently outperforms trajectory reuse and finetuned self-improvement methods, demonstrating that our proposed method enables self-improving agents to effectively leverage past experiences despite substantial context shifts. Our contributions are: • We reveal the context-trajectory correlation, where context shifts are highly parallel with trajectory shifts. Based on this observation, we propose Bifrost, a training-free method that induces context differences to guide agents leveraging previous trajectories as in-context examples toward a target task. • We provide theoretical analyses, showing that Bifrost can generalize experiences learned from prior trajectories to adapt toward a target task. • We conduct experiments on diverse benchmarks, including question answering, math reasoning, and code generation, with different model sizes. Our experiments show that Bifrost consistently outperforms other popular baselines and state-of-the-art methods based on trajectory reuse or finetuning. The rest of the paper is organized as follows. In Sec. 2, we demonstrate our methodology by revealing the context-trajectory correlation and presenting Bifrost. In Sec. 3, we conduct a theoretical analysis showing that Bifrost enables agents to generalize from prior trajectories to adapt to target tasks. Next, we conduct experiments and further analysis on Sec. 4, showing that Bifrost outperforms baseline and recent self-improving agent approaches. We present related work in Sec. A. Finally, we conclude our work in Sec. 5. 2Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

2 Methodology 

Our work focuses on how an LLM Agent can effectively leverage its successful trajectories solved in the past to adapt to a target task under a substantial context shift. Let M be a LLM that solved tasks under a previous context where successful trajectories are preserved as C =

{(qi, a i)}ki=1 . Here, each pair (qi, a i) denotes a trajectory consisting of a query and its corresponding reasoning process leading to the answer. Our objective is to find an optimal reasoning strategy by leveraging previous trajectories that maximizes the log likelihood of the LLM agent on the target task ˆC = {(ˆ qi, ˆai)}ni=1 ,

S∗ = arg max  

> S

E(ˆ q, ˆa)∈ ˆC [log MS (ˆ a | ˆq)] , (1) where MS denotes that the model M reasons following the reasoning strategy S. Although the strategy S succeeded under the previous context C, it might not align with the target context ˆC. Therefore, instead of fully fine-tuning the agent or naively reusing previous strategies, we aim to bridge the contextual gap between previous tasks and any target tasks by proposing Bifrost, which can improvise under any target contexts by properly steering the trajectory of an existing strategy. As a result, Bifrost improves in-context learning via an optimal reasoning strategy tailored for the target context, as shown in Fig. 1. To demonstrate Bifrost, we first reveal our findings of context-trajectory correlation in Sec. 2.1. Further, based on this finding, we perform trajectory steering to adapt previous trajectories to the target task in Sec. 2.2. 

2.1 Context-Trajectory Correlation 

To conduct proper reasoning under a target context, it is vital to employ an effective trajectory. Therefore, we investigate the correlation between trajectories and context under task shift. To accomplish this, we follow Linear Representation Hypothesis [45] by regarding LLMs as an embedding-unembedding process. Particularly, the embedding encodes context inputs into hidden states, e.g. , embedding vectors, which contain essential concepts that enable knowledge generalization across tasks. For example, “king” and “queen” are conceptually related to “man” and “woman”, respectively. Formally, for a context text x = ( qi, a i), we assume the hidden state mapped by an LLM is h(x) in a representation space Λ. Further, the model represents each word y in its vocabulary into an unembedding vector g(y) in a separate space Γ with the same dimension. To this end, the probability distribution for next-word prediction is formulated as: 

P (y | x) ∝ exp( h(x)⊤g(y)) . (2) Intuitively, the reasoning process can be explained as two representation spaces that are connected via an inner product, where LLMs aim to find the best match between a context input and a reasoning output. By conducting such a process iteratively, the input-output logic chain formulates a strategic trajectory that is optimized for a certain context. In agentic AI, it is common to apply agents across various tasks, thus encountering a novel context. To ensure the target task is solvable, we have the following assumption: 

Assumption 1. For an LLM agent M capable of solving task A and task B with contexts CA and CB , respectively, the concepts WA = {W Ai }ki=1 and WB = {W Bi }ni=1 extracted from task A and task B must satisfy: 

• The vocabulary space of M covers WA ∪ W B .

• The shared concepts WA ∩ W B are causally separable. 

• For any private concepts W A ∈ W A \ W B and W B ∈ W B \ W A, the embedding-unembedding vectors satisfy the subspace independence h⊤  

> WA

gW B = 0 .

To handle a target context ˆC specific to concept W , the contextual gap can be captured by a steering vector ¯hW scaled by α in the hidden states: 

ˆh = h + α¯hW . (3) Considering a binary-value concept, e.g. , man and woman, the probability of the output Y taking value Y (1) versus its counterfactual Y (0) is measured by the logit. According to the measurement representation theorem in Park et al. [ 45 ], such a logit is linear with respect to h:logit P (Y = Y (1) | h; Y ∈ { Y (0) , Y (1) }) = βh⊤ ¯gW , (4) where ¯gW is the unembedding representation (the output direction) of the task and β > 0 is a function of {Y (0) , Y (1) }.The following theorem shows that output tokens change linearly with the contextual shift. 3Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents Table 1: Hypothesis testing result.           

> α0.1 0.2 0.3 0.4 0.5
> p-value 0.00418 0.00665 0.01085 0.00802 0.02019

Theorem 2. Under the Linear Representation Hypothesis, for a concept W satisfying the conditions in Assumption 1, for an embedding vector ¯hW and unembedding vector ¯gW of W , the change of output logits is a linear function of the magnitude of the context shift β ¯hW .Proof. Given the optimal logit based on the target context hidden state ˆh, we substitute the shift according to Eq. (3): logit P (Y = Y (1) | ¯h) = β(h + α¯hW )⊤ ¯gW (5) 

= βh⊤ ¯gW + α(β ¯h⊤ 

> W

¯gW ). (6) Note that β is fixed under a certain concept, and ¯gW denotes the concept direction based on the model understanding, 

e.g., , “king” to “queen” and “man” to “woman” have the same shifting direction when given a fixed model. Thus the term β¯gW is a constant K. Therefore, we have logit P (Y = Y (1) | ¯h) = logit P (Y = Y (1) | h)+ αK ¯hW . (7) By defining the shift of logit distribution under different context hidden states ∆P = P (Y = Y (1) | ˆh) − P (Y =

Y (1) | h), we have ∆P ∝ αK ¯hW .Intuitively, a trajectory is composed of output tokens, which are a sequence of actions a. This theorem shows that selecting an optimal trajectory based on an existing one can be guided by the context shift. 

Discussion The above results are focused on one concept, which can be generalized to complex tasks containing multiple concepts. For concepts that are not shared across tasks, the subspace independence ensures that the embedding space are irrelavant to the unembedding space. Thus, the corresponding prediction logits are not affected by the context shift. Moreover, for multiple shared concepts, the causal separability ensures that they are not confusing each other. For example, adapting the context “He speaks English” to a target context “She speaks French” must be separated from adapting to another target“He speaks French”. 

Experimental Demonstration We conduct a small-scale experiment to demonstrate the context–trajectory correlation described in Theorem 2. We randomly select 100 context-separated samples from the AQUA and GSM8K datasets, forming 50 paired examples. These pairs are used to compute contextual direction vectors from AQUA to GSM8K. We hypothesize that when an agent solves AQUA tasks, the trajectories steered toward GSM8K exhibit higher similarity to GSM8K trajectories than to those generated by the non-steered model on AQUA. Tab. 1 shows a strong statistical significance over varied values of the control parameter α.

2.2 Improvised Trajectory Steering for Target Context 

Further, we build on the context-trajectory correlation and propose Bifrost, a training-free and context-guided strategy that enables agents to self-improve by employing prior successful task trajectories under substantial context shifts. As demonstrated in Sec. 2.1, trajectories exhibit behavioral adaptation in response to context shift toward a target task. This motivates a trajectory steering of the existing in-context learning mechanism using such trajectories. 

Context-Guided Adaptation under In-Context Learning Given a set of successful task trajectories from previous context C = {(qi, a i)}ki=1 solved by a LLM agent M. Our goal is to solve a target task ˆq ∈ ˆC from a different context by leveraging C as in-context examples and steering the model toward the target context. The steered model then produces the output ˆa = Ms(p), where Ms denotes the contextually steered model, and p = C ◦ ˆq denotes the in-context learning input prompt with C as demonstrations followed by the target question ˆq.

Contextual Representation Steering Let hℓ be the last token hidden state of the original model M, the trajectory steering can be applied across specific layers ℓ ∈ L, yielding the steered model Ms with corresponding hidden state 4Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

Algorithm 1 Bifrost                                                      

> 1: Input: C={(qi, a i)}ki=1 ,ˆC={ˆqj}nj=1 , model M, strength α, layers L.2: Precompute ¯hℓ=1
> k
> Pki=1 hℓ(ci)for all ℓ∈L.3: for each ˆq∈ˆCdo
> 4: p← C ◦ ˆq; obtain unsteered hℓ(p).5: Compute ∆ℓ=hℓ(ˆ q)−¯hℓand steer: hℓ
> s=hℓ+α∆ℓ,∀ℓ∈L.6: Generate ˆa=Ms(p)using steered states. 7: end for
> 8: Output: {ˆaj}nj=1

hℓ 

> s

= hℓ + α∆ℓ, where α ∈ R controls the steering strength. The vector ∆ℓ that represents the contextual direction from the previous trajectory to the target task is computed as 

∆ℓ = hℓ(ˆ q) − 1

k

> k

X

> i=1

hℓ(Ci), (8) where hℓ(ˆ q) is the target question hidden state extracted by M(ˆ q), and hℓ(Ci) extracts trajectory hidden state when the agent solve the corresponding task ai = M(qi).The contextual direction ∆ℓ represents a context-shift concept that appears as the difference between previous trajectory execution and the current task. Averaging the prior trajectory hidden states finds the shared patterns, while the steering model encourages such patterns adaptable to the target task, thereby enabling trajectory reuse under context shift and enhancing agent generalization. 

Implementation Details Algorithm 1 describes our proposed method in details. In practice, we precompute the average of previous trajectory hidden states to ensure computational efficiency. We apply Bifrost to each target task instead of computing a shared steering vector. This guarantees a precise adaptation of prior trajectories toward each task. In our experiments, we steer middle-to-late layers since they represent reasoning patterns optimal for steering, as stated in previous work [ 46 , 47 , 48 ]. Besides subtraction to obtain the steering vector, Bifrost also works well with other projection-based techniques such as principal component analysis (PCA), and Sparse Autoencoder (see Sec. 4.3). 

3 Theoretical Analysis 

We analyze the proposed method from a theoretical perspective, showing that Bifrost effectively leverages past trajectory experiences to adapt toward a target task. Our analysis is based on In-Context Learning (ICL) as Bayesian Inference [22 ], demonstrating that Bifrost preserves prior knowledge induced by previous trajectories while reducing posterior uncertainty toward the target tasks. Furthermore, we exceed the risk bound of Bifrost compared with an ordinary LLM agent that solely solves the target task. We begin by introducing In-Context Learning as Bayesian Inference. Proofs shown in Appendices C.1 and C.2. 

In-Context Learning as Bayesian Inference The LLM agent M generates a prediction for a given input x by implic-itly inferring a latent concept ϕ from a context C∗ represented by in-context examples, M(x) ≈ Eϕ∼p(ϕ|C ∗) [fϕ(x)] ,where p(ϕ|C∗) is the posterior distribution of concepts given a context. The effectiveness of agents stems from the latent concept inferred from the in-context examples. Thus, we focus on modeling the latent concept given previous trajectories. In our case, the context is represented by previous trajectories, C∗ = C. By the definition, the posterior probability of the concept given previous trajectories is defined as 

p(ϕ | C ) ∝ p(C | ϕ)p(ϕ) ∝ exp − 1

k

> k

X

> i=1

L(fϕ(qi), a i)

!

p(ϕ),

where L denotes the loss function. 

Posterior Inference Steering Effect The latent concept distribution can now be approximated by a Gaussian distribution over the context represented by previous context trajectories. Assuming L is convex and L-Lipschitz in fϕ,and fϕ is locally smooth in ϕ, we approximate the posterior of the latent concept via Laplace approximation, 

p(ϕ | C ) ≈ N ( ˜ϕ|μ, Σ−1), (9) 5Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents where N (ϕ|μ, Σ−1) denotes the Gaussian density with mean μ and inverse covariance matrix Σ−1, and Σ = 1 

> σ2

F⊤F,and F : X → Rd is the feature map. When leveraging previous trajectories as in-context examples, the latent concept is additionally conditioned on the target task ˆq ∈ ˆC. By further applying a trajectory steering ∆ ∈ Λ from Bifrost, the concept distribution mean shifts toward an ideal concept of such a target task accordingly. 

Lemma 1. Let ps(ϕ | C , ˆq) be the latent concept posterior probability of the steered model, conditioned on previous context trajectories C and the target task ˆq ∈ ˆC. The contextual shift ∆ shifts the concept distribution mean toward an ideal target task concept distribution by αP (∆) where P is the projection, while preserving uncertainty reduction induced by previous context trajectories, 

ps(ϕ | C , ˆq) ≈ N 

 ˜ϕ + αP (∆) |μ, Σ−1

(10) 

Excess Risk Bound We further analyze the risk bound of Bifrost compared to an ordinary agent that solely solves the target task without providing in-context examples from previous context tasks. Given that the target task shares a common representation with previous trajectories that are transferable, the following theorem shows that the bound of the proposed method reduces linearly with the number of induced previous trajectories. 

Theorem 3. Let Ms be the steered agent by Bifrost, and M be the ordinary agent without leveraging in-context learning. Assuming that the target task aligns with the previous trajectory context, and the contextual shift ∆ aligns with the shift of latent concept posterior probability as shown in Lemma 1. The steered agent by Bifrost satisfies 

E [L(Ms(C ◦ ˆq))] ≤ E [L(M(ˆ q))] − Ω

 1

k



. (11) 

4 Experiments 

We evaluate Bifrost against state-of-the-art baselines, including trajectory calibration, finetuning, and reinforcement learning, across diverse benchmarks. Sec. 4.1 describes the experimental setup under large context shifts. We then report main results showing that Bifrost consistently outperforms existing baselines across task domains in Sec. 4.2. Sec. 4.3 provides additional analyses on cross-domain adaptation, robustness to suboptimal trajectories, and compatibility with other contextual direction extraction methods. Finally, Sec. 4.4 presents an ablation study. Further details and hypothesis testing are deferred to the Appendix. 

4.1 Experimental Setup Datasets We evaluate Bifrost across diverse domains, including mathematical reasoning, question answering, and code generation. To study cross-context self-improvement, an agent first solves tasks on a source dataset and records successful trajectories, which are then provided as in-context examples when solving tasks on a target dataset. We consider source–target dataset pairs: (1) AQUA → GSM8K: AQUA [ 49 ] focuses on algebraic problem solving, while GSM8K [ 50 ] contains diverse grade-school math problems. This setting evaluates whether algebraic reasoning acquired from AQUA transfers to GSM8K. (2) ARC-Easy → GPQA-Diamond: ARC-Easy [ 51 ] consists of elementary science multiple-choice questions, whereas GPQA-Diamond [ 52 ] benchmarks graduate-level, multi-domain scientific reasoning. This pair tests whether foundational science reasoning generalizes to advanced queries. (3) HumanEval →

LiveCodeBench: HumanEval [ 53 ] evaluates code completion, while LiveCodeBench [ 54 ] is a dynamic, contamination-resistant code generation benchmark. It assesses whether code completion skills transfer to challenging programming tasks. 

Baseline Methods and LLMs We evaluate our method with Chain-of-Thought (CoT) [ 55 ], In-Context Learning (ICL) 

[25 ]. We also compare Bifrost with trajectory calibration methods, including Reflexion [26 ], Buffer of Thoughts (BoT) 

[ 27 ], Diversity of Though (DoT) [20 ], finetuning and reinforcement learning methods including RISE [36 ], Paprika 

[37 ]. We leverage the following models, Llama-3.2-1B-Instruct , Llama-3.2-3B-Instruct , and Llama-3.1-8B-Instruct .We conduct the steering in 10 th layer (Llama-1B), 14 th layer (Llama-3B), and 20 th layer (Llama-8B). The steering coefficient is chosen by a grid search α ∈ { 1, 2, 3, 4} (see Sec. D for more details). 

4.2 Main Results Quantitative Analysis Bifrost significantly outperforms all baselines across different domains, as shown in Tabs. 2, 3, and 4. In math problem solving experiments (Tab. 3), our method clearly surpasses from 2% to 3% the best performing 6Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents Table 2: Comparisons on LiveCodeBench code generation adapted from HumanEval trajectories measured by pass rates (%).                                                                                    

> Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Llama-3.1-8B-Instruct Method pass@1 pass@3 pass@5 pass@1 pass@3 pass@5 pass@1 pass@3 pass@5 Base 1.07 2.11 2.84 8.68 12.20 13.98 9.86 12.36 15.75 ICL 1.84 3.63 4.66 10.23 14.00 15.45 12.80 14.74 18.22 Reflexion 0.11 0.11 0.11 1.25 1.59 1.70 6.36 8.07 8.52 BoT 0.27 0.69 1.02 5.70 8.93 9.89 10.41 16.39 18.86 DoT 0.00 0.00 0.00 2.73 2.95 2.95 1.59 1.82 2.05 RISE 0.66 1.72 2.50 4.39 9.02 12.05 7.23 14.26 17.95 Paprika 1.30 2.55 3.30 8.59 12.33 14.20 14.98 20.30 22.61
> Bifrost 4.05 7.74 9.55 13.50 18.93 21.48 17.84 20.98 26.36

Table 3: Comparisons on GSM8K math problem solving, trajectories are adapted from AQUA. The results are mea-sured by solve rate (%). Method Llama-3.2 1B Llama-3.2 3B Llama-3.1 8B CoT 42.15 70.18 74.65 ICL 23.81 62.52 80.78 Reflexion 15.85 62.40 87.41 BoT 9.62 33.74 39.73 DoT 3.71 76.80 76.72 RISE (1 attempt) 14.48 36.92 48.29 RISE (5 attempts) 40.71 78.62 86.13 Paprika 21.08 40.26 55.27    

> Bifrost 44.66 81.41 90.22

Table 4: Comparisons on GPQA-Diamond question an-swering, trajectories are adapted from ARC-Easy. The results are measured by solve rate (%). Method Llama-3.2 1B Llama-3.2 3B Llama-3.1 8B CoT 4.04 12.63 17.68 ICL 23.23 25.25 22.22 Reflexion 1.10 20.20 33.84 BoT 16.67 12.12 11.11 DoT 1.01 13.64 30.81 RISE (1 attempt) 6.06 4.04 3.54 RISE (5 attempts) 21.21 19.19 12.63 Paprika 18.69 20.20 12.12    

> Bifrost 44.44 32.83 37.88

method. In multi-domain science question answering (Tab. 4), we clearly outperform the state-of-the-art methods from 4% to 16%. Similarly, in the code generation task (Tab. 2), we demonstrate a clear outperformance across all functional correctness metrics. Specifically, Bifrost surpasses supervised finetuning and reinforcement learning based methods without any training required, demonstrating that our method is flexible and efficient in leveraging their previous experiences across task contexts. 

Qualitative Analysis An interesting result in Fig. 2 shows that Bifrost successfully generalizes algebra knowledge from AQUA to solve a GSM8K task. While CoT gets stuck in a reasoning loop and RISE produces an unsatisfiable answer, Bifrost effectively demonstrates problem decomposition ability and symbolic manipulation induced from AQUA. The result confirms the success of latent concept transformation from prior in-context examples to adapt toward the target task as theoretically analyzed in Sec. 3. 

Steering Effect Illustration We demonstrate how latent concepts shift across context in Fig. 3. Specifically, we plot the agent hidden states before and after steering by Bifrost, where dashed lines indicate the adaptation direction. We estimate the target context by extracting hidden states when solving the target tasks without any in-context examples. As shown in Fig. 3, the mismatch between previous trajectories and the target task can be observed, which hinders agent generalization when solely relying on conventional ICL or untargeted trajectory calibration methods. Our method addresses this limitation by adapting such trajectories toward the target context effectively. 

4.3 Further Analysis Cross-Domain Context Adaptation We aim to further evaluate Bifrost on cross-domain adaptation by adapting successful GSM8K trajectories toward LiveCodeBench. We expect that latent concepts such as logical reasoning and symbolic manipulation are shared between such domains. Tab. 5 confirms that Bifrost remains effective on cross-domain adaptation, which outperforms the baseline methods. This result suggests that trajectories preserve context-independent patterns that can be adapted across domains. 7Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

Bifrost (algebra knowledge induced, correct answer) 

Let’s denote the number of enemies Emma jumps on as E,the number of berries she collects as B, and the time she has left when she finishes the level as T .Ava jumps on 8 more enemies than Emma, so the number of enemies Ava jumps on is E + 8 .Ava collects 3 more berries than Emma, so the number of berries Ava collects is B + 3 .Ava finishes the level 4 seconds slower than Emma, so the time Ava has left when she finishes the level is T − 4.The score for each player is calculated as follows: Emma’s score = 10 E + 5 B + 30 T

Ava’s score = 10( E + 8) + 5( B + 3) + 30( T − 4) 

Simplifying Ava’s score: Ava’s score = 10 E + 80 + 5 B + 15 + 30 T − 120 

= 10 E + 5 B + 30 T + 25 

Now let’s find the difference between their scores: Difference = Ava’s score - Emma’s score = (10 E + 5 B + 30 T + 25) − (10 E + 5 B + 30 T )

= 25 

Therefore, the difference between their two scores is 25 points. 

CoT (reasoning stuck, incorrect answer) 

First, let’s assign variables to the unknowns. Let e be the number of enemies Emma jumps on, and b be the number of berries she collects. Then, the number of enemies Ava jumps on is 

e + 8 , and the number of berries she collects is b + 3 . Since they finish the level at the same time, the time left on the timer for Emma is the same as for Ava. Therefore, the time left for Emma is 60 − (e + 8) = 52 − e, and for Ava is also 52 − e. The score for Emma is 10 e + 5 b + 30(52 − e), and for Ava is 

10( e + 8) + 5( b + 3) + 30(52 − e − 4) .The difference between their scores is 10 e +5b + 30(52 − e) − [10( e + 8) + 5( b +3) + 30(52 − e − 4)] . Distributing the negative sign gives 10 e + 5 b + 30(52 −

e) − 10 e − 80 − 5b − 30(52 − e − 4) .Distributing the 30 gives 10 e + 5 b + 30 ·

52 − 30 e − 30(52 − e − 4) . Continuing: 

10 e + 5 b + 1560 − 30 e + 30 e + 120 .Combining like terms: 10 e − 30 e + 30 e +5b + 1560 + 120 = . . . 

RISE (5 th attempt) (incorrect answer) 

To find the difference between Ava’s and Emma’s scores, we need to compare the points they receive from their various actions. Since Ava jumps on 8 more enemies and collects 3

more berries than Emma, she scores 8 more points from enemies and 3 more points from berries. However, Ava finishes 4 seconds slower, resulting in a loss of 4x30 = 120 points from the timer. The net gain for Ava is then 

8 + 3 − 120 = -109 points. This means Emma earns 109 more points than Ava. However, the question asks for the difference, so we should answer by dividing the net gain by −1 to repre-sent a net gain for Ava being actually a net loss. Therefore, the answer is −(−109) = 109 

and the correct answer is 109 . but I made another mistake and we have to just give the difference so it is just 109 not making a nega-tive sign so the answer is 109 . I noticed I made another inaccuracy. The correct answer should be 109 Wait this looks wrong then just 

109 − (109) = 0 The correct answer then is 0, I’ll stick with that. The correct answer is 

0 .

Figure 2: Success of latent concept transfer by Bifrost: it correctly solves a GSM8K task with algebra knowledge leveraged from AQUA trajectories, demonstrating problem decomposition and symbolic manipulation. Original 

> Steered by Bifrost
> Target context

(a) GSM8K, α = 3 Original  

> Steered by Bifrost
> Target context

(b) GSM8K, α = 4 Original  

> Steered by Bifrost
> Target context

(c) GPQA, α = 3 Original  

> Steered by Bifrost
> Target context

(d) LCB, α = 1 

Figure 3: Illustration of Latent Concept Shift by Bifrost: It steers prior context trajectories toward the target context of current tasks under agent hidden state representations. 

Number of ICL Examples Variants Fig. 4 demonstrates Bifrost behavior when leveraging different number of in-context examples. When evaluated on GSM8K, leveraging more in-context examples increases the agent effectiveness in general while it causes minor change in GPQA-Diamond. This can be explained as prior trajectories from ARC-Easy belongs to multi-domain science questions that are more challenging to find common transferable patterns to adapt toward GPQA-Diamond. Therefore, the skeptical effectiveness can stem from similarity of trajectory representations to extract and adapt useful patterns. 

Steering Layer Position Effect To understand behavioral change under different layer intervention positions, we conduct trajectory steering across layers. As shown in Fig. 5, agent effectiveness changes differently across task domains. For math problem solving, adapting mid-to-late layers brings optimal results, while the opposite behavior occurs in multi-domain science question answering. This behavior is explained by prior work [ 46 , 47 , 48 ] that steering early layers leads to more broad and foundational changes, while steering mid-to-late layers causes semantic concept changes. This well coincides with our experiments as multi-domain science requires foundational change, i.e., different science topic , at early layers, while problem solving requires concept adaptation, i.e., leveraging arithmetic concepts , that works best at mid-to-late layers. In code generation, steering mid-to-late layers also sightly increase the effectiveness. 

Suboptimal Trajectory Adaptation We reveal Bifrost ability to adapt suboptimal trajectories toward target tasks as shown in Fig. 6. Since contextual adaptation by Bifrost modifies trajectory behaviors by extracting transferable latent patterns, it is possible to leverage suboptimal trajectories, e.g., the ones producing incorrect answers in previous tasks , as long as such trajectories share common patterns with the target tasks. As shown in the figure, leveraging suboptimal trajectories surprisingly increases GSM8K solve rate; it could be explained as math-related patterns, such 8Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents Table 5: Bifrost effectiveness on cross-domain context adaptation: pass rates (%) on LiveCodeBench code generation adapted from GSM8K math problem-solving trajectories. 

Llama-3.2-3B-Instruct Method pass@1 pass@3 pass@5 ICL 8.75 11.93 13.64 BoT 4.67 7.41 8.52 RISE 4.81 8.85 10.74 Paprika 9.33 12.37 13.70 

Bifrost 12.73 16.99 18.75 2 3 4 6 7 8 9 12                 

> Number of In-Context Examples
> 55
> 60
> 65
> 70
> 75
> 80
> 85
> 90
> Solve Rate (%)
> GSM8K Ours ICL 23467
> Number of In-Context Examples
> 20
> 25
> 30
> 35
> 40
> 45
> Solve Rate (%)
> GPQA-Diamond Ours ICL 234
> # ICL Examples
> 8
> 10
> 12
> 14
> 16
> 18
> 20
> Pass (%)
> pass@1
> 234
> # ICL Examples
> pass@3
> 234
> # ICL Examples
> pass@5
> LiveCodeBench Ours ICL

Figure 4: Bifrost effectiveness when leveraging different numbers of in-context examples. as arithmetic remains transferable even though incorrect in the previous tasks. In addition, GPQA-Diamond matches their original solve rate, while LiveCodeBench’s effectiveness slightly degrades, possibly due to syntax errors in their defective trajectories, which could badly affect the adaptation. 

Alternative Contextual Direction Methods We evaluate Bifrost via various techniques to extract contextual direction. In particular, we replace the existing per-task subtraction in Bifrost with Shared Subtraction that computes a single contextual direction given prior trajectories and all target tasks. In addition, we leverage PCA and Sparse Autoencoder 

that are widely adopted in representation engineering [ 56 , 57 ]. Tab. 6 shows the adaptation results in GSM8K. Shared subtraction degrades effectiveness by 4.2% as a single steering vector hardly captures task specifics, while PCA further enhances Bifrost by 1.3% thanks to better transferable pattern extraction under lower dimensional levels, and Sparse Autoencoder nearly matches the original Bifrost effectiveness. Table 6: Integrability of Bifrost with alternative contextual direction extraction methods. 

Methods Solve Rate Diff. 

Original Bifrost 81.45 –

Shared Subtraction 77.25 -4.20 Principal Component Analysis (PCA) 82.75 +1.30 Sparse Autoencoder 80.95 -0.50 

Table 7: Ablation study on Bifrost. 

Ablation Configuration Solve Rate Diff. 

Original Bifrost 85.00 –

Without ICL prompt examples 84.25 -0.75 Opposite contextual direction 77.50 -7.50 Random contextual direction 78.00 -7.00 

9Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 2 4 6 8 10 14 16 28                              

> Steering Layer Position
> 20
> 30
> 40
> 50
> 60
> 70
> 80
> Solve Rate (%)
> 22.69
> 54.75
> 65.25
> 79.50 80.50 81.45 81.25 78.75
> GSM8K 246810 14 16 28
> Steering Layer Position
> 20
> 25
> 30
> 35
> 40
> 45
> 50
> 55
> 60
> 65
> Solve Rate (%)
> 58.33
> 65.00 65.00
> 60.00 60.00
> 40.00
> 46.67
> 30.00
> GPQA-Diamond 246810 14 16 28
> Steering Layer Position
> 5.0
> 7.5
> 10.0
> 12.5
> 15.0
> 17.5
> 20.0
> 22.5
> 25.0
> Pass (%)
> 10.53 10.83 10.83
> 12.65
> 11.06
> 13.03 12.58 12.58
> 14.05 14.55 14.85
> 17.08
> 14.77
> 17.58
> 16.67 16.67
> 15.91 16.29
> 17.05
> 19.32
> 16.67
> 19.70
> 18.94 18.94
> LiveCodeBench
> pass@1
> pass@3
> pass@5

Figure 5: Bifrost with different steering layer positions. Trajectory Type       

> 70
> 75
> 80
> 85
> 90
> Solve Rate (%)  86.50
> 81.45
> AQUA GSM8K
> Suboptimal
> Original Trajectory Type
> 30
> 32
> 34
> 36
> 38
> 40
> Solve Rate (%)
> 36.67 36.67
> ARC GPQA-Diamond
> Suboptimal
> Original pass@1 pass@3 pass@5
> Trajectory Type
> 10
> 15
> 20
> Pass (%)
> 11.06
> 16.02
> 18.56
> 13.64
> 17.77
> 19.70 HumanEval LiveCodeBench
> Suboptimal
> Optimal

Figure 6: Bifrost when leveraging suboptimal prior trajectories. 

4.4 Ablation Study 

We conduct an ablation study on three factors that affect contextual adaptation direction. Specifically, we keep the existing contextual representation steering as is, but remove corresponding in-context examples from prompts. In addition, we perform untargeted contextual adaptations that steer the agent in directions either opposite to or unrelated to the target tasks. Tab. 7 shows the agent effectiveness when adapted to GSM8K. Omitting in-context examples causes a slight decrease since most of the insightful patterns are embedded in their corresponding hidden states. Untargeted adaptation, such as opposite or random steering, significantly degrades agent effectiveness, which further confirms the role of target tasks as a contextual guidance to adapt meaningful trajectory behaviors. 

5 Conclusion 

In this work, we proposed Bifrost, a training-free method that enables self-improving agents to improvise their prior trajectories across tasks under substantial context shifts. We revealed a context-trajectory correlation that allows Bifrost to precisely steer any previous trajectories toward target tasks by exploiting context differences. Both theoretical analyses and empirical results demonstrate that Bifrost leverages latent concepts acquired from prior in-context examples to adapt to new tasks, consistently outperforming existing self-improvement methods across diverse domains. Despite these promising results, our theoretical analysis is grounded in the Linear Representation Hypothesis, which may limit its applicability where more expressive nonlinear representations are required in practice. In the future, we plan to extend Bifrost to multi-context adaptation, where multiple target contexts are presented. Such a direction introduces an intensified complexity, which is essential for general-purpose agentic AI under multi-task applications. 10 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

References 

[1] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X Lee, Maria Bauzá, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, et al. Robocat: A self-improving generalist agent for robotic manipula-tion. arXiv preprint arXiv:2306.11706 , 2023. [2] Archit Sharma, Ahmed M Ahmed, Rehaan Ahmad, and Chelsea Finn. Self-improving robots: End-to-end autonomous visuomotor reinforcement learning. arXiv preprint arXiv:2303.01488 , 2023. [3] Gokul Puthumanaillam, Paulo Padrao, Jose Fuentes, Pranay Thangeda, William E Schafer, Jae Hyuk Song, Karan Jagdale, Leonardo Bobadilla, and Melkior Ornik. Trace: A self-improving framework for robot behavior forecasting with vision-language models. arXiv preprint arXiv:2503.00761 , 2025. [4] Jianbiao Mei, Yukai Ma, Xuemeng Yang, Licheng Wen, Xinyu Cai, Xin Li, Daocheng Fu, Bo Zhang, Pinlong Cai, Min Dou, et al. Continuously learning, adapting, and improving: A dual-process approach to autonomous driving. 

arXiv preprint arXiv:2405.15324 , 2024. [5] Resul Dagdanov, Halil Durmus, and Nazim Kemal Ure. Self-improving safety performance of reinforcement learning based driving with black-box verification algorithms. arXiv preprint arXiv:2210.16575 , 2022. [6] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. In Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 2550–2575, 2023. [7] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems , 36:46534–46594, 2023. [8] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738 , 2023. [9] Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, and Yang Yu. Language model self-improvement by reinforcement learning contemplation. arXiv preprint arXiv:2305.14483 ,2023. [10] Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, and Ji-Rong Wen. Reflective multi-agent collaboration based on large language models. Advances in Neural Information Processing Systems ,37:138595–138631, 2024. [11] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and optimization. arXiv preprint arXiv:2402.17574 , 2024. [12] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anand-kumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291 ,2023. [13] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 19632–19642, 2024. [14] Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, et al. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286 , 2025. [15] Boyuan Zheng, Michael Y Fatemi, Xiaolong Jin, Zora Zhiruo Wang, Apurva Gandhi, Yueqi Song, Yu Gu, Jayanth Srinivasa, Gaowen Liu, Graham Neubig, et al. Skillweaver: Web agents can self-improve by discovering and honing skills. arXiv preprint arXiv:2504.07079 , 2025. [16] Gabriel Sarch, Lawrence Jang, Michael Tarr, William W Cohen, Kenneth Marino, and Katerina Fragkiadaki. Vlm agents generate their own memories: Distilling experience into embodied programs of thought. Advances in Neural Information Processing Systems , 37:75942–75985, 2024. [17] Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. Darwin godel machine: Open-ended evolution of self-improving agents. arXiv preprint arXiv:2505.22954 , 2025. [18] Yiyang Jin, Kunzhao Xu, Hang Li, Xueting Han, Yanmin Zhou, Cheng Li, and Jing Bai. Reveal: Self-evolving code agents via iterative generation-verification. arXiv preprint arXiv:2506.11442 , 2025. [19] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931 , 2023. 11 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents [20] Vijay Lingam, Behrooz Omidvar Tehrani, Sujay Sanghavi, Gaurav Gupta, Sayan Ghosh, Linbo Liu, Jun Huan, and Anoop Deoras. Enhancing language model agents using diversity of thoughts. In The Thirteenth International Conference on Learning Representations , 2025. [21] Vishnu Sarukkai, Zhiqiang Xie, and Kayvon Fatahalian. Self-generated in-context examples improve llm agents for sequential decision-making tasks. arXiv preprint arXiv:2505.00234 , 2025. [22] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080 , 2021. [23] Ziqian Lin and Kangwook Lee. Dual operating modes of in-context learning. In Forty-first International Conference on Machine Learning , 2024. [24] Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Hongzhang Liu, Ronghao Chen, Yangfan He, et al. Se-agent: Self-evolution trajectory optimization in multi-step reasoning with llm-based agents. arXiv preprint arXiv:2508.02085 , 2025. [25] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020. [26] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems , 36:8634–8652, 2023. [27] Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. Advances in Neural Information Processing Systems , 37:113519–113544, 2024. [28] Yurun Yuan and Tengyang Xie. Reinforce llm reasoning through multi-agent reflection. arXiv preprint arXiv:2506.08379 , 2025. [29] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , pages 2001–2010, 2017. [30] Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Self-adapting language models. arXiv preprint arXiv:2506.10943 , 2025. [31] Siddharth Narayanan, James D Braza, Ryan-Rhys Griffiths, Manu Ponnapati, Albert Bou, Jon Laurent, Ori Kabeli, Geemi Wellawatte, Sam Cox, Samuel G Rodriques, et al. Aviary: training language agents on challenging scientific tasks. arXiv preprint arXiv:2412.21154 , 2024. [32] Jing-Cheng Pang, Si-Hang Yang, Kaiyuan Li, Jiaji Zhang, Xiong-Hui Chen, Nan Tang, and Yang Yu. Kalm: Knowledgeable agents by offline reinforcement learning from large language model rollouts. Advances in Neural Information Processing Systems , 37:126620–126652, 2024. [33] Caia Costello, Simon Guo, Anna Goldie, and Azalia Mirhoseini. Think, prune, train, improve: Scaling reasoning without scaling models. arXiv preprint arXiv:2504.18116 , 2025. [34] Kareem Amin, Sara Babakniya, Alex Bie, Weiwei Kong, Umar Syed, and Sergei Vassilvitskii. Escaping collapse: The strength of weak data for large language model training. arXiv preprint arXiv:2502.08924 , 2025. [35] Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, and Waseem AlShikh. Reflect, retry, reward: Self-improving llms via reinforcement learning. arXiv preprint arXiv:2505.24726 , 2025. [36] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. Advances in Neural Information Processing Systems , 37:55249–55285, 2024. [37] Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, J Zico Kolter, Jeff Schneider, and Ruslan Salakhutdinov. Training a generally curious agent. arXiv preprint arXiv:2502.17543 , 2025. [38] Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, and Weiran Xu. Agentrefine: Enhancing agent generalization through refinement tuning. arXiv preprint arXiv:2501.01702 , 2025. [39] Yubin Ge, Salvatore Romeo, Jason Cai, Monica Sunkara, and Yi Zhang. Samule: Self-learning agents enhanced by multi-level reflection. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing , pages 16602–16621, 2025. [40] Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, and Dongmei Zhang. How do in-context examples affect compositional generalization? arXiv preprint arXiv:2305.04835 , 2023. 12 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents [41] Jingchu Gai, Guanning Zeng, Huaqing Zhang, and Aditi Raghunathan. Differential smoothing mitigates sharpening and improves llm reasoning. arXiv preprint arXiv:2511.19942 , 2025. [42] Nikita Afonin, Nikita Andriyanov, Nikhil Bageshpura, Kyle Liu, Kevin Zhu, Sunishchal Dev, Ashwinee Panda, Alexander Panchenko, Oleg Rogov, Elena Tutubalina, et al. Emergent misalignment via in-context learning: Narrow in-context examples can produce broadly misaligned llms. arXiv preprint arXiv:2510.11288 , 2025. [43] Zhuo Huang, Xiaobo Xia, Li Shen, Bo Han, Mingming Gong, Chen Gong, and Tongliang Liu. Harnessing out-of-distribution examples via augmenting content and style. In The Eleventh International Conference on Learning Representations , 2023. [44] Ziming Hong, Zhenyi Wang, Li Shen, Yu Yao, Zhuo Huang, Shiming Chen, Chuanwu Yang, Mingming Gong, and Tongliang Liu. Improving non-transferable representation learning by harnessing content and style. In The Twelfth International Conference on Learning Representations , 2024. [45] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv preprint arXiv:2311.03658 , 2023. [46] Constantin Venhoff, Iván Arcuschin, Philip Torr, Arthur Conmy, and Neel Nanda. Understanding reasoning in thinking language models via steering vectors. arXiv preprint arXiv:2506.18167 , 2025. [47] Yu Zhao, Alessio Devoto, Giwon Hong, Xiaotang Du, Aryo Pradipta Gema, Hongru Wang, Xuanli He, Kam-Fai Wong, and Pasquale Minervini. Steering knowledge selection behaviours in llms via sae-based representation engineering. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , pages 5117–5136, 2025. [48] Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J Vazquez, Ulisse Mini, and Monte MacDiarmid. Steering language models with activation engineering. arXiv preprint arXiv:2308.10248 , 2023. [49] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146 , 2017. [50] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021. [51] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1 , 2018. [52] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling , 2024. [53] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR ,abs/2107.03374, 2021. [54] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024. [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824–24837, 2022. [56] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: A top-down approach to ai transparency. arXiv preprint arXiv:2310.01405 , 2023. [57] Bertram Højer, Oliver Jarvis, and Stefan Heinrich. Improving reasoning performance in large language models via representation engineering. arXiv preprint arXiv:2504.19483 , 2025. [58] Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu. Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning. arXiv preprint arXiv:2410.02052 , 2024. 13 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents [59] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. 

Advances in Neural Information Processing Systems , 35:15476–15488, 2022. [60] Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, and Joseph J Lim. Bootstrap your own skills: Learning to solve new tasks with large language model guidance. arXiv preprint arXiv:2310.10021 , 2023. [61] Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging language model agents. arXiv preprint arXiv:2506.01716 , 2025. [62] Senkang Hu, Xudong Han, Jinqi Jiang, Yihang Tao, Zihan Fang, Yong Dai, Sam Tak Wu Kwong, and Yuguang Fang. Distribution-aligned decoding for efficient llm task adaptation. arXiv preprint arXiv:2509.15888 , 2025. [63] Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the pandora’s box of llms: Jailbreaking llms through representation engineering. CoRR , 2024. [64] Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, and Besmira Nushi. Improving instruction-following in language models through activation steering. arXiv preprint arXiv:2410.12877 , 2024. [65] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori B Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers) , pages 12286–12312, 2023. [66] Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv preprint arXiv:2311.06668 , 2023. [67] Weixuan Wang, Jingyuan Yang, and Wei Peng. Semantics-adaptive activation intervention for llms via dynamic steering vectors. arXiv preprint arXiv:2410.12299 , 2024. [68] Joschka Braun, Carsten Eickhoff, David Krueger, Seyed Ali Bahrainian, and Dmitrii Krasheninnikov. Understand-ing (un) reliability of steering vectors in language models. arXiv preprint arXiv:2505.22637 , 2025. [69] Jingran Su, Jingfan Chen, Hongxin Li, Yuntao Chen, Li Qing, and Zhaoxiang Zhang. Activation steering decoding: Mitigating hallucination in large vision-language models through bidirectional hidden state intervention. In 

Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 12964–12974, 2025. [70] Benoit Dherin, Michael Munn, Hanna Mazzawi, Michael Wunder, and Javier Gonzalvo. Learning without training: The implicit dynamics of in-context learning. arXiv preprint arXiv:2507.16003 , 2025. [71] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning , pages 35151–35174. PMLR, 2023. [72] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems , 36:45614–45650, 2023. [73] Jiayu Liu, Zhenya Huang, Chaokun Wang, Xunpeng Huang, Chengxiang Zhai, and Enhong Chen. What makes in-context learning effective for mathematical reasoning: A theoretical analysis. arXiv preprint arXiv:2412.12157 ,2024. [74] Huaze Tang, Tianren Peng, and Shao-lun Huang. On theoretical interpretations of concept-based in-context learning. arXiv preprint arXiv:2509.20882 , 2025. [75] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 11048–11064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [76] Jannik Kossen, Yarin Gal, and Tom Rainforth. In-context learning learns label relationships but is not conventional learning. arXiv preprint arXiv:2307.12375 , 2023. [77] Core Francisco Park, Andrew Lee, Ekdeep Singh Lubana, Yongyi Yang, Maya Okawa, Kento Nishi, Martin Wattenberg, and Hidenori Tanaka. Iclr: In-context learning of representations. arXiv preprint arXiv:2501.00070 ,2024. [78] Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. Advances in Neural Information Processing Systems , 36:36637–36651, 2023. 14 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents [79] Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak A Subramanian, and Alvin Chan. Can llms reason over non-text modalities in a training-free manner? a case study with in-context representation learning. 

arXiv preprint arXiv:2509.17552 , 2025. [80] Tomoya Wakayama and Taiji Suzuki. In-context learning is provably bayesian inference: a generalization theory for meta-learning. arXiv preprint arXiv:2510.10981 , 2025. [81] Zhuo Huang, Chang Liu, Yinpeng Dong, Hang Su, Shibao Zheng, and Tongliang Liu. Machine vision therapy: Multimodal large language models can enhance visual robustness via denoising in-context learning. In Forty-first International Conference on Machine Learning , 2024. [82] Haoyu Wang, Zhuo Huang, Zhiwei Lin, and Tongliang Liu. Noisegpt: Label noise detection and rectification through probability curvature. In The Thirty-eighth Annual Conference on Neural Information Processing Systems ,2024. [83] Madhur Panwar, Kabir Ahuja, and Navin Goyal. In-context learning through the bayesian prism. arXiv preprint arXiv:2306.04891 , 2023. [84] Allan Raventós, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. Advances in neural information processing systems , 36:14228– 14246, 2023. [85] Qixun Wang, Yifei Wang, Yisen Wang, and Xianghua Ying. Can in-context learning really generalize to out-of-distribution tasks? arXiv preprint arXiv:2410.09695 , 2024. [86] Chase Goddard, Lindsay M Smith, Vudtiwat Ngampruetikorn, and David J Schwab. When can in-context learning generalize out of task distribution? arXiv preprint arXiv:2506.05574 , 2025. [87] Hoyoon Byun, Gyeongdeok Seo, Joonseong Kang, Taero Kim, Jihee Kim, and Kyungwoo Song. Ccl: Causal-aware in-context learning for out-of-distribution generalization. In The Thirty-ninth Annual Conference on Neural Information Processing Systems .15 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

# Appendix for “Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents” 

In this Appendix, we first present related work in Sec. A. We then further elaborate the hypothesis testing on the context-trajectory correlation in Sec. B. Our proofs for the theoretical analysis is presented in Sec. C. For the experiments, we discuss further details of experimental setup and demonstrate additional illustration and experimental results in Sec. D. Finally, Sec E presents the prompts used for Bifrost in our work. 

A Related Work 

Self-Improving Agents Thanks to self-reflection ability [ 6, 7, 8, 9, 10 , 11 ], self-improving agents have emerged in leveraging their own experiences to solve complicated decision-making and reasoning problems through iterative prompt-based trial-and-error [ 12 , 26 , 13 , 20 , 24 , 37 , 58 ], supervised finetuning and reinforcement learning via data self-generation [ 59 , 60 , 61 , 32 , 35 , 18 , 16 , 21 ]. However, such approaches self-improve within target tasks [ 26 , 27 ,24 , 13 , 28 ], leaving cross-domain experience reuse strategies underexplored. While early work usually discards such insightful experiences to rely on supervised finetuning [ 29 , 30 , 31 , 32 , 33 , 34 ], recent work attempts to address this by naively reusing prior trajectories [ 26 , 20 , 13 , 38 , 7 ], generalizing thought templates [ 27 ] as guidance, or relying on costly reinforcement learning to let agent unfold strategic and adaptable information [ 35 , 36 , 37 , 38 , 24 , 39 ]. Unfortunately, they remain suboptimal when faced with contextual non-stationarity across tasks [ 40 , 23 , 41 , 42 ]. Bifrost addresses this problem by contextual adaptation of prior trajectories without any training required. 

Representation Engineering Previous work has demonstrated that representation manipulation under agent hidden states can change its behavior accordingly without any finetuning effort [ 45 , 48 , 56 , 62 , 63 ]. While most existing literature focus explicit control over specific emotion behaviors [ 64 , 48 ], it is more challenging to leverage representation engineering to enhance agent reasoning and problem-solving abilities, as such features are usually implicit [ 65 , 66 , 67 ,68 , 69 ]. Although recent work has revealed this possibility [ 57 ], a deterministic mechanism remains unexplored. In this work, we successfully exploit the representation engineering as a proxy of contextual adaptation to enable agents leverage their past experiences for target tasks effectively. 

In-Context Learning Many empirical and theoretical results have shown that in-context examples can enhance LLM Agent effectiveness [ 25 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 ]. In addition, recent work theoretically proves ICL from the perspective of Bayesian Inference [ 22 , 80 , 81 , 82 ] where the agent approximates a latent concept from such examples to solve the target when such tasks are context-related [ 83 , 84 ]. However, ICL becomes ineffective when examples are specific to their previous tasks [ 40 ]. This phenomenon is even worse for self-improving agents reusing their past experiences as in-context examples due to significant context mismatch between previous and target tasks [ 23 ], thus hindering generalization and further degrading agent behaviors [ 38 , 85 , 86 , 87 ]. Bifrost preserves the simplicity of ICL while further strengthening it by adapting the latent concept of such examples toward the target task. 

B Context-Trajectory Correlation 

B.1 Hypothesis Testing and Experimental Demonstration Details Hypothesis Testing Our research question is whether previous-to-target trajectory steering preserves a linear correla-tion with trajectory change, such that a steered trajectory is more similar to the natural trajectory associated with the target query than the trajectory generated by a non-steered model for the original query. Let (ˆ q, ˆa) be a target context pair consisting of a query and its corresponding natural trajectory, and let (q, a ) be a previous context counterpart. The contextual shift ∆, computed using Eq. (3) , is applied to the model to produce a steered trajectory as. We evaluate the relationship between context similarity and trajectory change by computing the Pearson correlation 

r = corr (sq , s a), (12) where sq denotes the context similarity obtained as 

sq = ⟨f (q), f (ˆ q)⟩,

16 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents and sa represents the similarity of trajectory change, 

sa = ⟨f (as), f (ˆ a)⟩ − ⟨ f (a), f (ˆ a)⟩.

Here, f (·) denotes an embedding extractor. As a result, we formulate the following hypotheses, • H0: There is no linear correlation between context shift and trajectory behavior shift. • H1: There is a confirmed linear correlation between context shift and trajectory behavior shift. 

Experiments To test the above hypothesis, we randomly select 100 samples from AQUA and GSM8K as two separate contexts. We then construct 50 sample pairs and apply trajectory steering from AQUA to GSM8K on each pair. For embedding extraction, we employ either their existing hidden states or the sentence transformer (all-MiniLM-L6-v2 in our work 1). We first generate original trajectories for each context (using Llama-3.1-8B-Instruct), then apply steering vectors using Eq. (3). Finally, we measure p-values from Pearson correlation as shown in Eq. (12). Tab. 8 and 9 show the hypothesis testing results when using hidden state and trajectory embeddings, respectively. The result demonstrates a strong statistical significance across different settings of α, showing our hypothesis of linear correlation between context shift and trajectory behavior shift exists. Table 8: Hypothesis testing results using hidden state embeddings                 

> α0.1 0.2 0.3 0.4 0.5 0.6 5.0 5.5
> p-value 0.00418 0.00665 0.01085 0.00802 0.02019 0.02202 0.04279 0.01746

Table 9: Hypothesis testing results using trajectory embeddings           

> α4.7 4.9 5.0 5.1 5.3
> p-value 0.035 0.012 0.017 0.012 0.007

C Theoretical Analysis 

C.1 Proof of Lemma 1 

After observing previous trajectories, we have approximated the latent concept posterior as a Gaussian 

p(ϕ | C ) ≈ N ( ˜ϕ, Σ−1),

where ˜ϕ = arg max  

> ϕ

(log p(C | ϕ) + log p(ϕ)) , Σ = 1

σ2 F⊤F,

where the mean ˜ϕ is usually centered at the mode of previous trajectories. When observing a new target query ˆq ∈ ˆC,we update the posterior and its mean accordingly, 

p(ϕ | C , ˆq) ∝ p(ˆ q | ϕ) · p(ϕ | C ).

˜ϕ′ = arg max  

> ϕ

(p(ˆ q | ϕ) + p(ϕ | C )) .

Let Lϕ = p(ˆ q | ϕ) + p(ϕ | C ), we update the mean of new posterior by optimizing Lϕ. To solve this, we will need to derive p(ˆ q | ϕ) and p(ϕ | C ).Deriving log p(ˆ q | ϕ) by Taylor expansion to the new log-likelihood term, 

log p(ˆ q | ϕ) ≈ log p(ˆ q | ˜ϕ) + g⊤(ϕ − ˜ϕ) + 12 (ϕ − ˜ϕ)⊤Q(ϕ − ˜ϕ),

where g = ∇ϕ log p(ˆ q | ϕ) ϕ= ˜ϕ

, Q = ∇2 

> ϕ

log p(ˆ q | ϕ) ϕ= ˜ϕ

.

> 1https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

17 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents Deriving log p(ϕ | C ) as 

log p(ϕ | C ) ≈ const − 12 (ϕ − μ)⊤Σ−1(ϕ − μ).

As a result, Lϕ becomes 

Lϕ ≈ − 12 (ϕ − ˜ϕ)⊤Σ−1(ϕ − ˜ϕ) + g⊤(ϕ − ˜ϕ) + 12 (ϕ − ˜ϕ)⊤Q(ϕ − ˜ϕ).

To optimize Lϕ, we take derivative w.r.t. ϕ and set to zero 

∇ϕLϕ = −Σ( ϕ − ˜ϕ) + g + Q(ϕ − ˜ϕ) = 0 .

Rearranging and simplifying the single-step update by discarding Q, we have 

ϕ − ˜ϕ ≈ Σ−1g = Σ −1∇ϕ log p(ˆ q | ϕ) ϕ= ˜ϕ

. (13) Therefore, adding the target query ˆq ∈ ˆC will update the latent concept posterior as 

p(ϕ | C , ˆq) ≈ N 

 ˜ϕ + Σ −1∇ϕ log p(ˆ q | ϕ), Σ−1

. (14) Under the Linear Representation Hypothesis, log p(ˆ q | ϕ) from Eq. (13) is generally a linear function of the hidden state expressed as 

log p(ˆ q | ϕ) ≈ h(ˆ q; ϕ)W,

where h is the hidden state of LLM M and W is a linear projection. Treating h as a function of ϕ, derive log p(ˆ q | ϕ),

∇ϕ log p(ˆ q | ϕ) = ∂ log p∂h · ∂h

∂ϕ .

The partial derivative of h can be approximated by a first-order Taylor expansion, assuming that h is locally smooth in 

ϕ, and ϕ∗ is the ideal concept of the target task context ˆC, we have 

h(ϕ∗) ≈ h(ϕ) + ∂h

∂ϕ (ϕ∗ − ϕ).

Since ∆ ≈ h(ϕ∗) − h(ϕ), we have 

∆ ≈ ∂h

∂ϕ (ϕ∗ − ϕ). (15) 

≈ J · Σ−1∇ϕ log p(ˆ q | ϕ) (from Eq. (13)) (16) 

= Σ −1 · J∇ϕ log p(ˆ q | ϕ). (17) Rearranging, 

Σ−1∇ϕ log p(ˆ q | ϕ) = J⊤∆ = P (∆) . (18) Therefore, applying a trajectory steering ∆ shifts the mean toward an ideal concept of the target context, the steered concept posterior is expressed as 

ps(ϕ | C , ˆq) ≈ N 

 ˜ϕ + αP (∆) , Σ−1

.

18 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

C.2 Proof of Theorem 3 

Let y∗ = fϕ(ˆ q) be the true label of target query ˆq, ˆys = Ms(C ◦ ˆq) be the prediction of the steered LLM agent, 

ps = ps(ϕ | C , ˆq) be the latent concept posterior of the steered model. Our target bound is 

E [L(ˆ ys, y ∗)] = Eϕ∼ps [L(fϕ(ˆ q), y ∗)] 

≤ L (Eϕ∼ps [fϕ(ˆ q)] , y ∗) + V ar [fϕ(ˆ q)] (Jensen inequality with smooth and convex L)

≈ L (fE[ϕ](ˆ q), y ∗) + O(V ar [fϕ(ˆ q)]) .

The target bound consists of the bias and variance terms. We first bound the variance term. From Lemma 1 we have 

ps(ϕ | · ) ≈ N (μ, Σ−1). Since fϕ is locally Lipschitz in ϕ with a constant G, we have 

V ar [fϕ(ˆ q)] ≤ G2 Tr(Σ −1) = G2 O( 1

k ) ≤ O ( 1

k ).

Next, we bound the bias term, 

L(fE[ϕ](ˆ q), y ∗) ≤ C || μ − ϕ∗|| 2 (L is smooth) 

= || αP (∆) − (ϕ∗ − ˜ϕ)|| 2

≤ O 



|| αP (∆) − (ϕ∗ − ˜ϕ)|| 2

As a result, we obtain the bound of the steered model as 

E [L(ˆ ys, y ∗)] ≤ O 

 1

k + || αP (∆) − (ϕ∗ − ˜ϕ)|| 2



.

Since ∆ ∝ (ϕ∗ − ˜ϕ), thus αP (∆) = c(ϕ∗ − ˜ϕ) for a scalar c > 0, then 

E [L(ˆ ys, y ∗)] ≤ O ( 1

k ).

For an ordinary model, its prediction y = M(ˆ q) induces a broader pretrained latent concept, then 

E[L(y, y ∗)] ≤ O (1) .

Subtracting the bounds, we have 

E[L(y, y ∗)] − E [L(ˆ ys, y ∗)] ≥ O (1) − O ( 1

k ).

Rearranging 

E [L(ˆ ys, y ∗)] ≤ E[L(y, y ∗)] − Ω

 1

k



.

D Experiments 

D.1 Experimental Setup Context Shift Experimental Setup To evaluate trajectory leverage of self-improving agents under substantial context shifts, we setup source - target dataset pairs. In particular, a self-improving agent will solve tasks on a source dataset to collect successful trajectories (and/or unsuccessful ones depending on baselines), then leverage such trajectories to adapt toward the target task in the sense that there is no ground-truth available on such tasks of the target dataset. In this way, the self-improving agent has to employ their past experiences learned from previous tasks via such trajectories to solve the target tasks. We now present how this setup applies to Bifrost and the baselines in our work. 19 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

Bifrost Setup Bifrost does not require any particular self-improving agent mechanism to solve tasks and collect corresponding trajectories. In our experiment, we use ordinary LLM agents as mentioned to solely solve source tasks and subsequently collect successful trajectories. We let the agent solve a subset of 20 tasks when using Llama-3.1-8B, and 50 tasks when using Llama-3.2-3B and Llama-3.2-1B, for all source tasks in our experiments (AQUA, ARC-Easy, HumanEval). We subsequently collect corresponding successful trajectories and randomly pick a number of them as in-context examples to solve target tasks. In particular, we randomly pick 7 examples in AQUA → GSM8K and ARC-Easy → GPQA-Diamond experiments, and 5 examples in HumanEval → GSM8K experiments. For the steering coefficient α, we get the optimal value by conducting a grid search α ∈ { 1, 2, 3, 4}. In our main experiments, we conduct the steering on residual stream of a single middle layer. In particular, we steer 20 th layer in Llama-3.1-8B, 14 th layer in Llama-3.2-3B, and 10 th layer in Llama-3.2-1B. Steering on other layers are discussed in Sec. 4.3 and Sec. D.5. 

Baselines Setup We compare Bifrost with the following baselines. • Chain-of-Though (CoT) [55 ] prompts an agent to perform step-by-step reasoning. We directly apply CoT to the target tasks. • In-Context Learning (ICL) [25 ], we let an agent solve tasks from a source dataset to collect successful trajectories. We then leverage such trajectories as in-context examples in the prompts to solve target tasks. We use 7 in-context examples for both the math problem-solving task and the multiple-choice question answering task, and we use 5 in-context examples for the code generation task. • Reflexion [26 ] is able to iteratively refine its own trajectories given environment feedback. We directly apply Reflexion to the target tasks. We allow up to 5 attempts for math problem solving and code generation problems, as in their original work. For the multi-choice question answering problem, we allow 3 attempts since more attempts can enable the agent to easily get the correct answer with 4-choice questions, which is unfair to other methods. • Buffer of Thoughts (BoT) [ 27 ] creates thought templates when solving tasks as guidance in future tasks. We let BoT solve the source dataset tasks, subsequently creating corresponding though templates, which then serve as guidance in target tasks. Following their original work [ 27 ], we let BoT continuously update its thought templates while solving the target tasks. • RISE [ 36 ] teaches an agent to self-improve by learning each fail-to-correct transition of trajectories via reinforcement learning. In our experiments, we let RISE solve source tasks and subsequently perform reinforcement learning on corresponding trajectories. We then leverage the finetuned agent to solve target tasks. We evaluate RISE within a single attempt and 5 attempts, respectively, as recommended by the original work [36]. • Diversity of Thought (DoT) [20 ] lets an agent generate diverse trajectories and self-reflect on given environment feedback. Successful trajectories are stored in a memory bank as is and subsequently retrieved as in-context examples when solving similar tasks. We let DoT solve source tasks, and employ it to solve the target tasks. • Paprika [ 37 ] learns strategic information by letting an agent self-explore and adjust its trajectories given environment feedback via a combined Supervised Finetuning and Direct Preference Optimization. Such information is prone to out-of-domain as shown in their original paper [ 37 ]. Similar to RISE, we learn Paprika on source tasks and apply it to the target tasks. 

Evaluation Metrics For AQUA → GSM8K and ARC-Easy → GPQA-Diamond experiments, we allow a single generation for each question, except Reflexion and RISE baselines, and perform exact matching between agent answers and ground-truths, then measure the solve rate. For HumanEval → GSM8K , we generate 5 solutions for each problem and measure functional correctness following previous work [ 53 , 54 ] to determine the solutions passing test cases via pass@1, pass@3, and pass@5. 

Datasets and LLMs We provide the dataset and LLM details as follows. For datasets, we leverage AQUA (Algebra Question Answering with Rationales) 2, GSM8K 3, ARC-Easy 4, GPQA-Diamond 5, HumanEval 6, and LiveCodeBech 7

> 2https://github.com/google-deepmind/AQuA
> 3https://huggingface.co/datasets/openai/gsm8k
> 4https://huggingface.co/datasets/allenai/ai2_arc
> 5https://huggingface.co/datasets/fingertap/GPQA-Diamond
> 6https://huggingface.co/datasets/openai/openai_humaneval
> 7https://huggingface.co/datasets/openai/openai_humaneval

20 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents (release_v5) with 880 problems. For the LLMs, we leverage different model sizes, including Llama 1B 8, 3B 9, and 8B 10 

Instruct versions. 

D.2 Latent Concept Shift Illustration 

We show more steering effect illustrations in AQUA → GSM8K (Fig. 7), ARC-Easy → GQPA-Diamond (Fig. 8), and HumanEval → LiveCodeBench (Fig. 9). original 

> steered
> target context original
> steered
> target context original
> steered
> target context original
> steered
> target context original
> steered
> target context original
> steered
> target context original
> steered
> target context

(a) 

Figure 7: Illustration of Latent Concept Shift from AQUA to GSM8K. 

D.3 Qualitative Analysis 

Fig. 10 and 11 show additional result demonstrating that Bifrost can effectively leverage latent concepts learned from their prior context trajectories to solve the current tasks. 

8https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct 

9https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct 

10 https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct 

21 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents original 

steered 

target context original 

steered 

target context original 

steered 

target context original 

steered 

target context original 

steered 

target context 

Figure 8: Illustration of Latent Concept Shift from ARC-Easy to GPQA-Diamond. original 

steered 

target context original 

steered 

target context original 

steered 

target context original 

steered 

target context original 

steered 

target context original 

steered 

target context 

Figure 9: Illustration of Latent Concept Shift from HumanEval to LiveCodeBench. 22 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

Bifrost 

Let’s denote the number of enemies Emma jumps on as E, the number of berries she collects as B,and the time she has left when she finishes the level as T .Ava jumps on 8 more enemies than Emma, so the number of enemies Ava jumps on is E + 8 .Ava collects 3 more berries than Emma, so the number of berries Ava collects is B + 3 .Ava finishes the level 4 seconds slower than Emma, so the time Ava has left when she finishes the level is T − 4.The score for each player is calculated as follows: Emma’s score = 10 E + 5 B + 30 T

Ava’s score = 10( E +8)+5( B +3)+30( T −4) 

Simplifying Ava’s score: Ava’s score = 10 E +80+5 B +15+30 T −120 

= 10 E + 5 B + 30 T + 25 

Now let’s find the difference between their scores: Difference = Ava’s score - Emma’s score = (10 E+5 B+30 T +25) −(10 E+5 B+30 T )

= 25 

Therefore, the difference between their two scores is 25 points. 

CoT 

First, let’s assign variables to the un-knowns. Let e be the number of en-emies Emma jumps on, and b be the number of berries she collects. Then, the number of enemies Ava jumps on is 

e + 8 , and the number of berries she collects is b + 3 . Since they finish the level at the same time, the time left on the timer for Emma is the same as for Ava. Therefore, the time left for Emma is 

60 − (e + 8) = 52 − e, and for Ava is also 52 − e. The score for Emma is 

10 e + 5 b + 30(52 − e), and for Ava is 

10( e+8)+5( b+3)+30(52 −e−4) .The difference between their scores is 

10 e + 5 b + 30(52 − e) − [10( e +8) + 5( b + 3) + 30(52 − e − 4)] . Dis-tributing the negative sign gives 10 e +5b + 30(52 − e) − 10 e − 80 −

5b − 30(52 − e − 4) . Distributing the 30 gives 10 e + 5 b + 30 · 52 −

30 e − 30(52 − e − 4) . Continuing: 

10 e + 5 b + 1560 − 30 e + 30 e + 120 .Combining like terms: 10 e − 30 e +30 e + 5 b + 1560 + 120 = . . . 

RISE (5th attempt) 

To find the difference between Ava’s and Emma’s scores, we need to compare the points they receive from their various ac-tions. Since Ava jumps on 8 more en-emies and collects 3 more berries than Emma, she scores 8 more points from enemies and 3 more points from berries. However, Ava finishes 4 seconds slower, resulting in a loss of 4x30 = 120 points from the timer. The net gain for Ava is then 8 + 3 − 120 = -109 points. This means Emma earns 109 more points than Ava. However, the question asks for the difference, so we should answer by di-viding the net gain by −1 to represent a net gain for Ava being actually a net loss. Therefore, the answer is −(−109) = 109 and the correct answer is 109 . but I made another mistake and we have to just give the difference so it is just 109 not making a negative sign so the an-swer is 109 . I noticed I made another inaccuracy. The correct answer should be 109 Wait this looks wrong then just 

109 − (109) = 0 The correct answer then is 0, I’ll stick with that. The correct answer is 0 .

Bifrost 

Let’s denote the original price of the jogger jeans as J. Since the jogger jeans cost $6 less than the tattered jeans, we can express this relationship as: 

J = 28 − 6

J = 22 

Now let’s calculate the total savings from buying both jeans. We know that Cole saved a total of $6 .Since he saved 1/3 of the total savings from the jogger jeans, we can set up an equation to find the amount saved from the jogger jeans: 

(1 /3) S = 2 

S = 6 

Now let’s calculate the amount saved from the tat-tered jeans. We know that the total savings is $6 ,and Cole saved $2 from the jogger jeans. Therefore, the amount saved from the tattered jeans is: 

6 − 2 = 4 

Since the tattered jeans originally cost $28 , the amount saved from them is: 

28 − 24 = 4 

Now let’s calculate the original price of the jogger jeans. We know that the jogger jeans originally cost 

$22 and Cole saved $2 from them. Therefore, the amount he paid for the jogger jeans is: 

22 − 2 = 20 

Now let’s calculate how much more the jogger jeans originally cost than the tattered jeans: 

J − 28 = 20 − 28 = −8

Therefore, the jogger jeans originally cost 8 less than the tattered jeans. 

CoT 

The jogger jeans cost $6 less than the tattered jeans, so the jogger jeans cost 

$28 − $6 = $22 . The total savings is $6 , and Cole saved 1/3 of the sav-ings from the jogger jeans, so he saved 

1/3 ∗ $6 = $2 from the jogger jeans. Since he saved $6 in total, he saved 

$6 − $2 = $4 from the tattered jeans. The tattered jeans cost $28 , so the jogger jeans cost $28 − $4 = $24 . The jog-ger jeans cost $24 − $22 = $2 more than the tattered jeans. Reasoning skill: This problem requires the student to use the information given to solve a series of problems. The student must first find the cost of the jogger jeans, then find the savings from each pair of jeans, and fi-nally find the difference in cost between the two pairs of jeans. This requires the student to use the information given to solve a series of problems, which is a key aspect of problem-solving. The student must also use mathematical operations such as subtraction and division to solve the problem, which is also a key aspect of problem-solving. The best answer is 2. 

RISE (5th attempt) 

Let’s break it down step by step. 1. The jogger jeans cost $6 less than the tattered jeans. So, the jogger jeans cost 

$28 − $6 = $22 .2. Cole saved 1/3 of the total savings from the jogger jeans. This means he saved 1/3$6 = $2 from the jogger jeans. 3. Since he saved a total of $6 , the amount saved from the tattered jeans is 

$6 − $2 = $4 .4. The tattered jeans cost $28 , and he saved $4 from the tattered jeans. So, the original price of the tattered jeans was 

$28 + $4 = $32 .5. Similarly, the jogger jeans cost $22 ,and he saved $2 from the jogger jeans. So, the original price of the jogger jeans was $22 + $2 = $24 .6. We are now asked to find the differ-ence in the original price of the jogger jeans and the tattered jeans. Jogger jeans originally cost $24 , and the tattered jeans cost $32 . Therefore, jogger jeans cost 

$32 − $24 = $8 less than the tattered jeans. 7. We are actually looking for how much more do jogger jeans originally cost than the tattered jeans. Since jogger jeans are cheaper, it will be a negative number. So, jogger jeans originally cost $8 less than the tattered jeans. Here is the final answer as requested: −8

Figure 10: Qualitative results from GSM8K dataset adapted from AQUA trajectories. 23 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

Bifrost 

The interplanar distance of a crystal plane can be calculated using the formula: dhkl =

> a

√h2+k2+l2

where dhkl is the interplanar distance, a is the lattice param-eter, and h, k, and l are the Miller indices of the plane. In this case, the Miller indices of the (111) plane are h = 1 ,

k = 1 , and l = 1 . The lattice parameter a is equal to the in-teratomic distance, which is 10 Angstrom. Plugging in these val-ues, we get: d111 =

> 10

√12+1 2+1 2 = 10 √3 =3.54 √3 = 9 .54 Angstrom Therefore, the correct answer is (C). 

CoT 

Step 1: To find the interplanar distance of the (111) plane in a rhombohedral crystal, we first need to understand the relationship between the interatomic distance, the angles between the planes, and the interplanar distance. Step 2: The rhombohedral crystal has angles α = β = γ = 30 ◦, which means it is a highly symmetri-cal crystal structure. Step 3: The interplanar distance, denoted as 

d, can be calculated using the formula d = a√h2+k2+l2 , where 

a is the interatomic distance, and h, k, l are the Miller indices of the plane. Step 4: In this case, the Miller indices of the (111) plane are h = 1 , k = 1 , l = 1 . Step 5: Substituting the given values, we get d = 10 √12+1 2+1 2 = 10 √3 . Step 6: To find the numerical value, we can use the value of √3 ≈ 1.732 . Step 7: Therefore, 

d ≈ 10 1.732 ≈ 5.774 . Step 8: However, we need to consider the rhombohedral structure, which has a different relationship between the interatomic distance and the interplanar distance. Step 9: The correct formula for the interplanar distance in a rhombohedral crys-tal is d = a√3 . Step 10: Substituting the given value of a = 10 

Angstrom, we get d = 10 √3 ≈ 5.774 . Step 11: However, we need to consider the rhombohedral structure, which has a different relationship between the interatomic distance and the interplanar distance. Step 12: The correct formula for the interplanar distance in a rhombohedral crystal is d = a√3 . Step 13: Substituting the given value of a =

ICL 

Step-by-step reasoning process: B The interplanar distance of a crystal plane is given by the equation: dhkl =

> a

√h2+k2+l2 where a is the inter-atomic distance, and h, k, and l are the Miller indices of the plane. In this case, the interatomic distance is 10 Angstrom, and the Miller indices of the (111) plane are h = 1 , k = 1 , and l = 1 . Substi-tuting these values into the equation, we get: d111 = 10 √12+1 2+1 2 = 10 √3 =5.77 Angstrom However, this is not one of the answer choices. We can try to find the closest answer by using the fact that the interplanar distance is inversely pro-portional to the square root of the sum of the squares of the Miller indices. Since the sum of the squares of the Miller in-dices is 3, we can expect the interplanar distance to be slightly less than 

Figure 11: Qualitative results from GPQA-Diamond dataset adapted from ARC-Easy trajectories. 24 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

D.4 In-Context Examples Number Variants 

Table 10: Bifrost vs. ICL on GSM8K with different number of in-context examples adapted from AQUA trajectories. 

# ICL Examples Cosine distance Bifrost ICL 

2 0.08 77.25 58.50 3 0.16 85.00 64.75 4 0.17 81.25 61.75 6 0.22 81.50 68.00 7 0.34 81.45 62.52 8 0.24 85.00 70.25 9 0.26 79.25 68.00 12 0.33 82.00 67.75 

Table 11: Bifrost vs. ICL on GPQA-Diamond with different number of in-context examples adapted from ARC-Easy trajectories. 

# ICL Examples Cosine distance Bifrost ICL 

2 0.13 38.33 30.00 3 0.22 38.33 28.33 4 0.29 35.00 26.67 6 0.29 36.67 31.67 7 0.40 40.00 26.67 

Table 12: Bifrost vs. ICL on LiveCodeBench with different number of in-context examples adapted from HumanEval trajectories. 

Bifrost ICL # ICL Examples Cosine distance pass@1 pass@3 pass@5 pass@1 pass@3 pass@5 2 0.27 13.03 17.23 18.94 8.94 12.20 13.64 3 0.43 13.11 17.20 19.32 9.70 13.30 14.77 4 0.51 12.73 17.77 19.32 9.92 13.86 15.15 

Tab. 10, 11, and 12 shows Bifrost effectiveness when leveraging different number of in-context examples from prior trajectories, evaluated on GSM8K, GPQA-Diamond, and LiveCodeBench, respectively. We compare Bifrost with existing in-context learning. Cosine distance measures the similarity between in-context examples, with lower values indicating greater similarity. 

D.5 Steering Player Positions 

Table 13: Steering layer intervention results on Bifrost. 

Steering Layer AQUA → GSM8K ARC → GPQA HumanEval → LiveCodeBench 

solve rate (%) solve rate (%) pass@1 pass@3 pass@5 Layer 2 22.69 58.33 10.53 14.05 15.91 Layer 4 54.75 65.00 10.83 14.55 16.29 Layer 6 65.25 65.00 10.83 14.85 17.05 Layer 8 79.50 60.00 12.65 17.08 19.32 Layer 10 80.50 60.00 11.06 14.77 16.67 Layer 14 81.45 40.00 13.03 17.58 19.70 Layer 16 81.25 46.67 12.58 16.67 18.94 Last layer 78.75 30.00 12.58 16.67 18.94 3 middle layers: [10 th ,14 th ,16 th ] 80.25 56.67 10.91 14.66 16.67 All mid-to-late layers: (10 th → 16 th ) 76.25 46.67 10.08 13.45 15.15 

25 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents Tab. 13 shows Bifrost effectiveness under different steering layers. We conduct experiment on single steering from 2 th 

to the last layer, and multiple steering layers from 10 th to 16 th layers. 26 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

E Bifrost Prompts 

We present all prompts used for Bifrost in our experiments. Sec E.0.1 demonstrates AQUA and GSM8K prompts used in math problem solving. Sec E.0.2 shows ARC-Easy and GPQA-Diamond prompts for multi-choice question answering. Finally, Sec E.0.3 presents the HumanEval and LiveCodeBench prompts for the code generation problem. 

E.0.1 AQUA and GSM8K Prompts for Math Problem Solving AQUA prompt template 

Solve the following problem: Question: {question} Answer: 

GSM8K with AQUA in-context examples (Llama-3.1-8B-Instruct) 

1. Question: Find out which of the following values is the multiple of X, if it is divisible by 9 and 12? 

Answer: The value that is a multiple of both 9 and 12 will be a multiple of their LCM (Least Common Multiple). Step 1: First, we need to find the prime factorization of each number. The prime factors of 9 are 3 × 3. The prime factors of 12 are 2 × 2 × 3. Step 2: Next, we determine the least common multiple (LCM) of 9 and 12. 9 = 3 2, 12 = 2 2 × 3 LCM = 22 × 32 = 4 × 9 = 36 Step 3: Any number that is a multiple of 36 will also be a multiple of both 9 and 12. Therefore, the correct answer is 36. 2. Question: A trader sold an article at a profit of 20% for Rs.360. What is the cost price of the article? 

Answer: Let Cost Price (CP) = x Profit = 20% of CP = 0 .2x Selling Price (SP) = x + 0 .2x = 1 .2x

Given SP = Rs.360 ∴ 1.2x = 360 x = 360 1.2 = 300 The Cost Price of the article is Rs.300. 3. Question: Find the total no. of distinct bike no.’s that can be formed using 2 letters followed by 2 no.’s. How many letters need to be distinct? 

Answer: Assuming the two letters must be distinct and digits allow repetition. Step 1: For the first letter, there are 26 possibilities (A-Z). Step 2: For the second letter, it must be distinct from the first, so 25 possibilities. Step 3: For the two digits, each has 10 choices (0-9) with repetition allowed, so 

10 × 10 = 100 possibilities. Step 4: Total distinct bike numbers = 26 × 25 × 100 = 65000 . Therefore, the total number is 65000. 4. Question: A train running at a speed of 100 miles/hour, takes 10 hours to reach its destination. After covering quarter of the distance, it starts raining and the train has to be slowed to speed of 75 miles/hour. What is the total journey duration? 

Answer: Let total distance = D. Step 1: Original time for full distance at 100 mph is 10 hours, so 

D = 100 × 10 = 1000 miles. Step 2: Quarter distance = 14 × 1000 = 250 miles at 100 mph, time =  

> 250 100

= 2 .5 hours. Step 3: Remaining distance = 750 miles at 75 mph, time = 750 75 = 10 hours. Step 4: Total time = 2.5 + 10 = 12 .5 hours. Therefore, the total journey duration is 12.5 hours. 5. Question: At a certain factory, 10 percent of the staplers produced on Monday were defective and 2 percent of the non-defective staplers were rejected by mistake. If 72 of the non-defective staplers were rejected, what was the number of staplers produced that day? 

Answer: Let x = total number of staplers produced. Non-defective staplers = 90% = 0 .9x Rejected non-defective = 2% of non-defective 0.02 × 0.9x = 72 0 .018 x = 72 x = 72 0.018 = 4000 The factory produced 4000 staplers that day. 6. Question: An exam is given in a certain class. The average (arithmetic mean) of the highest score and the lowest score is equal to x. If the average score for the entire class is equal to y and there are z

students in the class, where z > 5, then in terms of x, y, and z, what is the average score for the class excluding the highest and lowest scorers? 

Answer: Total sum of scores = yz Sum of highest and lowest = 2 x Sum excluding highest and lowest 

= yz − 2x Number of remaining students = z − 2 ∴ Average = yz − 2xz − 2

27 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 7. Question: [5+? × 19 − 15 − 7] /[13 × 13 − 156] = 6 

Answer: (5+? × 19 − 15 − 7) /(169 − 156) = 6 (5+? × 19 − 22) /13 = 6 5+? × 19 − 22 = 78 ? × 19 = 95 ? = 5 5 

8. Question: {question} 

Answer: 

GSM8K prompt template with AQUA in-context examples (Llama-3.2-3B-Instruct) 

1. Question: Find out which of the following values is the multiple of X, if it is divisible by 9 and 12? 

Answer: The value that is a multiple of both 9 and 12 will be a multiple of their LCM (Least Common Multiple). Step 1: First, we need to find the prime factorization of each number. The prime factors of 9 are 3 × 3.The prime factors of 12 are 2 × 2 × 3.Step 2: Next, we determine the least common multiple (LCM) of 9 and 12. 9 = 3 2, 12 = 2 2 × 3 LCM = 22 × 32 = 4 × 9 = 36 

Step 3: Any number that is a multiple of 36 will also be a multiple of both 9 and 12. Therefore, the correct answer is 36. 2. Question: A trader sold an article at a profit of 20% for Rs.360. What is the cost price of the article? 

Answer: Let Cost Price (CP) = x Profit = 20% of CP = 0 .2x Selling Price (SP) = x + 0 .2x = 1 .2x

Given SP = Rs.360 ∴ 1.2x = 360 x = 360 1.2 = 300 The Cost Price of the article is Rs.300. 3. Question: At a certain factory, 10 percent of the staplers produced on Monday were defective and 2 percent of the non-defective staplers were rejected by mistake. If 72 of the non-defective staplers were rejected, what was the number of staplers produced that day? 

Answer: Let x = total number of staplers produced. Non-defective staplers = 90% = 0 .9x Rejected non-defective = 2% of non-defective 0.02 × 0.9x = 72 0 .018 x = 72 x = 72 0.018 = 4000 The factory produced 4000 staplers that day. 4. Question: An exam is given in a certain class. The average (arithmetic mean) of the highest score and the lowest score is equal to x. If the average score for the entire class is equal to y and there are z

students in the class, where z > 5, then in terms of x, y, and z, what is the average score for the class excluding the highest and lowest scorers? 

Answer: Total sum of scores = yz Sum of highest and lowest = 2 x Sum excluding highest and lowest 

= yz − 2x Number of remaining students = z − 2 ∴ Average = yz − 2xz − 2

5. Question: [5+? × 19 − 15 − 7] /[13 × 13 − 156] = 6 

Answer: (5+? × 19 − 15 − 7) /(169 − 156) = 6 (5+? × 19 − 22) /13 = 6 5+? × 19 − 22 = 78 ? × 19 = 95 ? = 5 5 

6. Question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell? 

Answer: Let x = number of bags sold. Selling price per bag = $100 Profit = 25% of selling price 

=⇒ Profit per bag = $25 Cost per bag = $100 - $25 = $75 Total profit = $3000 ∴ x × 25 = 3000 

x = 3000 25 = 120 (Note: The original solution contained an error in interpretation; correct answer is 120 bags.) 

7. Question: While selling a watch, a shopkeeper gives a discount of 5%. If he gives a discount of 7%, he earns Rs. 15 less as profit. The marked price of the watch is: 

Answer: Let Marked Price = x (also treated as cost price in the solution provided) With 5% discount: SP = 0.95 x, Profit = 0.05 x With 7% discount: SP = 0.93 x, Profit = 0.07 x? (Note: original solution appears inconsistent) Difference in profit: 0.05 x − 0.07 x = −0.02 x = −15 0 .02 x = 15 x = 750 

Marked Price = Rs. 750 28 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 8. Question: {question} 

Answer: 

GSM8K with AQUA in-context examples (Llama-3.2-1B-Instruct) 

1. Question: Find out which of the following values is the multiple of X, if it is divisible by 9 and 12? 

Answer: The value that satisfies these conditions is 36. Step-by-step reasoning process: To find a number that is both a multiple of X and divisible by 9 and 12, we need to identify the least common multiple (LCM) of 9 and 12. Multiples of 9: 9, 18, 27, 36, 45, . . . Multiples of 12: 12, 24, 36, 48, . . . The smallest common multiple is 36. Therefore, any number divisible by both 9 and 12 is a multiple of 36, so X = 36. 36 

2. Question: Add: +45 and -30 

Answer: (+45) + (-30) = 45 - 30 = 15 The result is 15. 3. Question: [5 + ? × 19 - 15 - 7]/[13 × 13 - 156] = 6 

Answer: Denominator: 13 × 13 - 156 = 169 - 156 = 13 So the equation becomes: (5 + ? × 19 - 15 - 7) / 13 = 6 (5 + ? × 19 - 22) / 13 = 6 Multiply both sides by 13: 5 + ? × 19 - 22 = 78 ? × 19 - 17 = 78 ? × 19 = 95 ? = 95 / 19 = 5 5

4. Question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell? 

Answer: Profit is 25% of selling price. Selling price per bag = $100 Profit per bag = 25% of 100 = $25 Total profit = 3,000 Number of bags = total profit ÷ profit per bag = 3000 ÷ 25 = 120 The grocer sold 120 bags. 5. Question: On a normal day Bill usually averages about 15 mph when riding his bicycle. On a windy day, his speed is reduced by 4 mph. How far can Bill travel on a windy day in 21 minutes? Round to the nearest hundredth. 

Answer: Normal speed = 15 mph Windy day speed = 15 - 4 = 11 mph Time = 21 minutes = 21/60 hours = 0.35 hours Distance = speed × time = 11 × (21/60) = 231/60 = 3.85 miles 3.85 

6. Question: A jeep travels a certain distance taking 6 hours in the forward journey. During the return journey, it increased its speed by 12km/hr and took 4 hours. What is the distance travelled by the jeep? 

Answer: Let original speed = x km/h Distance = speed × time Forward: distance = x × 6 Return: speed = x + 12 km/h, distance = (x + 12) × 4 Since distance is same: 6x = 4(x + 12) 6x = 4x + 48 2x = 48 x = 24 km/h Distance = 24 × 6 = 144 km The distance travelled is 144 km. 7. Question: While selling a watch, a shopkeeper gives a discount of 5%. If he gives a discount of 7%, he earns Rs. 15 less as profit. The marked price of the watch is: 

Answer: Let Marked Price = x (assuming cost price is not needed for difference calculation) With 5% discount: SP = 0.95x With 7% discount: SP = 0.93x Profit difference = 0.95x - 0.93x = 0.02x Given difference = Rs. 15 0.02x = 15 x = 15 / 0.02 = 750 Marked price = Rs. 750 8. Question: {question} 

Answer: 

E.0.2 ARC-Easy and GPQA-Diamond Prompts for Mutli-Choice Question Answering ARC-Easy prompt template Question : {question} A) {option-a} B) {option-b} C) {option-c} D) {option-d} 

Answer :

29 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

GPQA-Diamond with ARC-Easy in-context examples (Llama-3.1-8B-Instruct) 

1. Question: Which technology was developed most recently? A) cellular telephone B) television C) refrigerator D) airplane 

Answer: AThe best answer is A. Cellular telephones are a relatively recent technology, developed in the 1970s and 1980s. Televisions were developed in the 1920s and 1930s, refrigerators in the 1910s and 1920s, and airplanes in the 1900s and 1910s. Therefore, (A) is correct. (B), (C), and (D) are incorrect because these technologies were developed earlier than cellular telephones. (Note: While the first smartphone was released in 1992, the first cellular telephone was released in 1983. Therefore, (A) is still the best answer.) The development of technology is a continuous process, with new technologies being developed all the time. However, the development of technology is not always linear. New technologies can be developed from existing technologies, and existing technologies can be improved upon. For example, the development of the airplane was influenced by the development of the automobile and the bicycle. The development of the automobile was influenced by the development of the steam engine and the internal combustion engine. The development of the steam engine was influenced by the development of the waterwheel and the windmill. The development of the waterwheel and. 2. Question: Planets in the solar system are in constant motion. What factor has the greatest effect on the orbits of the planets? A) the size of the planets B) gravitational pull of the Sun C) the composition of the planets D) electromagnetic radiation from the Sun 

Answer: BThe gravitational pull of the Sun is the primary force that keeps the planets in orbit around the Sun. The best answer is B. Step 1: Identify the key concept The key concept in this question is the factor that has the greatest effect on the orbits of the planets in the solar system. Step 2: Analyze the options We need to analyze each option to determine which one has the greatest effect on the orbits of the planets. Step 3: Eliminate incorrect options Option A, the size of the planets, is not a significant factor in determining the orbits of the planets. Option C, the composition of the planets, also does not have a significant impact on the orbits. Option D, electromagnetic radiation from the Sun, is not a primary force that affects the orbits of the planets. Step 4: Identify the correct option The gravitational pull of the Sun is the primary force that keeps the planets in orbit around the Sun. This is the correct option. The final answer is: B

3. Question: Which of the following is produced during the process of cellular respiration? A) carbon dioxide B) sodium chloride C) oxygen D) sugar 

Answer: ACarbon dioxide is produced during cellular respiration. The best answer is A. Step 1: Identify the process mentioned in the question. The process mentioned in the question is cellular respiration. Step 2: Recall what cellular respiration produces. Cellular respiration is the process by which cells break down glucose to produce energy. It involves the breakdown of glucose to produce carbon dioxide, water, and ATP (adenosine triphosphate), which is the energy currency of the cell. Step 3: Match the products of cellular respiration with the options given. The products of cellular respiration include carbon dioxide, water, and ATP. Among the options given, carbon dioxide is the correct answer. Step 4: Eliminate incorrect options. Sodium chloride (table salt) is not a product of cellular respiration. Oxygen is actually a reactant in cellular respiration, not a product. Sugar is a reactant in cellular respiration, not a product. The final answer is: A

30 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 4. Question: A sample of sulfur forms crystals when it A) melts. B) freezes. C) evaporates. D) condenses. 

Answer: BSulfur is a solid at room temperature. The best answer is B. Step 1: Understand the properties of sulfur Sulfur is a solid at room temperature. Step 2: Determine the process that forms crystals Crystals form when a substance solidifies from a liquid state. Step 3: Identify the correct process for sulfur Since sulfur is a solid at room temperature, it will form crystals when it freezes. The final answer is: B

5. Question: Which form of solar radiation causes sunburn? A) Visible B) Ultraviolet C) Infrared D) X-rays 

Answer: BUltraviolet radiation causes sunburn. The best answer is B. Step 1: Identify the types of solar radiation There are several types of solar radiation, including visible light, ultraviolet (UV) radiation, infrared (IR) radiation, and X-rays. Step 2: Determine which type of radiation causes sunburn Sunburn is a result of exposure to a specific type of radiation that damages the skin’s outer layer. Step 3: Eliminate incorrect options Visible light is what we can see, infrared radiation is what we feel as heat, and X-rays are a form of ionizing radiation that is not typically associated with sunburn. Step 4: Select the correct answer Ultraviolet radiation is the type of solar radiation that causes sunburn. The final answer is: B

6. Question: About 75% of the world’s active volcanoes are the result of tectonic activity around which plate? A) North American Plate B) African Plate C) Pacific Plate D) Nazca Plate 

Answer: CThe Pacific Ring of Fire is a horseshoe-shaped zone of intense seismic and volcanic activity that stretches from New Zealand, along the eastern edge of Asia, through the Philippines, Japan, and the Aleutian Islands, and down the western coast of North and South America. The Pacific Plate is the largest of the Earth’s tectonic plates, covering an area of about 155.6 million square kilometers (60.1 million square miles). It is also the most volcanically active plate, with about 75% of the world’s active and dormant volcanoes located on this plate. Plate tectonics is the theory that the Earth’s outer shell is divided into several plates that glide over the mantle, the rocky inner layer above the core. Plate movement is the result of convection currents in the mantle, which are caused by heat from the core. The movement of the plates is responsible for the creation of mountain ranges, volcanoes, and earthquakes. The best answer is C. 7. Question: Oak trees produce seeds that are contained in acorns. Blue jays eat the seeds in acorns. Blue jays also collect acorns and hide them in the ground, often far away from the parent oak tree. Blue jays do not eat the seed of every acorn they hide. How do oak trees benefit from blue jays’ collecting and hiding acorns? A) The oak trees are pollinated by the blue jays. B) The oak trees are protected from other herbivores. C) The seeds of oak trees are protected from the sun. D) The seeds of oak trees are planted in new environments. 

Answer: DThe blue jays are helping the oak trees by planting the seeds in new environments. The other options are not correct because blue jays do not pollinate oak trees, protect them from herbivores, or protect the seeds from the sun. (Skill 1b) The best answer is D. (Skill 1b) - Science in Personal and Social Perspectives. This question requires 31 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents the student to think about the relationship between the oak trees and the blue jays. The student must understand that the blue jays are helping the oak trees by planting the seeds in new environments. This is an example of symbiosis, where one organism benefits and the other is not harmed. This is a key concept in science. 8. Question: {question} A) {option-a} B) {option-b} C) {option-c} D) {option-d} 

Answer :

GPQA-Diamond with ARC-Easy in-context examples (Llama-3.2-3B-Instruct) 

1. Question: Which technology was developed most recently? A) cellular telephone B) television C) refrigerator D) airplane 

Answer: AA cellular telephone was developed most recently. The other options were developed before the invention of the cellular telephone. The best answer is A.2. Question: Planets in the solar system are in constant motion. What factor has the greatest effect on the orbits of the planets? A) the size of the planets B) gravitational pull of the Sun C) the composition of the planets D) electromagnetic radiation from the Sun 

Answer: 

Step-by-step reasoning process: B

Planets in the solar system are in constant motion. What factor has the greatest effect on the orbits of the planets? (A) The size of the planets has no effect on the orbits of the planets. (C) The composition of the planets has no effect on the orbits of the planets. (D) Electromagnetic radiation from the Sun has no effect on the orbits of the planets. The gravitational pull of the Sun has the greatest effect on the orbits of the planets. The best answer is B.3. Question: Which of the following is produced during the process of cellular respiration? A) carbon dioxide B) sodium chloride C) oxygen D) sugar 

Answer: ACarbon dioxide is a byproduct of cellular respiration. The best answer is A.4. Question: Which form of solar radiation causes sunburn? A) Visible B) Ultraviolet C) Infrared D) X-rays 

Answer: BUltraviolet radiation is the cause of sunburn. The best answer is B.5. Question: Which of the following is the most accurate measurement for the length of an object that is actually 15.0 m long? A) 15.35 m B) 15.2 m C) 14.55 m D) 14.5 m 

Answer: 

Step-by-step reasoning process: (B) 

The uncertainty in the length of the object is ±0.01 m. The most accurate measurement is the one that is closest to the true length of the object. The true length of the object is 15.0 m, so the most accurate measurement is 15.2 m. The best answer is B.6. Question: Which type of graph would best display the changes in temperature over a 24 hour period? A) line graph B) pictograph C) circle (pie) graph D) stem-and-leaf graph 32 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

Answer: 

Step-by-step reasoning process: A line graph is used to show changes over time. A pictograph is used to show the number of people in different groups. A circle (pie) graph is used to show the number of people in different groups. A stem-and-leaf graph is used to show the number of people in different groups. Therefore, (A) is correct. (B)–(D) are incorrect because they are not used to show changes over time. The best answer is A.7. Question: Which is a true statement about cells? A) Plant cells contain chloroplasts. B) Animal cells are missing a nucleus. C) Only plant cells have a cell membrane. D) Animal cells include a rigid wall structure. 

Answer: APlant cells contain chloroplasts, which are organelles that contain the pigment chlorophyll. This pigment is necessary for photosynthesis. The best answer is A.8. Question: {question} A) {option-a} B) {option-b} C) {option-c} D) {option-d} 

Answer :

GPQA-Diamond with ARC-Easy in-context examples (Llama-3.2-1B-Instruct) 

1. Question: Which technology was developed most recently? A) cellular telephone B) television C) refrigerator D) airplane 

Answer: Step-by-step reasoning process: A- The cellular telephone was first developed in the 1970s. The best answer is A. 2. Question: Which form of solar radiation causes sunburn? A) Visible B) Ultraviolet C) Infrared D) X-rays 

Answer: Step-by-step reasoning process: B -Ultraviolet radiation is the primary cause of sunburn. The best answer is B. 3. Question: Oak trees produce seeds that are contained in acorns. Blue jays eat the seeds in acorns. Blue jays also collect acorns and hide them in the ground, often far away from the parent oak tree. Blue jays do not eat the seed of every acorn they hide. How do oak trees benefit from blue jays’ collecting and hiding acorns? A) The oak trees are pollinated by the blue jays. B) The oak trees are protected from other herbivores. C) The seeds of oak trees are protected from the sun. D) The seeds of oak trees are planted in new environments. 

Answer: Step-by-step reasoning process: D -Blue jays collect acorns and hide them in the ground, often far away from the parent oak tree. This protects the seeds from other herbivores. Choice A is incorrect because the blue jays do not pollinate the oak trees. Choice B is incorrect because the blue jays do not protect the oak trees from other herbivores. Choice C is incorrect because the seeds of oak trees are not protected from the sun. The best answer is D. 4. Question: The changing appearances of the nighttime sky over the surface of Earth and eclipses of the Moon have provided evidence that A) Earth is a sphere. B) Earth supports life. C) Earth has a layered atmosphere. D) Earth is covered mostly with water. 

Answer: A The changing appearances of the nighttime sky over the surface of Earth and eclipses of the Moon have provided evidence that Earth is a sphere. The best answer is A. 5. Question: If the environment in an area is destroyed, a new community of organisms sometimes takes the place of previous communities. The introduction of a new community of organisms is known as A) evolution. B) adaptation. C) biological diversity. D) ecological succession. 33 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

Answer: D -Ecological succession is the process of change in the species composition of a community over time. The best answer is D. 6. Question: Which of these is a characteristic of the metal gold? A) malleable B) lighter than water C) magnetic D) harder than a diamond 

Answer: Step-by-step reasoning process: A -Gold is malleable, meaning it can be pounded into thin sheets. It is also denser than water, so it is heavier than water. The best answer is A. 7. Question: When placed in a glucose solution, yeast cells will produce bubbles. Which process are the yeast cells carrying out? A) chemosynthesis B) photosynthesis C) respiration D) transpiration 

Answer: Step-by-step reasoning process: C -Yeast cells produce carbon dioxide gas as a byproduct of their metabolism. The best answer is C. 8. Question: {question} A) {option-a} B) {option-b} C) {option-c} D) {option-d} 

Answer :

E.0.3 HumanEval and LiveCodeBench Prompts for Code Generation HumanEval prompt template 

Solve the following code problem: {prompt} Complete the function below. def {entry_point} 

LivdeCodeBench with HumanEval in-context examples (Llama-3.1-8B-Instruct) 

You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests. ### Examples: 

def has_close_elements(numbers: List[float], threshold: float) -> bool: """ Check if in given list of numbers, are any two numbers closer to each other than ,→

given threshold. >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True """ for i in range(len(numbers)): for j in range(i + 1, len(numbers)): if abs(numbers[i] - numbers[j]) < threshold: return True return False def separate_paren_groups(paren_string: str) -> List[str]: # Remove spaces from the input string 

34 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

paren_string = paren_string.replace(" ", "") # Initialize an empty list to store the separated groups separated_groups = [] # Initialize an empty string to build the current group current_group = "" # Initialize a counter to keep track of the balance of parentheses balance = 0 # Iterate over each character in the input string for char in paren_string: # If the character is an open parenthesis, increment the balance if char == "(": balance += 1 current_group += char # If the character is a close parenthesis, decrement the balance elif char == ")": balance -= 1 current_group += char # If the balance is zero, it means we 've found a complete group if balance == 0: # Add the current group to the list of separated groups separated_groups.append(current_group) # Reset the current group and balance current_group = "" balance = 0 # Return the list of separated groups return separated_groups def truncate_number(number: float) -> float: """ Given a positive floating point number, it can be decomposed into and integer part (largest integer smaller than given number) and decimals (leftover part always smaller than 1). Return the decimal part of the number. >>> truncate_number(3.5) 0.5 """ # First, we need to separate the integer part from the decimal part. # We can do this by using the built-in int() function to get the integer part. integer_part = int(number) # Now, we need to get the decimal part. We can do this by subtracting the integer part from the original number. ,→

# This will give us the decimal part, which is always smaller than 1. decimal_part = number - integer_part # Finally, we return the decimal part. return decimal_part def below_zero(operations: List[int]) -> bool: """ You 're given a list of deposit and withdrawal operations on a bank account that starts with ,→

zero balance. Your task is to detect if at any point the balance of account falls below zero, and ,→

35 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

at that point function should return True. Otherwise it should return False. >>> below_zero([1, 2, 3]) False >>> below_zero([1, 2, -4, 5]) True """ balance = 0 for operation in operations: balance += operation if balance < 0: return True return False def mean_absolute_deviation(numbers: List[float]) -> float: """ For a given list of input numbers, calculate Mean Absolute Deviation around the mean of this dataset. Mean Absolute Deviation is the average absolute difference between each element and a centerpoint (mean in this case): MAD = average | x - x_mean | >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0]) 1.0 """ # Calculate the mean of the input list mean = sum(numbers) / len(numbers) # Calculate the absolute difference between each element and the mean absolute_diffs = [abs(num - mean) for num in numbers] # Calculate the mean of the absolute differences mad = sum(absolute_diffs) / len(absolute_diffs) return mad 

### Question: {question} ### Format: Implement a function called main() which orchestrates the solution by reading inputs from stdin as sys.stdin.read().split() , do not use input() , and writing the answer to stdout. Feel free to use additional functions as necessary. Next do NOT forget to call main function at the end of the program otherwise you will not be awarded any points. 

``` python # YOUR CODE HERE 

``` 

### Answer: (use the provided format with backticks) 

LivdeCodeBench with HumanEval in-context examples (Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct) 

You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests. ### Examples: 

def has_close_elements(numbers: List[float], threshold: float) -> bool: """ Check if in given list of numbers, are any two numbers closer to each other than ,→

given threshold. 

36 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

>>> has_close_elements([1.0, 2.0, 3.0], 0.5) False >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True """ # Sort the list of numbers in ascending order numbers.sort() # Iterate over the sorted list of numbers for i in range(len(numbers) - 1): # For each number, check if the difference between it and the next number is less than the threshold ,→

if numbers[i + 1] - numbers[i] < threshold: # If the difference is less than the threshold, return True return True # If the function hasn 't returned True after iterating over the entire list, return False ,→

return False def truncate_number(number: float) -> float: """ Given a positive floating point number, it can be decomposed into and integer part (largest integer smaller than given number) and decimals (leftover part always smaller than 1). Return the decimal part of the number. >>> truncate_number(3.5) 0.5 """ # First, we need to find the integer part of the number, which is the largest # integer smaller than the given number. We can use the math.floor function # for this. import math integer_part = math.floor(number) # Now, we need to find the decimal part of the number. This is simply the # difference between the original number and the integer part. decimal_part = number - integer_part # Finally, we return the decimal part of the number. return decimal_part def below_zero(operations: List[int]) -> bool: """ You 're given a list of deposit and withdrawal operations on a bank account that starts with ,→

zero balance. Your task is to detect if at any point the balance of account fallls below zero, and ,→

at that point function should return True. Otherwise it should return False. >>> below_zero([1, 2, 3]) False >>> below_zero([1, 2, -4, 5]) True """ balance = 0 for operation in operations: 

37 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents 

balance += operation if balance < 0: return True return False def mean_absolute_deviation(numbers: List[float]) -> float: """ For a given list of input numbers, calculate Mean Absolute Deviation around the mean of this dataset. Mean Absolute Deviation is the average absolute difference between each element and a centerpoint (mean in this case): MAD = average | x - x_mean | >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0]) 1.0 """ # Calculate the mean of the input list mean = sum(numbers) / len(numbers) # Initialize a variable to store the sum of absolute deviations sum_absolute_deviation = 0 # Iterate over each number in the input list for num in numbers: # Calculate the absolute deviation of the current number from the mean absolute_deviation = abs(num - mean) # Add the absolute deviation to the sum sum_absolute_deviation += absolute_deviation # Calculate the mean absolute deviation by dividing the sum by the length of the list ,→

mad = sum_absolute_deviation / len(numbers) # Return the mean absolute deviation return mad def filter_by_substring(strings: List[str], substring: str) -> List[str]: """ Filter an input list of strings only for ones that contain given substring >>> filter_by_substring([], 'a')[] >>> filter_by_substring([ 'abc ', 'bacd ', 'cde ', 'array '], 'a')['abc ', 'bacd ', 'array ']""" # Initialize an empty list to store the filtered strings filtered_strings = [] # Iterate over each string in the input list for string in strings: # Check if the substring is present in the current string if substring in string: # If the substring is found, append the string to the filtered list filtered_strings.append(string) # Return the filtered list of strings return filtered_strings 

### Question: {question} 38 Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents ### Format: Implement a function called main() which orchestrates the solution by reading inputs from stdin as sys.stdin.read().split() , do not use input() , and writing the answer to stdout. Feel free to use additional functions as necessary. Next do NOT forget to call main function at the end of the program otherwise you will not be awarded any points. 

``` python # YOUR CODE HERE 

``` 

### Answer: (use the provided format with backticks) 39