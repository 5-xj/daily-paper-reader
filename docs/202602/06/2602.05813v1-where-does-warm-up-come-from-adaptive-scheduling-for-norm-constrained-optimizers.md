# Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers
# 预热从何而来？范数约束优化器的自适应调度

**Authors**: Artem Riabinin, Andrey Veprikov, Arman Bolatov, Martin Takáč, Aleksandr Beznosikov \\
**Date**: 2026-02-05 \\
**PDF**: https://arxiv.org/pdf/2602.05813v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 6.0 \\
**Evidence**: Adaptive scheduling and automatic algorithm tuning for optimizers \\

---

## Abstract
We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.   Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup

## 摘要
我们研究了范数约束优化器（如 Muon 和 Lion）的自适应学习率调度。我们引入了一种广义光滑性假设，在该假设下，局部曲率随次优间隙（suboptimality gap）的减小而降低，并从经验上验证了这种

---

## 速览摘要（自动生成）

**问题**：规范约束优化器（如Muon/Lion）的预热机制缺乏理论支撑且依赖手动调优。

**方法**：提出广义平滑假设，揭示曲率随优化过程降低的规律，并