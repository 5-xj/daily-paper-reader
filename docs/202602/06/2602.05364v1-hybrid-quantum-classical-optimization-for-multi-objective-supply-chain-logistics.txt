Title: Hybrid Quantum-Classical Optimization for Multi-Objective Supply Chain Logistics

URL Source: https://arxiv.org/pdf/2602.05364v1

Published Time: Fri, 06 Feb 2026 01:42:40 GMT

Number of Pages: 27

Markdown Content:
# Hybrid Quantum-Classical Optimization for Multi-Objective Supply Chain Logistics 

## Raoul Heese 1,* , Timothée Leleu 2,3 , Sam Reifenstein 2,Christian Nietner 1, and Yoshihisa Yamamoto 2

> 1

NTT Data Germany 

> 2

NTT Research 

> 3

Stanford University 

> *

corresponding author: raoul.heese@nttdata.com 

Abstract 

A multi-objective logistics optimization problem from a real-world supply chain is formulated as a Quadratic Unconstrained Binary Optimization Problem (QUBO) that minimizes cost, emissions, and de-livery time, while maintaining target distributions of supplier workshare. The model incorporates realistic constraints, including part dependencies, double sourcing, and multimodal transport. Two hybrid quantum-classical solvers are proposed: a structure-aware informed tree search (IQTS) and a modular bilevel frame-work (HBS), combining quantum subroutines with classical heuristics. Experimental results on IonQ’s Aria-1 hardware demonstrate a methodology to map real-world logistics problems onto emerging combinatorial optimization-specialized hardware, yielding high-quality, Pareto-optimal solutions. 

# 1 Introduction 

The interdependent structure of global supply chains for complex manufacturing is characterized by multi-factorial requirements that address economic, envi-ronmental, social, and political constraints. These factors require careful balancing of potentially con-flicting objectives, such as cost, sustainability, and re-silience [7]. The underlying combinatorial problems in global supply chain optimization are typically NP-hard, making them notoriously difficult to solve at scale. Recent research has turned to novel comput-ing paradigms, including specialized hardware like Ising machines [39] and quantum annealers [1], to address these challenges more efficiently. However, solving real-world optimization prob-lems with these emerging platforms remains a con-siderable open problem due to current technological limitations [1]. As a result, identifying suitable ap-plications for non-classical computing approaches is an active area of research [28]. In this context, it is im-portant to explore use cases that are both sufficiently hard to solve and capable of delivering real-world im-pact if solved more effectively. These criteria apply to supply chain optimization, making it a promis-ing use case candidate for further research. In this paper, a hybrid quantum-classical approach is devel-oped to solve a real-world, multi-objective logistics optimization problem. Our approach combines specialized model-ing with hybrid quantum-classical and quantum-inspired solvers. We integrate high-performance classical optimization techniques with advanced quantum methods, allowing us to estimate the Pareto frontier of a multi-objective optimization problem. For our numerical evaluations, we use Aria-1 [11], a state-of-the-art and commercially deployed quan-tum hardware trapped-ion quantum computer from IonQ. Since we focus on scalability and modularity, our findings are also applicable to a more general scope, opening up opportunities to leverage special-purpose Ising machines for complex real-world op-timization problems. Our main contributions are as follows. We de-veloped a multi-objective Quadratic Unconstrained Binary Optimization Problem (QUBO) model, aim-ing to optimize four Key Performance Indicators (KPIs) of a logistics network: carbon dioxide emis-sions, costs, time, and supplier target workshare fulfillment. The model integrates real-world con-1

> arXiv:2602.05364v1 [math.OC] 5 Feb 2026

straints such as mandatory workshare limits, mul-tiple transportation modes, and dependencies be-tween parts. The formulation as a QUBO was driven by the enabling of quantum algorithms. Next, we designed specialized solver tools—Informed Solu-tion Generator (ISG), Informed Solution Fixer (ISF), and Informed Solution Improver (ISI)—that exploit problem-specific knowledge to construct, repair, and refine solutions iteratively, improving solution qual-ity and feasibility. We propose two hybrid quantum-classical solvers: Informed Quantum-Enhanced Tree Solver (IQTS) and Hybrid Quantum-Classical Bilevel Solver (HBS). IQTS aims at exploiting our knowledge about the problem structure to achieve a very fast convergence. It is tightly tailored to the problem by integrating the Quantum Approximate Optimization Algorithm (QAOA) into an explorative tree decom-position. HBS is a general high-performance strat-egy with a highly modular architecture. It is aimed at providing computational speed, and scalability by combining quantum methods such as QAOA, quantum-inspired methods such as Chaotic Ampli-tude Control with Momentum (CACm), and clas-sical methods such as Iterative Belief Propagation (IBP) and Dynamic Anisotropic Smoothing (DAS) into a bilevel optimization framework. We test our algorithms on an industrially relevant logistic net-work provided by Airbus and are able to find Pareto-optimal solution candidates. The remaining paper is structured a follows. In Section 2, we outline related work. Subsequently, in Section 3, we describe the use case. Our solution ap-proaches are then presented in Section 5. In Section 6, we show the results of our numerical evaluations and experiments. We end with a conclusion in Section 7. 

# 2 Related work 

Quantum optimization in an emerging field [1] with many different application domains, mostly focusing on QUBOs [46]. Multi-objective QUBOs have already been considered [5], also with respect to different scalarisation [4] and standardization techniques [30]. Significant effort has been dedicated in recent years to advancing hybrid quantum-classical methodolo-gies for logistics optimization, particularly to address the computational limitations of current quantum hardware. A hybrid workflow has been proposed to tackle real-scale multi-truck vehicle routing prob-lems through iterative quantum computations, mak-ing it feasible for Noisy Intermediate-Scale Quantum (NISQ) devices by segmenting complex tasks into manageable quadratic optimization instances suit-able for quantum annealers [48]. Similarly, quantum annealing has been used effectively for the Trans-port Network Design Problem (TNDP), formulated as QUBO, providing computational benefits over classical Tabu search methods, with implications ex-tending across various network optimization scenar-ios [15]. Furthermore, a quantum-classical strategy called Q4RPD has been developed, specifically ad-dressing realistic constraints in vehicle routing, such as heterogeneous vehicle fleets, utilizing the D-Wave Leap solver [42]. In addition, QAOA has been ap-plied to the Heterogeneous Vehicle Routing Problem (HVRP), demonstrating potential for combinatorial optimization, despite facing scalability challenges re-lated to the required number of qubits [18]. Variational approaches, such as QAOA and ad-vanced forms of Variational Quantum Eigensolver (VQE), have been employed to solve job shop scheduling and vehicle routing problems, demon-strating the feasibility of these techniques on current quantum hardware and achieving near-optimal re-sults within small-scale instances [3, 29]. In paral-lel, hybrid annealing frameworks have proven ef-fective in addressing multi-agent pathfinding and complex delivery routing under realistic constraints, with iterative quantum subproblem solving enabling scalability to industry-relevant scenarios [19]. More-over, hybrid learning architectures integrating quan-tum circuits into reinforcement learning agents or neural networks have achieved competitive perfor-mance in logistics applications like fleet dispatch and backorder prediction, revealing promising intersec-tions between quantum machine learning and sup-ply chain analytics [12, 23]. 

# 3 Use case 

The 2024 Quantum Computing Challenge from Airbus and BMW [2] contained a task in which the goal was to optimize a logistics network for a sustainable air-craft manufacturing process within a global supply chain. One aspect of the supply chain of the Airbus air-craft manufacturing process is to decide, by which supplier at which global production sites, the air-craft parts are produced and how they are trans-ported between these sites for assembling. The goal is to minimize costs, production time, carbon diox-ide emissions, and supplier target workshare fulfill-ments while ensuring certain constraints that require that the assignments are sufficiently distributed both between all suppliers and all production sites. This logistics problem is a particularly challenging aspect 2of the supply chain optimization and was proposed by Airbus. In the following, the Airbus supply chain problem is used to illustrate the proposed method, which is general and applicable to a broader class of optimization problems. 

# 4 Model 

To begin with, we first present a detailed prob-lem description, followed by a formal definition of the properties of a problem instance. Subsequently, we provide a multi-objective Quadratic Constrained Binary Optimization Problem (QCBO) formulation and show how it can be reduced to a scalarized QUBO form. 

## 4.1 Problem description 

The supply chain logistics model we consider is based on the production of a single product. This product is assembled from a number of individual parts—48 in the case of the Airbus aircraft. The so-called Product Breakdown Structure (PBS) shown in Fig. 1a describes the dependency of the parts as a tree structure. Each node is a part and its child nodes indicate which other parts are required for its pro-duction. The parts represented by leaf nodes can therefore be produced without any dependencies. The completed aircraft is shown as the root. Each part can be assigned a level (from 0 to 4) that indi-cates how many production steps it takes for it to be integrated into the final product. In total, there are 29 suppliers, each of which can produce a specific selection of parts at specific pro-duction sites. To produce a part at a specific site, all parts it depends on must be available at the same site. To that end, parts can be transported from one site to another. Transportation can be done by differ-ent means depending on the start and end site and the part to be transported. Options include trucks, ships, and cargo planes. Each mode of transportation is associated with carbon dioxide emissions, costs, and transportation time. A part can either be trans-ported directly or indirectly via certain hubs, called warehouses, where it can be transshipped. Conse-quently, a large variety of transportation options can be selected for each part. We consider 43 production sites and 28 warehouses as sketched in Fig. 1b, which are distributed globally across 18 different regions. To improve the supply chain resilience, it is nec-essary to install double sourcing, if possible. This means that the production of each part is proportion-ally split between two production sites (and suppli-ers), the primary site (supplier) and the secondary site (supplier). While the two sites (primary production site and secondary production site) must be differ-ent, the respective suppliers (primary supplier and secondary supplier) can be the same. Additionally, if possible, the two sites must be located in different regions. Regardless of whether it is produced as a primary or secondary production, each part is trans-ported to both the primary and secondary processing sites, resulting in up to four necessary transportation routes per part. The only exception are two immobile parts, which can only be produced and processed at the same site, which they will never leave. Each part is assigned a relative cost that measures its value in relation to the other parts. This relative cost is used to ensure a shared distribution of the production process among different production sites and suppliers. For this purpose, each manufacturing site and each supplier has a predefined lower and upper limit for the cumulative costs of the products produced, known as the workshare . In addition, each supplier has a fixed target for the workshare that should ideally be achieved. In summary, the task is a multi-objective assign-ment problem in which one needs to (i) assign each part to a primary and secondary production site and supplier, and (ii) assign transportation routes to move parts between production sites (possibly via warehouses). The assignments must meet the fol-lowing criteria: (i) compliance with the PBS depen-dencies, (ii) adherence to workshare fulfillment con-straints for production sites and suppliers, and (iii) the primary and secondary sites for each part must be distinct and, if possible, located in different regions. The goal is to jointly optimize four KPIs: cumulative carbon dioxide emissions, costs, production time, and supplier target workshare fulfillments. The first two KPIs are determined by the chosen transporta-tion routes alone, while production time is affected by both the chosen transportation routes and the se-lected production sites and suppliers. Lastly, target workshare fulfillment is determined by the supplier choices. 

## 4.2 Problem instances 

A problem instance is defined by the following quan-tities: 

• The set of parts I with |I| = 48 . Each part 

i ∈ I is assigned a value Vi ∈ R>0, which can be used to calculate the relative value vi := 100 Vi/V (in percentage) with the total value 

V := ∑ 

> i∈I

Vi. Furthermore, each part is as-signed a volume Vi ∈ R>0 (assuming a rectan-3aircraft    

> 0
> 1
> 2
> 3
> 4
> part’s level different route options for each part
> production sites warehouses PBS sites
> assign each part to a production site
> (a) (b)

Figure 1: Problem sketch: (a) The PBS showing dependencies of all 48 parts with the assembled product, an aircraft, as the root. (b) Supply chain sites comprised of 43 production sites and 28 warehouses. The shown positions do not reflect geographical locations. gular cuboid shape). The production dependen-cies between parts are defined by a set of tuples 

ϕ ∈ I2, where (i, j ) ∈ ϕ indicates that the pro-duction of part j (called “parent”) requires part 

i (called “child”). Every child has only one par-ent, which leads to a dependency tree, the PBS. The level Li := ∑ 

> (i′,j )∈ϕ

δi′i(Lj + 1) ∈ [0 , 4] of each part i ∈ I represents the necessary num-ber of production steps until the final product as assembled. 

• The set of production sites K with |K| = 43 .Each site k ∈ K is assigned one region r ∈ R out of the set of all regions V with |V | = 18 . To this end, we define Vvk ∈ { 0, 1} with ∑ 

> v∈V

Vvk = 1 

for all k ∈ K, where Vvk = 1 indicates that region v ∈ V is assigned to site k ∈ K (and 

Vvk = 0 that it is not assigned). Furthermore, 

Kmin  

> k

∈ [0 , 100] ⊂ N0 and Kmax  

> k

∈ [0 , 100] ⊂ N0

define the minimum and maximum workshare (in percentage), respectively, for site k ∈ K. The production processes at the sites is not further concerned, it is, for example, presumed that there is always sufficient capacity for immedi-ate production. 

• The set of warehouses W with |W | = 28 that can be used as intermediate transportation hubs but don’t produce anything. 

• The set of suppliers U with |U | = 29 . For each supplier u ∈ U , U min  

> u

∈ [0 , 100] ⊂ Z, U max  

> u

∈

[0 , 100] ⊂ N0, and U target  

> u

∈ [0 , 100] ⊂ N0 define the minimum, maximum, and target workshare (in percentage), respectively. 

• The set of feasible site-supplier combinations 

fi ⊆ K × U that are capable of producing part 

i ∈ I with |fi| ≥ 1. The set of assignable sites for a part i is denoted by Ki := {k|k ∈ K ∧ ∃ u ∈ U :(k, u ) ∈ fi} with |Ki| ≥ 1. Any part i that can be assigned to at least two sites (i.e., |Ki| ≥ 2)can be used for double sourcing. Any other part can not be used for double sourcing. The set of assignable regions for a part i is denoted by 

Vi := {v|v ∈ V ∧ Vvk = 1 ∧ ∃ u ∈ U : ( k, u ) ∈ fi}

with |Vi| ≤ | Ki|. Any part i that can be as-signed to at least two regions (i.e., |Vi| ≥ 2) can be double sourced in two distinct regions. Fur-thermore, for each tuple (k, u ) ∈ fi, the produc-tion time of the corresponding manufacturing process is given by ti→(k,u ) ∈ R≥0.

• The (possible empty) set of transportation meth-ods M ip⇒q ⊂ Mi ⊆ M for transporting part i ∈ I

from site or warehouse p ∈ K ∪ W to site or warehouse q ∈ K ∪ W , where |M | = 17 073 . The mappings cn := M → R≥0 with n ∈ { 1, 2, 3}

yield the carbon dioxide emissions ( c1), costs (c2), and transportation time ( c3), respectively, for each transportation method. In addition, each transportation method is assigned a total cargo volume Vm ∈ R>0 (assuming a rectangu-lar cuboid shape). We denote the set of trans-portation methods for part i ∈ I by Mi and the set of all transportation methods by M .

• The primary source share αi ∈ [0 .5, 1] is the frac-tion of the production of part i ∈ I at the pri-mary production site. The secondary share is consequently given by 1 − αi ∈ [0 , 0.5] and rep-resents the fraction of the production of part i at the secondary production site. Without loss of generality, the primary source share is by defini-4tion always larger than or equal to the secondary source share, i.e., αi ≥ 1 − αi for all i ∈ I.The data provided by Airbus contains a single lo-gistics network for the production of an aircraft with the PBS shown in Fig. 1a and the production location and warehouses shown in Fig. 1b. This single in-stance can be varied by a modification of the primary source shares (denoted αi). However, we mainly fo-cus on the solution of a single instance version. 

## 4.3 Multi-Objective QCBO Model 

We use an index n ∈ { 1, . . . , 4} to label the the four KPIs as carbon dioxide emissions ( n = 1), costs (n = 2 ) production time ( n = 3 ), and supplier tar-get workshare fulfillment ( n = 4 ). We model these quantities in the following way: 1. To determine the carbon dioxide emissions ( n =1) and costs ( n = 2 ), we take all chosen trans-portation methods of all parts into account. For each mode of transportation, we count the emis-sions and pricing of the shipment in proportion to the relative shipping space occupied by the shipped part. This approach takes into account the fact that each part takes up only a fraction of the available cargo space, so that the remaining space can be filled with other goods and is not wasted. It has a significant impact on the cost of large-capacity transportation modes, such as ships, which would otherwise be counted with their full emissions even if only a tiny fraction of their cargo space is used to ship a part. 2. For the production time ( n = 3 ), we consider the weighted cumulative transport times of all selected transportation methods instead of the actual lead time of the assembled product for two reasons. First, modeling the exact lead time greatly increases the complexity of the model. Second, we aim for a robust result, taking into account that in an efficient and resilient supply chain, all transportation modes should be cho-sen as small as possible independently of each other. As a weight of the transportation times we choose the level of the product in the PBS. This choice recognizes that parts deeper in the tree have more (indirect) dependencies, so their transport time is more important and should re-sult in a relatively larger contribution. 3. For the supplier target workshare fulfillments (n = 4 ), we choose the cumulative quadratic deviation from the relative target workshare for each supplier. Next, we describe how we realize this modeling approach for the KPIs. To that end, we first discuss the shipment of parts. For each part i ∈ I, the avail-able transportation methods Mi ⊂ M and the joint set of production sites K and warehouses W can also be considered as a graph Gi := ( K ∪ W, M i), where an edge M ip⇒q ⊂ Mi ⊆ M is meant to understood to connect node p to node q. In short, the graph Gi

defines how part i can be moved between different locations. Transportation methods can be combined into routes, where each route r ∈ Rik⇒l from site k ∈ K

to site l ∈ K represent a distinct path on the graph 

Gi, i.e., it consists of a sequence of ν ∈ N transporta-tion methods (graph edges) r := ( m1, . . . , m ν ) with 

m1 ∈ M ik⇒w, ms ∈ M iw⇒w′ for s ∈ { 2, . . . , ν − 1},and mν ∈ M iw⇒l with w, w ′ ∈ K ∪ W . For ν > 1, r

is an indirect route, where for ν = 1 , r := ( m) with 

m ∈ M ik⇒l is a direct route. By definition, routes al-ways start and end at production sites, but they may involve transportation methods which pass through warehouses. Here, we use Rik⇒l to denote the set of all available routes from k to l which visit each pro-duction site and warehouse (graph nodes) at most once along the path. Although this set can be quite extensive, it is not explicitly used in our final model. This will be clarified further below. If Rik⇒l = ∅,there is no available connection between k and l. The set of all available routes for a part i is denoted by 

Ri := ∪k,l ∈K Rik⇒l. A part i is considered immobile, if Ri = ∅ (and mobile otherwise). To take these different routing options into account in our modeling approach, we define a set of terminals 

for each part at each site, which is to be understood as a set of labels that can be used to identify the different outgoing routes from this site to other sites, consider-ing all possible direct and indirect routes. We define the set of terminals for part i ∈ I at site k ∈ K by 

T ik := {1, . . . , t ik} with the total number of terminals 

tik ≥ 1. First of all, we consider a mobile part (with 

Ri̸ = ∅), for which the number of terminals tik is given by the maximum number of outgoing routes over all destinations such that |T ik| = max l∈K |Rik⇒l|.Then, there exists a surjective but not necessarily in-jective mapping τ ikl : T ik → Rik⇒l for all l ∈ K with 

Rik⇒l̸ = ∅, such that each each terminal is assigned to a (not necessarily unique) route. In other words, given a part i ∈ I, a start site k ∈ K, and a destina-tion site l ∈ K with Rik⇒l̸ = ∅, each terminal t ∈ T ik

is associated with a route r ∈ Rik⇒l. Conversely, all routes starting from site k can be associated with at least one terminal in t ∈ T ik. We define a correspond-ing inverse mapping τ ikl : Rik⇒l → T ik from a route to a terminal, which is to be understood as an arbitrary but fixed realization of the inversion. We only pre-sume that suitable mappings τ ikl and τ ikl exist, but do 5not need to define them explicitly. For an immobile part (i.e., Ri = ∅), there are by definition no outgoing routes, independent of where the part is produced. In this case, we presume that there is a single terminal that represents the absence of any transportation. Summarized, we define the the total number of terminals as 

tik := 

{

max i∈I,l ∈K |Rik⇒l| , if Ri̸ = ∅

1 , if Ri = ∅ (1) for any part i ∈ I, either mobile or immobile. The concept of terminals allows us to integrate dif-ferent routing options into the existing assignment task. For this purpose, in addition to the require-ment that each part must be assigned to a site and supplier, we additional require that it must also be assigned to a terminal at the site. That means that a site-terminal-supplier tuple (k, t, u ) with k ∈ K,

t ∈ T ik and u ∈ U must be assigned to each part i ∈ I

(both for the primary and secondary production). Based on the feasible site-supplier combinations fi,the feasible site-terminal-supplier tuples are given by 

Fi := {(k, t, u ) | t ∈ T ik ∀ (k, u ) ∈ fi}.Presume a site-terminal-supplier assignment 

(k, t, u ) ∈ Fi for part i ∈ I and a site-terminal-supplier assignment (l, s, v ) ∈ Fj for part j ∈ I,where (i, j ) ∈ ϕ. Then, τ ikl (t) = r assigns a route 

r := ( m1, . . . , m ν ) ∈ Rik⇒l, which is used to transport part i from site k to site l, where part j is produced. By definition, the assigned route r depends on the sites k and l as well as the terminal t, but not on the suppliers u and v, and also not on the terminal v.Each transportation method within the route r im-plies a shipment that creates carbon dioxide emis-sions, increase costs and takes time. The total con-tribution of r to the first three KPIs is consequently given by the cumulative contributions of the trans-portation methods along the path, which we define as 

cin  

> (k,t )⇒(l,s )

:= ∑ 

> m∈τikl (t)

cn(m)γin (m) (2) for Rik⇒l̸ = ∅, based on the the prescribed KPI contri-butions cn from the problem instance and a custom scaling factor γin ≥ 0. While cin  

> (k,t )⇒(l,s )

is formally independent of the terminal s, we still include it in the notation for clarity. We use the scaling factor 

γin (m) := 

{ Vi 

> Vm

, if n ∈ { 1, 2}

Li , if n = 3 (3) as explained above. Since every part has a non-vanishing volume and must fit into the cargo space, 

Vi/Vm ∈ (0 , 1] .We aim to model the optimization problem as a QUBO. To this end, we define a set of binary vari-ables y, each identifiable by five indices. Specifically, the binary variable yai→(k,t,u ) ∈ { 0, 1} represents the assignment of part i ∈ I to terminal t ∈ T ik of site 

k ∈ K and supplier u ∈ U , with (k, t, u ) ∈ Fi. Here, 

a ∈ A := {1, 2} indicates either whether the assign-ment refers to the the primary production ( a = 1 )or the secondary production ( a = 2 ). If a part i ∈ I

does not allow double sourcing (i.e., |Ki| = 1 ), we set ya=2  

> i→(k,t,u )

:= ya=1 

> i→(k,t,u )

, effectively reducing the decision space. With these premises, we can formulate the opti-mization problem as: 

min  

> y

(C1(y), C 2(y), C 3(y), C 4(y)) (4) s.t. ∑

> a,b ∈A,u,v ∈U

μab i yai→(k,t,u )ybj→(l,s,v ) = 0 ∀ (i, j ) ∈ ϕ ∀ k, l ∈ K ∀ t ∈ T ik ∀ s ∈ T jl ∧ Rik⇒l = ∅ (4a) 

∑

> (k,t,u )∈Fi

yai→(k,t,u ) = 1 ∀ i ∈ I ∀ a ∈ A (4b) 

( ∑

> (k′,t,u )∈Fi

δk′kya=1 

> i→(k,t,u )

)( ∑

> (k′,t,u )∈Fi

δk′kya=2 

> i→(k,t,u )

)

= 0 ∀ i ∈ I ∀ k ∈ K ∧ | Ki| ≥ 2 (4c) 

( ∑

> (k,t,u )∈Fi

Vvk ya=1 

> i→(k,t,u )

)( ∑

> (k,t,u )∈Fi

Vvk ya=2 

> i→(k,t,u )

)

= 0 ∀ i ∈ I ∀ v ∈ V ∧ | Vi| ≥ 2 (4d) 

∑

> i∈I, (k′,t,u )∈Fi,a ∈A

δk′kviαai yai→(k,t,u ) ∈ [Kmin  

> k

, K max  

> k

] ∀ k ∈ K (4e) 

∑

> i∈I, (k,t,u ′)∈Fi,a ∈A

δu′uviαai yai→(k,t,u ) ∈ [U min  

> u

, U max  

> u

] ∀ u ∈ U (4f) 6In summary, this is a multi-objective QCBO with four KPIs and six types of constraints. As KPIs, we use carbon dioxide emissions ( C1), costs ( C2), and times ( C3) of all used means of transportation for all parts, with the former considered in proportion to the relative volume of the load, and the latter con-sidered in proportion to the level of each part as a measure of its priority. The fourth KPI is the supplier workshare target fulfillment ( C4), taken into account as the quadratic deviation from the target workshare of each supplier. Formally, the KPIs are defined by 

Cn(y) := 1

dn

∑  

> (i,j )∈ϕ
> (k,t,u )∈Fi,a ∈A
> (l,s,v )∈Fj,b ∈ARik⇒l̸=0

αai cin  

> (k,t )⇒(l,s )

(5a) 

× yai→(k,t.u )ybj→(l,s,v )

for dioxide emissions ( n = 1 ), costs ( n = 2 ), and transportation times ( n = 3 ), whereas the supplier target workshare fulfillments is defined by 

C4(y) := 1

d4

∑

> u∈U

[ ∑

> i∈I
> (k,t,v )∈Fi,a ∈A

δuv viαai yai→(k,t,u ) (6a) 

− U target 

> u

]2

,

where we recall the relative value vi. Furthermore, we make use of the abbreviation 

αai := 

{

αi , if a = 1 1 − αi , if a = 2 (7) for the primary (secondary) source share αi (1 − αi)and introduce the empirically chosen rescaling factor 

dn := 



ˆcn , if n ∈ { 1, 2}

ˆcn=3 ˆL , if n = 3 100 , if n = 4 

(8) to align the magnitudes of the KPIs without alter-ing the model structure. It contains the maximum contributions 

ˆcn := max  

> i∈Ip,q ∈K∪Wm∈Mip⇒q

cn(m) (9) and the level mid-range 

ˆL := min i∈I Li + max i∈I Li

2 = 2 , (10) which is used as an additional factor for the trans-portation times based on the observation that the scaling factor from Eq. (3) does not contain a rela-tive value for n = 3 . The constant value 100 for 

n = 4 is motivated by the percentage measure of the target workshares. By construction, all KPIs are non-negative. The six types of constraints ensure the require-ments of a feasible supply chain: avoid non-existing connections (Eq. (4a)), allow one one assignment for each production source (Eq. (4b)), enforce different sites for primary and secondary source (Eq. (4c)), enforce different regions for primary and secondary sources (Eq. (4d)), respect the site workshare limits (Eq. (4e)), and respect the supplier workshare limits (Eq. (4f)). 

## 4.4 QUBO Model 

To transform Eq. (4) into a QUBO, we perform the following steps: 1. Equation (4) involves multiple objectives. To reduce the multi-objective assignment problem to a single-objective assignment problem, we perform a scalarization of the KPIs with the weights w := ( w1, w 2, w 3, w 4) ∈ [0 , 1] 4, where ∑ 

> n

wn = 1 , such that the new optimization ob-jective reads ∑4 

> n=1

wnCn(y). Different choices of weights w represent different compromises between the objectives in the usual sense. 2. The explicit use of terminals results in a large number of terms in some expressions, which make them computationally challenging to han-dle. To address this, we employ a two-step pre-processing of the instance: first, a pathfinding step using Dĳkstra’s algorithm to eliminate sub-optimal terminals (based on the chosen scalar-ization weights w), and second, a refinement of the feasible solution space to reduce the overall number of variables (indepentent of the weights 

w). The preprocessing is explained in detail in Section A. As a result, we obtain a reduced model with less terms and variables. 3. The reduced model still contains constraints, which need to be transformed into penalty terms. For the equality constraints, this can be realized straightforwardly. The box constraints, however, must first be converted into equal-ity constraints. To achieve this, we employ a standard method [6], which requires us to ap-proximate the relatives values vi and the pri-mary source shares αi as rational numbers for all parts i ∈ I. Using auxiliary (ancilla) vari-ables z ∈ { 0, 1}Nz , we can then reformulate the 7box constraints as equality constraints and trans-form them into penalty terms. This method is described in Section B. As a result, we arrive at the final QUBO form 

min  

> x

Q(x) (11a) with the objective 

Q(x) := Q(w, λ, x ) (11b) 

:= 

> 4

∑

> n=1

Cn(w, y ) + 

> 6

∑

> n=1

λnPn(x),

which consists of the four weighted KPIs Cn(wn, y )

and the six penalty terms Pn(x) with the correspond-ing Lagrange multipliers λn > 0, where we use the notation λ := ( λ1, . . . , λ 6). The set of binary variables 

x ∈ { 0, 1}Nx with Nx := Ny + Nz fully describes the supply chain configuration. All expressions from Eq. (11b) are defined in Section C. The resulting QUBO objective, Eq. (11b), can also be written in a quadratic form 

Q(x) = x⊺Qx (12) with a matrix Q := Q(w, λ ) ∈ RNx×Nx . Alternatively, using the coordinate transformation s := 2 x − 1, the QUBO can also be straightforwardly converted into an Ising optimization problem [34] 

min  

> s

H(w, λ, s ) (13a) based on the Ising model (also called Ising Hamilto-nian) 

H(w, λ, s ) := H(s) = s⊺Js + hs (13b) with spin variables s ∈ {− 1, 1}Nx , a matrix J := 

J(w, λ ) ∈ RNx×Nx , and a vector h := h(w, λ ) ∈ RNx .

# 5 Methods 

To solve Eq. (11), we propose two hybrid quantum-classical solvers: Informed Quantum-Enhanced Tree Solver (IQTS) and Hybrid Quantum-Classical Bilevel Solver (HBS). While IQTS leverages the domain knowledge of the problem structure by integrat-ing QAOA within an exploratory tree decomposi-tion, HBS is a bilevel optimization framework with a highly modular architecture, designed to combine various methods such as QAOA, IBP, CACm, and DAS to enhance flexibility, performance, and scala-bility. Both methods rely on similar solver compo-nents, as sketched in Fig. 2. In the following, we first describe these components, which then allows us to present IQTS and HBS. 

## 5.1 Solver Components 

In this section, we provide a brief summary over all seven solver components, which can be grouped into four categories: 

• Three classical procedures based on our struc-tural knowledge about the problem, which is why we also call them “informed” procedures: ISG, ISF, and ISI. 

• A quantum optimization algorithm, QAOA, which is fine-tuned for our problem. 

• Two classical Ising solvers, IBP and CACm. The latter can also be considered as a quantum-inspired approach. 

• A derivative-free hyperparameter tuning proce-dure, DAS. All of these components are heuristic algorithms. 

5.1.1 Informed Solution Generator (ISG) 

Since we have a detailed knowledge about the prob-lem structure, we can use this knowledge to gen-erate randomized feasible solutions (that fulfill all constraints). For that, we propose the following pro-cedure, which we call Informed Solution Generator (ISG): 1. Start with all parts unassigned. 2. Iteratively draw an unassigned part i ∈ I from the lowest available PBS level (the reason for this order is that lower levels tend to have fewer available options). 3. Collect all feasible site-supplier pairs gi for the part i that satisfy the constraints, given the as-signments made previously. Randomly assign the part i to one of these options. Repeat this step for the primary and secondary sources. 4. Repeat from step 2 until all parts are assigned, resulting in y. Calculate the correct values for 

z and return the corresponding solution x =(y, z ), which is a feasible by construction. 

5.1.2 Informed Solution Fixer (ISF) 

We can also exploit the problem structure to project an infeasible solution back to the feasible domain without disturbing it more than necessary. Our pro-posed procedure Informed Solution Fixer (ISF) works as follows: 1. Start with x, a candidate solution to Eq. (11) that violates the constraints. 2. Iterate through all sites k ∈ K and suppliers 

u ∈ U and mark whether the workshare is overassigned (too much workshare) or underas-signed (not enough workshare). If overassigned, 8HBS IQTS 

ISG ISF ISI IBP CACm QAOA DAS 

Tuning Classical / Quantum-Inspired Ising Quantum Classical Informed Figure 2: Sketch of the algorithmic architecture. We propose two hybrid quantum-classical solvers, IQTS (Section 5.2) and HBS (Section 5.3), which make use of various components: ISG (Section 5.1.1), ISF (Sec-tion 5.1.2), ISI (Section 5.1.3), QAOA (Section 5.1.4), IBP (Section 5.1.5), CACm (Section 5.1.6), DAS (Sec-tion 5.1.7). randomly unassign parts until the workshare is satisfied. This step can lead to an update of y.3. Iterate through all parts i ∈ I of decreasing PBS level and mark whether they are unassigned or wrongfully assigned (i.e., in a way that vi-olates the constraints). Try to reassign each wrongfully assigned part: If there are underas-signed sites/suppliers, try to reassign the mis-matched products to those sites/suppliers and avoid overassigned sites/suppliers. Reassign-ments must meet all constraints. If a reassign-ment cannot be resolved, skip it. Again, this step can lead to an update of y.4. Repeat from step 2 until all assignments are fea-sible, resulting in y, or until a certain number of iterations have been performed, at which point the method has ran out of budget and no fixed solution can be returned. In case that a set of fea-sible assignments has been found, calculate the correct values for z and return a fixed solution 

x = ( y, z ), which is feasible by construction. 

5.1.3 Informed Solution Improver (ISI) 

Finally, we can also use our knowledge to iteratively improve the objective of a feasible solution without leaving the feasible domain. For that, we propose a procedure called Informed Solution Improver (ISI), which works in the following way: 1. Start with x, a feasible candidate solution to Eq. (11). 2. Randomly draw a part i ∈ I.3. Find all feasible combinations of primary source and secondary source assignments for part i.4. Iterate through the feasible options in a random order and, if they would lead to an improvement in the optimization objective, reassign the part 

i accordingly, which leads to an update of y.In case of a reassignment, stop exploring the feasible options with a predefined probability or continue otherwise. Stop when all feasible options have been explored. 5. Repeat from step 2 until a certain number of it-erations has been reached. Calculate the correct values for z and return an improved x = ( y, z ),which is feasible by construction and leads to an objective at least as good as the original candi-date solution. 

5.1.4 Quantum Approximate Optimization Algo-rithm (QAOA) 

QAOA [16, 9] is a hybrid quantum-classical heuris-tic to solve Ising optimization problems of the form of Eq. (13) on a gate-based quantum computer. As a special form of Variational Quantum Algorithm (VQA), the key idea is to optimize the parameters of a parameterized quantum circuit iteratively using a classical optimizer until convergence is achieved. The measurement results of the quantum device then correspond to candidate solutions s of Eq. (13a). We make use of a variant of QAOA [16] with a problem-specific setup: 

• Mixer: We choose a XY -mixer [47], which con-sists of correlated Pauli-x and Pauli-y terms  

> 12

(ˆ σix ˆσjx + ˆ σiy ˆσjy ), for every combination of vari-ables (i, j ) that correspond the an assignment of the same product for the same source. This mixer enforces the constraint P2(x). In addition, we choose the uncorrelated Pauli-x terms ˆσix for all ancilla variables. 

• Initialization: The initial state is chosen to be a W state [13, 41] for every set of variables that corre-spond the an assignment of the same product for the same source in analogy to the mixer choice. W states can be prepared with logarithmic com-plexity [13]. The state for all ancilla variables is prepared in the usual way with a Hadamard gate. 

• Schedule: Motivated by recent findings [40], we make use of a fixed linear ramp QAOA proto-col, for which the QAOA parameters [16], typ-ically called γ and β, are set to fixed values. This means that no additional classical opti-9mization loop is necessary to tune these param-eters, which greatly reduces the required quan-tum hardware access time. Specifically, for a depth p ≥ 2, we choose constant cost param-eters γi = ( i − 1) /(p − 1) and constant mixer parameters βi = 1 − γi for i ∈ [2 , p ]. For the special case p = 1 , we define γ1 = β1 = 12 .This setup requires n qubits to solve a QUBO with 

n binary variables. The only hyperparameter is the depth p ∈ N. As the size of our problem formulation, Eq. (11), is too large to be solved with the quantum hardware available to us, we apply QAOA only on sub-problems. While QAOA is genuinely a hybrid quantum-classical algorithm, the choice of fixed pa-rameters reduce our variant to a pure quantum algo-rithm. 

5.1.5 Iterative Belief Propagation (IBP) 

Belief propagation (BP) is an algorithm which ap-proximately samples from a high dimensional dis-crete probability distribution when the probability function can be expressed as a product of factors that involve a small subset of the variables [10, 43]. If the graph that connects factors and variables (called the factor graph) forms a tree, BP is know to converge ex-actly in polynomial time. In the context of QUBOs, the probability distribution we would like to sam-ple from is the Boltzmann distribution for a given inverse temperature β, defined as P (x) ∝ e−βQ (x)

where Q(x) is the QUBO cost function of the form of Eq. (11b). Because Q(x) is a linear combination of terms involving edges in the QUBO graph these edges will then become factors in the graph used by BP. Thus, if we have a QUBO with tree connectivity, BP provides a polynomial time algorithm for finding the ground state (by setting β to a very large value). BP can be outlined as follows. Consider a QUBO in quadratic form, Eq. (12), with Nx variables x and a corresponding matrix Q. For each ordered pair of variables i, j ∈ { 1, . . . , N x} such that Qij ̸ = 0 , BP stores two “messages” μaij ∈ [0 , 1] with a ∈ { 0, 1}.These messages are typically initialized randomly and updated iteratively according to the classical be-lief propagation equations [38]: 

μaij → (μaij )∗

∑

> b∈{ 0,1}

(μbij )∗ (14a) with 

(μaij )∗ := ∏  

> k∈N (i)
> k̸=j

∑

> b∈{ 0,1}

e−βQ ik ab ∏  

> l∈N (k)
> l̸=j

μblk 

 . (14b) The set N (i) refers to all of the connected QUBO variables (which can include i itself). Once the mes-sages have converged, they can be used to compute marginal probabilities which allow us to sample from the desired (Boltzmann) distribution. On the other hand, Simulated Annealing (SA) pro-vides a different approach for sampling from the Boltzmann distribution of a QUBO [27]. In SA, the current solution x is iteratively updated by choos-ing a random variable and then flipping it according to the Metropolis Hastings criterion. This can also be interpreted as choosing a random sub-problem of the full problem (of just one variable in this case) and sampling from the Boltzmann distribution of that sub-problem while keeping all other variables fixed. Unlike BP, SA has the useful property that it works for any QUBO connectivity given enough iterations, however in some cases convergence time can be ex-ponential in the problem size. The proposed IBP procedure attempts to bridge the gap between BP and SA as follows [44]. Given a QUBO graph with sparse connectivity, it is possible to choose a subset of variables such that the sub-graph formed by them is a tree. If the given QUBO graph is sparse and more tree-like, these sub-graphs will be larger, whereas if the graph has full connec-tivity these subgraphs will have size 2 at most. IBP works by randomly choosing one of these subgraphs (which can be done efficiently) and applying BP on it. This allows us to sample from the Boltzmann distribution for a subset of variables similar to SA. However, unlike SA, many variables will be updated at once. Iteratively choosing these sub-trees and up-dating x using BP will allow us to construct a Markov chain which converges to the Boltzmann distribution as desired. A convenient property of IBP is that in the limiting case of a QUBO that is already a tree, the al-gorithm will be identical to regular BP, and thus con-verge in polynomial time (given the hyper-parameter 

β is chosen correctly). On the other hand, if the prob-lem is fully connected, IBP will reduce to a slightly modified version of SA in which two variables are updated at once instead of one. Similar to SA, IBP is parameterized by the hyper-parameter β which can be annealed for better performance. Many problems of industrial and academic interest lie somewhere in-between these two extremes thus it is reasonable to postulate that IBP can outperform both BP and SA in some practically relevant cases. In particular, since the optimization problem studied in this work has an inherent tree structure to it from the PBS, it is reasonable that IBP may provide some improve-ment in solution speed and accuracy by exploiting this structure. 10 5.1.6 Chaotic Amplitude Control with Momen-tum (CACm) 

The CACm algorithm belongs to the class of Ising solvers based on Ordinary Differential Equations (ODEs) [33, 31]. It has been shown to perform com-petitively against other state-of-the-art solvers such as SA and Parallel Tempering (PT) [31, 45, 32], but its performance strongly depends on the choice of its hyperparameters [45]. Presuming an Ising opti-mization problems of the form of Eq. (13), CACm is based on the concept of relaxing the binary space of 

s ∈ {− 1, 1}Nx to the real space m ∈ RNx , which leads to a relaxed problem of the form min m H(m).The operator F 1, which represents the flow map of the deterministic trajectory of CACm, is defined as follows [32]: 

γ d2udt 2 + du dt = −λu − βe ◦ ∇ mV, (15a) 

de dt = −ξ(m ◦ m − a) ◦ e, (15b) where the symbol ◦ denotes the Hadamard product, 

∇mV the gradient of V with respect to the vector 

m, β a positive parameter and e a vector of positive auxiliary variables. The variable u is a real-valued vector representing the internal state of the CACm and t the continuous time. The term proportional to 

γ represent the momentum which has been utilized for non-convex optimization [26]. In practice, the operator F 1 used the bilevel optimization scheme for the contribution of CACm represents the map given as F 1 : sn := sign[ m(t = 0)] → sn+1 := sign[ m(t =

T )] .In the current work, the potential V is equal to the Ising Hamiltonian of the model H, Eq. (13b), relaxed to the continuous domain. The vector 

θ := ( λ1, λ 2, γ, β, ξ, a, T ) contains the hyperparam-eters, which are briefly explained in the following. First of all, λ1 and λ2 are the initial and final val-ues of the parameter λ that is taken to be a linear function of time to perform “annealing.”. Further-more, γ represents the momentum and β a positive parameter. We set mi := ϕ( ˜βu i), ∀i ∈ { 1, · · · , N x},where ϕ(m) := 21+ e−m − 1 is a sigmoidal function normalized to the domain m ∈ [−1, 1] . Moreover, ξ

represents the speed of the error correction dynamics and a represents target amplitude of the m variables. Finally, T denotes the number of steps of a single run of CACm. 

5.1.7 Dynamic Anisotropic Smoothing (DAS) 

DAS is a recently proposed derivative-free optimiza-tion approach for automated hyperparameter tun-ing. Presume that an algorithm has a hyperparame-ter space θ. DAS involves iteratively sampling a set of solutions L[θ] := {θ1, θ 2, · · · , θ R}, using the Ising solvers to infer an optimal value of θ. This is achieved by iterating the following equations [45]: 

dL dt = αL

(

LL ⊤ ∂h (L, θ )

∂L + λL + ηL

)

, (16a) 

dθ dt = αθ

(

LL ⊤ ∂h (L, θ )

∂θ + ηθ

)

, (16b) The variables θ and L represent a distribution in the hyper-parameter space in which the trajectories are sampled from, where the vector θ and matrix L corre-spond to the center and covariance of the distribution respectively. R is the number of hyperparameters sampled per step of DAS. Equations (16b) and (16b) describe the gradient descent dynamics of the opti-mal parameter estimate θ, as well as the estimation of the local curvature L of the algorithm’s performance in the hyperparameter space at θ. The function h

and the hyperparameter λ are described in more de-tail in [45]. In each step n of DAS, an ensemble of parameters 

L[θγ ] is updated conditionally on set of solution Sγn

found by the Ising solver as follows: 

L[θγn+1 ] = G(L[θγn]; Sγn ). (17) In DAS, the operator G estimates the gradient and local curvature of Cn (that is, DAS is an approximate second order method). 

## 5.2 Informed Quantum-Enhanced Tree Solver (IQTS) 

Our first hybrid quantum-classical solver focuses on knowledge about the problem structure and how it can be used to decompose the problem into smaller sub-problems, which can then be tackled using quantum optimization. Since the decomposition approach is centered around the tree structure of the PBS, we call this approach Informed Quantum-Enhanced Tree Solver (IQTS). The method is largely I/O-bound and can achieve a very fast convergence by keeping the exploration close to the feasible do-main. The task of the solver is to provide a heuristic so-lution to the proposed QUBO, Eq. (11). Given a user-defined subtree size m and variable count n,11 the solver workflow can be summarized in the fol-lowing way, where we recall the solver components from Section 5.1: 1. Start with a candidate solution x from ISG. 2. Randomly select a part i ∈ I from the PBS that has not yet been selected, always prefer the part with the highest available level. 3. For each part i, perform the following steps: a. Randomly select a sub-tree of size m from the PBS that contains part i.b. Randomly select n variables for the assign-ment of parts, which are contained in the sub-tree. Set all remaining variables to zero in order to nullify the product assignments. c. Build a sub-problem from the selected vari-ables and solve it with QAOA. Due to the selection of the sub-problem based on a sub-tree of the PBS, the sub-problem vari-ables are highly correlated. d. If necessary, repair x using ISF. This step can lead to an update of x.e. Perform a predefined number of ISI itera-tions. This step can lead to an update of 

x.4. If all parts have been randomly selected once in step 2, make all of them freely available again. 5. Repeat from step 2 until κ repetitions have been reached, then return x.

## 5.3 Hybrid Quantum-Classical Bilevel Solver (HBS) 

The second solver we propose is a high performance framework that leverages multiple sub-solvers in a collaborative way, which allows us to make use of a combination of classical, quantum-inspired and pure quantum optimizers. The framework is mostly CPU-bound and focused on computational efficiency, scal-ability, and generalizability for tackling complex models with bilevel optimization. We call this approach Hybrid Quantum-Classical Bilevel Solver (HBS). Specifically, the task of HBS is to provide a heuristic solution to an Ising optimization problem of the form of Eq. (13). We outline the framework in the follow-ing, where we recall the components from Section 5.1. First of all, HBS makes use of three specialized Ising solvers: 

• CACm [32, 33], as a general-purpose, state-of-the-art quantum-inspired method, 

• IBP [44], as an iterative message-passing tech-nique, designed to quickly approximate solu-tions for problems with a tree-like structure, and 

• QAOA [17, 38] to include the exploitation of quantum effects. Each solver leverages different properties of the prob-lem instances, and together, they maximize the ad-vantages of each approach. We denote the solvers by Alg γ with γ ∈ { CACm, IBP, QAOA}. In addition, HBS also uses ISG to generate solution candidates, ISF to improve the feasibility of solution candidates and DAS to tune hyperparameters. Algorithm 1 pro-vides a summary of the framework. 

Algorithm 1 Hybrid Quantum-Classical Bilevel Solver  

> 1:

Input: Ising problem, Eq. (13), with Ising Hamil-tonian H(s) 

> 2:

Initialize s0 ∈ S 0 using ISG  

> 3:

while not converged do  

> 4:

for γ ∈ { CACm, IBP, QAOA} do  

> 5:

Refine solution: sn+1 = F γ (sn, θ γn) 

> 6:

Generate candidate set: Sγn+1  

> 7:

end for  

> 8:

Combine solutions: Sn+1 = S

[⋃ 

> γ

Sγn+1 

] 

> 9:

Tune: θ∗ 

> n+1

≈ arg min θ u(H∗ −H (sn)) via DAS step  

> 10:

given as Ln+1 [θ] = G(Ln[θ]; Sn) 

> 11:

Fix solutions sn+1 ∈ S n+1 using ISF  

> 12:

Record candidate solutions Sn+1  

> 13:

end while 

Initially, ISG is used to generate a candidate solu-tion s0 ∈ S 0 ⊂ {− 1, 1}Nx , where S0 denotes the fea-sible set of states generated by ISG. Then, a sequence of iterations n ≥ 1 follows, during which the solution candidate is progressively refined until convergence is reached—that is, when further iterations no longer improve the solution quality. The task of each algo-rithm Alg γ is to refine a solution from an initial guess 

sn ∈ S n to an improved solution sn+1 ∈ S γn+1 , en-suring H(sn+1 ) ≤ H(sn). Formally, this action is represented as a operator F γ given as follows: 

sn+1 := F γ (sn, θ γn), ∀sn ∈ Sn, ∀γ, (18) where θγn denotes the hyperparameters of solver γ at step n. In the case of CACm for example, the operator 

F CACm consists in the flow map of a deterministic trajectory represented by an ODE as described in Section 5.1.6. The solutions found by all algorithms are then re-combined to be the starting point for the next iter-ations, which involves sampling the subset of best solutions found among the union set of all solutions, 12 as follows: 

Sn := S

[⋃

> γ

Sγn

]

, (19) where S is a sampling operation selecting with uniform distribution from candidate solutions with smallest Ising Hamiltonian H(sn+1 ).We then adopt a bilevel optimization approach, where the parameters θγn are adaptively tuned to maximize the quality of the solution found. Specifi-cally, we seek 

θγ∗ 

> n

:= arg min  

> θ

Cγn (θ), (20) where 

Cγn (θ) := u(H∗ − H(F γ (sn−1, θ ))) (21) is a cost associated with the solution quality found after one step of the Ising solver and based on the the Heaviside step function u. The reference energy 

H∗ is defined as the minimum objective of Eq. (13) found so far. Since the functional dependency of 

Cγn (θ) on the hyperparameters θ is not known a priori, we optimize θ using an estimation of the gradient of 

Cγn based on DAS. In practice, the two levels described in Eqs. (17) and (18) are iterated one after the other as depicted in Fig. 3. To ensure feasibility, ISF is applied on all solution candidates sn ∈ S n at the end of each iteration (which are first transformed back into the QUBO domain). Sn {Sγn } Sn+1 

L(θn) L(θn+1 )

{F γ } ⋃

G

Ising solvers (CACm, IBP, QAOA) Merge 

Hyperparameters online tuning (DAS) 

Figure 3: Sketch of how HBS combines Ising solvers (CACm, IBP, and QAOA) with hyperparameter tun-ing (DAS). The current implementation of HBS does not uti-lize ISI, but it could be included in future imple-mentations. Similarly, the Lagrangian multipliers λ

have been considerd as constants, but could in prin-ciple also be included within the list of hyperparam-eters tuned automatically by DAS. Penalty terms as-sociated with box constraints are currently not in-cluded in the Ising Hamiltonian utilized by HBS. While we tailor HBS towards its best performance for the present use case, it can, unlike IQTS, be easily be adapted to be applicable to any QUBO. An exem-plary demonstration of the convergence behavior of HBS is provided in Section D. 

# 6 Experiments 

In this section, we describe our numerical experi-ments. We use the IonQ device Aria-1 [22] to ex-ecute quantum circuits, which is based on trapped ion technology and operates on 25 qubits with all-to-all connectivity. We access the system via AWS Braket. We also conduct quantum simulations, for which we use the (noiseless) AWS Braket universal state vector simulator SV1 . In total, we perform four types of experiments: E1. IQTS for different scalarization weights using QAOA on Aria-1 .E2. IQTS for the same scalarization weights as in experiment E1 using SA. E3. IQTS for different scalarization weights using QAOA on SV1 .E4. HBS for different scalarization weights using CACm. E5. HBS for different scalarization weights using IBP, CACm, and QAOA on SV1 .E6. IQTS for different primary source shares us-ing QAOA on SV1 .A summary of all experiments is listed in Table 1. Detailed settings are provided in Section E. Table 1: Experiment summary: solver, number of instances and resulting objective hypervolume over all instances. The hypervolume is calculated with respect to the reference point (3 , 5, 4.5, 5.5) for the four weighted KPIs in Eq. (11). Experiment Solver Instances Hypervolume Experiment E1 IQTS 8 55 .20 

Experiment E2 IQTS 8 54 .44 

Experiment E3 IQTS 286 73 .05 

Experiment E4 HBS 206 58 .74 

Experiment E5 HBS 156 63 .90 

Experiment E6 IQTS 20 53 .85 

We particularly choose primary source shares of 

α := αi := 0 .8 for all i ∈ I in experiments E1 to E5, which is a suitable value for real-world applications. In experiment E6, on the other hand we randomly sample values for α ∈ [.5, . 8] . Based on these spec-ifications, we end up with Nz := 494 ancilla vari-ables, leading to a total of Nx := Ny + Nz = 2416 

13 variables in experiments E1 to E5 (while the num-ber of variables for experiment E6 depends on the sampled value of α, which may affect the number of ancilla variables). This number can be compared to the number of variables a naive approach to the as-signment problem to get a feeling for the complexity reduction we achieved with our modeling approach. If we take into account all 48 parts, 43 manufactur-ing sites, and 29 suppliers of the problem instance, we arrive at around (43 × 29) 48 ×2 ≈ 10 300 possi-ble assignments, when taking double sourcing into account. In addition, the instance includes 28 ware-houses and 17 073 transportation methods between sites and warehouses, resulting in a large number of possible routes to which the products must also be properly assigned. Our model captures the entire problem with only 2416 binary variables. In the following, we first we present the results from experiment E1, which we subsequently com-pare with the results from experiment E2. Next, we show the resulting solution space exploration from experiments E3 to E5. Finally, we provide the results from experiment E6. 

## 6.1 Experiment E1 on Aria-1 

The solutions from experiment E1 are shown in Fig. 4 as pairwise projections within the four-dimensional KPI space. Each solution is feasible (i.e., it fulfills the constraints) and has been obtained by choosing a dif-ferent set of scalarization weights and running IQTS. The reported KPIs for each solution are to be un-derstood as the non-scalarized expressions Cn from Eq. (5), which we refer to as costs ( n = 1 ), emissions (n = 2 ), time ( n = 3 ), and workshare ( n = 4 ). For each two-dimensional projection, we highlight both the overall Pareto-optimal solutions as well as the solutions that are Pareto-optimal with respect to the projected plane (i.e., ignoring the other dimensions). We also mark two Pareto-optimal example solu-tions, A and B, which are visualized in Fig. 5. Each visualization shows Below the map, two plots show the workshare for the individual sites and suppliers, respectively, together with the required upper and lower bounds (triangles), which are fulfilled. For the suppliers, we also show the target workshares (crosses), which have to be reached as close as possi-ble. In Fig. 5a and Fig. 5b, we show an abstract map with production sites (dots) and warehouses (squares) in the style of Fig. 1b. The connecting lines show the streams of goods: the line thickness indi-cates the cumulative costs value of the transported parts (a bigger line means more expensive), whereas 1.5 1.6 1.7              

> 1.7
> 1.8
> 1.9A
> B
> emissions
> costs
> Solution Pareto-optimal Optimal in Projection
> 1.71.81.9
> 1.5
> 2
> 2.5
> 3
> A
> B
> costs
> time
> 1.51.61.7
> 1
> 2
> 3
> 4
> 5
> A
> B
> emissions
> workshare
> 1.71.81.9
> 1
> 2
> 3
> 4
> 5
> A
> B
> costs
> workshare
> 1.51.61.7
> 1.5
> 2
> 2.5
> 3
> A
> B
> emissions
> time
> 1.522.53
> 1
> 2
> 3
> 4
> 5
> A
> B
> time
> workshare

Figure 4: Solutions from experiment E1, shown as pairwise projections of the four KPIs. Pareto-optimal solutions are highlighted. We mark two solutions, A and B, which are visualized in Fig. 5. Each solution represents a supply chain configuration. the redness indicates the contribution of this con-nection to the overall carbon dioxide emission (the redder, the more emissions). In Fig. 5c and Fig. 5d, we show the workshare fulfillments for the sites and suppliers. The sites and suppliers are are both shown on the same horizontal axis, where each supplier is marked (triangle). Also shown are the required up-per and lower limits, which are all respected by the ef-fective workshares from the solutions (the horizontal order of sites and suppliers is with ascending feasible window size). For the suppliers, we also show the target workshares (crosses), which are to be reached as close as possible. The visualizations demonstrate the complexity be-hind each individual solution and could in a next step be used to select between different competing sup-ply chain configurations that focus on different KPIs. For example, the solution A fulfills the supplier tar-get workshares much better than solution solution B at a cost of a much higher carbon dioxide emission. In Fig. 6, we present the iterative convergence of 14 production sites warehouses          

> low
> high
> emissions costs (a) Solution A: Routes with emissions and costs production sites warehouses
> low
> high
> emissions costs (b) Solution B: Routes with emissions and costs effective workshare workshare limits
> workshare target supplier workshare
> (c) Solution A: Workshare fulfillments effective workshare workshare limits
> workshare target supplier workshare (d) Solution B: Workshare fulfillments

Figure 5: Visualization of two Pareto-optimal solutions from experiment E1, which we call solution A and solution B. In (a) and (b), we show routes between production sites and warehouses, the thickness and color indicate cumulative costs and emissions, respectively. In (c) and (d), we show workshare fulfillments for sites and suppliers (both on the same horizontal axis, suppliers are marked). The KPIs of the two solutions are shown in Fig. 4. IQTS for one instance from experiment E1 (which leads to solution A). Here, Q(x) stands for the objec-tive from Eq. (11), which is chosen as the weighted mean of the four KPIs from Eq. (32): carbon diox-ide emissions ( C1(y)), costs ( C2(y)), time ( C3(y)), and supplier workshare target fulfillment ( C4(y)). It turns out that after 50 iterations, convergence is al-most reached, which is why we use this as the maxi-mum iteration count for experiment E6. 

## 6.2 Experiment E2 with SA 

The quantum device we use is capable of solving sub-problems involving up to 25 optimization vari-ables using the proposed QAOA approach. QUBOs of this size can also be solved relatively easy with a brute-force methods. Therefore, no concrete ad-vantage is expected from applying a quantum opti-0 20 40 60 80 100 

> 2
> 3
> iteration
> Q(x)
> C1(y)
> C2(y)
> C3(y)
> C4(y)

Figure 6: Convergence of IQTS for one instance within experiment E1 using QAOA on Aria-1 .mization strategy to such small instances. However, solving sub-problems to optimality does not neces-sarily yield the best overall performance. In fact, sub-optimal solutions to individual sub-problems may contribute to a better overall outcome. Consequently, the inherent imperfections of the quantum optimiza-15 1.5 1.6 1.7

1.6

1.8

2

> (0,1,0,0)
> (0.4,0.6,0,0)
> (0.25,0.25,0.25,0.25)

emissions 

> costs

E1: IQTS (LR-QAOA on Aria-1 ) E2: IQTS (SA) 

1.6 1.8 2

1.5

2

2.5

3

3.5

> (0,1,0,0)
> (0.25,0.25,0.25,0.25)

costs 

> time

1.5 1.6 1.7

1

2

3

4

5

> (0,1,0,0)
> (0.5,0.5,0,0)
> (0.25,0.25,0.25,0.25)

emissions 

> workshare

1.6 1.8 2

1

2

3

4

5

> (0,1,0,0)
> (0.4,0.6,0,0)
> (0.25,0.25,0.25,0.25)

costs 

> workshare

1.5 1.6 1.7

1.5

2

2.5

3

3.5 (0,1,0,0) 

> (0.4,0.6,0,0)
> (0.3,0.7,0,0)

emissions 

> time

1.5 2 2.5 3 3.5

1

2

3

4

5

> (0.3,0.7,0,0)
> (1,0,0,0)
> (0.25,0.25,0.25,0.25)

time 

> workshare

Figure 7: Comparison of the solutions from experi-ments E1 and E2. Solution pairs from instances with the same scalarization weights w are connected with a line. Hence, each line shows the KPI shifts from using IQTS with QAOA against IQTS with SA. For some solution pairs, the corresponding choice of w

is shown. tion process may prove beneficial to the performance of IQTS. In experiment E2, we replace QAOA as a sub-solver with SA to test if any performance difference can be observed. We use the same scalarization weights for the 8 instances as in experiment E1. A direct comparison of the results is shown in 7, where we particularly highlight the KPI shifts of solutions with the same scalarization weights. There occur significant KPI shifts, which is no sur-prise as the feasible solution space is very complex and many compromises may lead to similar over-all objectives. We observe that the overall solution space exploration is very similar for experiments E1 and E3. In Table 1, we evaluate the hypervolume of the solutions with respect to a reference point (Nadir point). The hypervolume serves as an indicator of the Pareto frontier expansion, with higher values reflect-ing a better performance. We find a hypervolume of 

55 .20 for experiment E1 and 54 .44 for experiment E2, indicating a comparable Pareto frontier expansion. This quantitative result aligns with the qualitative observations presented in Fig. 7. In summary, no significant difference between us-ing QAOA or SA in IQTS can be observed, as ex-pected. However, it should be emphasized that we only consider 8 instances per experiment here. Astudy with more data might be necessary to statis-tically validate these observations and draw more robust conclusions. 

## 6.3 Feasible Solution Space from E3, E4, and E5 

To visualize the solution space exploration for exper-iments E3 to E5, we show the combined results as projections on the KPIs in Fig. 8 in analogy to Fig. 4. We also include the results from experiment E1 as a reference. All presented solutions are feasible. For the sake of clarity, we do not explicitly indicate the Parto optimality of individual solutions here. These results are provided in Section F. We observe that all of our approaches lead to com-parable solution space exploration with only minor difference. From Table 1 we find that the hypervol-ume of experiment E3 is largest with 73 .05 followed by experiment E5 with 63 .90 and experiment E4 with 

58 .74 . However, with 286 instances, experiment E3 also includes more solutions than experiment E4 with 206 instances and experiment E5 with only 156 

instances. Overall, we conclude that the performance of all considered approaches is competitive. As in our previous observations, using Aria-1 in experi-ment E1 does not yield a significant computational advantage over the classical algorithms. It is evident that most objectives are in conflict with one another, such as costs-time, emissions-workshare, costs-workshare, emissions-time, and time-workshare. In contrast, emissions and costs are hardly conflicting. This observation suggests that optimized supply chains can be environmentally friendly and cost-effective at the same time. 

## 6.4 Source Share Variation from E6 

In contrast to the previous experiments, the goal of experiment E6 is to explore the effect of different values of the primary source share α (for constant, balanced scalarization weights w). We present the resulting solutions in Fig. 9 for one pair of KPIs, the other projections are shown in Sec-tion F. The colors of the dots indicate the correspond-ing choice of α, grouped into five distinct clusters, as 16 1.5 2 2.5                   

> 2
> 3
> 4
> 5
> emissions
> costs
> E1: IQTS (LR-QAOA on Aria-1 )
> E3: IQTS (LR-QAOA on SV1 )
> E4: HBF (CACm)
> E5: HBF (IBP, CACm, LR-QAOA on SV1 )
> 2345
> 1
> 2
> 3
> 4
> costs
> time
> 1.522.5
> 2
> 4
> emissions
> workshare
> 2345
> 2
> 4
> costs
> workshare
> 1.522.5
> 1
> 2
> 3
> 4
> emissions
> time
> 1234
> 2
> 4
> time
> workshare

Figure 8: Explored solution space for different solvers. Each solution represents a supply chain con-figuration. Data from experiments E1 and E3 to E5. defined in the colorbar. The marks on the colorbar indicate the sampled values of α, each of which leads to a problem instance to be solved. As expected, different choices of α lead to a distri-bution of solutions. While there is no clear distinc-tion, higher values of α have a slight tendency to lead to solutions with higher emissions and lower work-share (and vice-versa). This aligns with the intuitive understanding that higher values of α together with the workshare constraints reduce the flexibility of choosing emission-efficient routes. 

# 7 Conclusions 

We address a multi-objective logistics optimization problem inspired by a real-world supply chain sce-nario. To model the assignment problem in a form suitable for quantum optimization, we first formu-late a QUBO. By preprocessing the instance data 1.6 1.7 1.8

> 1.5
> 2
> emissions
> workshare
> 0.5
> 0.6
> 0.7
> 0.8
> α
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶
> ▶

Figure 9: Solutions for different primary source shares α. Data from experiment E6. through pathfinding and feasibility exploration, we substantially reduce the solution space that must be searched. Building on this foundation, we propose two hybrid quantum-classical solvers to tackle the optimization problem effectively: The knowledge-based IQTS and the high-performance approach HBS. Both approaches leverage special-purpose sys-tems to solve sub-problems of scalable size, enabling the use of Ising machines, NISQ devices, and poten-tially more efficient quantum or non-classical plat-forms in the future. We demonstrate experimentally that the two heuristic solvers are able to find solutions of com-parable quality. Both have their own strengths and weaknesses. IQTS is highly efficient in finding fea-sible solutions and its focus on the exploration of the solution space close to the feasible domain typ-ically leads to a very fast convergence behavior. Its weakness, on the other hand, is that it cannot be di-rectly parallelized in its current design. The strength of HBS is its flexibility in combining multiple sub-solvers within a bilevel framework, which ensures scalability and generalization. A weakness of the method is that its adaptability to a general problem setting comes at the cost of slower convergence to an optimal solution. In the course of our experiments, however, the two solvers demonstrate similar solu-tion quality, suggesting that these weaknesses were minor in practice. Our experimental results do not indicate any per-formance advantage from using the Aria-1 quantum hardware within the proposed solver frameworks. We attribute this primarily to the small scale of the device of only 25 qubits, which cannot be expected to be competitive to classical computations [14, 49]. Nevertheless, the design of our solver frameworks al-lows for straightforward adaptation to more power-ful quantum hardware in the future, where quantum optimization may lead to meaningful performance 17 gains. There are numerous opportunities to extend and generalize our modeling approach from multiple perspectives. One important aspect would be the packing problem of placing multiple parts within the limited container space of transport vehicles based on the geometries. Additional elements could also be taken into account, such as production times, precise transportation scheduling, manufacturing capaci-ties, warehouse capacities, and additional regional restrictions related to transportation and regulatory compliance. Some of these elements, such as pro-duction times, could be easily integrated into our ex-isting formalism. Others, such as the packing prob-lem, would lead to entirely new sub-problems that could be integrated as a future extensions. Lastly, the primary source share, which we treated an instance-dependent example, could also be taken into account as an additional optimization parameter. Possibly, in combination with another KPI, which measures the resilience of the supply chain. In the current work, we have not combined all elements of IQTS and HBS, as integrating calls to the quantum hardware with parallel CPU process-ing is non-trivial in the current setup. However, this integration could be achieved with improved inter-facing. HBS can be extended to include a broader set of special-purpose algorithms, either classical, quantum-inspired or quantum. Incorporating state-of-the-art algorithms into the solver mix ensures that the method achieves optimal performance at present. As quantum hardware scales to include more qubits, it becomes possible to allocate an increasing portion of the solver mix to quantum algorithms. In the meantime, the hybrid approach facilitates the effi-cient utilization of these schemes. Moreover, HBS can be improved by incorporating more sophisti-cated methods for merging candidate solutions from solvers (such as mutation rules) and by exploring al-ternative parameter tuning techniques, like Bayesian optimization, where applicable. For classical hard-ware, this study has primarily focused on CPU-based parallel computing, as the sparsity of the problem’s connectivity made CPUs a suitable choice. However, algorithms such as CACm and IBP could benefit from GPU acceleration in cases of denser connectivity. Another potential non-classical platform for Ising machines, specifically of the CACm type, is the Co-herent Ising Machine (CIM). The CIM is a nonlinear optical system that leverages optical parametric am-plification which can be implemented on the lithium niobate platform [20, 35, 37, 21]. A next-generation CIM should be implemented as a Thin Film Lithium Niobate (TFLN) photonic integrated circuit. Several key components toward this goal have successfully developed such as optical parametric amplifiers [24], optical frequency converters [25] and optical para-metric oscillators [36]. 

# 8 Acknowledgments 

The authors acknowledge Airbus and BMW Group for providing the problem statements under the Air-bus–BMW Group Quantum Computing Challenge 2024 and for supporting the validation of the results. We also extend our appreciation to the teams at Air-bus, Amazon, and IonQ for their invaluable guidance in clarifying the use case and for providing access to IonQ’s computing resources via AWS. Their readi-ness to address our questions throughout the de-velopment process was especially appreciated. We would like to give special thanks (in alphabetical or-der) to Andreas Mitschke (Airbus), Andreas Zindel (Airbus), Denny Dahl (IonQ), Heidi Nelson-Quillin (IonQ), Howard Lock (IonQ), Martin Schuetz (Ama-zon), Michael Brett (Amazon), Richard Ashworth (Airbus), and Sebastian Stern (Amazon). Further-more, we express our sincere gratitude to Marwan Channab, Raymond Harding, and Matteo Conterno for their active participation in discussions and the exchange of ideas. 

# References 

[1] Amira Abbas, Andris Ambainis, Brandon Au-gustino, et al. “Challenges and opportunities in quantum optimization”. In: Nature Reviews Physics 6.12 (Dec. 2024), pp. 718–735. issn : 2522-5820. doi : 10.1038/s42254-024-00770-9 . url :

https : / / doi . org / 10 . 1038 / s42254 - 024 -00770-9 .[2] Airbus and BMW Group. The Quantum Mobil-ity Quest . https://qcc.thequantuminsider. com . Accessed: 2025-05-20. 2024. [3] David Amaro, Matthias Rosenkranz, Nathan Fitzpatrick, Koji Hirano, and Mattia Fiorentini. “A case study of variational quantum algo-rithms for a job shop scheduling problem”. In: 

EPJ Quantum Technology 9.1 (2022), p. 5. [4] Mayowa Ayodele, Richard Allmendinger, Manuel López-Ibáñez, and Matthieu Parizy. “A Study of Scalarisation Techniques for Multi-objective QUBO Solving”. In: Opera-tions Research Proceedings 2022 . Springer Inter-national Publishing, 2023, pp. 393–399. isbn :9783031249075. doi : 10 . 1007 / 978 - 3 - 031 -

18 24907- 5\_47 . url : http://dx.doi.org/10. 1007/978-3-031-24907-5%5C_47 .[5] Mayowa Ayodele, Richard Allmendinger, Manuel López-Ibáñez, and Matthieu Parizy. “Multi-objective QUBO solver: bi-objective quadratic assignment problem”. In: Proceed-ings of the Genetic and Evolutionary Computa-tion Conference . GECCO ’22. ACM, July 2022, pp. 467–475. doi : 10.1145/3512290.3528698 .

url : http://dx.doi.org/10.1145/3512290. 3528698 .[6] Ryan Babbush, Bryan O’Gorman, and Alán Aspuru-Guzik. “Resource efficient gadgets for compiling adiabatic quantum optimization problems”. In: Annalen der Physik 525.10–11 (Sept. 2013), pp. 877–888. issn : 1521-3889. doi :

10.1002/andp.201300120 . url : http://dx. doi.org/10.1002/andp.201300120 .[7] Ana Paula Barbosa-Póvoa, Cátia da Silva, and Ana Carvalho. “Opportunities and challenges in sustainable supply chain: An operations re-search perspective”. In: European Journal of Op-erational Research 268.2 (2018), pp. 399–431. issn :0377-2217. doi : https://doi.org/10.1016/ j . ejor . 2017 . 10 . 036 . url : https : / / www . sciencedirect.com/science/article/pii/ S0377221717309499 .[8] Ville Bergholm, Josh Izaac, Maria Schuld, et al. PennyLane: Automatic differentiation of hy-brid quantum-classical computations . 2022. arXiv: 

1811 . 04968 [quant-ph] . url : https : / / arxiv.org/abs/1811.04968 .[9] Kostas Blekos, Dean Brand, Andrea Ceschini, et al. “A review on Quantum Approximate Optimization Algorithm and its variants”. In: 

Physics Reports 1068 (June 2024), pp. 1–66. issn :0370-1573. doi : 10.1016/j.physrep.2024.03. 002 . url : http://dx.doi.org/10.1016/j. physrep.2024.03.002 .[10] A. Braunstein, M. Mézard, and R. Zecchina. “Survey propagation: An algorithm for satis-fiability”. In: Random Struct. Algorithms 27.2 (Sept. 2005), pp. 201–226. issn : 1042-9832. [11] Jwo-Sy Chen, Erik Nielsen, Matthew Ebert, et al. “Benchmarking a trapped-ion quantum computer with 30 qubits”. In: Quantum 8(2024), p. 1516. [12] Randall Correll, Sean J Weinberg, Fabio Sanches, Takanori Ide, and Takafumi Suzuki. “Quantum neural networks for a supply chain logistics application”. In: Advanced Quantum Technologies 6.7 (2023), p. 2200183. [13] Diogo Cruz, Romain Fournier, Fabien Gremion, et al. “Efficient Quantum Al-gorithms for GHZ and W States, and Implementation on the IBM Quantum Computer”. In: Advanced Quantum Tech-nologies 2.5–6 (Apr. 2019). issn : 2511-9044. 

doi : 10 . 1002 / qute . 201900015 . url : http : //dx.doi.org/10.1002/qute.201900015 .[14] Alexander M. Dalzell, Aram W. Harrow, Dax Enshan Koh, and Rolando L. La Placa. “How many qubits are needed for quantum computa-tional supremacy?” In: Quantum 4 (May 2020), p. 264. issn : 2521-327X. doi : 10.22331/q-2020-05 - 11 - 264 . url : http : / / dx . doi . org / 10 . 22331/q-2020-05-11-264 .[15] Vinayak V Dixit and Chence Niu. “Quantum computing for transport network design prob-lems”. In: Scientific Reports 13.1 (2023), p. 12267. [16] Edward Farhi, Jeffrey Goldstone, and Sam Gut-mann. A Quantum Approximate Optimization Al-gorithm . 2014. arXiv: 1411 . 4028 [quant-ph] .

url : https://arxiv.org/abs/1411.4028 .[17] Edward Farhi, Jeffrey Goldstone, and Sam Gut-mann. “A quantum approximate optimization algorithm”. In: arXiv preprint arXiv:1411.4028 

(2014). [18] David Fitzek, Toheed Ghandriz, Leo Laine, Mats Granath, and Anton Frisk Kockum. “Ap-plying quantum approximate optimization to the heterogeneous vehicle routing problem”. In: Scientific Reports 14.1 (2024), p. 25415. [19] Thore Gerlach, Loong Kuan Lee, Frédéric Barbaresco, and Nico Piatkowski. “Hybrid Quantum-Classical Multi-Agent Pathfinding”. In: arXiv preprint arXiv:2501.14568 (2025). [20] Toshimori Honjo, Tomohiro Sonobe, Ken-suke Inaba, et al. “100,000-spin coherent Ising machine”. In: Science advances 7.40 (2021), eabh0952. [21] Takahiro Inagaki, Kensuke Inaba, Ryan Hamerly, et al. “Large-scale Ising spin network based on degenerate optical parametric oscilla-tors”. In: Nature Photonics 10.6 (2016), pp. 415– 419. [22] IonQ. IonQ Aria . https://ionq.com/quantum-systems/aria . [Accessed 29-10-2024]. 19 [23] Md Abrar Jahin, Md Sakib Hossain Shovon, Md Saiful Islam, et al. “QAmplifyNet: pushing the boundaries of supply chain backorder pre-diction using interpretable hybrid quantum-classical neural network”. In: Scientific Reports 

13.1 (2023), p. 18246. [24] Marc Jankowski, Nayara Jornod, Carsten Lan-grock, et al. “Quasi-static optical parametric amplification”. In: Optica 9.3 (2022), pp. 273– 279. [25] Marc Jankowski, Carsten Langrock, Boris Desi-atov, Marko Lončar, and MM Fejer. “Supercon-tinuum generation by saturated second-order nonlinear interactions”. In: APL Photonics 8.11 (2023). [26] Kirill P Kalinin, George Mourgias-Alexandris, Hitesh Ballani, et al. “Analog Iterative Machine (AIM): using light to solve quadratic optimiza-tion problems with mixed variables”. In: arXiv preprint arXiv:2304.12594 (2023). [27] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. “Optimization by Simulated Annealing”. In: 

Science 220.4598 (May 1983), pp. 671–680. doi :

10.1126/science.220.4598.671 .[28] Thorsten Koch, David E. Bernal Neira, Ying Chen, et al. Quantum Optimization Benchmark Library – The Intractable Decathlon . 2025. arXiv: 

2504 . 03832 [quant-ph] . url : https : / / arxiv.org/abs/2504.03832 .[29] Krzysztof Kurowski, Tomasz Pecyna, Mateusz Slysz, et al. “Application of quantum ap-proximate optimization algorithm to job shop scheduling problem”. In: European Journal of Operational Research 310.2 (2023), pp. 518–528. [30] Loong Kuan Lee, Thore Thassilo Gerlach, and Nico Piatkowski. Standardization of Multi-Objective QUBOs . 2025. arXiv: 2504 . 12419 [cs.LG] . url : https://arxiv.org/abs/2504. 12419 .[31] Timothée Leleu, Farad Khoyratee, Timothée Levi, et al. “Scaling advantage of chaotic ampli-tude control for high-performance combinato-rial optimization”. In: Communications Physics 

4.1 (2021), p. 266. [32] Timothée Leleu and Samuel Reifenstein. “Non-Equilibrium Dynamics of Hybrid Continuous-Discrete Ground-State Sampling”. In: arXiv preprint arXiv:2410.22625 (2024). [33] Timothée Leleu, Yoshihisa Yamamoto, Peter L McMahon, and Kazuyuki Aihara. “Destabi-lization of local minima in analog spin systems by correction of amplitude heterogeneity”. In: 

Physical review letters 122.4 (2019), p. 040607. [34] Andrew Lucas. “Ising formulations of many NP problems”. In: Frontiers in Physics Volume 2 (2014). issn : 2296-424X. doi : 10.3389/fphy. 2014.00005 . url : https://www.frontiersin. org/journals/physics/articles/10.3389/ fphy.2014.00005 .[35] Alireza Marandi, Zhe Wang, Kenta Takata, Robert L Byer, and Yoshihisa Yamamoto. “Net-work of time-multiplexed optical parametric oscillators as a coherent Ising machine”. In: Na-ture Photonics 8.12 (2014), pp. 937–942. [36] Timothy P McKenna, Hubert S Stokowski, Vahid Ansari, et al. “Ultra-low-power second-order nonlinear optics on a chip”. In: Nature Communications 13.1 (2022), p. 4532. [37] Peter L McMahon, Alireza Marandi, Yoshitaka Haribara, et al. “A fully programmable 100-spin coherent Ising machine with all-to-all con-nections”. In: Science 354.6312 (2016), pp. 614– 617. [38] Marc Mezard and Andrea Montanari. Informa-tion, physics, and computation . Oxford Univer-sity Press, 2009. [39] Naeimeh Mohseni, Peter L McMahon, and Tim Byrnes. “Ising machines as hardware solvers of combinatorial optimization problems”. In: 

Nature Reviews Physics 4.6 (2022), pp. 363–379. [40] J. A. Montanez-Barrera and Kristel Michielsen. 

Towards a universal QAOA protocol: Evidence of a scaling advantage in solving some combinatorial optimization problems . 2024. arXiv: 2405.09169 [quant-ph] . url : https://arxiv.org/abs/ 2405.09169 .[41] Chandra Sekhar Mukherjee, Subhamoy Maitra, Vineet Gaurav, and Dibyendu Roy. 

On Actual Preparation of Dicke State on aQuantum Computer . 2020. arXiv: 2007 . 01681 [quant-ph] . url : https://arxiv.org/abs/ 2007.01681 .[42] Eneko Osaba, Esther Villar-Rodriguez, and Antón Asla. “Solving a real-world package de-livery routing problem using quantum anneal-ers”. In: Scientific Reports 14.1 (2024), p. 24791. 20 [43] Judea Pearl. “Reverend bayes on inference en-gines: a distributed hierarchical approach”. In: 

Proceedings of the Second AAAI Conference on Ar-tificial Intelligence . AAAI’82. Pittsburgh, Penn-sylvania: AAAI Press, 1982, pp. 133–136. [44] Sam Reifenstein and Timothée Leleu. “It-erative Belief Propagation for Sparse Com-binatorial Optimization”. In: arXiv preprint arXiv:2411.00135 (2024). [45] Sam Reifenstein, Timothee Leleu, and Yoshi-hisa Yamamoto. “Dynamic Anisotropic Smoothing for Noisy Derivative-Free Opti-mization”. In: arXiv preprint arXiv:2405.01731 

(2024). [46] Dario De Santis, Salvatore Tirone, Stefano Marmi, and Vittorio Giovannetti. Optimized QUBO formulation methods for quantum comput-ing . 2024. arXiv: 2406.07681 [quant-ph] . url :

https://arxiv.org/abs/2406.07681 .[47] Zhihui Wang, Nicholas C. Rubin, Jason M. Dominy, and Eleanor G. Rieffel. “XY mixers: Analytical and numerical results for the quan-tum alternating operator ansatz”. In: Physical Review A 101.1 (Jan. 2020). issn : 2469-9934. doi :

10.1103/physreva.101.012320 . url : http:// dx.doi.org/10.1103/PhysRevA.101.012320 .[48] Sean J Weinberg, Fabio Sanches, Takanori Ide, Kazumitzu Kamiya, and Randall Correll. “Supply chain logistics with quantum and clas-sical annealing algorithms”. In: Scientific Re-ports 13.1 (2023), p. 4770. [49] Xiaosi Xu, Simon Benjamin, Jinzhao Sun, Xiao Yuan, and Pan Zhang. A Herculean task: Clas-sical simulation of quantum computers . 2023. arXiv: 2302 . 08880 [quant-ph] . url : https : //arxiv.org/abs/2302.08880 .

# Appendix A Preprocessing 

We consider here Eq. (4) with scalarized objectives 

∑4 

> n=1

wnCn(y). As described in Section 4.4, we per-form a two-step preprocessing step to remove sub-optimal terminals from the formulation and reduce the overall number of variables. These two prepreo-cessing steps are explained in the following. 

## A.1 Pathfinding 

So far, we have not specified how to determine the set of all routes from the set of transportation meth-ods, which is necessary to define the terminals for each site. It is clear that an explicit enumeration of all routes is computationally expensive due to the many combinatorial possibilities. However, it turns out that with our scalarization approach, no explicit representation is necessary because it allows us to reduce the number of routes for each part and each pair of sites to only one, as will be explained in the following. By definition, the first three KPIs, Eq. (5a), con-sist of a sum of non-negative contributions from the transport costs, Eq. (2). Therefore, when omitting the contribution from C4(y), the scalarized objective reads 

∑

> n∈{ 1,2,3}

wnCn(y) (22) 

= ∑  

> (i,j )∈ϕ
> (k,t,u )∈Fi,a ∈A
> (l,s,v )∈Fj,b ∈ARik⇒l̸=0

αai χikl (w, t, s ) yai→(k,t.u )ybj→(l,s,v ),

where 

χikl (w, t, s ) := ∑

> n∈{ 1,2,3}

wn

dn

cin  

> (k,t )⇒(l,s )

(23) represents the contribution of transporting part i ∈ I

from production site k ∈ K and terminal t ∈ T ik to production site l ∈ K and terminal s ∈ T jl . The end terminal s has in fact no influence on Eq. (23), so that we can also write χikl (w, t ) := χikl (w, t, s ).Moreover, Eq. (23) and hence Eq. (22) is independent of the chosen suppliers. We use w := ( w1, w 2, w 3) to denote the first three scalarization weights. The start terminal t represents the chosen route 

r = τ ikl (t) ∈ Rik⇒l (as a sequence of transportation options) for part i. However, since every part is to be transported independently, there is in fact a univer-sally optimal route for each part i between each pair of production sites (k, l ), depending on the chosen weights w, which is formally given by the optimal terminal 

ˆtikl (w) := arg min  

> t∈Tik

χikl (w, t ). (24) This optimal terminal can in principle be found by enumerating all possible terminals T ik. Alternatively, finding the best terminal is equivalent to finding the most cost-efficient path between k and l on the graph 

Gi, where each edge m ∈ Mi is assigned the costs 

ξi(w, m ) := ∑

> n∈{ 1,2,3}

wn

dn

cn(m)γin (m) (25) 21 since 

χikl (w, t ) = ∑ 

> m∈τikl (t)

ξi(w, m ), (26) where we recall Eqs. (2) and (23). Rik⇒l̸ = ∅, the most cost-efficient path on Gi corresponds to a route 

ˆrikl (w) := arg min 

> r∈Rik⇒l

∑

> m∈r

ξi(w, m ) (27) with 

ˆtikl (w) = τ ikl (ˆ rikl (w)) . (28) For Rik⇒l = ∅, the optimal terminal is simply the only available terminal, Eq. (1). We consider the determination of optimal termi-nals, as described above, as a preprocessing step, which we perform for each choice of weights w. To this end, Eq. (27) is solved with Dĳkstra’s algorithm. This pathfinding step eliminates the search for the best terminal (or, equivalently, best route) from the optimization problem and allows us to switch to a new set of of binary variables y, each identifiable by four indices. Specifically, the binary variable 

yai→(k,u ) ∈ { 0, 1} represents the assignment of part 

i ∈ I to the best terminal ˆtikl (w) ∈ T ik of site k (pre-suming that parent part jinI is assigned to produc-tion site l ∈ K) and supplier u with (k, u ) ∈ fi, where 

a ∈ A indicates either the primary production ( a = 1 )or the secondary production ( a = 2 ). Furthermore, we write 

cin k⇒l(w1, w 2, w 3) := cin  

> (k, ˆtikl (w)) ⇒(l,s )

(29) for the cumulative contributions of the transporta-tion methods, where we recall the independence from the end terminal s.

## A.2 Feasibility Reduction 

In addition to the pathfinding, we apply a second preprocessing step in which we refine the feasible solution space. This step is independent of the cho-sen weights and only has to be performed once per problem instance. Specifically, for each part i ∈ I, we consider only those elements from the predefined set of feasible site-supplier combinations fi that contain sites for which every child of i can be transported to 

i (as its parent) and, at the same time, i can also be transported to its parent. Given these conditions, the resulting reduced set is given by the self-referencing definition 

gi := {(k, u ) | (k, u ) ∈ fi ∧ [∀(i′, j ) ∈ ϕ (30) 

∧ i′ = i∃(l, v ) ∈ gj : Rik⇒l̸ = ∅]

∧ [∀(j, i ′) ∈ ϕ ∧ i′ = i∃(l, v ) ∈ gj

: Rjl⇒k̸ = ∅]} ⊆ fi

and can be straightforwardly determined with a re-cursive search of fi. We find that ∑ 

> i∈I

|gi| = 1922 .Since all options from fi \ gi violate Eq. (4a), we only use the choices provided by gi in the following, lead-ing to a total of Ny := 1922 binary variables. Fur-thermore, gi also allows us to fix assignments with a single feasible option, i.e., yai→(k,u ) := 1 for all i ∈ I

with |gi| = 1 . However, this approach does not lead to a reduction of variables for the problem instance we consider. 

## A.3 Reduced Model 

As a consequence of the two preprocessing steps, the reduced model is given by: 

min 

> y
> 4

∑

> n=1

Cn(w, y ) (31) s.t. ∑

> a,b ∈A,u ′,v ′∈U

μab iδu′uδv′v yai→(k,u )ybj→(l,v ) = 0 ∀ (i, j ) ∈ ϕ ∀ (k, u ) ∈ gi ∀ (l, v ) ∈ gj ∧ Rik⇒l = ∅ (31a) 

∑

> (k,u )∈gi

yai→(k,u ) = 1 ∀ i ∈ I ∀ a ∈ A (31b) 

( ∑

> (k′,u )∈gi

δk′kya=1 

> i→(k,u )

)( ∑

> (k′,u )∈gi

δk′kya=2 

> i→(k,u )

)

= 0 ∀ i ∈ I ∀ k ∈ K ∧ | Ki| ≥ 2 (31c) 

( ∑

> (k,u )∈gi

Vvk ya=1 

> i→(k,u )

)( ∑

> (k,u )∈gi

Vvk ya=2 

> i→(k,u )

)

= 0 ∀ i ∈ I ∀ v ∈ V ∧ | Vi| ≥ 2 (31d) 

∑

> i∈I, (k′,u )∈gi,a ∈A

δk′kviαai yai→(k,u ) ∈ [Kmin  

> k

, K max  

> k

] ∀ k ∈ K (31e) 

∑

> i∈I, (k,u ′)∈gi,a ∈A

δu′uviαai yai→(k,u ) ∈ [U min  

> u

, U max  

> u

] ∀ u ∈ U (31f) 22 Here, we use the simplified KPIs 

Cn(w, y ) := Cn(w1, w 2, w 3, y ) (32a) 

:= wn

dn

∑  

> (i,j )∈ϕ
> (k,u )∈gi,a ∈A
> (l,v )∈gj,b ∈ARik⇒l̸=0

αai cin k⇒l(w1, w 2, w 3)yai→(k,u )ybj→(l,v )

for n ∈ { 1, 2, 3} and 

C4(w, y ) := C4(w4, y ) (32b) 

:= w4

d4

∑

> u∈U

[ ∑ 

> i∈I
> (k,v )∈gi
> a∈A

δuv viαai yai→(k,u ) − U target 

> u

]2

,

respectively, in analogy to Eqs. (32a) and (32b). 

# Appendix B Box Constraints 

As described in Section 4.4, the box constraints from Eq. (31), Eqs. (31e) and (31f), must first be converted into equality constraints before they can be trans-formed into penalty terms. We describe the proce-dure in the following [6]. Equations (31e) and (31f) can also be written in terms of inequalities f (y) ≥ 0 using functions of the form 

f (y) := ∑

> i∈I, (k,u )∈gi,a ∈A

bi

> (k,u )

viαai yai→(k,u ) + ν (33) with coefficients bi 

> (k,u )

∈ {− 1, 0, 1} for (k, u ) ∈ gi and an integer ν ∈ Z.As an initial step, and for reasons that will be-come clear further below, we approximate the real-valued coefficients (the relative values and the global primary source shares) by rational numbers. To that end, we define a part-dependent numerators 

Pi, P i ∈ Z for each part i ∈ I and global denom-inators R, R ∈ Z̸=0 , which allow us to express the relative value as 

vi = Pi

R + ϵi (34) and the primary source share as 

αi = P i

R + ϵi, (35) respectively, with error terms ϵi, ϵ i ∈ R for all i ∈ I.Thus, we can write 

f (y) = fQ(y) + fϵ(y) (36) with a rational-valued part 

fQ(y) := fZ(y)

RR ∈ Q (37) and a real-valued part 

fϵ(y) := ∑

> i∈I
> (k,u )∈gi
> a∈A

[ Pi

R ϵi + P i

R ϵi + ϵiϵi

]

yai→(k,u ) ∈ R.

(38) The rational-valued part is based on the integer-valued expression 

fZ(y) := ∑

> i∈I
> (k,u )∈gi
> a∈A

bi

> (k,u )

PiP iyai→(k,u ) + ν ∈ Z. (39) In the following, we presume that the numerators and denominators are chosen in such a way that 

fϵ(y) ≈ 0 for all y ∈ { 0, 1}Ny , hence f (y) ≈ fQ(y).In the next step, we define the expression 

gZ(y, z ) := fZ(y) −

> nz

∑

> n=1

2n−1zn ∈ Z. (40) Here, zn ∈ { 0, 1} for n ∈ { 1, . . . , n z } denote the nz

ancilla variables with nz := ⌈log 2 f Z⌉ with a con-stant f Z ∈ Z, which can be freely chosen under the requirement f Z ≥ 1 + max y fZ(y). As a con-sequence, ∀y ∈ { 0, 1}Ny ∃ z : gZ(y, z ) = 0 (since 

fZ(y) ∈ [0 , f Z]) and we can replace the inequal-ity constraint fQ(y) ≥ 0 by the equality constraint 

gZ(y, z ) = 0 . We use this approach to replace all box constraints, Eqs. (31e) and (31f), by equality con-straints. By definition, the minimal number of ancilla vari-ables depends on the magnitude of the global de-nominators R and R such that smaller values lead to less variables. More ancilla variables make the problem harder to solve. Therefore, the rational ap-proximation in Eqs. (34) and (35) needs to be bal-anced between sufficiently small error terms and a sufficiently small number of ancilla variables. In the following, we omit the dependency on the specific choices of numerators and denominators to simplify the notation. Additionally, we aggregate all ancilla variables which emerge from the conversion of the box constraints into a set of binary variables, denoted by z. We then represent the combined vector of y and 

z as x := ( y, z ).The resulting model only consists of equality con-straints, which can then be transformed into penalty terms. 23 Appendix C QUBO specifications 

Equation (11b) from Eq. (31) makes use of the fol-lowing notations. The weighted KPIs Cn(wn, y ) are provided in Eqs. (32a) and (32b). The penalty terms are defined by 

P1(y) := ∑

> (i,j )∈ϕ
> (k,u )∈gi,(l,v )∈gj
> Rik⇒l=∅

( ∑

> a,b ∈Au′,v ′∈U

μab i (41a) 

× δu′uδv′v yai→(k,u )ybj→(l,v )

)

,P2(y) := ∑

> i∈I,a ∈A

( ∑

> (k,u )∈gi

yai→(k,u ) − 1

)2

, (41b) 

P3(y) := ∑

> i∈I,k ∈K
> |Ki|≥ 2

( ∑

> (k′,u )∈gi

δk′kya=1 

> i→(k,u )

)

(41c) 

×

( ∑

> (k′,u )∈gi

δk′kya=2 

> i→(k,u )

)

,P4(y) := ∑

> i∈I,v ∈V
> |Vi|≥ 2

( ∑

> (k,u )∈gi

Vvk ya=1 

> i→(k,u )

)

(41d) 

×

( ∑

> (k,u )∈gi

Vvk ya=2 

> i→(k,u )

)

,P5(x) := P ≥ 

> 5

(x) + P ≤ 

> 5

(x), (41e) 

P6(x) := P ≥ 

> 6

(x) + P ≤ 

> 6

(x) (41f) based on the abbreviations 

P ≥ 

> 5

(x) := ∑

> k∈K

2−n(5 ,≥)

> k

(

p5(y) − Kmin  

> k

RR (42a) 

−

> n(5 ,≥)
> k

∑

> n=1

2n−1z(5 ,≥)

> kn

)2

,P ≤ 

> 5

(x) := ∑

> k∈K

2−n(5 ,≤)

> k

(

Kmax  

> k

RR − p5(y) (42b) 

−

> n(5 ,≤)
> k

∑

> n=1

2n−1z(5 ,≤)

> kn

)2

,P ≥ 

> 6

(x) := ∑

> u∈U

2−n(6 ,≥)

> u

(

p6(y) − U min  

> u

RR (42c) 

−

> n(6 ,≥)
> u

∑

> n=1

2n−1z(6 ,≥)

> un

)2

,P ≤ 

> 6

(x) := ∑

> u∈U

2−n(6 ,≤)

> u

(

U max  

> u

RR − p6(y) (42d) 

−

> n(6 ,≤)
> u

∑

> n=1

2n−1z(6 ,≤)

> un

)2

.

The exponential factors in the sums are empirically chosen to suitably scale the penalty terms. The ex-pressions are based on 

p5(y) := ∑

> i∈I, (k′,u )∈gi,a ∈A

δk′kPiyai→(k,u )Dai (43) 

p6(y) := ∑

> i∈I, (k,u ′)∈gi,a ∈A

δu′uPiyai→(k,u )Dai (44) with 

Dai := 

{

P i , if a = 1 

R − P i , if a = 2 (45) and the ancilla counters 

n(5 ,≥) 

> k

:= 

⌈

log 2

(

p5(y = 1) − Kmin  

> k

RR + 1 

)⌉ 

, (46a) 

n(5 ,≤) 

> k

:= 

⌈

log 2

(

Kmax  

> k

RR + 1 

)⌉ 

, (46b) 

n(6 ,≥) 

> u

:= 

⌈

log 2

(

p6(y = 1) − U min  

> u

RR + 1 

)⌉ 

, (46c) 

n(6 ,≤) 

> u

:= 

⌈

log 2

(

U max  

> u

RR + 1 

)⌉ 

. (46d) The ancilla counters can be summed up to obtain the total number of ancilla variables 

Nz := ∑

> k∈K

(

n(5 ,≥) 

> k

+ n(5 ,≤)

> k

)

(47) 

+ ∑

> u∈U

(

n(6 ,≥) 

> u

+ n(6 ,≤)

> u

)

,

where the vector of ancilla variables is to be under-stood as 

z := 

(

z(5 ,≥) 

> kn

∀n ∈ { 1, . . . , n (5 ,≥) 

> k

}, k ∈ K, (48) 

z(5 ,≤) 

> kn

∀n ∈ { 1, . . . , n (5 ,≤) 

> k

}, k ∈ K, z(6 ,≥) 

> un

∀n ∈ { 1, . . . , n (6 ,≥) 

> u

}, u ∈ U, z(6 ,≤) 

> un

∀n ∈ { 1, . . . , n (6 ,≤) 

> u

}, u ∈ U

)

. (49) In summary, the following steps are necessary to arrive at Eq. (11) from Eq. (4): 1. Choose the scalarization weights w.2. Find the optimal routes for each part with Dĳk-stra’s algorithm (depends on w). 3. Refine the feasible solution space (independent of w), eliminating all options that cannot be re-alized because of a missing connectivity. This leads to Eq. (31). 4. Choose numerators and denominators to obtain a rational approximation of the relative values and the primary source shares for all parts. 5. Transform inequality constraints into penalties using Nz ancilla variables. 6. Choose the corresponding Lagrange multipliers 

λ. This leads to Eq. (11). 24 Appendix D HBS Demonstration 

For demonstration purposes, the time progression of the solution quality at each step n of HBS from Sec-tion 5.3 is exemplarily shown in Fig. 10. The decrease of the Ising Hamiltonian H (Fig. 10a) is accelerated by the online tuning of hyperparameters achieved by DAS (Fig. 10b). At each step, the Ising solvers CACm, IBP, and QAOA are applied to improve the solution quality. The solution candidates are then fixed using ISF and their objective value is evaluated. 50 100 150 200 250          

> DAS step n
> H (Ising) [a.u.] a
> H*
> min H
> Q1.55
> 1.60
> Q (QUBO)
> 50 100 150 200 250
> n(DAS step )
> 0.0
> 0.5
> 1.0
> (Solver parameters )
> b
> 1
> 2
> (CAC)
> a
> (IBP)

Figure 10: Simulation results of the bilevel opti-mization approach using CACm, IBP, and QAOA. (a) Ising and QUBO objective vs. steps of DAS. (b) Step-dependent hyperparameters θ of the Ising solvers. ω1 = 0.5, ω2 = 0.0,ω3 = 0.0,

ω4 = 0 .5. See description of the hyperparameters 

{λ1, λ 2, γ, β (IBP ), β (CAC ), ξ, a, T } in Sections 5.1.5 and 5.1.6. H and Q are obtained before and after the application of ISF, respectively. The reference energy H∗ is defined as the minimum objective of Eq. (13) found so far. 

# Appendix E Experimental Setup 

For the experiments described in Section 6, we have used the following settings (using the notation from Sections 3 and 5): E1. Model: R := 5, P i := 4 for all i ∈ I,

λi = 2 for i ∈ {1, 6}, weights for each in-stance are chosen manually with w2 = 1 − w2

and w3 = w4 = 0: w = (0 , 1, 0, 0) , w =(0 .5, 0.5, 0, 0) , w = (1 , 0, 0, 0) , w = (0 .4, 0.6, 0, 0) ,

w = (0 .6, 0.4, 0, 0) , w = (0 .3, 0.7, 0, 0) , and twice 

w = (0 .25 , 0.25 , 0.25 , 0.25) . IQTS: m = 4 , n = 15 ,

κ = 50 (κ = 100 for w = (0 .25 , 0.25 , 0.25 , 0.25) ). QAOA: p = 1 . Aria-1 : 256 shots per circuit. Total number of instances: 8.E2. Model, IQTS: as in experiment E1, but instead of QAOA, SA is used. SA: 1000 annealing steps. Total number of instances: 8.E3. Model: R := 5 , P i := 4 for all i ∈ I, λi = 2 for 

i ∈ { 1, 6}, weights are chosen as a uniform grid 

wi ∈ { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1} for 

i ∈ { 1, 4} with ∑ 

> i

wi = 1 . IQTS: m = 4 , n = 25 ,

κ = 50 . QAOA: p = 1 . SV1 : 1024 shots per circuit. Total number of instances: 286 .E4. Model: R := 5 , P i := 4 for all i ∈ I, λi = 3 

for i ∈ { 1, 4}, λi = 0 for i ∈ { 5, 6}, uniformly sampled weights wi ∈ [0 , 1] for i ∈ { 1, 4} with ∑ 

> i

wi = 1 for each instance. DAS: n = 250 steps. Total number of instances: 206 .E5. Model: R := 5 , P i := 4 for all i ∈ I, λi = 3 

for i ∈ { 1, 4}, λi = 0 for i ∈ { 5, 6}, uniformly sampled weights wi ∈ [0 , 1] for i ∈ { 1, 4} with ∑ 

> i

wi = 1 for each instance. DAS: n = 250 steps. QAOA: p = 3 . SV1 : 32 shots per circuit. Total number of instances: 156 .E6. Model: uniformly sampled denominators 

R ∈ {2, 3, 5, 10 , 15 , 20 , 25 , 30 , 50 , 100 } for each instance, P i := ˜P (R) for all i ∈ I

with uniformly sampled values ˜P (R) ∈{⌈ αmin R⌉, . . . , ⌊αmax R⌋} with αmin := 0 .5 and 

αmax := 0.8 for each instance, λi = 2 for 

i ∈ { 1, 6}, wi = 0 .25 for i ∈ { 1, 4}. IQTS: m = 4 ,

n = 15 , κ = 50 . QAOA: p = 3 . SV1 : 1024 shots per circuit. Total number of instances: 20 .In all experiments, we choose a global denomi-nator R := 10 for the relative values, Eq. (34), and set the corresponding part-dependent numerators to their optimal values 

ˆPi := arg min 

> Pi∈Z

∣∣∣∣vi − Pi

Q

∣∣∣∣ (50) for all i ∈ I. This choice is particularly independent of the scalarization weights w and leads to a maxi-mum error of max i∈I ϵi = 0 .051 .All algorithms are implemented in Python using common libraries. Pennylane [8] is used to realize quantum circuits. Quantum circuits for QAOA are either executed on the IonQ device Aria-1 or simulated with the AWS Braket universal state vector simulator SV1 . Classical algorithms also run on AWS. For IQTS, we use 4 instances of AWS EC2 in total, each with 4 (total 16) vCPUs (3.6 GHz Intel Xeon ) and 8 GiB of memory. For HBS, we use 22 instances of AWS EC2 in total with each 36 (total 792) vCPUs (2.9 GHz Intel Xeon E5-2666 ) and 60 GiB of memory. 25 Appendix F Additional Results 

In the following, we provide additional results for Section 6. Firstly, we present in Figs. 11 to 15 the Pareto frontiers for experiments E2 to E6 in anal-ogy to Fig. 4 for experiment E1. In each projection, we highlight the Pareto-optimal points in the four-dimensional KPI space as well as the optimal points for this specific projection (when ignoring the other dimensions). Note that all presented solutions are feasible. Secondly, we present the remaining projec-tions from experiment E6 in Fig. 16 as a supplement to Fig. 9. 1.5 1.6 1.7

1.6

1.8

2

emissions 

> costs

1.6 1.8 2

1.5

2

2.5

3

3.5

costs 

> time

1.5 1.6 1.7

1

2

3

4

emissions 

> workshare

1.6 1.8 2

1

2

3

4

costs 

> workshare

1.5 1.6 1.7

1.5

2

2.5

3

3.5

emissions 

> time

1.5 2 2.5 3 3.5

1

2

3

4

time 

> workshare

Figure 11: Solutions from experiment E2 with the same symbols as in Fig. 4. 1.5 2 2.5

2

3

4

5

emissions 

> costs

2 3 4 5

1

2

3

4

costs 

> time

1.5 2 2.5

2

4

emissions 

> workshare

2 3 4 5

2

4

costs 

> workshare

1.5 2 2.5

1

2

3

4

emissions 

> time

1 2 3 4

2

4

time 

> workshare

Figure 12: Solutions from experiment E3 with the same symbols as in Fig. 4. 1.4 1.6 1.8 2 2.2

2

3

4

5

emissions 

> costs

2 3 4 5

2

3

4

costs 

> time

1.4 1.6 1.8 2 2.2

1

2

3

4

emissions 

> workshare

2 3 4 5

1

2

3

4

costs 

> workshare

1.4 1.6 1.8 2 2.2

2

3

4

emissions 

> time

2 3 4

1

2

3

4

time 

> workshare

Figure 13: Solutions from experiment E4 with the same symbols as in Fig. 4. 26 1.4 1.6 1.8 2 2.2 2.4

2

3

4

emissions 

> costs

2 3 4

1

2

3

4

costs 

> time

1.4 1.6 1.8 2 2.2 2.4

2

4

emissions 

> workshare

2 3 4

2

4

costs 

> workshare

1.4 1.6 1.8 2 2.2 2.4

1

2

3

4

emissions 

> time

1 2 3 4

2

4

time 

> workshare

Figure 14: Solutions from experiment E5 with the same symbols as in Fig. 4. 1.6 1.7 1.8

1.9

2

2.1

2.2

emissions 

> costs

1.9 2 2.1 2.2

1.5

2

2.5

costs 

> time

1.6 1.7 1.8

1.5

2

emissions 

> workshare

1.9 2 2.1 2.2

1.5

2

costs 

> workshare

1.6 1.7 1.8

1.5

2

2.5

emissions 

> time

1.5 2 2.5

1.5

2

time 

> workshare

Figure 15: Solutions from experiment E6 with the same symbols as in Fig. 4. 1.6 1.7 1.8

1.9

2

2.1

2.2

emissions 

> costs

1.9 2 2.1 2.2

1.5

2

2.5

costs 

> time

1.6 1.7 1.8

1.5

2

emissions 

> workshare

1.9 2 2.1 2.2

1.5

2

costs 

> workshare

1.6 1.7 1.8

1.5

2

2.5

emissions 

> time

1.5 2 2.5

1.5

2

time 

> workshare

Figure 16: Solutions from experiment E6 with the same symbols as in Fig. 9. 27