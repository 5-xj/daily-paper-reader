# Selecting Hyperparameters for Tree-Boosting
# 树提升算法的超参数选择

**Authors**: Floris Jan Koster, Fabio Sigrist \\
**Date**: 2026-02-05 \\
**PDF**: https://arxiv.org/pdf/2602.05786v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 7.0 \\
**Evidence**: Empirical comparison of automatic algorithm configuration methods \\

---

## Abstract
Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using $59$ regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than $100$ is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks.

## 摘要
树提升（Tree-boosting）是一种广泛用于表格数据的机器学习技术。然而，其

---

## 速览摘要（自动生成）

**问题**：提升树模型性能高度依赖超参数，但缺乏最优优化方法的实证对比。

**方法**：在59个数据集上对比了SMAC、TPE、GP-BO、Hyperband及网格搜索等主流方法。

**结论**：SMAC表现显著优于其他方法。研究指出需超100次迭代，默认参数效果差，所有