Title: Selecting Hyperparameters for Tree-Boosting

URL Source: https://arxiv.org/pdf/2602.05786v1

Published Time: Fri, 06 Feb 2026 02:24:35 GMT

Number of Pages: 20

Markdown Content:
# Selecting Hyperparameters for Tree-Boosting 

Floris Jan Koster ∗ Fabio Sigrist ∗‡ 

Abstract 

Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using 59 regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than 100 is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks. 

Keywords: Tabular data; hyperparameter optimization; gradient boosting; LightGBM 

# 1 Introduction 

Tree-boosting [Friedman et al., 2000, Friedman, 2001, B¨ uhlmann and Hothorn, 2007, Sigrist, 2021] is a widely used machine learning technique that achieves state-of-the-art prediction accuracy on tab-ular data sets [Nielsen, 2016, Shwartz-Ziv and Armon, 2022, Januschowski et al., 2022, Grinsztajn et al., 2022, McElfresh et al., 2023]. However, the out-of-sample accuracy of tree-boosting depends critically on multiple hyperparameters, such as the number of trees and the learning rate. In this ar-ticle, we empirically compare several popular methods for selecting hyperparameters for tree-boosting using 59 OpenML regression and classification data sets. Specifically, we consider the following hyper-parameter optimization methods: random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based al-gorithm configuration (SMAC) method, a deterministic full grid search, and using default hyperpa-rameters. We find that the SMAC method outperforms all other considered methods in terms of accuracy versus tuning budget. Moreover, we obtain the following findings. First, a relatively large number of trials (i.e., number of candidate hyperparameter sets) is required for accurate tuning, and using less than approximately 100 trials often yields inaccurate models. Second, using default values for hyperparameters often yields very inaccurate predictions. Furthermore, we find that all considered hyperparameters have an effect on the accuracy of tree-boosting. This means that the sometimes observed practice of only tuning a small set of hyperparameters likely often results in inferior models. In addition, for regression tasks, we find that choosing the number of boosting iterations (i.e., the number of trees) using early stopping yields more accurate results compared to including the number of iterations in the search space of a hyperparameter selection method. 

1.1 Related literature 

Putatunda and Rama [2018] compare the TPE method to random and deterministic full grid search for hyperparameter optimization for tree-boosting (XGBoost) on six classification data sets. They find 

> ∗Seminar for Statistics, ETH Zurich, Switzerland
> ‡Corresponding author: fabio.sigrist@stat.math.ethz.ch

1

> arXiv:2602.05786v1 [cs.LG] 5 Feb 2026

that the TPE method achieves a superior accuracy–time trade-off compared to random and full grid search. Motz et al. [2022] compare hyperparameter optimization techniques on five industrial produc-tion data sets for various machine learning methods. They find that the SMAC and TPE methods perform best for tree-boosting. Recently, Meaney et al. [2025] compare multiple hyperparameter opti-mization methods for tree-boosting using a single healthcare data set with a relatively small number of features and a strong signal to noise ratio. They find that all methods yield similar results. In ad-dition, there are public benchmark suites which include large collection of machine learning tasks for hyperparameter optimization comparisons such as Eggensperger et al. [2021] and Pfisterer et al. [2022]. Both Eggensperger et al. [2021] and Pfisterer et al. [2022] report that model-based search with resource allocation such as SMAC tends to outperform random or pure bandit baselines on tree-boosting sce-narios. Several other articles have evaluated hyperparameter optimization (HPO) for general machine learning methods besides tree-boosting [e.g., Falkner et al., 2018]. Bergstra and Bengio [2012] find that random grid search outperforms deterministic grid search for neural networks. 

# 2 Experimental settings 

2.1 Hyperparameter selection methods and software used 

We consider the following methods for hyperparameter optimization: (i) deterministic full grid search, (ii) random grid search [Bergstra and Bengio, 2012], (iii) Gaussian-process-based Bayesian optimization [Mockus et al., 1978] (GP-BO), (iv) the tree-structured Parzen estimator (TPE) [Bergstra et al., 2011], (v) Hyperband [Li et al., 2018], and (vi) sequential model-based algorithm configuration (SMAC) [Hutter et al., 2011]. In addition, we compare this to using the default hyperparameters of LightGBM 

[Ke et al., 2017] reported in Table 4. Tree-boosting is done using the GPBoost Python package [Sigrist et al., 2021, Sigrist, 2022] ver-sion 1.4.0 whose tree-boosting algorithm is the same as the one of LightGBM [Ke et al., 2017]. For TPE and Hyperband, we use the Optuna Python package [Akiba et al., 2019] version 3.5.0. For SMAC, we use the SMAC3 Python package [Lindauer et al., 2022] version 2.3.1, and for GP-BO, we use the scikit-optimize Python version 0.9.0. Unless stated otherwise, we use the default setting of all software packages. In particular, for all Bayesian optimization methods, we use the default acquisition functions in the above-mentioned software packages which are the expected im-provement for TPE and SMAC and “gp hedge” for GP-BO (each iteration randomly chooses among expected improvement, lower confidence bound, and probability of improvement). For SMAC, no specific multi-fidelity scheduling was enabled, and any early stopping was applied uniformly at the model-training level. Code to reproduce the experiments in this article can be found on https: //github.com/fl0risk/HPOTreeBoosting .

2.2 Data sets 

We use the same 59 data sets as in Grinsztajn et al. [2022] available on OpenML, out of which 36 are regression and 23 are classification tasks. We follow the pre-processing steps used in Grinsztajn et al. [2022]. This includes dropping entries with missing values and removing categorical features with more than 20 levels and numerical features with fewer than 10 unique values. One-hot-encoding is used for categorical features. For the classification data sets, the response variable is binarized if there are multiple classes, by only including the two most prevalent classes. In contrast to Grinsztajn et al. [2022], we allow for larger sample sizes up to n = 100 , 000. For data sets with more than n = 100 , 000 samples, a random subsample of size 100 , 000 is used. 

2.3 Train-test splits 

We use 5-fold cross-validation on every data set to compare the different methods. For every such 80 /20 train-test data split, we further split the training data into inner training and validation data sets using a 80 /20 split ratio, and the hyperparameters are chosen by learning on the inner training data and using a validation score on the validation data. As validation scores, the RMSE and accuracy are used for the regression and classification data sets, respectively. For a chosen set of hyperparameters, the models are then retrained on the entire training data sets. Most of the methods have a source of randomness such as randomly chosen initial values for the GP-BO, TPE, Hyperband, and SMAC 2methods and the random order in the random grid search. To analyze the impact of this, we repeat the hyperparameter searches 20 times using different random number generator seeds. Specifically, for every data set and hyperparameter selection method, we use 20 different random number generator seeds to generate 20 random initial values for the adaptive methods and 20 random orders for the random grid search. We then repeat the hyperparameter searches for each of the five 80 /20 train-test splits 20 times using these random initial values and orders. 

2.4 Hyperparameters and search spaces 

We consider the following hyperparameters: the number of iterations (= number of trees), the learning rate, the number of leaves, the maximal depth, the minimal number of samples per leaf (‘Min data in leaf’), the ℓ2 penalty on the leaf values (‘Lambda ℓ2’), the maximal number of bins for the histogram-based splitting approach for continuous features (‘Max bin’), and the bagging and feature sub-sampling fractions. For the TPE, GP-BO, Hyperband and SMAC methods, we use the hyperparameter search space given in Table 1. Note that “( l, u )” denote intervals for continuous parameters, and “ {nl, n l+1 , . . . , n u−

1, n u}” denote sets of integers for discrete parameters. “Max depth = -1” means no maximal tree-depth restriction, and n denotes the sample size. For the random grid search, we use the hyperparameter grid shown in Table 2. The hyperparameter grid for the deterministic full grid search is shown in Table 3, and the default hyperparameter values are reported in Table 4. For the full grid search, default values are used for the hyperparameters not included in Table 3. Both the maximal number of leaves and the maximal tree depth restrict the size of the trees. By default, we only include the maximal number of leaves in the hyperparameter search space and impose no limit on the maximal tree depth (“Max depth = -1”) since LightGBM uses a leaf-wise tree growth algorithm. However, we repeat the experiments by including the maximal depth in the search space and fixing the maximal number of leaves to a large number (1024) and also by jointly including the maximal number of leaves and the maximal tree depth in the hyperparameter search space. Furthermore, unless stated otherwise, we choose the number of boosting iterations using early stopping by monitoring a validation loss on the validation data sets. This means that the number of iterations is not explicitly contained in the search space of the hyperparameter selection methods, but for every combination of hyperparameters, the optimal number of iterations is determined using early stopping. To analyze the impact of this, we additionally perform the experiments by explicitly including the number of iterations in the search space instead of using early stopping. For this analysis, both the maximal number of leaves and the maximal tree depth are included in the search space, and for the random grid search, we use the set of candidate values for the number of iterations shown in Table 2. A total number of 135 trials, which corresponds to the size of the deterministic full grid, is used in all experiments for all hyperparameter selection methods except for Hyperband. The Hyperband method has two main tuning parameters R and η. We choose R and η such that the resulting maximal number of iterations is close to 135 , 000 = 135 × 1, 000, which is the maximal number of boosting iterations without taking into account early stopping for the other methods. Specifically, we use R = 2150 and 

η = 2 .8 which yields 128 , 816 boosting iterations. With this choice, the runtimes of random grid search and Hyperband are approximately equal across all data sets (results not tabulated). 

Parameter Search Space 

Learning rate (0 .001 , 1) Num leaves {2, 3, 4, . . . 1023 , 1024 }

Max depth {− 1, 1, 2, 3, . . . , 9, 10 }

Num iterations {1, 2, . . . , 999 , 1000 }

Min data in leaf {1, 2, . . . , 999 , 1000 }

Lambda ℓ2 (0 , 1000) Max bin {255 , 256 , 257 , . . . , min {10000 , n }} 

Bagging fraction (0 .5, 1) Feature fraction (0 .5, 1) Table 1: Hyperparameter search space for the TPE, GP-BO, Hyperband, and SMAC methods. 3Parameter Values 

Learning rate {0.001 , 0.01 , 0.1, 1}

Num leaves {2, 4, 8, 16 , 32 , 64 , 128 , 256 , 512 , 1024 }

Max depth {− 1, 1, 2, 3, 5, 10 }

Num iterations {1, 2, 5, 10 , 20 , 50 , 100 , 200 , 500 , 1000 }

Min data in leaf {1, 10 , 100 , 1000 }

Lambda ℓ2 {0, 1, 10 , 1000 }

Max bin {255 , 500 , 1000 , min {10000 , n }} 

Bagging fraction {0.5, 0.75 , 1}

Feature fraction {0.5, 0.75 , 1}

Table 2: Hyperparameter grid for random grid search. 

Parameter Values 

Learning rate {0.01 , 0.1, 1}

Min data in leaf {10 , 100 , 1000 }

Lambda ℓ2 {0, 1, 10 }

Num leaves {2, 4, 8, 32 , 1024 }

Table 3: Hyperparameter grid for deterministic full grid search. Default values reported in Table 4 are used for the hyperparameters not included in this table. 

Parameter Values 

Learning rate 0.1Num leaves 31 Max depth −1 (=no limit) Num iterations 100 Min data in leaf 20 Lambda ℓ2 0Lambda ℓ1 0Max bin 255 Bagging fraction 1Feature fraction 1Table 4: Default hyperparameters. 

2.5 Evaluation scores and aggregation across data sets 

For evaluating the accuracy of the different methods, we use the following scores. For the regression tasks, we use the root mean squared error (RMSE) and test R2 given by RMSE = 

vuut 1

N

> N

X

> i=1

(yi − ˆyi)2

and 

R2 = 1 − SS res 

SS tot 

,

respectively, where SS res = PNi=1 (yi − ˆyi)2, SS tot = PNi=1 (yi − y)2, yi are the true values, ˆ yi the predicted values, y = 1

> N

PNi=1 yi, and N is the number of test samples. For the classification tasks, we use the accuracy and log loss given by Accuracy = 

PNi=1 1{yi=ˆ yi}

N4and Log loss = 1

N

> N

X

> i=1

−(yi log( pi) + (1 − yi) log(1 − pi)) ,

respectively, where pi are the predicted probabilities for class 1. As mentioned in Section 2.3, we use the RMSE and accuracy on the validation data sets for the regression and classification data sets, respectively, for choosing hyperparameters. These scores are calculated on the hold-out test data sets after every trial t using the currently best hyperparameters, which are determined only on the validation data sets for every method. Specifi-cally, we calculate sequences of scores stkmlj for every data set k = 1 , . . . , 59, hyperparameter selection method m = 1 , . . . , 6, random initial values and orders l = 1 , . . . , 20, train-test split j = 1 , . . . , 5, and trial number t = 1 , . . . , T m. For all hyperparameter optimization methods except Hyperband, the number of trials is Tm = 135. For Hyperband, we calculate the scores stkmlj for Tm = 9 rungs as follows. Hyperband runs smax + 1 brackets consisting of multiple randomly sampled hyperparameter configurations. For each bracket, the algorithm allocates resources (number of boosting iterations) across configurations, and it performs successive halving: after evaluating configurations at a given budget, the poorest performers are discarded and the budget is increased for the remaining configu-rations, typically until a single configuration remains. After each successive-halving rung, we record the overall incumbent configuration (best validation performance among all configurations evaluated so far across all brackets) and use it to compute the corresponding test score stkmlj . In our case, this results in Tm = 9 recorded rungs. For aggregating the scores stkmlj across the different data sets, we normalize them for better com-parability across data sets as described in the following. First, we calculate averages over the five different folds: 

stkml = 15

> 5

X

> j=1

stkmlj .

We then follow Grinsztajn et al. [2022] and use the average distance to the minimum (ADTM) nor-malization [Wistuba et al., 2015]. Specifically, for the scores where lower values are better (RMSE and log loss), we use the ADTM normalization ˜stkml = stkml − min k

Q0.9 

> k

− min k

,

where “min k” and “ Q0.9 

> k

” denote the minimum and 90% quantile of all scores {stkml : m = 1 , . . . , 6, l =1, . . . , 20 , t = 1 , . . . , T m} for task k = 1 , . . . , 59. We use the 90% quantile instead of the maximum since a few scores are very large in the first few trials of some methods. Using the maximum would distort the normalization in the sense that most normalized scores were close together and small. Analogously, for the scores where higher values are better ( R2 and accuracy), we use the ADTM normalization ˜stkml = stkml − Q0.1

> k

max k − Q0.1

> k

,

where “max k” and “ Q0.1 

> k

” denote the maximum and 10% quantile of all scores {stkml : m = 1 , . . . , 6, l =1, . . . , 20 , t = 1 , . . . , T m} for task k. Similarly as for the lower-better scores, we use the 10% quantile instead of the minimum since a few scores are very small in the first few trials of some methods. Note that the results presented in this article are not sensitive to the specific choice of the 10% and 90% quantiles, and other quantiles yield qualitatively very similar results (results not shown). Finally, we calculate averages across the data sets and random initial values and orders ˜stm = 1

K

120 

> K

X

> k=1 20

X

> l=1

˜stkml ,

where K = 36 and K = 23 for the regression and classification data sets, respectively, In addition to the ADTM-normalized scores, we use ranks and relative differences to the best score for comparison across data sets. Specifically, ranks rtkm and relative differences to the best 5score δtkm are calculated using the sets {stkm : m = 1 , . . . , 6} for every iteration t and data set k,where stkm = 120 

P20  

> l=1

stkml are average scores over the different random initial values and orders. These ranks and relative differences are then averaged across data sets to give rtm = 1

> K

PKk=1 rtkm and 

δtm = 1

> K

PKk=1 δtkm . However, relative differences have the disadvantage of being sensitive to the scale. For instance, two methods with accuracies of 0 .1 and 0 .2 have a large relative difference. However, when equivalently using error rates (given by 0 .9 and 0 .8 in this example) instead of accuracies, the relative difference becomes small. Ranks, on the other hand, have the disadvantage that they potentially neglect useful information in the sense that ranked scores are different even if the differences are tiny and practically negligible. 

# 3 Results 

Table 5 reports the average ADTM-normalized scores, relative differences to the best score, and ranks aggregated across all datasets after the maximal number of trials Tm. Specifically, the table contains the average normalized scores ˜ sTm, relative differences δTm, and ranks rTm for the final best hyperparameters obtained at the end of the optimization for every method; see Section 2.5 for more details. Moreover, Figure 1 shows the sequences ˜ stm of average normalized incumbent scores as a function of the number of trials t. Note that we exclude the first 44 trials as some methods yield very inaccurate results which would impair the visibility of the plots. We add 95% confidence intervals obtained as ˜ stm ± 1.96SE tm,where SE tm are standard errors representing uncertainty across data sets obtained by first averaging the normalized scores ˜ stkml over the 20 different random initial values and orders, ˜ stkm = 120 

P20  

> l=1

˜stkml ,and then using this to calculate standard errors across the data sets k. That is, the confidence intervals represent uncertainty across data sets but not uncertainty due to randomness in the hyperparameter selection methods. Below, we also analyze the latter uncertainty. In Figure 3 in Appendix A, we additionally report the average relative differences δtm as a function of the number of trials t. Note that for the Hyperband method, there are not Tm = 135 trials but Tm = 9 rungs as explained in Section 2.5. For better visual comparability, we uniformly place the Tm = 9 rungs on the x-axis and linearly interpolate the test scores obtained by Hyperband in all figures that show the accuracy measures as a function of the number of trials t such as Figures 1 and 3. Default Deterministic GP-BO Hyperband Random Grid SMAC TPE norm  R2 0.181 0.664 0.775 0.584 0.814 0.898 0.845 RMSE 0.829 0.353 0.234 0.432 0.195 0.108 0.166 Accuracy 0.115 0.572 0.755 0.502 0.645 0.782 0.766 Log Loss 0.843 0.341 0.265 0.604 0.333 0.191 0.257 Rel. diff.  R2 0.046 0.013 0.006 0.017 0.004 0.001 0.003 RMSE 0.154 0.062 0.014 0.059 0.013 0.004 0.022 Accuracy 0.029 0.007 0.001 0.009 0.004 0.000 0.002 Log Loss 0.222 0.051 0.019 0.097 0.037 0.005 0.023 Rank  R2 6.889 4.444 3.722 5.778 3.167 1.417 2.583 RMSE 6.889 4.472 3.694 5.778 3.167 1.417 2.583 Accuracy 6.783 4.870 2.304 5.652 4.087 1.870 2.435 Log Loss 6.652 3.696 2.957 6.043 3.826 1.652 3.174 Table 5: Average normalized scores, relative differences, and ranks. We find that SMAC clearly outperforms all other methods in terms of all metrics for both regression and classification tasks. Overall, the TPE method gives the second most accurate results. For the regression data sets, random grid search is almost equally accurate as the TPE method, followed by Gaussian-process-based Bayesian optimization (GP-BO). For the classification tasks, GP-BO yields essentially equal accuracy as the TPE method, and random grid search is less accurate. A deterministic full grid search and Hyperband give considerably worse results. Moreover, using default values for the hyperparameters results in very inaccurate predictions. Finally, we also observe that using a low number of trials, say below 100, in the hyperparameter selection method yields worse results for all 645 60 75 90 105 120 135 

> 0.2
> 0.4
> 0.6
> 0.8

R2      

> 45 60 75 90 105 120 135
> 0.2
> 0.4
> 0.6
> 0.8

RMSE       

> 45 60 75 90 105 120 135
> 0.2
> 0.4
> 0.6
> 0.8

Accuracy       

> 45 60 75 90 105 120 135
> 0.2
> 0.4
> 0.6
> 0.8

Log Loss 

> Regression Classification
> Default
> Hyperband
> TPE
> Deterministic Grid
> Random Grid
> GP-BO
> SMAC

Figure 1: Normalized scores as a function of number of trials. The confidence intervals represent the uncertainty across data sets. methods. To assess the variability due to randomness in the methods, we additionally show in Figure 4 in Appendix A “seed randomness” confidence intervals obtained by first averaging over the different data sets, 1

> K

PKk=1 ˜stkml , and then calculating standard errors based on these averages. Despite the relatively small number of different random initial values and orders (20), the confidence intervals are very small, and we conclude that the uncertainty in our results due to randomness in the methods is almost negligible. In Figures 5, 6, 7, 8, 9, and 10 in Appendix A we additionally report the R2, RMSE, accuracy, and log-loss as a function of the number of trials for every data set separately. No normalization is applied here as the results are not aggregated over the different data sets. In line with the results reported above, SMAC often yields the most accurate results for most data sets. However, there is some variability across the different data sets, and no method is universally best for all data sets. The results discussed so far are obtained by including the maximal number of leaves in the hyper-parameter search space without directly limiting the maximal tree depth and by choosing the number of boosting iterations using early stopping as described in Section 2.4. In Figure 11 in Appendix A, we report additional results obtained by (i) including the maximal tree depth in the search space and fixing the maximal number of leaves to a large number (“Max Depth”), (ii) jointly including the max-imal number of leaves and the maximal tree depth in the hyperparameter search space (“Joint”), and (iii) additionally including the number of iterations in the search space instead of using early stopping (“Num Iter”). For comparison, the figure also reports the results when including the maximal number of leaves in the hyperparameter search space without imposing a maximal tree depth limit (“Num 7Leaves”). We first observe that SMAC yields the best results irrespective of which hyperparameter search space option is used. Overall, including the maximal number of leaves in the hyperparameter search space without imposing an explicit limit on the maximal tree depth yields the best results. Including the number of iterations in the search space of the methods instead of using early stopping clearly results in worse results for the regression data sets. On the other hand, including the number of iterations in the search space gives the best results for the classification tasks for the random grid search, TPE, and SMAC methods. 

# 4 The importance of individual hyperparameters 

In the following, we try to understand whether some hyperparameters are more important than others for the prediction accuracy of tree-boosting. This is motivated by the observation that some empirical studies in various applied fields only tune a few hyperparameters while fixing others at default values. For this, we create a “meta” data set consisting of all hyperparameter combinations and validation losses in all trials done for all data sets and methods in the above reported experiments when including both the maximal number of leaves and the maximal tree depth in the search space and using early stopping for the number of boosting iterations. We then analyze how the validation loss depends on the hyperparameters. To this end, the validation loss is considered as the response variable and the corresponding hyperparameters are the predictor variables in a tree-boosting regression model. Specifically, we fit a mixed-effects tree-boosting model [Sigrist, 2022] with data set specific grouped random effects to this meta data set. We apply the SMAC method to find the hyperparameters using an 80 /20 train-test split and the “maximal number of leaves” search space option described in Section 2.4 and early stopping for choosing the number of boosting iterations in this meta analysis. In Figure 2, we report SHAP values for the tree-boosting model trained on the meta data set described above. Separate SHAP values are reported for the regression and classification tasks. We find that all hyperparameters have relatively large average SHAP values and the differences in the SHAP values are small across the hyperparameters. This means that all considered hyperparameters can have a large effect on the accuracy of a tree-boosting model. 4 2 0 2      

> SHAP value (impact on model output)
> max_depth: 0.16
> min_data_in_leaf: 0.18
> num_leaves: 0.2
> learning_rate: 0.21
> feature_fraction: 0.24
> lambda_l2: 0.4
> max_bin: 0.44
> bagging_fraction: 0.61
> Low
> High
> Feature value 3210123
> SHAP value (impact on model output)
> max_depth: 0.17
> num_leaves: 0.19
> lambda_l2: 0.2
> min_data_in_leaf: 0.2
> feature_fraction: 0.24
> learning_rate: 0.25
> max_bin: 0.3
> bagging_fraction: 0.32
> Low
> High
> Feature value

Figure 2: SHAP summary plot for regression (top) and classification (bottom) tasks. 

# 5 Conclusion 

In this article, we benchmarked several widely used hyperparameter optimization methods for tree-boosting on 59 OpenML regression and classification tasks under a common evaluation protocol and 8a comparable tuning budget. Overall, the results show a clear ranking: SMAC consistently achieves the best predictive performance across all considered metrics, with TPE typically being the second strongest approach, while GP-based Bayesian optimization and random grid search form a competitive middle tier depending on the task type. In contrast, deterministic grid search and Hyperband are markedly less reliable, and default hyperparameters often lead to substantially inferior accuracy. Beyond this headline comparison, several practical lessons emerge. First, accurate tuning gen-erally requires a non-trivial number of trials: performance often continues to improve up to (and beyond) roughly 100 trials for methods such as SMAC, TPE, GP-based Bayesian optimization, and random grid search. This means that small trial budgets can materially distort conclusions about both models and tuning methods. Second, our analysis of hyperparameter importance suggests that there is no single “small” subset of hyperparameters that can be tuned while safely leaving others at defaults—all investigated parameters (learning rate, tree size/regularization controls, histogram bin-ning, and subsampling) meaningfully affect performance, implying that partial tuning strategies are frequently suboptimal. Third, regarding the number of boosting iterations, we find that selecting it via early stopping is generally preferable to treating it as a standard search parameter for regression tasks, whereas for classification tasks including the iteration count in the search space can be competitive for some hyperparameter optimization methods. Taken together, these findings support a simple recommendation for practitioners who can afford moderate tuning effort: use SMAC (or a closely related model-based method) with a sufficiently large trial budget, avoid relying on defaults, and prefer early stopping for determining the number of boosting rounds in regression tasks. At the same time, the best method is not universal at the individual data-set level, so the observed average advantages should be interpreted as guidance rather than a guarantee. There are also limitations that point to future work. Our experiments focus on one tree-boosting im-plementation (LightGBM), and we primarily assess predictive performance rather than full cost–benefit trade-offs (e.g., wall-clock time under varying degrees of parallelism, robustness under strict compute limits, or tuning under alternative objectives such as calibration or fairness). Future studies could ex-tend these comparisons to additional boosting libraries, investigate principled ways to allocate budgets across folds and seeds, and explore hybrid approaches that combine multi-fidelity resource allocation with strong model-based search and meta-learning/warm-starting across related tasks. 

# Acknowledgments 

This research was partially supported by the Swiss Innovation Agency - Innosuisse (grant number ‘57667.1 IP-ICT’). 

# References 

T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2019. J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. J. Mach. Learn. Res. ,13(null):281–305, Feb. 2012. ISSN 1532-4435. J. Bergstra, R. Bardenet, Y. Bengio, and B. K´ egl. Algorithms for hyper-parameter op-timization. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, Advances in Neural Information Processing Systems , volume 24. Curran Asso-ciates, Inc., 2011. URL https://proceedings.neurips.cc/paper_files/paper/2011/file/ 86e8f7ab32cfd12577bc2619bc635690-Paper.pdf .P. B¨ uhlmann and T. Hothorn. Boosting algorithms: Regularization, prediction and model fitting. 

Statistical Science , pages 477–505, 2007. K. Eggensperger, M. Lindauer, N. J. Mallik, A. Biedenkapp, P. Gijsbers, N. Sch¨ ornig, A. Tornede, J. Vanschoren, and F. Hutter. Hpobench: A collection of reproducible multi-fidelity benchmark 9problems for hyperparameter optimization. In NeurIPS 2021 Datasets and Benchmarks Track ,2021. URL https://arxiv.org/abs/2109.06716 .S. Falkner, A. Klein, and F. Hutter. Bohb: Robust and efficient hyperparameter optimization at scale. In Proceedings of the 35th International Conference on Machine Learning , pages 1437–1446, 2018. J. Friedman, T. Hastie, R. Tibshirani, et al. Additive logistic regression: a statistical view of boosting. 

The Annals of Statistics , 28(2):337–407, 2000. J. H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics ,pages 1189–1232, 2001. L. Grinsztajn, E. Oyallon, and G. Varoquaux. Why do tree-based models still outperform deep learning on tabular data? In Neural Information Processing Systems Datasets and Benchmarks Track , 2022. F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for gen-eral algorithm configuration. In Proceedings of the 5th International Conference on Learning and Intelligent Optimization , LION’05, page 507–523, Berlin, Heidelberg, 2011. Springer-Verlag. ISBN 9783642255656. doi: 10.1007/978-3-642-25566-3 40. URL https://doi.org/10.1007/ 978-3-642-25566-3_40 .T. Januschowski, Y. Wang, K. Torkkola, T. Erkkil¨ a, H. Hasson, and J. Gasthaus. Forecasting with trees. International Journal of Forecasting , 38(4):1473–1481, 2022. G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y. Liu. Lightgbm: A highly efficient gradient boosting decision tree. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-gus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems ,volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf .L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization, 2018. URL https://arxiv.org/abs/1603.06560 .M. Lindauer, K. Eggensperger, M. Feurer, A. Biedenkapp, D. Deng, C. Benjamins, T. Ruhkopf, R. Sass, and F. Hutter. Smac3: A versatile bayesian optimization package for hyperparameter optimization. 

Journal of Machine Learning Research , 23(54):1–9, 2022. URL http://jmlr.org/papers/v23/ 21-0888.html .D. McElfresh, S. Khandagale, J. Valverde, V. Prasad C, G. Ramakrishnan, M. Goldblum, and C. White. When do neural nets outperform boosted trees on tabular data? Advances in Neu-ral Information Processing Systems , 36:76336–76369, 2023. C. Meaney, X. Wang, J. Guan, and T. A. Stukel. Comparison of methods for tuning machine learning model hyper-parameters: with application to predicting high-need high-cost health care users. BMC Medical Research Methodology , 25(1):134, 2025. J. Mockus, V. Tiesis, and A. Zilinskas. The application of Bayesian methods for seeking the extremum. 

Towards Global Optimization , 2(117-129):2, 1978. M. Motz, J. Krauß, and R. H. Schmitt. Benchmarking of hyperparameter optimization techniques for machine learning applications in production. Advances in Industrial and Manufacturing Engineering ,5:100099, 2022. D. Nielsen. Tree boosting with XGBoost-Why does XGBoost win” every” machine learning competi-tion? Master’s thesis, NTNU, 2016. F. Pfisterer, L. Schneider, J. Moosbauer, M. Binder, and B. Bischl. Yahpo gym - an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization. In I. Guyon, M. Lindauer, M. van der Schaar, F. Hutter, and R. Garnett, editors, Proceedings of the First International Con-ference on Automated Machine Learning , volume 188 of Proceedings of Machine Learning Research ,pages 3/1–39. PMLR, 25–27 Jul 2022. 10 S. Putatunda and K. M. Rama. A comparative analysis of hyperopt algorithms for optimizing machine learning algorithms. arXiv preprint arXiv:1807.02822 , 2018. URL https://arxiv.org/abs/1807. 02822 .R. Shwartz-Ziv and A. Armon. Tabular data: Deep learning is not all you need. Information Fusion ,81:84–90, 2022. F. Sigrist. Gradient and Newton boosting for classification and regression. Expert Systems With Applications , 167:114080, 2021. F. Sigrist. Gaussian Process Boosting. Journal of Machine Learning Research , 23(232):1–46, 2022. F. Sigrist, T. Gyger, and P. Kuendig. GPBoost: Combining tree-boosting with Gaussian process and mixed effects models, 2021. URL https://github.com/fabsig/GPBoost .M. Wistuba, N. Schilling, and L. Schmidt-Thieme. Learning hyperparameter optimization initializa-tions. In 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA) ,pages 1–10, 2015. doi: 10.1109/DSAA.2015.7344817. 11 Appendix A Additional results 45 60 75 90 105 120 135 

0.01 

0.02 

0.03 

0.04 

R2

45 60 75 90 105 120 135 

0.05 

0.10 

0.15 

0.20 

0.25 

RMSE 

45 60 75 90 105 120 135 

0.01 

0.02 

0.03 

0.04 Accuracy 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

Log Loss 

> Regression Classification

Default 

Hyperband 

TPE 

Deterministic Grid 

Random Grid 

GP-BO 

SMAC 

Figure 3: Relative differences to the best score as a function of the number of trials. 12 45 60 75 90 105 120 135 

0.2 

0.4 

0.6 

0.8 

R2

45 60 75 90 105 120 135 

0.2 

0.4 

0.6 

0.8 

RMSE 

45 60 75 90 105 120 135 

0.2 

0.4 

0.6 

Accuracy 

45 60 75 90 105 120 135 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 Default = 0.843 

Log Loss 

> Regression Classification

Default 

Hyperband 

TPE 

Deterministic Grid 

Random Grid 

GP-BO 

SMAC Figure 4: Normalized scores as a function of the number of trials. The confidence intervals represent uncertainty due to the randomness in the hyperparameter selection methods. 13 45 60 75 90 105 120 135 

0.9820 

0.9825 

0.9830 

0.9835 

Default = 0.981 

analcatdata_supreme 

45 60 75 90 105 120 135 

0.0005 

0.0004 

0.0003 

0.0002 

0.0001 

0.0000 +1 visualizing_soil 

45 60 75 90 105 120 135 

0.9920 

0.9922 

0.9924 

0.9926 

0.9928 

0.9930 

Default = 0.992 

diamonds 

45 60 75 90 105 120 135 

0.52 

0.53 

0.54 

0.55 

0.56 

0.57 

Mercedes_Benz_Greener_Manufacturing 

45 60 75 90 105 120 135 

0.994 

0.995 

0.996 

0.997 

0.998 

Default = 0.99 

Brazilian_houses 

45 60 75 90 105 120 135 

0.925 

0.930 

0.935 

0.940 

0.945 

Bike_Sharing_Demand 

45 60 75 90 105 120 135 

0.60 

0.62 

0.64 

0.66 

Default = 0.581 

nyc-taxi-green-dec-2016 

45 60 75 90 105 120 135 

0.8875 

0.8900 

0.8925 

0.8950 

0.8975 

0.9000 

0.9025 

house_sales 

45 60 75 90 105 120 135 

0.695 

0.700 

0.705 

0.710 

particulate-matter-ukair-2017 

45 60 75 90 105 120 135 

0.99974 

0.99976 

0.99978 

0.99980 

0.99982 

SGEMM_GPU_kernel_performance 

45 60 75 90 105 120 135 

0.060 

0.065 

0.070 

0.075 

Default = 0.0525 

topo_2_1 

45 60 75 90 105 120 135 

0.555 

0.560 

0.565 

0.570 

Default = 0.545 

abalone 

45 60 75 90 105 120 135 

0.1898 

0.1900 

0.1902 

0.1904 

0.1906 

0.1908 

Default = 0.189 

seattlecrime6 

45 60 75 90 105 120 135 

0.084 

0.086 

0.088 

delays_zurich_transport 

45 60 75 90 105 120 135 

0.540 

0.545 

0.550 

0.555 

0.560 

Allstate_Claims_Severity 

45 60 75 90 105 120 135 

0.058 

0.059 

0.060 

0.061 

Airlines_DepDelay_1M 

45 60 75 90 105 120 135 

0.9787 

0.9788 

0.9789 

0.9790 

0.9791 

0.9792 

medical_charges 

45 60 75 90 105 120 135 

0.9835 

0.9840 

0.9845 

0.9850 

0.9855 

0.9860 

cpu_act 

45 60 75 90 105 120 135 

0.96 

0.97 

0.98 

0.99 

pol 

45 60 75 90 105 120 135 

0.8900 

0.8925 

0.8950 

0.8975 

0.9000 

0.9025 

0.9050 

Default = 0.884 

elevators 

45 60 75 90 105 120 135 

0.425 

0.450 

0.475 

0.500 

0.525 

0.550 

wine_quality 

45 60 75 90 105 120 135 

0.844 

0.846 

0.848 

0.850 

0.852 

0.854 

0.856 

Ailerons 

45 60 75 90 105 120 135 

0.835 

0.840 

0.845 

0.850 

0.855 

0.860 

houses 

45 60 75 90 105 120 135 

0.54 

0.55 

0.56 

0.57 

0.58 

0.59 

0.60 

house_16H 

Default 

Hyperband 

TPE 

Deterministic Grid 

Random Grid 

GP-BO 

SMAC Figure 5: R2 as a function of the number of trials per data set. The confidence intervals represent uncertainty due to the randomness in the hyperparameter selection methods. 14 45 60 75 90 105 120 135 

0.9460 

0.9465 

0.9470 

0.9475 

diamonds 

45 60 75 90 105 120 135 

0.994 

0.995 

0.996 

0.997 

0.998 

Default = 0.99 

Brazilian_houses 

45 60 75 90 105 120 135 

0.695 

0.700 

0.705 

Bike_Sharing_Demand 

45 60 75 90 105 120 135 

0.60 

0.62 

0.64 

Default = 0.562 

nyc-taxi-green-dec-2016 

45 60 75 90 105 120 135 

0.880 

0.885 

0.890 

0.895 house_sales 

45 60 75 90 105 120 135 

0.85 

0.86 

0.87 

0.88 

0.89 

0.90 

Default = 0.821 

sulfur 

45 60 75 90 105 120 135 

0.9787 

0.9788 

0.9789 

0.9790 

0.9791 

0.9792 

medical_charges 

45 60 75 90 105 120 135 

0.9225 

0.9250 

0.9275 

0.9300 

0.9325 

0.9350 

MiamiHousing2016 

45 60 75 90 105 120 135 

0.905 

0.910 

0.915 

0.920 

0.925 

0.930 

superconduct 

45 60 75 90 105 120 135 

0.065 

0.070 

0.075 

0.080 

0.085 

yprop_4_1 

45 60 75 90 105 120 135 

0.545 

0.550 

0.555 

0.560 

0.565 abalone 

45 60 75 90 105 120 135 

0.036 

0.038 

0.040 

delays_zurich_transport 

Default 

Hyperband 

TPE 

Deterministic Grid 

Random Grid 

GP-BO 

SMAC Figure 6: R2 as a function of the number of trials per data set. The confidence intervals represent uncertainty due to the randomness in the hyperparameter selection methods. 15 45 60 75 90 105 120 135 

0.068 

0.069 

0.070 

0.071 

0.072 

0.073 Default = 0.0756 

analcatdata_supreme 

45 60 75 90 105 120 135 

0.05 

0.10 

0.15 

0.20 

0.25 

visualizing_soil 

45 60 75 90 105 120 135 

0.084 

0.085 

0.086 

0.087 

0.088 

0.089 

0.090 Default = 0.0925 

diamonds 

45 60 75 90 105 120 135 

8.3 

8.4 

8.5 

8.6 

8.7 

8.8 

Mercedes_Benz_Greener_Manufacturing 

45 60 75 90 105 120 135 

0.035 

0.040 

0.045 

0.050 

0.055 

Default = 0.0732 

Brazilian_houses 

45 60 75 90 105 120 135 

42 

44 

46 

48 

50 

Bike_Sharing_Demand 

45 60 75 90 105 120 135 

0.34 

0.35 

0.36 

0.37 

Default = 0.385 

nyc-taxi-green-dec-2016 

45 60 75 90 105 120 135 

0.164 

0.166 

0.168 

0.170 

0.172 

0.174 

0.176 

house_sales 

45 60 75 90 105 120 135 

0.368 

0.370 

0.372 

0.374 

0.376 

0.378 

particulate-matter-ukair-2017 

45 60 75 90 105 120 135 

0.015 

0.016 

0.017 

0.018 

SGEMM_GPU_kernel_performance 

45 60 75 90 105 120 135 

0.02850 

0.02855 

0.02860 

0.02865 

0.02870 

0.02875 Default = 0.0289 

topo_2_1 

45 60 75 90 105 120 135 

2.11 

2.12 

2.13 

2.14 

2.15 

2.16 Default = 2.17 

abalone 

45 60 75 90 105 120 135 

379.75 

379.80 

379.85 

379.90 

379.95 

380.00 Default = 380 

seattlecrime6 

45 60 75 90 105 120 135 

2.988 

2.990 

2.992 

2.994 

2.996 

2.998 

delays_zurich_transport 

45 60 75 90 105 120 135 

0.5375 

0.5400 

0.5425 

0.5450 

0.5475 

0.5500 

0.5525 Allstate_Claims_Severity 

45 60 75 90 105 120 135 

1.910 

1.911 

1.912 

1.913 

Airlines_DepDelay_1M 

45 60 75 90 105 120 135 

0.0814 

0.0816 

0.0818 

0.0820 

0.0822 

0.0824 

medical_charges 

45 60 75 90 105 120 135 

2.20 

2.25 

2.30 

2.35 

cpu_act 

45 60 75 90 105 120 135 

4

5

6

7

8

9 pol 

45 60 75 90 105 120 135 

0.00205 

0.00210 

0.00215 

0.00220 

Default = 0.00228 

elevators 

45 60 75 90 105 120 135 

0.60 

0.62 

0.64 

0.66 

wine_quality 

45 60 75 90 105 120 135 

0.000156 

0.000158 

0.000160 

Ailerons 

45 60 75 90 105 120 135 

0.210 

0.215 

0.220 

0.225 

0.230 

houses 

45 60 75 90 105 120 135 

0.56 

0.57 

0.58 

0.59 

house_16H 

Default 

Hyperband 

TPE 

Deterministic Grid 

Random Grid 

GP-BO 

SMAC Figure 7: RMSE as a function of the number of trials per data set. The confidence intervals represent uncertainty due to the randomness in the hyperparameter selection methods. 16 45 60 75 90 105 120 135 

0.233 

0.234 

0.235 

0.236 diamonds 

45 60 75 90 105 120 135 

0.035 

0.040 

0.045 

0.050 

0.055 

Default = 0.0731 

Brazilian_houses 

45 60 75 90 105 120 135 

98.0 

98.5 

99.0 

99.5 

100.0 

100.5 

Bike_Sharing_Demand 

45 60 75 90 105 120 135 

0.35 

0.36 

0.37 

0.38 

Default = 0.394 

nyc-taxi-green-dec-2016 

45 60 75 90 105 120 135 

0.1725 

0.1750 

0.1775 

0.1800 

0.1825 

0.1850 house_sales 

45 60 75 90 105 120 135 

0.017 

0.018 

0.019 

0.020 

0.021 Default = 0.0222 

sulfur 

45 60 75 90 105 120 135 

0.0814 

0.0816 

0.0818 

0.0820 

0.0822 

0.0824 

medical_charges 

45 60 75 90 105 120 135 

0.1450 

0.1475 

0.1500 

0.1525 

0.1550 

0.1575 

MiamiHousing2016 

45 60 75 90 105 120 135 

9.0 

9.5 

10.0 

10.5 

superconduct 

45 60 75 90 105 120 135 

0.0283 

0.0284 

0.0285 

0.0286 

0.0287 yprop_4_1 

45 60 75 90 105 120 135 

2.13 

2.14 

2.15 

2.16 

2.17 

abalone 

45 60 75 90 105 120 135 

3.064 

3.066 

3.068 

3.070 

3.072 

3.074 

delays_zurich_transport 

Default 

Hyperband 

TPE 

Deterministic Grid 

Random Grid 

GP-BO 

SMAC Figure 8: RMSE as a function of the number of trials per data set. The confidence intervals represent uncertainty due to the randomness in the hyperparameter selection methods. 17 45 60 75 90 105 120 135 

0.86 

0.88 

0.90 

0.92 

electricity 

45 60 75 90 105 120 135 

0.62 

0.64 

0.66 

0.68 

eye_movements 

45 60 75 90 105 120 135 

0.84 

0.86 

0.88 

0.90 

0.92 

0.94 

covertype 

45 60 75 90 105 120 135 

0.659 

0.660 

0.661 

0.662 

0.663 

albert 

45 60 75 90 105 120 135 

0.712 

0.714 

0.716 

0.718 

0.720 

Default = 0.71 

default-of-credit-card-clients 

45 60 75 90 105 120 135 

0.77 

0.78 

0.79 

0.80 

0.81 

road-safety 

45 60 75 90 105 120 135 

0.678 

0.680 

0.682 

0.684 

0.686 

0.688 

Default = 0.67 

compas-two-years 

45 60 75 90 105 120 135 

0.774 

0.776 

0.778 

0.780 

0.782 

0.784 credit 

45 60 75 90 105 120 135 

0.84 

0.86 

0.88 

0.90 

0.92 

electricity 

45 60 75 90 105 120 135 

0.80 

0.82 

0.84 

0.86 

0.88 

0.90 

0.92 

covertype 

45 60 75 90 105 120 135 

0.965 

0.970 

0.975 

0.980 

0.985 

pol 

45 60 75 90 105 120 135 

0.8725 

0.8750 

0.8775 

0.8800 

0.8825 

0.8850 

0.8875 

house_16H 

45 60 75 90 105 120 135 

0.8500 

0.8525 

0.8550 

0.8575 

0.8600 

0.8625 

0.8650 

MagicTelescope 

45 60 75 90 105 120 135 

0.802 

0.804 

0.806 

0.808 

bank-marketing 

45 60 75 90 105 120 135 

0.9300 

0.9325 

0.9350 

0.9375 

0.9400 

0.9425 

MiniBooNE 

45 60 75 90 105 120 135 

0.7175 

0.7200 

0.7225 

0.7250 

0.7275 

0.7300 

0.7325 

Higgs 

45 60 75 90 105 120 135 

0.61 

0.62 

0.63 

0.64 

0.65 

0.66 

0.67 

eye_movements 

45 60 75 90 105 120 135 

0.6075 

0.6080 

0.6085 

0.6090 

0.6095 

0.6100 

Diabetes130US 

45 60 75 90 105 120 135 

0.780 

0.785 

0.790 

0.795 

0.800 

jannis 

45 60 75 90 105 120 135 

0.714 

0.716 

0.718 

0.720 

Default = 0.71 

default-of-credit-card-clients 

45 60 75 90 105 120 135 

0.77 

0.78 

0.79 

Bioresponse 

45 60 75 90 105 120 135 

0.895 

0.900 

0.905 

0.910 

california 

45 60 75 90 105 120 135 

0.720 

0.722 

0.724 

0.726 

Default = 0.715 

heloc 

Default 

Hyperband 

TPE 

Deterministic Grid 

Random Grid 

GP-BO 

SMAC Figure 9: Accuracy as a function of the number of trials per data set. The confidence intervals represent uncertainty due to the randomness in the hyperparameter selection methods. 18 45 60 75 90 105 120 135 

0.20 

0.25 

0.30 

0.35 

electricity 

45 60 75 90 105 120 135 

0.60 

0.62 

0.64 

0.66 

0.68 

eye_movements 

45 60 75 90 105 120 135 

0.15 

0.20 

0.25 

0.30 

0.35 

0.40 

0.45 

covertype 

45 60 75 90 105 120 135 

0.614 

0.616 

0.618 

0.620 

albert 

45 60 75 90 105 120 135 

0.56 

0.57 

0.58 

0.59 

0.60 

0.61 

default-of-credit-card-clients 

45 60 75 90 105 120 135 

0.42 

0.44 

0.46 

0.48 

0.50 

0.52 

0.54 

road-safety 

45 60 75 90 105 120 135 

0.604 

0.606 

0.608 

0.610 Default = 0.709 

compas-two-years 

45 60 75 90 105 120 135 

0.48 

0.50 

0.52 

0.54 

0.56 

credit 

45 60 75 90 105 120 135 

0.25 

0.30 

0.35 

0.40 

electricity 

45 60 75 90 105 120 135 

0.25 

0.30 

0.35 

0.40 

0.45 

covertype 

45 60 75 90 105 120 135 

0.06 

0.08 

0.10 

0.12 

pol 

45 60 75 90 105 120 135 

0.32 

0.34 

0.36 

0.38 

0.40 

house_16H 

45 60 75 90 105 120 135 

0.350 

0.375 

0.400 

0.425 

0.450 

0.475 

0.500 MagicTelescope 

45 60 75 90 105 120 135 

0.425 

0.450 

0.475 

0.500 

0.525 

0.550 

0.575 

bank-marketing 

45 60 75 90 105 120 135 

0.20 

0.22 

0.24 

0.26 

MiniBooNE 

45 60 75 90 105 120 135 

0.54 

0.55 

0.56 

0.57 

0.58 

0.59 

Higgs 

45 60 75 90 105 120 135 

0.62 

0.64 

0.66 

0.68 

eye_movements 

45 60 75 90 105 120 135 

0.65775 

0.65800 

0.65825 

0.65850 

0.65875 

0.65900 

Diabetes130US 

45 60 75 90 105 120 135 

0.46 

0.48 

0.50 

0.52 

jannis 

45 60 75 90 105 120 135 

0.56 

0.57 

0.58 

0.59 

Default = 0.6 

default-of-credit-card-clients 

45 60 75 90 105 120 135 

0.50 

0.55 

0.60 

0.65 

0.70 

Bioresponse 

45 60 75 90 105 120 135 

0.26 

0.28 

0.30 

0.32 

0.34 

california 

45 60 75 90 105 120 135 

0.55 

0.56 

0.57 

0.58 

0.59 

0.60 Default = 0.623 

heloc 

Default 

Hyperband 

TPE 

Deterministic Grid 

Random Grid 

GP-BO 

SMAC Figure 10: Log Loss as a function of the number of trials per data set. The confidence intervals represent uncertainty due to the randomness in the hyperparameter selection methods. 19 45 60 75 90 105 120 135 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

> Random Grid

Default = 0.181 

R2

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 Default = 0.829 

RMSE 

45 60 75 90 105 120 135 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

Default = 0.115 

Accuracy 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

Log Loss 

45 60 75 90 105 120 135 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

> TPE

Default = 0.181 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 Default = 0.829 

45 60 75 90 105 120 135 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

Default = 0.115 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

45 60 75 90 105 120 135 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

> GP-BO

Default = 0.181 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 Default = 0.829 

45 60 75 90 105 120 135 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

Default = 0.115 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

45 60 75 90 105 120 135 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

> Hyperband

Default = 0.181 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 Default = 0.829 

45 60 75 90 105 120 135 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

Default = 0.115 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

45 60 75 90 105 120 135 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

> SMAC

Default = 0.181 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 Default = 0.829 

45 60 75 90 105 120 135 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

Default = 0.115 

45 60 75 90 105 120 135 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

Default 

Num Iter 

Joint 

Num Leaves 

Max Depth Figure 11: Comparison of hyperparameter search space options regarding the maximal number of iterations, the maximal tree depth, and the number of boosting iterations. 20