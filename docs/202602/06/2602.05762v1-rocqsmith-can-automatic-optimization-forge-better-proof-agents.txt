Title: RocqSmith: Can Automatic Optimization Forge Better Proof Agents?

URL Source: https://arxiv.org/pdf/2602.05762v1

Published Time: Fri, 06 Feb 2026 02:06:49 GMT

Number of Pages: 6

Markdown Content:
Preprint. Under review 

# ROCQ SMITH : C AN AUTOMATIC OPTIMIZATION 

# FORGE BETTER PROOF AGENTS ?

Andrei Kozyrev 1, Nikita Khramov 1, Denis Lochmelis 1, Valerio Morelli 1

Gleb Solovev 1, Anton Podkopaev 2,31JetBrains Research Germany, 2JetBrains Research Netherlands, 

> 3

Constructor University Bremen 

# ABSTRACT 

This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how dif-ferent automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tun-ing of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent. 

# 1 INTRODUCTION 

Recent advances in Generative Artificial Intelligence (AI) have significantly accelerated software de-velopment workflows. At the same time, empirical studies indicate that code generated or assisted by large language models may still exhibit security weaknesses and correctness issues in real-world software projects (Fu et al., 2025). Formal methods provide a way to mitigate these risks by offering strong correctness guarantees and explicit acceptance criteria for generated software, assuming a well-formed specification. Interactive Theorem Provers (ITPs) support the development of formal specifications and machine-checked proofs. Several mature ITPs exist today, including Rocq (for-merly Coq) (Bertot & Cast´ eran, 2013), Lean (De Moura et al., 2015), Agda (Kokke et al., 2020), and Isabelle (Nipkow et al., 2002). Among them, Rocq is a particularly mature system with over 30 years of continuous development and a strong track record in high-impact verification efforts, including the formal verification of the CompCert C compiler (Leroy et al., 2016), the only widely used C compiler for which an extensive study reported no miscompilation bugs (Yang et al., 2011). Formal software verification remains a demanding process that requires substantial human expertise, motivating extensive work on automating theorem proving in Rocq. One promising direction is the development of autonomous agentic systems for interactive theorem proving (Xu & Odersky, 2026). Agentic systems are a natural fit for this setting, as their interaction loop closely resembles that of a human Rocq programmer. An agent observes the current environment state, performs intermediate reasoning, and selects an action to apply. In Rocq, proof construction follows an analogous process: the user (or agent) iteratively inspects the current proof state , consisting of the active goal and its context, and applies a tactic that transforms the proof state into simpler subgoals until the proof is complete. The classical ReAct (Reason-and-Act) paradigm (Yao et al., 2023) aligns well with this workflow, alternating between reasoning about the proof state and selecting tactics to apply in a step-by-step manner. Several attempts have been made to build autonomous agentic systems for interactive theorem proving (Yang et al., 2023; Teodorescu et al., 2024; Kozyrev et al., 2025). Among them, RocqS-tar (Kozyrev et al., 2025) proposes an agentic system for generating Rocq proofs and reports strong empirical results on the IMM-300 benchmark. The RocqStar agent consists of a multi-component pipeline that combines planning, execution, and auxiliary control mechanisms, and relies on a sub-stantial amount of carefully engineered prompts and agent interactions. Although such systems demonstrate the potential of agentic approaches to theorem proving, their design and implementa-1

> arXiv:2602.05762v1 [cs.AI] 5 Feb 2026

Preprint. Under review tion involve considerable engineering effort. Constructing and tuning a production-grade proof agent typically requires manual prompt design, task decomposition, tool orchestration, and the curation of structured contextual knowledge. This motivates the study of whether automatic agent optimization methods can reduce the amount of manual engineering required to build and adapt such agents. Recent work on automatic agent optimization aims to reduce manual agent engineering by plac-ing agents into feedback-driven optimization loops. Depending on the approach, optimization may target prompts and few-shot demonstrations (Opsahl-Ong et al., 2024), external contextual knowl-edge (Ouyang et al., 2025; Zhang et al., 2025), or the agent’s control flow structure (Hu et al., 2025). Most of the existing evaluations of automatic agent optimization methods focus on benchmark-driven tasks such as SWE-Bench or AppWorld. However, it remains unclear how these methods generalize to agents in formal domains where correctness is strictly verified rather than empirically tested. Interactive theorem proving serves as a representative example of such a task; it provides a strong verifier that allows evaluation of the agent’s actions and returns a clear feedback signal from the environment, while at the same time posing a challenging problem that cannot be solved in an ad hoc manner and requires structured reasoning. In this work, we take a simplified but representative core of a production-grade Rocq proof agent and use it as a test case to evaluate automatic agent optimization methods. We systematically study how different optimizers perform when applied to this setting and assess their ability to improve the agent toward the performance level of a carefully human-designed baseline. 

# 2 PRELIMINARIES 

RocqStar (Kozyrev et al., 2025) is an agentic system for Rocq proof generation built as a multi-component pipeline that combines planning, execution, reflection, critique, and retrieval-based re-planning, and relies on carefully constructed prompts, auxiliary control logic, and a rich tool inter-face for interacting with the prover. Although this design achieves strong empirical performance, it requires substantial manual engineering effort to construct, tune, and maintain. In this work, we omit these hand-crafted components and consider a simplified agent core as our baseline. In partic-ular, we study a single-loop agent topology, where a minimal control structure repeatedly observes the current proof state, performs lightweight reasoning, selects a tool call, and executes it until ter-mination. This simplified agent captures the essential interaction pattern between an LLM and the Rocq environment while avoiding additional planning or control modules. Our goal is to evalu-ate whether automatic agent optimization methods can bridge the performance gap between such a minimal agent and a carefully engineered, human-designed agentic pipeline. We begin our study by evaluating the automatic agent optimization methods available in the DSPy framework (Khattab et al., 2023). We implement a simplified RocqStar agent in DSPy and apply a range of built-in optimizers, including BootstrapFewShot , MIPROv2 (Opsahl-Ong et al., 2024), 

SIMBA , and GEPA (Agrawal et al., 2025). BootstrapFewShot constructs few-shot prompts by col-lecting successful execution traces on a training set and including the first k (4 in evaluation due to context limitations) of such demonstrations directly into the prompt, providing a simple base-line for in-context learning. MIPROv2 jointly optimizes instructions and few-shot demonstrations by proposing multiple instruction candidates and performing a discrete search over their combina-tions using Bayesian optimization (Snoek et al., 2012) guided by task performance. SIMBA applies a random search over prompt variants, keeping changes that lead to improved task performance. 

GEPA applies an evolutionary optimization strategy to the textual components of the agent, such as prompts or instructions, using selection mechanisms and optional reflective feedback to progres-sively improve performance. In addition to prompt-centric optimizers, we consider approaches that optimize an agent’s contex-tual knowledge based on past behavior. Both ACE (Zhang et al., 2025) and ReasoningBank (Ouyang et al., 2025) operate by running the agent on a training dataset, analyzing successful and failed ex-ecutions using a language model, and storing the resulting reflections in a structured knowledge base that can be reused at test time. This knowledge captures high-level guidance about effective strategies and common failure modes. The two approaches differ primarily in how this contextual knowledge is represented and retrieved. ReasoningBank maintains a memory bank of past expe-riences and uses an encoder model to embed the current query at test time and retrieve the most similar memory items, which are then injected into the agent’s context. ACE instead maintains a cu-2Preprint. Under review                                                                                                                                         

> Impl Opt. Time Agent Description Model Proof Length Total
> ≤45–8 9–20 Koog —Simple —GPT-4.1 10% 10% 0% 6% Koog 45 min Simple w/ BootstrapFewShot GPT-4.1 24% 10% 6% 13% Koog 3h Simple w/ ACE GPT-4.1 22% 6% 2% 10% Koog 2h Simple w/ RB GPT-4.1 32% 14% 4% 17% Koog —ReAct —GPT-4.1 34% 16% 10% 20% Koog 45 min ReAct w/ BootstrapFewShot GPT-4.1 54% 28% 18% 33%
> Koog 6h ReAct w/ ACE GPT-4.1 28% 16% 6% 17% Koog 6h ReAct w/ RB GPT-4.1 40% 22% 12% 25% Koog 6h ReAct w/ RB (no retrieval) GPT-4.1 32% 20% 12% 21% ADAS 70h ADAS —GPT-4.1 30% 14% 8% 17% DSPy —ReAct —GPT-4.1 38% 16% 4% 19% DSPy 45 min ReAct w/ BootstrapFewShot GPT-4.1 66% 32% 22% 40% DSPy 12h ReAct w/ MIPROv2 GPT-4.1 66% 42% 22% 43%
> DSPy 14h ReAct w/ SIMBA GPT-4.1 60% 42% 26% 43%
> DSPy 32h ReAct w/ GEPA GPT-4.1 30% 18% 4% 17% Koog —SOTA Hand-written agent GPT-4.1 72% 56% 32% 53%

Table 1: Performance of different agent optimization methods on Rocq proof generation, grouped by proof length (number of tactics). RB denotes ReasoningBank .rated, agent-specific context that is incrementally refined across training iterations, and provides the resulting context to the agent in full at inference time in order to better utilize the available context window, rather than relying on similarity-based retrieval. Finally, we also evaluate ADAS (Hu et al., 2025), which targets the agent’s control flow rather than its prompts or context. ADAS represents the agent as executable code and optimizes its topology by prompting a language model with the available tools and task description to generate code defining the agent’s overall decision logic. 

# 3 EXPERIMENTS 

We study automatic optimization methods for agentic proof generation in Rocq and address two research questions. (i) Which automatic AI agent optimization methods lead to measurable perfor-mance gains over a fixed baseline agent in a production-oriented Rocq theorem proving setting? (ii) How does an automatically optimized proof agent compare to a state-of-the-art, human-engineered proof-agent pipeline in a Rocq setting? We use the IMM theorem proving benchmark introduced in prior work on proof agents and reuse the dataset split released by CoqPilot (Kozyrev et al., 2024). The dataset contains 300 theorems from the IMM project (Podkopaev et al., 2019). Theorems are split by difficulty using the length (in tactics) of the human-written reference proof as a proxy: shorter proofs tend to correspond to easier theorems, while longer proofs indicate harder instances. For evaluation, we select 50 theorems from each difficulty group. We allocate a separate set of 60 theorems for training the optimizers. Due to differences in experimental setup, the exact training subset used in the DSPy experiments slightly differs from the one used in the Koog experiments (the DSPy training split is mildly skewed toward easier theorems). This does not affect comparisons within each framework, where the training set is fixed, but may introduce minor variance when comparing results across frameworks. All experiments use GPT-4.1 due to its large context window and reliable tool use. For all experiments, we use the simplified RocqStar agent described in § 2 as the baseline archi-tecture. We evaluate this agent in two implementations. In DSPy , we consider a ReAct strategy, which produces intermediate reasoning before making tool calls and serves as the basis for applying DSPy’s built-in optimization methods. In Koog , we evaluate both a Simple variant, which directly predicts tool calls without generating explicit reasoning traces, and a ReAct variant, allowing us to assess whether explicit reasoning is necessary for effective optimization in this setting. We train the optimization methods described in § 2 and evaluate them on the test set with a fixed resource budget of at most 20 tool calls. We report success rate (percentage of theorems solved) for each difficulty group (A – C) and overall ( Total ), as shown in Table 1. We also report the 

optimization time , i.e., the wall-clock time spent in the optimizer’s training phase. 3Preprint. Under review In Table 1, RB denotes ReasoningBank . Unless stated otherwise, ReasoningBank retrieves the top 5 memory items at test time based on encoder similarity and injects them into the agent context. The 

RB (no retrieval) disables similarity-based retrieval and instead includes the entire memory bank in the context. For BootstrapFewShot , we use at most four demonstrations due to context length constraints. We note that absolute performance differences between BootstrapFewShot in Koog and DSPy should be interpreted with caution, as they reflect minor framework-specific differences, including additional implicit prompting and tool-handling logic. GEPA is run in its heavy optimiza-tion mode, which enables full evolutionary search with reflective feedback, while MIPROv2 is run in the light mode due to the substantially higher computational cost of the heavy configuration. 1 For 

ADAS , we perform ten optimization runs and report the best-performing agent obtained. The experimental results reveal several patterns. Few-shot demonstrations lead to reliable perfor-mance improvements in all evaluated settings, making them the most robust optimization method in our study. When applied to agents that explicitly produce intermediate reasoning traces, as in the ReAct variants, few-shot demonstrations yield particularly strong gains. At the same time, few-shot demonstrations must be used with care, as longer traces can quickly exhaust the available context budget and substantially increase token usage. For context-centric methods, ReasoningBank bene-fits from retrieving a small number of relevant memory items rather than injecting the entire memory bank into the prompt, in contrast to ACE, which injects up to 200 memories into the context. This observation aligns with prior findings (Huang et al., 2025; Xu et al., 2024) that uncurated or weakly relevant context can degrade model performance by introducing noise. At the same time, effec-tive use of ReasoningBank requires careful consideration of agent inputs: similarity-based retrieval assumes that the agent input is semantically meaningful for encoding, whereas in our setting the raw input initially consisted only of a theorem identifier and file path, with the actual proof state resolved later by the environment. As a result, contextual memory methods may require additional engineering to be applicable in theorem-proving pipelines. Among prompt-centric optimizers, we find that more sophisticated methods do not consistently out-perform simpler baselines in this setting. MIPROv2 is potentially well-suited for multi-component pipelines, where joint optimization of instructions and demonstrations across several agent nodes can be beneficial, but yields limited gains for the single-prompt agents studied here. SIMBA and MIPROv2 achieve improvements comparable to BootstrapFewShot in DSPy, suggesting that much of the observed performance gain stems from the inclusion of successful few-shot demonstrations rather than from deeper prompt evolution. Finally, ADAS, which attempts to optimize the agent’s control flow by generating executable agent code, performs inconsistently: despite multiple op-timization iterations, we do not observe monotonic improvement, and the best-performing agent emerges early in the process. Moreover, the resulting agents exhibit strong bias toward the training data and rely on brittle, hard-coded decision logic, limiting their generality. Overall, none of the evaluated optimizers matches the performance of the hand-engineered state-of-the-art agent, indicating that fully automated optimization of agent architectures for formal ver-ification remains an open challenge. Nevertheless, BootstrapFewShot emerges as a simple yet sur-prisingly strong optimization method, achieving consistent improvements with minimal engineering effort. 

# 4 CONCLUSION 

In this work, we evaluated a range of automatic agent optimization methods on a Rocq theorem proving task, using a simplified proof agent as the evaluation setting. Our results show that while several optimizers can yield measurable improvements over a fixed baseline, simple few-shot boot-strapping is the most consistently effective, especially for ReAct-style agents, improving overall success rates from 20% to 33% in Koog and from 19% to 40% in DSPy; however, none of the studied methods reaches the performance of a carefully engineered, state-of-the-art proof agent. Context-centric approaches such as ReasoningBank can be effective when contextual information is selectively retrieved, but sometimes require slight pipeline adaptations. More complex optimiz-ers, including topology-level approaches such as ADAS, did not demonstrate consistent gains in this domain. Overall, our findings suggest that current automatic agent optimization techniques 

> 1MIPROv2 with heavy mode is left for future experiments.

4Preprint. Under review can partially reduce manual tuning effort but are not yet sufficient to fully automate the design of high-performing agentic systems for formal verification. 

# REFERENCES 

Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, and Omar Khattab. Gepa: Reflective prompt evolution can outperform reinforcement learning, 2025. URL https:// arxiv.org/abs/2507.19457 .Yves Bertot and Pierre Cast´ eran. Interactive theorem proving and program development: Coq’Art: the calculus of inductive constructions . Springer Science & Business Media, 2013. doi: 10.1007/ 978-3-662-07964-5. Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25 , pp. 378–388. Springer, 2015. doi: https://doi.org/10.1007/978-3-319-21401-6 26. Yujia Fu, Peng Liang, Amjed Tahir, Zengyang Li, Mojtaba Shahin, Jiaxin Yu, and Jinfu Chen. Security weaknesses of copilot-generated code in github projects: An empirical study. ACM Trans. Softw. Eng. Methodol. , 34(8), October 2025. ISSN 1049-331X. doi: 10.1145/3716848. URL https://doi.org/10.1145/3716848 .Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems, 2025. URL https: //arxiv.org/abs/2408.08435 .Yue Huang, Yanbo Wang, Zixiang Xu, Chujie Gao, Siyuan Wu, Jiayi Ye, Xiuying Chen, Pin-Yu Chen, and Xiangliang Zhang. Breaking focus: Contextual distraction curse in large language models. ArXiv , abs/2502.01609, 2025. URL https://api.semanticscholar.org/ CorpusID:276107466 .Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vard-hamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. Dspy: Compiling declarative language model calls into self-improving pipelines, 2023. URL https://arxiv.org/abs/2310.03714 .Wen Kokke, Jeremy G. Siek, and Philip Wadler. Programming language foundations in agda. 

Science of Computer Programming , 194:102440, 2020. ISSN 0167-6423. doi: https://doi. org/10.1016/j.scico.2020.102440. URL https://www.sciencedirect.com/science/ article/pii/S0167642320300502 .Andrei Kozyrev, Gleb Solovev, Nikita Khramov, and Anton Podkopaev. Coqpilot, a plugin for llm-based generation of proofs. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering , ASE ’24, pp. 2382–2385, New York, NY, USA, 2024. Associ-ation for Computing Machinery. ISBN 9798400712487. doi: 10.1145/3691620.3695357. URL 

https://doi.org/10.1145/3691620.3695357 .Andrei Kozyrev, Nikita Khramov, Gleb Solovev, and Anton Podkopaev. Rocqstar: Leverag-ing similarity-driven retrieval and agentic systems for rocq generation, 2025. URL https: //arxiv.org/abs/2505.22846 .Xavier Leroy, Sandrine Blazy, Daniel K¨ astner, Bernhard Schommer, Markus Pister, and Christian Ferdinand. Compcert-a formally verified optimizing compiler. In ERTS 2016: Embedded Real Time Software and Systems, 8th European Congress , 2016. Tobias Nipkow, Markus Wenzel, and Lawrence C Paulson. Isabelle/HOL: a proof assistant for higher-order logic . Springer, 2002. doi: https://doi.org/10.1007/3-540-45949-9 5. Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, and Omar Khattab. Optimizing instructions and demonstrations for multi-stage language model programs, 2024. URL https://arxiv.org/abs/2406.11695 .5Preprint. Under review Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, and Tomas Pfister. Reasoningbank: Scaling agent self-evolving with reasoning memory, 2025. URL https://arxiv.org/abs/2509.25140 .Anton Podkopaev, Ori Lahav, and Viktor Vafeiadis. Bridging the gap between programming lan-guages and hardware weak memory models. Proceedings of the ACM on Programming Lan-guages , 3(POPL):1–31, 2019. Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine learning algorithms, 2012. URL https://arxiv.org/abs/1206.2944 .Laetitia Teodorescu, Guillaume Baudart, Emilio Jes´ us Gallego Arias, and Marc Lelarge. Nlir: Natu-ral language intermediate representation for mechanized theorem proving. In MathAI@NeuRIPS 2024 - 4th Workshop on Mathematical Reasoning and AI , Vancouver, Canada, December 2024. URL https://hal.science/hal-04886208 .Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian guang Lou, and Shuai Ma. Re-reading improves reasoning in large language models, 2024. URL https: //arxiv.org/abs/2309.06275 .Yichen Xu and Martin Odersky. Agentic proof automation: A case study, 2026. URL https: //arxiv.org/abs/2601.03768 .Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems , 36:21573– 21612, 2023. Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. Finding and understanding bugs in c com-pilers. In Proceedings of the 32nd ACM SIGPLAN conference on Programming language design and implementation , pp. 283–294, 2011. doi: https://doi.org/10.1145/1993498.1993532. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv. org/abs/2210.03629 .Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kama-nuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, and Kunle Olukotun. Agentic context engineering: Evolving contexts for self-improving language models, 2025. URL https://arxiv.org/abs/2510.04618 .6