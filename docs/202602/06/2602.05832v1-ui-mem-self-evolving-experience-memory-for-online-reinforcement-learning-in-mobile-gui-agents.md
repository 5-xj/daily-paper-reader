# UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents
# UI-Mem：用于移动端 GUI 智能体在线强化学习的自进化经验记忆

**Authors**: Han Xiao, Guozhi Wang, Hao Wang, Shilong Liu, Yuxiang Chai, Yue Pan, Yufeng Zhou, Xiaoxin Chen, Yafei Wen, Hongsheng Li \\
**Date**: 2026-02-05 \\
**PDF**: https://arxiv.org/pdf/2602.05832v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EOH</span> \\
**Score**: 6.0 \\
**Evidence**: self-evolving experience memory for agent improvement \\

---

## Abstract
Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: https://ui-mem.github.io

## 摘要
在线强化学习（RL）

---

## 速览摘要（自动生成）

**问题**：移动GUI智能体在线强化学习面临长步长任务信用分配难、缺乏经验迁移导致重复犯错的问题。
**方法**：提出UI-Mem框架，构建包含工作流与失败模式的分层经验记忆，通过分层分组采样引导策略学习，并利用自进化循环持续更新记忆。
**结论**：显著提升了在线RL性能，在未知应用上具有极强的泛化能力。