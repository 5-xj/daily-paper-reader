Title: Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning

URL Source: https://arxiv.org/pdf/2602.01983v1

Published Time: Tue, 03 Feb 2026 03:22:02 GMT

Number of Pages: 17

Markdown Content:
# Li Auto | Base Model 

# Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning 

Xintian Shen âˆ— Jiawei Chen âˆ— Lihao Zheng âˆ—

Hao Ma â€  Tao Wei â€  Kun Zhan â€¡

Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLMâ€™s responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86% â†‘ and +23.04% â†‘ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent. 

{ Last Update Date: February 2, 2026 

# Correspondence: zhankun@lixiang.com 

## 1 Introduction 

In recent years, Large Language Models (LLMs) [ 23 , 1, 10 , 36 , 3 , 7 , 29 , 18 ] have achieved significant breakthroughs, demonstrating robust knowledge capabilities in tasks such as language understanding and complex reasoning [ 2 ]. To further enhance the practical utility of LLMs, existing research has primarily focused on incorporating external tools to transcend their inherent limitations. Traditional tool-augmented approaches [ 13 , 26 ] typically rely on predefined workflows to orchestrate tool invocation. However, such rigid paradigms struggle to generalize to open and uncertain environments. While multi-agent systems [ 34 , 25 , 15 , 14 ] enhance flexibility by employing a central model for 

> âˆ—

Co-first author 

> â€ 

Technique Leader 

> â€¡

Supervisor Preprint.                

> arXiv:2602.01983v1 [cs.AI] 2 Feb 2026 <think>â€¦ Therefore, 6/7 * 6.535 â‰ˆ
> 5.601, subtracting 34/7 â‰ˆ 4.857,
> the result is approximately 5.601
> -4.857 â‰ˆ 0.744 .</think>
> (a) COT
> (c) Ours
> Algebra Solver
> Geometry 2D Shapes
> Statistics Analysis
> Geometry Relations
> ...
> Retrieve Tools
> def _calculus():
> def _arithmetic():
> def _ equation_solver () :
> ...
> Algebra Solver
> Calculate âˆ«!
> "ð‘¥ ^1/6
> The tool now returns
> a result of 0.747 , so
> the final answer
> should be this value.
> Query: Find the area of
> the region bounded by the
> curves y =ð‘¥ ^1/6, y = 1,
> and x = 5. GT:0.747

QQ          

> Queries
> Tools
> Creator Self -Evolving
> Tool Library
> (b) Tool Only -be -Used
> Step1: Force Create Tool
> Step2: [Single -used Tool]
> ``` term = 5 ** (7/6)
> numerator = 6 * term -34
> area = numerator / 7
> print( f"Final Answer: {area:.6f}") ```
> Answer: 0.747

Figure 1: Comparison of tool-creating agents. (a) For this specific math problem, the standard Chain-of-Thought (CoT) [ 33 , 12 ] method fails and makes errors even during simple calculations. (b) Previous tool creation methods typically solve problems by generating code specific to the current instance. However, these tools are tailored solely to the immediate problem, making them non-reusable for other tasks and still prone to errors. (c) Ours. We propose a framework capable of reusing tool creation experience. During the inference process for task solving, UCT can utilize, create, and self-evolve existing tools. Furthermore, we design an offline memory consolidation module to generalize tool memory and transform it into reusable tool experience assets. planning and delegating sub-tasks to tool-using sub-agents, the deployment of multiple models incurs additional computational costs and introduces interaction latency. With the advancement of thought augmented models [ 33 , 10 ], Tool Integrated Reasoning (TIR) methods [ 21 , 4] exemplified by the ReAct [ 37 ] paradigm have emerged. The core philosophy of TIR involves enabling the model to explicitly generate reasoning traces during the inference process, autonomously invoke tools, and make iterative decisions based on feedback from the external environment. Consequently, TIR agents are capable of dynamically planning multi-step operations, which solves more end-to-end problems in open-world tasks. However, tools employed in existing TIR or tool-using frameworks typically manifest in two forms. The first relies on manual definition, which entails laborious tool construction efforts. Moreover, such hand-crafted tools inevitably fail to cover the exhaustive range of problem-solving requirements during reasoning [ 19 , 22 ]. The second approach involves generating ad-hoc code to address the immediate problem [ 8, 5]. However, these methods introduce significant uncertainty, as the generated code may be erroneous, and even when a valid tool is produced, the lack of persistence mechanisms restricts the agent to single-use scenarios. Although tool creation has emerged in agent research that allows for the creation of autonomous tools during reasoning [ 20 , 38 , 30 ], these methods are inherently limited. Figure 1 compares existing tool-creating agents. They tend to construct tools bespoke to specific tasks, rendering them single-use. This prevents the agent from internalizing these resource-intensive creations into a reusable library of experiential assets. To overcome the shortcomings of current agents, we introduce a self-evolving tool construction paradigm. Mimicking human problem-solving, our agent autonomously explores potential strategies when confronting complex tasks and encapsulates these experiences into persistent tools. By consolidating recurring sub-capabilities into a reusable library, the agent ensures their availability for future instances. This dynamic mechanism fosters continuous evolution during reasoning, effectively breaking through the rigid boundaries of existing frameworks. In this paper, we propose a self-evolving agent that transforms from a tool User to a Creator via 

Training-Free experience reuse (UCT). This framework enables the flexible and autonomous creation and execution of tools on demand, allowing the agent to absorb experience from reasoning and evolve accordingly. Built upon the ReAct paradigm, our architecture consists of three distinct modules: the Online Task Loop, the Online Build Loop, and Offline Memory Consolidation. The online task loop focuses on online problem-solving and triggers the online build loop whenever the agent requests a tool that does not yet exist. To ensure system stability, the tool creation process is constrained and incorporates rigorous testing and review mechanisms to guarantee the quality of the generated tools. To further crystallize reasoning experience, we introduce the memory consolidation module, 2which refines and comprehensively organizes tool memories retained during execution to facilitate iterative tool upgrades. To maintain the stability of the online tool library, we perform tool experience optimization as an offline process, separate from active inference tasks, by utilizing usage logs for prioritization. Our approach enables the collection of experience during reasoning, the evolution of that experience, and the iterative upgrade of agent capabilities, achieving performance improvements without additional training. To summarize, we make the following contributions: â€¢ We introduce a training-free framework UCT for reusing reasoning experience, facilitating the self-evolution of the agent during inference. By encapsulating effective experiences into tool assets, the framework offers robust guidance for future reasoning. â€¢ We establish an automated pipeline for high-quality tool library construction with minimal redundancy, which can be readily extended to diverse domains. We release TRBench, which includes 959 instances for evaluating tool-use reasoning tasks. â€¢ Extensive experiments demonstrate the superior performance of our method across multiple domains, including mathematical, scientific, and general VQA. Notably, our approach achieves state-of-the-art results on cross-domain tasks. 

## 2 Related Work 

2.1 TIR Agent 

TIR Agents have recently witnessed rapid advancements. By autonomously selecting and invoking tools, these models have significantly expanded the capability boundaries of Large Language Models (LLMs). As the premier user-facing TIR Agent, OpenAI o3 [ 17 ] has demonstrated robust capabilities, enabling functions such as image manipulation, code execution, and file system accessâ€”thereby prompting researchers to explore the immense research potential of TIR Agents. However, compared to the extensive human knowledge and reasoning capabilities possessed by current foundation models, the actionable abilities of most LLM Agents remain in a nascent stage. A series of existing works have drawn inspiration from the paradigm set by OpenAI o3, including code agents, search agents, deep-research agents, and agents designed for general-purpose TIR tasks. For instance, rstar2-agent [ 24 ] leverages code tools to enhance mathematical reasoning; deepeyes [ 40 ] introduces image manipulation tools (e.g., image zoom-in) to evaluate the ability of multimodal agents to resolve fine-grained understanding tasks when empowered by such tools. Furthermore, the Qwen DeepResearch [ 15 , 9, 28 ] team has addressed multi-dimensional challenges inherent in deep-research agents, making significant contributions to the open-source TIR agent ecosystem. Nevertheless, there remains significant room for improvement in existing TIR Agents regarding tool invocation, context management, and historical memory management. From the perspective of a tool creator, this paper proposes a training-free approach. We enable autonomous tool creation and memory management within the inference pipeline, realizing a unified agent framework that integrates reasoning, invocation, and memory. 

2.2 Tool Creation 

Recently, a series of studies have focused on the tool creation capabilities of agents, aiming to extend their reach to a more flexible spectrum of tools. For instance, CREATOR [ 20 ]and LIVE-SWE-AGENT [ 35 ] both leverage code generation to create tools, whereas CRAFT [ 38 ] custom-designs tools specifically for tasks during the inference phase. However, tools generated through these methods are typically ephemeral. Produced via a one-off process, they are not retained and thus cannot be internalized as experiential assets for the agent. With the growing exploration of agent self-evolution, Voyager [ 30 ] enables agents to accumulate code-based tools within embodied environments. Similarly, ToolACE-DEV [ 11 ] introduces an agent with self-evolutionary mechanisms tailored for operating systems. Nevertheless, these works primarily target embodied or gamified scenarios. There remains insufficient research on how Large Language Models (LLMs) can directly extend their inherent general question-answering and reasoning logic through tool invocation. Furthermore, many existing methods lack robust structural constraint mechanisms, which may lead to instability or even failure during the evolutionary process. 3Online Task Loop 

> Run
> Test

Online Build Loop 

> Build
> Ticket
> Created
> Tool
> Package
> Generation

ReAct Model 

Query 

> Radium -226 has a half -
> life of 1620 years. Find
> the time period during
> which a given amount of
> this material is reduced
> by one -quarter.

Answer            

> 672.4 Year
> Tool Retriever
> Step i
> success
> fail
> def radioactive_decay_time (half_life ,â€¦):
> ...
> half_life = 1620
> remaining_fraction = 0.75
> t=radioactive_decay_time (harf_life , ...)
> think
> Execute Tool
> Core Tool
> Create Tool

Tools Library  

> ReAct
> Model passed

... 

Offline Memory 

Consolidation  

> discard
> New
> Tools Analysis
> Critic
> Model
> Okay, let's see. The problem
> is about Radium -226 with a
> half -life of 1620 years. ...
> <tool_call >...</ tool_call >
> organize

Figure 2: The overall architecture of the proposed self-evolving agent framework. The system operates through three coupled phases: (1) The Online Task Loop governs the problem-solving process using the ReAct paradigm. At step t, the policy model Ï€Î¸ predicts the optimal action 

at+1 = arg max PÎ¸ (a | ht, o t, T ) based on the interaction history ht and current observation 

ot. The action space A dynamically integrates reasoning thoughts, tool execution ( Tcore âˆª T cre ), and tool creation requests. (2) The Online Build Loop is triggered by a creation ticket cticket to iteratively synthesize new tool code. This isolated refinement process is formalized as C(k) =Î¨build (C(kâˆ’1) , Rcritic , Rsandbox ), where the generator optimizes the code C(k) by fusing feedback from the critic model ( Rcritic ) and the sandbox execution environment ( Rsandbox ). (3) The Offline Memory Consolidation module asynchronously evolves the tool library by merging, classifying, and pruning tool assets to ensure long-term scalability and retrieval efficiency. 

## 3 Methodology 

In this section, we detail our self-evolving agent, which transforms from a tool user to a creator via Training-Free experience reuse. In Figure 2, the framework of our system consists of three key modules: the Online Task Loop, the Online Tool Creation Loop, and Offline Memory Consolidation. The online task loop handles real-time problem-solving by planning reasoning paths and determining the next action. When a tool is required, the system triggers the retrieval mechanism to search both the core and self-created toolsets; a retrieval failure prompts a tool creation request. The online tool creation loop then processes these requests to generate new tools. Crucially, this module incorporates testing and verification procedures to ensure the quality and usability of any newly generated tool. Meanwhile, the offline memory consolidation module refines and organizes tools stored in the library, facilitating the iterative upgrade of the tool construction process. The collaborative computation across these three modules ensures secure tool generation and execution while integrating tool memory, ultimately driving the autonomous evolution of the intelligent agent. 

3.1 Online Task Loop 

The online task loop within our system adopts the ReAct paradigm. The backbone is a multimodal policy model parameterized by Î¸, which generates thoughts for complex problems and performs interleaved reasoning to autonomously select and invoke tools. The process for the t-th step in the task loop is formulated as follows: 

at+1 = arg max 

> aâˆˆA

PÎ¸ (a | ht, o t, Tcre âˆª T core ) , (1) where at+1 denotes the decision (action) predicted by the model for the next step, ht represents the interaction history, and ot denotes the tool execution result (observation) returned from the 4Algorithm 1 UCT Online Task Loop Workflow 

Require: User Query 

Ensure: Final Answer  

> 1:

Initialize System Prompt  

> 2:

loop  

> 3:

Decision â† ReActModel (M essages ) 

> 4:

if Decision is Answer then  

> 5:

return Final Answer  

> 6:

break loop  

> 7:

else if Decision is Tool Call then  

> 8:

Identify Tool Source  

> 9:

if Source is Core Toolset then  

> 10:

Execute Core Tool  

> 11:

else if Source is Built Tools then  

> 12:

Retrieve and Execute Existing Tool  

> 13:

Record Execution Log  

> 14:

else  

> 15:

Generate in Build Loop  

> 16:

Register to Built Tools  

> 17:

Execute New Tool  

> 18:

end if  

> 19:

OBS â† Get Execution Result (Observation)  

> 20:

M essages â† M essages + OBS  

> 21:

end if  

> 22:

end loop 

environment in the current round. We define the action space as A = Athought âˆª (Tcore âˆª T cre )

| {z }

> Atool

âˆªA create ,where Atool comprises two categories: Core Tools ( Tcore ) and Create d Tools ( Tcre ). We impose a maximum limit of n rounds to derive the final answer. Core Tools are grounded in the native capabilities of the base model, serving as the foundation of the tool library. In contrast, Created Tools are autonomously constructed by the agent during reasoning tasks. This process consolidates reasoning experience into reusable assets that are iteratively updated, thereby achieving the evolution of the modelâ€™s capabilities. When the agent selects the action to create a tool, the system generates a build ticket to validate the construction requirement, subsequently triggering the Build Loop to produce the memory tool. Upon completing the multi-round iterations, the model yields a final answer enclosed within answer tags. 

3.2 Online Build Loop 

When the model within the Task Loop generates a build ticket, the system transitions into the Build Loop for tool creation. The Build Loop operates as a distinct workflow, fully isolated from the original task. This isolation serves two primary purposes: it prevents the extensive context generated during the creation process from interfering with the main task, and it enhances the controllability of the automated tool construction process within this production environment. We have established a standardized tool interface protocol. The created build ticket encapsulates a refined summary of the task context and the specific requirements of the sub-problem to be solved. Within the Build Loop, we continue to employ the main model based on the ReAct paradigm. Upon receiving the build ticket, the model generates both the executable tool code and the corresponding test script in a single pass. Furthermore, we establish a sandbox environment to execute these tests. The immediate runtime results, along with the generated code, are submitted to a specialized code model for critique and review suggestions. In this loop, we iterate to produce a preliminary usable tool, which must satisfy the dual verification of runtime testing and the critic modelâ€™s review. If the tool fails this review, the critique results, execution outcomes, and the current tool code are fed back into the ReAct model for regeneration: 5Algorithm 2 UCT Online Build Loop Workflow 

Require: Build Ticket 

Ensure: High-Quality Code (Passed Review)  

> 1:

Initialize M essages â† { Build Ticket } 

> 2:

Code â† ReActModel (M essages ) 

> 3:

loop  

> 4:

T estResult â† RunTests (Code ) 

> 5:

if T estResult is Success then  

> 6:

ReviewResult â† (Code, T estResult ) 

> 7:

if ReviewResult is Approved then  

> 8:

return Code  

> 9:

break loop  

> 10:

else  

> 11:

F eedback â† ReviewResult  

> 12:

end if  

> 13:

else  

> 14:

Activate Critic Model  

> 15:

Critique â† CriticModel (Code, T estResult ) 

> 16:

F eedback â† Critique  

> 17:

end if  

> 18:

M essages â† M essages + F eedback  

> 19:

Code â† ReActModel (M essages ) {Refine Code}  

> 20:

end loop 

C(k) = Î¨ build 



C(kâˆ’1) , Rcritic , Rsandbox | cticket 



, (2) where C(k) denotes the tool code generated in the current iteration, while Rcritic and Rsandbox represent the critique from the code model and the runtime execution results, respectively. Specifically, the critique includes a score for the current tool along with suggestions for revision. The observation within the Build Loop is a composite of â€œexecution feedbackâ€ and â€œcode review suggestions.â€ Based on this observation, the model iteratively fixes bugs, refactors code, and addresses boundary conditions until the tool meets the acceptance criteria for registration. The registration process yields a structured Tool Package, which encapsulates the tool code, invocation instructions, environment dependencies, and test results. Subsequently, the system reverts to the Task Loop, where the model utilizes the newly created tool to resolve the current problem and proceeds to complete the overall task workflow. 

3.3 Offline Memory Consolidation 

The unconstrained expansion of the tool library inevitably introduces redundancy into the memory of the LLM, complicating retrieval and degrading performance. While immediate integration of new tools could address this, performing operations such as deduplication and conflict resolution within the online Build Loop incurs unacceptable computational latency and potential instability. To reconcile the need for efficient task completion with the necessity of memory maintenance, we relegate the evolution of the toolset to an independent offline phase. This evolutionary process is formalized as a state update equation: 

Mt+1 = Î¦ offline (Mt âˆª T gen | L ) (3) where Mt represents the existing tool memory, and Tgen denotes the set of raw tools generated during the online inference phase. The evolution function Î¦offline executes a series of optimization operations conditioned on usage logs and tool descriptions, denoted by L. Specifically, Î¦offline performs two key tasks: (1) Organize , where tools of similar types are categorized and merged while duplicates are eliminated; and (2) Analysis & Discard , where rarely used or high-failure-rate tools are deprecated. This offline mechanism ensures that Mt+1 retains only high-utility experiences, thereby reducing retrieval complexity for future tasks without impacting online inference speed. 64 TRBench: Tool-Reasoning Benchmark 

With the rapid advancement of model capabilities in recent years, mainstream benchmarks have been continuously evolving. However, existing evaluation datasets are not primarily constructed to assess tool creation and usage. They contain a significant number of simple instances that do not require tool invocation, as well as knowledge-based questions irrelevant to computation or tool use. To verify tool usability, following the previous methodology [ 38 ], we constructed a standard tool reasoning benchmark by repurposing the test sets of existing authoritative benchmarks. For mathematical and scientific reasoning tasks, we excluded problems involving proofs or pure reasoning that are not directly computable. Regarding difficulty, we retained only challenging instances that necessitate tool-assisted solutions, covering cases of medium to high complexity. The specific procedure is as follows: 1. We employed a model to filter out all questions that can be answered solely using the modelâ€™s internal knowledge, resulting in a filtered candidate set C.2. To prevent the homogenization of the toolset, we adopted an iterative Min-Max sampling strategy. Initially, n questions were randomly sampled from C to form the initial set Q0,with the remaining questions serving as the candidate pool C0 = C \ Q0.3. We iteratively calculated the cosine similarity between questions in the candidate set and the current selected set to ensure diversity. In each iteration t, we select the instance xâˆ— that minimizes the maximum similarity to any instance in the current set Qt:

xâˆ— = argmin 

> xâˆˆC t



max 

> qâˆˆQt

CosSim (ex, eq )



(4) where ex and eq represent the embedding vectors of the questions. We then update Qt+1 =

Qt âˆª { xâˆ—} and Ct+1 = Ct \ { xâˆ—}. The number of iterations was set to 5 for mathematical and scientific reasoning tasks, and 10 for VQA tasks. 4. Finally, we categorized all collected problem sets by task type. The specific distribution of the data is illustrated in Figure 4. Specifically, for scientific and mathematical reasoning tasks, we filtered out problems involving proofs or pure reasoning that are not amenable to direct computation. We exclusively retained challenging instances that necessitate tool-assisted solutions, covering both medium and high difficulty levels. This resulted in a final set of 959 samples. This curation ensures a balanced difficulty distribution while enabling a more rigorous comparison of tool capabilities. 

## 5 Experiment 

5.1 Experimental Settings Datasets. To validate the effectiveness of UCT, we selected tasks from diverse domains for evaluation, including Visual Question Answering (VQA), mathematical reasoning, and scientific problems. Specifically, for mathematical reasoning, we employed four mainstream benchmarks: DynaMath [ 41 ], MathVerse [ 39 ], MathVista [ 16 ], and MathVision [ 31 ]. For scientific benchmarks, we employ reasoning related qa-pairs in Scibench [ 32 ] and Scieval [ 27 ]. SimpleVQA [ 6] is used to measure the general VQA ability of the agent. To better evaluate tool usage, we constructed the cross-domains Tool-Reasoning Benchmark using the above datasets. 

Implementation Details. With the iterative development of models, capabilities in coding, reasoning, and planning have gradually enhanced. Leveraging these advancements, we employ Qwen3-VL-235B-Thinking as our base model. Following standard model configurations, we set the sampling temperature to 1. All experiments are conducted on 8 NVIDIA H20 GPUs. 

Evaluation Metrics. To evaluate the efficacy of our constructed toolset and the system as a whole, two metrics are selected. The Correctness metric assesses the validity of the final answer, utilizing Qwen3-VL-235B-Instruct as the judge. We allow a numerical tolerance of 10 âˆ’6 for floating-point answers. For questions requiring multiple numerical outputs, strict correctness across all values is enforced. 7Figure 3: Data distribution of TRBenchmark. TRBench is a multimodal tool-use reasoning benchmark spanning Mathematics, Science, and General Question Answering. It comprises 959 challenging tool reasoning problems organized into 11 sub-categories across 3 major domains. Table 1: Comparisons across three sub-datasets of TRBench. Best results are in bold. Cate. Method Science General Bio. Chem. Phys. Avg. Cult. Reason. Recog. Avg.                                                                                                                    

> Basic-COT Gemini-2.5-pro 84.00 43.93 37.96 45.42 46.00 70.83 66.67 63.27 Qwen3-VL-235B-thinking 84.00 75.70 80.56 78.75 32.00 58.33 50.34 48.16 Vanilla 48.00 29.91 26.85 30.42 38.00 62.50 54.42 52.65 CREATOR [20] 76.00 76.64 80.56 78.34 28.00 77.08 45.58 48.16 CRAFT [38] 80.00 71.96 81.48 77.08 36.00 83.33 48.98 48.98 Gemini-2.5-pro + UCT 88.00 71.03 85.19 79.17 40.00 79.17 68.03 64.49 Tool-based Qwen3-VL-235B-thinking + UCT 88.00 83.18 90.74 87.08 46.00 87.50 65.31 65.71
> Cate. Method Mathematics Overall Alg. Disc. Geo. Prob. Stats. Avg.
> Basic-COT Gemini-2.5-pro 53.76 53.42 42.52 63.16 82.86 50.21 52.35 Qwen3-VL-235B-thinking 40.86 45.21 64.96 73.68 57.14 56.96 60.17 Vanilla 58.06 72.60 54.33 57.89 40.00 56.96 49.22 CREATOR [20] 79.57 75.34 65.35 89.47 82.86 71.94 67.47 CRAFT [38] 74.19 76.71 68.9 94.74 85.71 73.42 69.13 Gemini-2.5-pro + UCT 80.65 64.38 72.05 100.00 91.43 75.11 73.41( +20.86 â†‘)Tool-based Qwen3-VL-235B-thinking + UCT 93.55 91.78 87.40 94.74 97.14 90.30 83.21 (+23.04 â†‘)

5.2 Effectiveness and Superiority of UCT 

To validate the effectiveness of our UCT method, we categorize our comparative analysis into the following four dimensions: 1). Baseline (Basic-CoT): We utilize Large Language Models (LLMs) employing only basic Chain-of-Thought (CoT) without external tools as our baseline. In this setup, we rely solely on the LLMâ€™s CoT reasoning capabilities to solve problems, where the model generates answers directly after reasoning. 2). Vanilla Tool Version: We introduce a vanilla tool-augmented version that incorporates only a code interpreter. This setup involves creating tools that lack test verification, are applicable only to the current turn, and possess no memory retention. 3). Existing Tool-Creation Methods: We compare our approach against established methods for tool-creating agents, including CREATER [ 20 ] and CRAFT [ 38 ]. Since the base model significantly impacts the agentâ€™s foundational knowledge, we upgraded all base models to Qwen3-VL-235B-thinking to ensure a fair comparison. 4). Our Method: Our approach distinguishes itself by not requiring ground truth data during the tool creation phase. Instead, we leverage extensive data from the 8Algebra 

> Chemistry
> Geometry_2D
> Geometry_3D
> Trig
> Relations
> Sta.

Figure 4: The tool library generated by UCT. The library comprises 7 major categories, 64 sub-categories, and 207 specific computational tools. The pie chart illustrates the distribution of these specific tools relative to the total collection, highlighting the richness and hierarchical organization of our generated toolset. Table 2: Ablation study of our approach with different components Math Bench Sci Bench General Avg. Baseline 56.96 78.75 48.16 60.17 + BL w/o CM 73.42 77.08 53.06 69.13 + BL 83.97 80.00 61.63 77.27 ++ MC 90.30 87.08 65.71 83.21 

Internet for the initial creation of tools. We have developed tools across seven major categories, including algebraic calculation, geometric operations, and statistical analysis, which also encompass 64 sub-categories of extension package functionalities. By equipping the system to propose tickets and create tools dynamically during the reasoning process, the observed performance improvements effectively validate our systemâ€™s capability for self-evolution. As shown in Table 1, we compare the performance of our method against Basic-CoT, the vanilla tool, and other existing state-of-the-art (SOTA) tool creation methods on TRBench. Observing the experimental results, we can summarize the following conclusions: 1) First, by comparing the Vanilla tool with Basic-CoT, we observe that directly employing a code interpreter for tool creation does not yield significant performance gains. This suggests that the relevance of the generated tools critically impacts the underlying the performance of LLM. 2) Second, our method achieves substantial improvements over basic-CoT across all tested models. This validates that the tool library constructed by our approach effectively realizes self-evolution during the reasoning process. Compared to the baseline, our methods based on Qwen3-VL-235B-thinking and Gemini2.5-pro achieve improvements of +20.86% â†‘ and +23.04% â†‘, respectively, demonstrating substantial and significant gains. In Figure 5, we also show the tool call rounds and accuracy in UCT. Our task success rate remains high even as the number of tool invocation rounds increases. 3) Third, compared to other tool generation baselines, our approach achieves state-of-the-art (SOTA) performance across all metrics on TRBench. 

5.3 Ablation Studies Effectiveness of the Framework Components. We further conduct experiments to verify the effectiveness of components in UCT. Note that modules in UCT have dependencies, we can only show gradually added ablation studies. As the results in Table 2, our full framework obtains the highest performance on all metrics when the online build loop, critic module, and offline memory consolidation work together. Table 2 also illustrates the necessity of every phase. 

Effectiveness of Created Tools. To validate the effectiveness of our generated tools, we statistically analyzed the frequency of their correct usage within the dataset. A higher tool utilization rate (the ratio of utilized tools to the total toolset) indicates lower redundancy, demonstrating that the tools are designed for system-level utility rather than being tailored to task-specific purposes. We also evaluated the overall correct usage rate in Table 3. In the table, reuse@k denotes the proportion of tools in the entire tool set that are used at least k times in the test set. Notably, 93.1% of the tools 9512               

> 351
> 61 17 18
> 632
> 303
> 19 41
> 58.4%
> 91.7%
> 85.2% 88.2% 88.9%
> 79.4%
> 89.1%
> 94.7%
> 100.0% 100.0%
> 01234+
> 0
> 100
> 200
> 300
> 400
> 500
> 600
> 0%
> 20%
> 40%
> 60%
> 80%
> 100%
> Ours(Gemini-2.5-Pro) Ours(Gemini-2.5-Pro) Acc Ours(Qwen3-VL-235B-A22B-Thinking) Ours(Qwen3-VL-235B-A22B-Thinking) Acc
> Tool-call Rounds
> Case Count
> Accuracy
> Ours(Gemini-2.5-Pro): 73.4%
> Ours(Qwen3-VL-235B-A22B-Thinking): 82.9%

Figure 5: Tool call rounds and accuracy of UCT. 

Figure 6: Model performance on the mathematical subset through tool creation and memory consoli-dation Table 3: Reuse rate of created tools in UCT. Reuse@1 Reuse@5 Reuse@10 Tools Library 93.1 86.0 77.1 are used at least once, which reveals the high-quality of the tool library. Furthermore, as illustrated in Figure 4, we visualized the categories and descriptions of the generated tools to demonstrate the diversity and richness of our toolset. 

Explanation of Model Evolution. We utilized a massive-scale dataset of multimodal reasoning QA pairs for toolset creation. As the volume of reasoning queries increased, the tool library was progressively refined via a memory consolidation module. We conducted an experimental analysis on the Math subset using snapshots of the tool library generated at different reasoning milestones. Figure 6 illustrates the system performance across different steps. The red line denotes UCT with Qwen3-VL-235B-thinking, and the blue line denotes UCT with Gemini-2.5-pro. The upward trend of the curve indicates that our framework undergoes continuous self-evolution during the reasoning process. However, due to the finite variety of problem types within the dataset, the signs of evolution tend to plateau after a certain number of iterations. Nevertheless, this evolvability suggests that when applied to a broader spectrum of multimodal reasoning tasks, the system possesses the potential to construct an even more comprehensive toolset. 10 6 Conclusion 

In this work, we introduce a novel training-free framework that transitions agents from the role of tool users to that of tool creators, thereby realizing the self-evolving of reasoning agents during inference and retaining tool memory as a reusable asset. Our methodology integrates three core modules: an online task main loop, an online tool creation loop, and an offline memory consolidation module. This architecture achieves autonomous path planning and action execution, creates tools on demand during inference to continuously enrich the tool library, and utilizes the offline memory consolidation module to iteratively upgrade constructed tools for enhanced quality and usability. Experiments conducted on extensive datasets across diverse domains validate the effectiveness and generalization of our framework. Moreover, this paradigm shift grants tools the ability to evolve continuously and paves the way for autonomous agents to tackle increasingly complex problems in open-world environments. 

## Acknowledgments and Disclosure of Funding 

We thank all colleagues at LiAuto Base Model for their support of the MindWatcher Team. 11 A Appendix 

A.1 Tool Descriptions for UCT Core Library 

This paper primarily investigates the effectiveness of dynamically constructing a tool library by leveraging reasoning experience. We provide a detailed description of the core tool library here. Specifically, this core library comprises five categories of multimodal image and text tools. Given their prevalent usage in existing agent-based research, we have incorporated them as foundational components of our core tool library. 

Tool: Region Croping/Zooming 

Description: Zoom into a specific area of **the first input image** based on your provided bounding box. 

input: the image and bounding box. 

output: a new image. 

Arguments: 

â€¢ bbox : [x1, y1, x2, y2], # The bounding box you provided, where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner. 

Tool: Object Grounding & Visual Search 

Description: Retrieve new similar images and their descriptions based on the provided bounding box area of **the first input image**. 

input: image and bounding box. 

output: only the most similar targetâ€™s type name and the confidence score. 

Arguments: 

â€¢ bbox_2d: [x1, y1, x2, y2], # The bounding box you provided, where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner. â€¢ category: "the category" # The category of the image you want to search for, which can only be one of {plant, animal, car, person, landmark, vegetable, cuisine, logo}. 

Tool: External Text Retrieval 

Description: Retrieve external text information from the internet based on your provided text query. 

input: only text query. 

output: text. 

Arguments: 

â€¢ query: "the content". 

Tool: Webpage Content Retrieval 

Description: Visit a specified web page URL under 3 modes: you can read its full content, the content within the window you provided, or use an AI assistant to generate a structured summary based on the specific goal you provide. 

input: A JSON object containing three arguments: url, window, goal. 

output: A JSON object containing the visited URL and a structured result. 

Arguments: 

â€¢ url: "https://example.com/article", # The webpage url you want to visit. â€¢ window: [a, b], # Select the content you want to read within [a, b] (Optional) â€¢ goal: "". # What you want to get or find. (Optional) 

12 A.2 Prompt Design 

In this section, we display the prompts utilized by policy model and online build loop. 

Prompt: Policy Model 

You are a ReAct paradigm AI assistant capable of solving problems through reasoning and tool execution. You can interact using the following format: 

1. Thought Process: 

<think> Your thought content </think> 

2. Tool Call: Tool Call Guidelines: 

â€¢ wrapper: The content must be wrapped in <tool_call> tags. â€¢ name: The unique, descriptive name of the tool. â€¢ arguments: A dictionary containing the input parameters. Example: <tool_call> {{ "name": "tool_name", "arguments": {{ "arg": value, "arg2": value2 }} }} </tool_call> You may call these tools as needed. A list of currently available tools will be provided at the end of the user message for your reference. {core_tool_instruction} 

3. Final Answer: 

<answer> Your final answer </answer> 

Rules: 

â€¢ After every output of <tool_call>, you must wait for the tool to return results. â€¢ End the conversation immediately after outputting <answer>. â€¢ You may engage in multiple rounds of thinking and tool calling. â€¢ DO NOT call multiple tools consecutively in a single response; you must proceed step-by-step. â€¢ You must provide the answer at the end. 

IMPORTANT: Handling Tool Failures 

â€¢ If a tool returns empty or fails to execute, do not completely negate your previous reasoning .â€¢ If you have already derived an answer through logical reasoning in <think>, but the tool cannot verify it or returns empty, you need to re-input parameters that match the format based on the feedback. â€¢ Correct actions when a tool fails: 

â€“ First, check if the tool call parameters are incorrect; try adjusting the parameters and calling again. 

â€“ If the tool continues to fail after multiple attempts, fall back to simulating the calculation yourself. 

â€“ Retain the answer you derived through rigorous logical reasoning in <think>; do not change it arbitrarily just because the tool failed. 

â€“ Directly output the answer you reasoned previously. You may state, "Tool verification failed, but based on logical reasoning, the answer is..." 

13 Prompt: Principal Software Engineer 

Role Definition 

You are a Principal Software Engineer. You are not a chat assistant; you are a high-precision autonomous coding engine. Your goal is to design, implement, and debug complex software systems with expert-level proficiency across 100+ programming languages. 

Operational Directives 

â€¢ Expertise Activation: Leverage your Mixture-of-Experts architecture to utilize specialized sub-networks for specific languages (e.g., Rust memory safety, Python async patterns). Always adhere to the latest stable language standards (e.g., ES2024, Python 3.12). â€¢ Reasoning First: You must utilize the <think> tag to perform deep reasoning before generating any code. Code without preceding reasoning is strictly prohibited. â€¢ No Filler: Do not use conversational filler ("Certainly", "I can help with that"). Be terse, technical, and objective. â€¢ Full Implementation: Never use placeholders like //... rest of code or pass. You must generate complete, functional, and production-ready implementations. 

The Thinking Protocol 

Inside the <think> block, you must strictly follow this cognitive process: â€¢ Requirement Analysis: Deconstruct the userâ€™s request into atomic technical requirements. â€¢ Execution Plan: Step-by-step plan for the code generation artifacts. 

The Artifact Protocol (Claude-Style) 

When generating code, configuration files, or substantial documentation, you must encapsulate the content within an <artifact> XML block. 

XML Schema: 

<artifact identifier="unique-id" type="mime-type" path="file-path" ac-tion="create|update"> [Content goes here] </artifact> 

Attributes Guidelines: 

â€¢ identifier : A unique, descriptive ID. â€¢ type : The standard MIME type (e.g., text/x-python ). â€¢ path : The relative file path. 

14 References 

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. [3] Shuai Bai, Yuxuan Cai, Ruizhe Chen, et al. Qwen3-vl technical report, 2025. URL https: //arxiv.org/abs/2511.21631 .[4] Jiawei Chen, Xintian Shen, Lihao Zheng, Zhenwei Shao, Hongyuan Zhang, Pengfei Yu, Xudong Rao, Ning Mao, Xiaobo Liu, Lian Wen, et al. Mindwatcher: Toward smarter multimodal tool-integrated reasoning. arXiv preprint arXiv:2512.23412 , 2025. [5] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 , 2022. [6] Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 4637â€“4646, 2025. [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 , 2025. [8] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning , pages 10764â€“10799. PMLR, 2023. [9] Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, et al. Webwatcher: Breaking new frontier of vision-language deep research agent. arXiv preprint arXiv:2508.05748 , 2025. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. [11] Xu Huang, Weiwen Liu, Xingshan Zeng, Yuefeng Huang, Xinlong Hao, Yuxian Wang, Yirong Zeng, Chuhan Wu, Yasheng Wang, Ruiming Tang, et al. Toolace-dev: Self-improving tool learning via decomposition and evolution. arXiv preprint arXiv:2505.07512 , 2025. [12] Yue Jiang, Jiawei Chen, Dingkang Yang, Mingcheng Li, Shunli Wang, Tong Wu, Ke Li, and Lihua Zhang. Comt: Chain-of-medical-thought reduces hallucination in medical report generation. In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 1â€“5, 2025. doi: 10.1109/ICASSP49660.2025.10887699. [13] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024 , 2022. [14] Mingcheng Li, Xiaolu Hou, Ziyang Liu, Dingkang Yang, Ziyun Qian, Jiawei Chen, Jinjie Wei, Yue Jiang, Qingyao Xu, and Lihua Zhang. Mccd: Multi-agent collaboration-based compositional diffusion for complex text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 13263â€“13272, 2025. [15] Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, et al. Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312 , 2025. 15 [16] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 , 2023. [17] OpenAI. Introducing openai o3 and o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/ , April 2025. Accessed: 2025-12-19. [18] MindGPT ov Team. Mindgpt-4ov: An enhanced mllm via a multi-stage post-training paradigm. 

arXiv preprint arXiv:2512.02895 , 2025. [19] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems , 37: 126544â€“126565, 2024. [20] Cheng Qian, Chi Han, Yi R Wei, Dahua Lin, and Zhiyuan Liu. Creator: Disentangling abstract and concrete reasonings of large language models through tool creation. arXiv preprint arXiv:2305.14318 , 2023. [21] Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, et al. Webresearcher: Unleashing unbounded reasoning capability in long-horizon agents. arXiv preprint arXiv:2509.13309 , 2025. [22] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 , 2023. [23] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training, 2018. [24] Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, et al. rstar2-agent: Agentic reasoning technical report. arXiv preprint arXiv:2508.20722 , 2025. [25] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems , 36:38154â€“38180, 2023. [26] Yuchen Shi, Siqi Cai, Zihan Xu, Yuei Qin, Gang Li, Hang Shao, Jiawei Chen, Deqing Yang, Ke Li, and Xing Sun. Flowagent: Achieving compliance and flexibility for workflow agents. 

arXiv preprint arXiv:2502.14345 , 2025. [27] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. Scieval: A multi-level large language model evaluation benchmark for scientific research. In 

Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 19053â€“19061, 2024. [28] Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701 , 2025. [29] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-thÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. [30] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. 

arXiv preprint arXiv:2305.16291 , 2023. [31] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. 

Advances in Neural Information Processing Systems , 37:95095â€“95169, 2024. 16 [32] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635 ,2023. [33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. 

Advances in neural information processing systems , 35:24824â€“24837, 2022. [34] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling , 2024. [35] Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, and Lingming Zhang. Live-swe-agent: Can software engineering agents self-evolve on the fly? arXiv preprint arXiv:2511.13646 , 2025. [36] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115 , 2024. [37] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations , 2022. [38] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R Fung, Hao Peng, and Heng Ji. Craft: Customiz-ing llms by creating and retrieving from specialized toolsets. arXiv preprint arXiv:2309.17428 ,2023. [39] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision , pages 169â€“186. Springer, 2024. [40] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing" thinking with images" via reinforcement learning. arXiv preprint arXiv:2505.14362 , 2025. [41] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: A dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836 , 2024. 17