Title: Self-Consolidation for Self-Evolving Agents

URL Source: https://arxiv.org/pdf/2602.01966v1

Published Time: Tue, 03 Feb 2026 03:20:00 GMT

Number of Pages: 15

Markdown Content:
# Self-Consolidation for Self-Evolving Agents 

Hongzhuo Yu 1*, Fei Zhu 2*, Guo-Sen Xie 3, Ling Shao 1

> 1

UCAS-Terminus AI Lab, University of Chinese Academy of Sciences 

> 2

Centre for Artificial Intelligence and Robotics, HKISI-CAS 

> 3

School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China 

> yuhongzhuo24@mails.ucas.ac.cn, zhfei2018@gmail.com, gsxiehm@gmail.com, ling.shao@ieee.org

Abstract 

While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static sys-tems, lacking the ability to evolve through life-long interaction. Existing attempts to bridge this gap primarily rely on retrieving success-ful past trajectories as demonstrations. How-ever, this paradigm faces two critical limita-tions. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual expe-riences not only increases the time consump-tion during retrieval but also inevitably intro-duces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical expe-rience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution. 

1 Introduction 

Recent years have witnessed the rapid development of large language models (LLMs) (Team et al., 2023; Yang et al., 2025a; Liu et al., 2024a; Touvron et al., 2023), enabling AI agents to tackle complex tasks ranging from code generation (Nijkamp et al., 2022; Zheng et al., 2023) to mathematical reason-ing (Lu et al., 2023b). Despite these successes, a fundamental bottleneck remains: most agents op-erate under a task-isolation paradigm (Deng et al.,   

> *These authors contributed equally to this work. Environment LLM Agent
> Observation
> Action
> Long-Term
> Parametric Memory
> Short-Term
> Non-Paramet ric Memory
> Consolidation

Figure 1: Illustration of the proposed self-consolidation framework for LLM agents lifelong evolution. 

2023; Yoran et al., 2024). They function as state-less entities that reset after every session, failing to accumulate knowledge or learn from experience in a human-like manner (Zheng et al., 2025a). To achieve evolutionary test-time learning, re-cent studies (Wang et al., 2024b; Zheng et al., 2025b) have explored explicit textual replay, where historical interactions are retrieved to guide fu-ture actions. However, current approaches suf-fer from two major limitations that hinder their practical scalability and effectiveness. First, most approaches focus exclusively on successful expe-riences (Zheng et al., 2025b; Yang et al., 2025b; Wang et al., 2024b), thereby overlooking the in-formative value of failure cases. Agents’ faulty problem-solving processes often contain critical information for preventing repeated failures; how-ever, such information is rarely incorporated into learning mechanisms. Second, the fixed and lim-ited context window of LLMs (Jin et al., 2024) imposes a strict constrain on the amount of experi-ential information that can be incorporated at infer-ence time. As a result, only a small subset of past interactions can be retrieved or replayed, forcing agents to rely on truncated trajectories or heuristic experience selection strategies. This constraint not only leads to the loss of contextual dependencies across tasks but also weakens the agent’s ability to integrate cumulative knowledge over time, ulti-

> arXiv:2602.01966v1 [cs.LG] 2 Feb 2026

mately limiting robust reasoning and long-term per-formance. Besides, incorporating too many textual demonstrations might introduce contextual noise (Hsieh et al., 2024), where redundant information dilutes the model’s attention (Liu et al., 2024b) and impairs inference accuracy. To address these challenges, we introduce EvoSC, a self-consolidation framework designed to continuously internalize experience and knowl-edge from test-time interactions into the agent’s parameters, as illustrated in Figure 1. Specifically, EvoSC mimics the principle of human cognitive learning (Tamnes et al., 2013; Spens and Burgess, 2024) through two synergistic components. On the one hand, we design a contrastive experience extraction mechanism that prompts the LLM to an-alyze successful and failed trajectories, highlight-ing the subtle divergences in reasoning that lead to errors. Consequently, the agent extracts two types of explicit guidance: error-prone insights to avoid pitfalls and successful patterns to replicate correct logic. On the other hand, we propose a self-consolidation mechanism to enable rich, explicit knowledge contained in the textual trajectories to be compressed into compact, learnable parameters. This effectively converts long and potentially redun-dant interaction trajectories into implicit parametric memory, allowing the agent to utilize vast historical experience without occupying token slots. We summarize our contributions as follows: • We propose EvoSC, a model-agnostic, plug-and-play agent test-time learning frame-work that integrates hierarchical textual non-parametric and lightweight parametric experi-ence, providing a unified interface for agents to maintain both immediate context and con-solidated historical experience. • We introduce a dual-evolution mechanism. It mines valuable insights from both successful and failed trajectories through contrastive re-flection. Then, explicit past experiences are consolidated into learnable prompts to enable fast, intuition-like reasoning at test-time. • Extensive experiments demonstrate that EvoSC significantly outperforms static base-line agents and conventional experience replay methods, establishing a scalable and context-efficient paradigm for evolutionary test-time learning in LLM-based agents. 

2 Related Works 

2.1 Agent Lifelong Learning 

Evolutionary lifelong learning is a core capability driving the advancement of LLM-based agents, en-abling dynamic adaptation and self-improvement under complex, evolving environments with min-imal human intervention. Recently, numerous re-searchers have adopted diverse approaches to real-ize the transition from static model deployments to dynamic, adaptive agent systems. Wang et al. (2024a) proposed a four-module autonomous agent framework comprising profile, memory, planning, and action, while Xi et al. (2025) decomposed agents into brain, perception, and action but did not address long-term adaptation. Recently, AWM (Wang et al., 2024b) enhances the efficiency of web navigation agents by analyzing their past experi-ences and extracting reusable workflows therefrom. By simply storing historical interaction examples to build a memory repository, TER (Zheng et al., 2025b) enables agents to tackle highly relevant do-main tasks effectively. 

2.2 Memory for Agent 

With the rapid advancement of large language models and embodied agents, memory modules have emerged as a core component for bridging perception, decision-making, and long-term inter-actions. Recently, an increasing number of re-searchers have conducted in-depth studies on the memory mechanisms of agents. MemoryBank (Zhong et al., 2024) achieves enhanced develop-ment of agents for AI emotional companionship by mimicking human long-term memory. Agentic Memory (Xu et al., 2025) establishes dynamically interconnected memory networks through mem-ory note construction, association and evolution-ary iteration, thereby providing effective support for large language model agents. Self-Controlled Memory (Wang et al., 2025) devises a dedicated memory framework consisting of the agent, stream and controller modules, which is specialized for processing ultra-long input sequences. Moreover, some research approaches propose to enhance the knowledge capabilities of large models by incorpo-rating read and write memory modules (Hu et al., 2023; Modarressi et al., 2023; Lu et al., 2023a) and leveraging structured knowledge bases. Like-wise, several methods have explored cooperative memory frameworks for multi-agent systems (Li et al., 2023b,a), enabling the accomplishment of Llama 3.1-8B-Instruct Qwen 2.5-7B-Instruct       

> 60
> 70
> 80
> 90
> Performance (%)
> 74.2
> 70.2
> 68.4
> 74.8
> 77.8
> 76.0
> 74.0
> 75.8 74.6
> 76.8
> Llama 3.1-8B-Instruct Qwen 2.5-7B-Instruct
> AWM TER SCM A-MEM EvoSC (Ours)

(a) Llama 3.1-8B-Instruct Qwen 2.5-7B-Instruct        

> 40
> 50
> 60
> 70
> Performance (%)
> 51.0
> 46.8
> 49.0
> 51.4
> 53.0 52.6
> 58.2
> 52.0
> 57.2
> 59.4
> Llama 3.1-8B-Instruct Qwen 2.5-7B-Instruct
> AWM TER SCM A-MEM EvoSC (Ours)

(b) Llama 3.1-8B-Instruct Qwen 2.5-7B-Instruct          

> 0
> 10
> 20
> 30
> 40
> 50
> 60
> 70
> Performance (%)
> 32.8 35.3
> 31.1 31.8
> 47.7
> 17.7
> 34.1 33.3
> 18.4
> 45.2
> Llama 3.1-8B-Instruct Qwen 2.5-7B-Instruct
> AWM TER SCM A-MEM EvoSC (Ours)

(c) 

Figure 2: Performance comparison of different methods across the (a) Database (DB), (b) Operating System (OS), and (c) Knowledge Graph (KG) agent lifelong learning benchmarks using Llama 3.1-8B-Instruct (Dubey et al., 2024) and Qwen 2.5-7B-Instruct (Yang et al., 2024). EvoSC surpasses strong baselines such as AWM (Wang et al., 2024b), TER (Zheng et al., 2025b), SCM (Wang et al., 2025), and A-MEM (Xu et al., 2025). 

collaborative multi-agent tasks. In summary, while substantial progress has been achieved in agent memory mechanisms, existing methods often lack effective strategies for long-term memory consol-idation and knowledge reinforcement across ex-tended interaction histories. 

3 Problem Formulation 

Task Stream and Input. A LLM-based lifelong learning agent deals with a stream of tasks. We define a domain-specific dataset D = ⟨P sys , T ⟩ ,where Psys is the universal system prompt which encodes domain rules (e.g., SQL syntax or OS com-mands) shared across all tasks in the domain, and 

T = {t1, t 2, ..., t N } denotes a sequence of N task instances arriving sequentially. When the agent addresses task tk, the initial input context Ik is con-structed by concatenating the system prompt and the specific task description: Ik = Psys ⊕ tk.

Sequential Interaction. The agent interacts with the environment to solve tk within a fixed maxi-mum of r rounds. At each time step s (1 ≤ s ≤ r), the agent observes the interaction history Hk,s −1

and the task input Ik. The LLM-based agent func-tions as a policy πθ, generating action Ak,s ∈ A :

Ak,s ∼ πθ (Ak,s | H k,s −1, Ik) . (1) Upon executing Ak,s , the environment transitions to a new state based on transition function T and returns a feedback (observation) Fk,s ∈ Ω (e.g., execution logs or error messages). The interac-tion history is then updated to Hk,s = Hk,s −1 ∪{(Ak,s , F k,s )}, which serves as the context for the subsequent step. 

Lifelong Learning Objective. For each task 

tk, the agent generates a trajectory ξ(k) =(Ik, A k, 1, F k, 1, . . . , A k,T , F k,T ). The environment assigns a binary reward R(tk) ∈ { 0, 1} upon task completion, indicating success or failure. The ob-jective of lifelong learning is to maximize the cu-mulative expected reward across the sequence of all tasks, leveraging past experiences accumulated in history (Zheng et al., 2025b): 

max 

πθ

NX

k=1 

Eξ(k)∼πθ [R(tk)] . (2) In this work, we aim to enhance πθ dynamically by consolidating experiences extracted from prior trajectories {ξ(1) , . . . , ξ (k−1) } into non-parametric textual prompt or parametric parameters, thereby improving the success rate on future tasks. 

4 Method 

Overview. Figure 3 illustrates the proposed EvoSC framework, which orchestrates agent lifelong learning through two complementary mechanisms: non-parametric contrastive experi-ence extraction and parametric long-term self-consolidation. To enhance immediate reasoning capabilities, EvoSC first employs a non-parametric contrastive reflection mechanism. Instead of solely replaying successful demonstrations, the agent re-trieves and analyzes both successful and failed his-torical trajectories. By contrasting these outcomes, the agent explicitly extracts two types of textual guidance: error-prone insights that identify specific pitfalls to avoid and success patterns that highlight effective strategies to replicate. These insights are injected as textual prompts to guide the inference on the fly. To ensure long-term scalability and circumvent context window constraints, EvoSC employs parametric experience consolidation. At periodic intervals, the framework triggers a self-consolidation process that distills accumulated tra-jectories into learnable parameters. By converting verbose, explicit memories into compact paramet-ric intuition, EvoSC allows the agent to internalize Conditions such as pulmonary              

> heart disease have how many
> prevention factors?
> Agent reasoning process
> Experience
> injection Interaction
> Act
> Feedback
> ×N
> Reflection
> Guide
> Error-prone
> knowledge
> Experience repository Past trajectory
> Task M : Correct interaction
> Successful experience Error-prone knowledge
> - Specific question
> - Error analysis
> - Error avoidance
> - Specific question
> - Success insight
> - Solution pattern
> Reflection
> Success
> Failure
> Amplify differences through contrast Error-prone knowledge extraction Successful experience extraction
> Knowledge retrieve
> Succsessful
> experience
> Many trajectories
> Long term experience consolidation
> A few trajectories
> Input
> Correct answer Output
> Model backbone
> Textual &learnable experience enhanced
> Inference
> Question
> Trajectories
> Learnable prompt
> Model Output
> Question Question
> Input

Figure 3: Pipeline of EvoSC. The framework orchestrates a dual-memory system: (Left) Non-parametric contrastive extraction draws explicit error-prone and successful insights from past trajectories to guide immediate reasoning; (Right) Parametric trajectory consolidation internalizes extensive historical knowledge into compact learnable prompts via trajectory distillation, which effectively resolves the context-length explosion. This hybrid approach ensures scalable lifelong learning by leveraging both textual experience and parametric long-term knowledge. 

extensive interaction history, transforming raw ex-perience into intrinsic knowledge without the linear growth in computational overhead. 

4.1 Non-parametric Contrastive Extraction Error-Prone Experience Extraction. Drawing on principles of contrastive learning, we posit that the semantic difference between a correct and an in-correct solution contains the highest informational value. While successful interactions within a task category often exhibit correlated reasoning patterns, failed interactions tend to diverge at critical deci-sion points. Therefore, juxtaposing a failed trajec-tory ( Cf ) against a successful one ( Cs) allows the model to pinpoint the exact logical step where the reasoning is flawed. To this end, we design a set of contrastive prompt templates Pc that instruct the LLM to analyze the divergence between success and failure. The model extracts specific error-prone points and correspond-ing avoidance strategies, defined as: 

Exp c = LLM( Pc ∪ C s ∪ C f ), (3) where Cs and Cf represent the complete interaction dialogs of a successful and a failed task attempt, respectively. To maintain memory efficiency and relevance, these extracted insights are stored in a first-in-first-out (FIFO) queue, ensuring the agent prioritizes recent lessons while automatically prun-ing outdated error information. These insights then guide the agent in tackling subsequent tasks. In Figure 4, we provide the contrastive experience extraction prompt and an error-prone knowledge example for the KG benchmark. 

Successful Experience Extraction. While error analysis prevents the repetition of mistakes, identi-fying efficient solution paths is equally critical for capability improvement. To this end, we perform successful experience extraction to distill gener-alizable strategies from positive interactions. Un-like the contrastive module which focuses on diver-gence, this module focuses on abstraction. It aims to condense specific execution traces into high-level methodological summaries. When the agent completes tasks successfully, the interaction tra-jectories are collected. We utilize specific prompt templates Ps to guide the LLM in abstracting es-sential structural patterns and effective reasoning steps from these raw trajectories. The extraction of reusable successful experience (Exp s) is formulated as: 

Exp s = LLM( Ps ∪ C (i)

s ∪ C (j)

s ), (4) where C(i)

s and C(j)

s denote the complete interaction dialogs from two distinct successful task instances (ti, t j ). This multi-shot extraction encourages the model to find commonalities across different suc-cessful attempts, fostering robust generalization. Similar to the error module, these successful ex-periences are managed via a FIFO queue to adapt Contrastive Prompt design for knowledge graph   

> You are a knowledge graph expert, skilled at resolving various knowledge graph API calls. You will be presented with two
> interactive dialogues: one where an agent correctly helps a user solve a knowledge graph problem, and another where an agent
> incorrectly solves another problem.
> The following are successful interactions and incorrect interactions:
> ·Successful interactions: {success_text}
> ·Failed interactions: {failed_text}
> Your task is to identify potential causes of errors by comparing two problem-solving patterns in different interaction processes,
> and to extract successful experiences that enable you to effectively complete the task and avoid repeating the same mistakes.
> Error-prone knowledge example
> - Use get_neighbors() with the exact relation type that matches the knowledge graph structure.
> - Follow the traversal order: get_relations() -> get_neighbors() -> get_attributes() -> aggregation functions (count, argmax,
> argmin).
> - When counting entities, first get the relation entity using get_relations(), then get the entities connected through that relation
> using get_neighbors(), before applying count().

Figure 4: Contrastive prompt and knowledge example for KG dataset. 

to the agent’s evolving capabilities. The specific prompt templates and extracted knowledge exam-ples are provided in the Appendix. 

4.2 Parametric Trajectory Consolidation 

To mitigate the computational and storage over-head introduced by excessive textual experience, which may lead to prompt explosion and exceed the LLM’s fixed context window, we propose para-metric trajectory consolidation that internalizes the knowledge embedded in many long-term multi-round interaction trajectories into compact, learn-able prompt tokens Pθ, enabling the agent to trans-form explicit step-by-step deliberation into a com-pact, parametric memory. Denote E = Csucc i Ki=1 the set of historical in-teraction trajectories. Given a task input tk with specific task description ( Ik = Psys ⊕ tk), let 

Hk = {(Ak,s , F k,s )}rs=1 be a successful trajectory for task tk. For each round s ∈ { 1, . . . , r }, we first define an expert action A∗

> k,s

. This action is generated by the LLM when it is provided with the many historical trajectory set Emany ⊂ E and the current interaction history Hk,s −1:

A∗ 

> k,s

= LLM  Emany ∪ H k,s −1 ∪ I k). (5) Simultaneously, the agent attempts to reconstruct this reasoning logic using only a few trajectories 

Efew ⊂ E many and the learnable prompt Pθ:

ˆAk,s = LLM  Pθ ∪ E few ∪ H k,s −1 ∪ I k). (6) The consolidation process aims to synchronize the student’s parametric response with the ex-pert’s contextual reasoning at every decision point. 

Algorithm 1 Inference Workflow 

Require: Universal system prompt Psys , task set 

T = {tk}, max interaction rounds r, con-trastive prompt templates Pc, success prompt templates Ps, retrieval hyperparameter K.

1: Initialization :

2: Rsucc ← ∅ , Qerr ← ∅ , Qsucc ← ∅ 

3: for all tk ∈ T do 

4: Step 1: Experience Retrieval & Extraction 

5: Retrieve top-K recent successful dialogs 

Crec succ from Rsucc 

6: Exp c = LLM( Pc ∪ C rec succ ∪ C fail )

7: Exp s = LLM( Ps ∪ C rec succ )

8: Step 2: Experience-Augmented Prompt 

9: Ik = Psys ⊕ Exp c ⊕ Exp s ⊕ C rec succ ⊕ tk

10: Step 3: Interactive Task Execution 

11: Hk, 0 ← ∅ 

12: for s = 1 to r do 

13: Ak,s = π(Hk,s −1, Ik)

14: Fk,s = EnvFeedback (Ak,s )

15: Hk,s = Hk,s −1 ∪ { (Ak,s , F k,s )}

16: end for 

17: Step 4: Update Experience Repository 

18: if EnvVerify (tk) = Success then 

19: Add full dialog Hk,r to Rsucc 

20: Push Exp s to Qsucc (FIFO) 

21: else 

22: Push Exp c to Qerr (FIFO) 

23: end if 

24: end for 

The objective function minimizes the cumulative token-level cross-entropy loss across all interaction Model Dataset Method Exp=0 Exp=1 Exp=4 Exp=16 Exp=32 Avg Llama 3.1-8B AWM (Wang et al., 2024b) 19.0 45.4 71.6 66.7 74.2 55.4 TER (Zheng et al., 2025b) 19.8 41.6 68.2 69.0 70.2 53.8 DB SCM (Wang et al., 2025) 19.8 23.4 63.0 61.0 68.4 47.1 A-MEM (Xu et al., 2025) 19.8 57.0 67.0 74.8 73.4 58.4 EvoSC (ours) 24.8 71.2 74.4 77.2 77.8 65.1 (+6.7)                                                                                              

> AWM (Wang et al., 2024b) 42.8 51.0 45.2 49.2 47.4 47.1 TER (Zheng et al., 2025b) 42.2 42.8 46.8 46.4 41.2 43.9 OS SCM (Wang et al., 2025) 42.2 44.0 43.2 43.6 49.0 44.4 A-MEM (Xu et al., 2025) 42.2 46.6 51.4 51.2 50.8 48.4 EvoSC (ours) 45.2 50.2 52.0 51.8 51.2 50.1 (+1.7)
> Qwen 2.5-7B AWM (Wang et al., 2024b) 73.8 72.6 74.0 76.0 OOM 74.1 TER (Zheng et al., 2025b) 74.0 71.2 72.8 72.6 OOM 72.7 DB SCM (Wang et al., 2025) 74.0 74.6 75.6 75.8 OOM 75.0 A-MEM (Xu et al., 2025) 74.0 73.0 73.0 74.6 OOM 73.7 EvoSC (ours) 75.4 75.0 76.8 75.2 76.2 75.7 (+0.7)
> AWM (Wang et al., 2024b) 47.2 50.6 52.0 52.6 OOM 50.6 TER (Zheng et al., 2025b) 41.2 51.8 53.6 58.2 OOM 51.2 OS SCM (Wang et al., 2025) 41.2 44.2 52.0 49.0 OOM 46.6 A-MEM (Xu et al., 2025) 41.2 44.2 53.0 57.2 OOM 48.9 EvoSC (ours) 48.4 57.4 56.2 59.4 56.4 55.6 (+4.4)

Table 1: Performance comparison (on Database (DB) and Operating System (OS) datasets (Zheng et al., 2025b)) between our proposed EvoSC Framework and other methods, evaluated on Llama 3.1-8B-Instruct (Dubey et al., 2024) and Qwen 2.5-7B-Instruct models (Yang et al., 2024). “Exp” denotes the number of recent successful trajectories provided to the agent. “OOM” denotes out of memory, which is caused by the limited context window of LLMs. When more trajectories are used, the number of tokens exceeds the context window limit of LLM. 

rounds s and all tokens j within each action: 

Lconsolid =

−

> r

X

> s=1

X

> j

log Pθ

 A∗ 

> k,s,j

| P θ, Ik, Hk,s −1, A k,s,<j 

,

where Ak,s,j denotes the j-th token of the expert action at round s. By optimizing Pθ over the entire sequence of interactions, the agent effectively in-ternalizes the successfully trajectories augmented reasoning process into its parameter space. This ensures that even when the context window is constrained, the agent can leverage Pθ to main-tain high-fidelity, intuition-like decision-making throughout the multi-round task execution. 

4.3 Experience Enhanced Inference 

To balance immediate relevance with long-term internalization, EvoSC adopts a hybrid injection strategy that combines explicit textual retrieval with consolidated parametric guidance. For each new task tk, the agent retrieves the top-K most relevant entries from the experience queues to construct an augmented input. Consequently, the final in-put representation Ik is defined as a multi-level composition: 

Ik = Pθ ⊕ P sys ⊕ Exp c ⊕ Exp s ⊕C s ⊕ tk, (7) where Pθ provides the implicit parametric intuition consolidated from long-term history, while Exp c,

Exp s, and Cs provide explicit textual experience from recent successful and failed interactions. The complete inference workflow of the REC frame-work is detailed in Algorithm 1. 

5 Experiments 

5.1 Experimental Setup Benchmark. For our experimental evaluation, we adopt the LifelongAgentBench (Zheng et al., 2025b), which is specifically designed to assess the learning capabilities of LLM-based agents. This benchmark comprises three domain-specific datasets that measure capabilities across distinct ar-eas: Database (DB, 500 tasks), Operating System (OS, 500 tasks), and Knowledge Graph (KG, 396 tasks). These three datasets are deployed across three interactive environments, simulating dynamic real-world scenarios that demand knowledge accu-mulation, retention, and transfer. Model Dataset Method Exp=0 Exp=1 Exp=4 Exp=16 Avg Llama 3.1-8B AWM (Wang et al., 2024b) 12.6 26.5 32.6 OOM 23.9 TER (Zheng et al., 2025b) 28.0 35.1 32.8 OOM 32.0 KG SCM (Wang et al., 2025) 28.0 28.0 31.1 OOM 29.0 A-MEM (Xu et al., 2025) 28.0 31.8 19.9 OOM 26.6 EvoSC (ours) 32.1 39.4 36.7 42.7 37.7 (+5.7)                           

> Qwen 2.5-7B AWM (Wang et al., 2024b) 17.7 13.4 15.7 OOM 15.6 TER (Zheng et al., 2025b) 16.4 34.1 32.8 OOM 27.8 KG SCM (Wang et al., 2025) 16.4 27.2 33.3 OOM 25.6 A-MEM (Xu et al., 2025) 16.4 18.4 10.6 OOM 15.1 EvoSC (ours) 29.3 39.4 39.6 45.2 38.4 (+10.6)
> Table 2: Performance comparison (KG dataset) between our proposed EvoSC and strong baselines such as AWM (Wang et al., 2024b), TER (Zheng et al., 2025b), SCM (Wang et al., 2025) and A-MEM (Xu et al., 2025) with Llama 3.1-8B-Instruct and Qwen 2.5-7B-Instruct models.

Implement Details. We adopt the same maxi-mum allowed interaction rounds as (Zheng et al., 2025b) for tasks across different datasets. Specifi-cally, 3 rounds for the Database dataset, 5 rounds for the Operating System dataset, and 15 rounds for the Knowledge Graph dataset. We evaluate two LLM-based agents: Llama 3.1-8B-Instruct and Qwen 2.5-7B-Instruct. In our method, the length of the learnable prompt is set to 20. Under the experimental setting of “Exp = 32”, the teacher model uses 20 trajectories for reasoning, whereas the student model uses 8 trajectories. Specifically, the experience from 12 trajectories is internalized into the latent space of the prompts and paired with an additional 20 trajectory experiences for reason-ing. To mitigate experimental randomness, we run each experiment three times with random seeds and report the averaged results. All experiments are conducted on Linux servers, with each experi-ment utilizing two NVIDIA A40 (48G) GPUs. 

Baselines and Metrics. We compare our method with strong baselines: Agent workflow memory (AWM) (Wang et al., 2024b) extracts workflows from past experiences to guide reasoning. Tex-tual experience replay (TER) (Zheng et al., 2025b) enhances the reasoning accuracy of agents via tra-jectory experience replay. Self-Controlled Memory (SCM) (Wang et al., 2025) constructs a memory framework with an agent, stream, and controller to handle ultra-long inputs. Agentic Memory (A-Mem) (Xu et al., 2025) constructs dynamic inter-connected memory networks via note construction, linking and evolution to support LLM agents. We evaluate the framework under various configura-tions, where different configurations involve the replay of different amounts of historical trajecto-ries. The evaluation metric is the task success rate, which is defined as the proportion of correct action sequences that complete the task successfully. 

5.2 Main Results and Analysis Performance on DB and OS Tasks. Table 1 summarizes the performance comparison on the DB and OS datasets. The results demonstrate that our proposed EvoSC framework consistently out-performs strong baselines, including AWM (Wang et al., 2024b) and TER (Zheng et al., 2025b), across different LLM backbones. Specifically, on the DB dataset, EvoSC achieves a substantial average per-formance boost of +9.7% on Llama 3.1-8B com-pared to the best-performing baselines. On the OS dataset, the performance gains are also signifi-cant, with improvements of +3.0% and +4.4% on Llama and Qwen models, respectively. Notably, as the number of provided successful trajectories (Exp) increases from 0 to 32, EvoSC maintains a stable upward trend in success rate. In contrast, the performance of AWM and TER often fluctu-ates or degrades as more trajectories are introduced. This suggests that raw textual replay introduces dis-tracting noise and redundant information, whereas EvoSC’s contrastive reflection mechanism effec-tively filters high-value insights. 

Performance on KG Tasks. The superiority of EvoSC is even more pronounced in the Knowledge Graph (KG) domain, as presented in Table 2. KG tasks typically involve longer reasoning chains and more complex environmental interactions, which pose significant challenges for experience manage-ment. EvoSC achieves a remarkable average im-Model Dataset EE SE PTC Exp=0 Exp=1 Exp=4 Exp=16 Exp=32 Avg Llama 3.1-8B DB 

✓ 24.4 66.8 70.4 72.2 74.6 61.7 

✓ 24.2 65.4 72.8 73.4 75.2 62.2 

✓ ✓ 24.8 74.0 73.8 75.6 77.8 65.2 

✓ ✓ ✓ 24.8 71.2 74.4 77.2 77.8 65.1 OS 

✓ 45.8 50.0 49.2 48.2 47.6 48.2 

✓ 45.0 50.4 50.8 50.4 48.8 49.2 

✓ ✓ 45.2 51.2 50.8 50.8 49.4 49.5 

✓ ✓ ✓ 45.2 50.2 52.0 51.8 51.2 50.1 

Qwen 2.5-7B DB ✓ ✓ 75.4 75.2 76.8 74.2 OOM 75.4 

✓ ✓ ✓ 75.4 75.0 76.8 75.2 76.2 75.7 

OS ✓ ✓ 48.4 53.8 56.2 58.8 OOM 54.4 

✓ ✓ ✓ 48.4 57.4 56.2 59.4 56.4 55.6 

Table 3: Ablation study of EvoSC on DB and OS datasets. “EE” and “SE” denote the error-prone experience and successful experience, respectively, while “PTC” denotes the parametric trajectory consolidation. 

provement of +5.7% on Llama 3.1-8B and +10.6% on Qwen 2.5-7B. Most importantly, while all base-line methods fail when Exp increases, EvoSC con-tinues to benefit from expanded historical data. This demonstrates that EvoSC effectively internal-izes the complex logic of long-horizon trajectories into its parametric memory, enabling the agent to solve tasks that are difficult for standard context-based replay methods. 

Overcoming Context Limitations. A critical ad-vantage of EvoSC is its robustness to the physical constraints of LLMs during lifelong learning. As shown in both Table 1 and Table 2, baseline meth-ods frequently encounter OOM errors as the num-ber of experiences increases. For instance, on the Qwen 2.5-7B model, which has a more constrained context window, the baselines fail to operate when Exp = 32 for DB/OS and Exp = 16 for KG. This failure mode is inevitable for methods relying on raw trajectory replay, as the prompt length expands linearly with history. EvoSC effectively circum-vents this bottleneck by consolidating extensive textual experiences into compact, learnable prompt parameters. This ensures that the agent can lever-age vast historical wisdom without exceeding hard-ware limits, maintaining a constant and efficient context length regardless of the task’s duration. 

Visualization of Learning Stability. To further investigate the learning process, Figure 5 visualizes the cumulative correct task counts on DB tasks using Llama 3.1-8B as the backbone. Under a fixed evaluation window, our method exhibits a steeper and more sustained growth curve compared to the window 1 window 2 window 3 window 4 window 5  

> 0
> 10
> 20
> 30
> 40
> 50
> 60
> 70
> 80
> 90
> Total correct count
> 48
> 27
> 50
> 41
> 46
> 68 70
> 65
> 72
> 81
> TER
> Ours

Figure 5: Correct count comparison for dataset database (window size=100, 1 trajectory) 

baseline, indicating that the experiences extracted and consolidated by EvoSC provide higher-quality guidance for subsequent tasks. 

5.2.1 Ablation Study 

To evaluate the individual contributions of the two experience extraction methods (error-prone and successful) and the parametric consolidation, we conduct a series of ablation studies in Tables 3 and Figures 6. The results indicate that both the error-prone and successful experience extraction modules are vital for performance. This confirms that analyzing failures is essential for the agent to identify and avoid recurrent logical pitfalls, and the absence of successful experience extraction im-pairs the agent’s ability to replicate efficient rea-soning patterns. The exclusion of the parametric trajectory consolidation results in the most signifi-cant performance bottleneck, particularly in long-horizon tasks. Without this, the agent is forced to rely solely on raw textual replay, which not only risks exceeding the context window but also in-troduces irrelevant noise that dilutes the model’s Exp=0 Exp=1 Exp=4 Exp=16   

> 25.0
> 27.5
> 30.0
> 32.5
> 35.0
> 37.5
> 40.0
> 42.5
> 45.0
> Performance (%)
> 28.8
> 37.1
> 34.1
> 31.1
> 38.4
> 35.9
> 32.1
> 39.1
> 36.4
> 32.1
> 39.4
> 36.7
> 42.7
> OOM OOM OOM
> SE
> EE
> EE+SE
> EE+SE+PTC
> OOM

(a) Exp=0 Exp=1 Exp=4 Exp=16   

> 25
> 30
> 35
> 40
> 45
> 50
> Performance (%)
> 29.3
> 40.2
> 36.9
> 29.3
> 39.4 39.6
> 45.2
> OOM
> EE+SE
> EE+SE+PTC
> OOM

(b) 

Figure 6: Ablation study on KG based on (a) Llama 3.1-8B-Instruct and (b) Qwen 2.5-7B-Instruct. 

focus. Overall, the ablation study confirms that the full EvoSC framework, through the integration of explicit reflection and implicit consolidation, achieves a superior balance between learning depth and operational scalability. 

6 Conclusion 

In this paper, we introduced the EvoSC framework, a novel dual-stage paradigm for agent evolution. a dual-stage agent evolution paradigm that not only autonomously extracts multifaceted experiential in-sights but also internalizes historical experiences into the latent space of learnable prompts. Empir-ical evaluations on several agent lifelong bench-marks demonstrate that EvoSC achieves state-of-the-art performance across multiple domains and maintains stability in long-horizon learning sce-narios where existing methods fail. In conclusion, EvoSC provides a versatile and scalable founda-tion, paving the way for developing evolutionary lifelong learning agents. 

7 Limitations 

Although our approach provides an evolutionary framework for agents, the relatively simplistic ex-perience retrieval mechanism inevitably constrains the reasoning capabilities of agents. For future work, we plan to explore advanced methodolo-gies to refine and optimize the retrieval mechanism. Additionally, due to constrained computational re-sources, our current experiments were primarily focused on 7B and 8B scale models. Evaluating the scalability of EvoSC on larger-scale models (e.g., 70B+) and a broader diversity of LLM ar-chitectures remains a key priority for our future research to fully unlock the potential of parametric experience consolidation. 

References 

Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards a generalist agent for the web. 

Advances in Neural Information Processing Systems ,36:28091–28114. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. 

arXiv preprint arXiv:2407.21783 .Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shan-tanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. Ruler: What’s the real context size of your long-context language models? 

arXiv preprint arXiv:2404.06654 .Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901 .Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. 2024. Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325 .Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. 2023a. Tradinggpt: Multi-agent system with layered memory and distinct char-acters for enhanced financial trading performance. 

arXiv preprint arXiv:2309.03736 .Yuan Li, Yixuan Zhang, and Lichao Sun. 2023b. Metaa-gents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collab-orative generative agents. arxiv, arxiv231006500. 

URL: https://doi. org/10.48550/arXiv , 2310. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024a. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 .Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b. Lost in the middle: How language models use long contexts. Transactions of the Asso-ciation for Computational Linguistics , 12:157–173. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yu-lan He, Di Yin, Xing Sun, and Yunsheng Wu. 2023a. Memochat: Tuning llms to use memos for consis-tent long-range open-domain conversation. arXiv preprint arXiv:2308.08239 .Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023b. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 .Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. 2023. Ret-llm: Towards a general read-write memory for large language models. arXiv preprint arXiv:2305.14322 .Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. 

arXiv preprint arXiv:2203.13474 .Eleanor Spens and Neil Burgess. 2024. A generative model of memory construction and consolidation. 

Nature human behaviour , 8(3):526–543. Christian K Tamnes, Kristine B Walhovd, Mari Torstveit, Victoria T Sells, and Anders M Fjell. 2013. Performance monitoring in children and adolescents: a review of developmental changes in the error-related negativity and brain maturation. Develop-mental cognitive neuroscience , 6:1–13. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Mil-lican, and 1 others. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 .Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. Llama: Open and effi-cient foundation language models. arXiv preprint arXiv:2302.13971 .Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Zhenhe Wu, ShuangZhi Wu, Zejun Ma, and Zhoujun Li. 2025. Scm: Enhancing large language model with self-controlled memory framework. In Interna-tional Conference on Database Systems for Advanced Applications , pages 188–203. Springer. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, and 1 others. 2024a. A survey on large language model based autonomous agents. 

Frontiers of Computer Science , 18(6):186345. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. 2024b. Agent workflow memory. 

arXiv preprint arXiv:2409.07429 .Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yi-wen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, and 1 others. 2025. The rise and potential of large language model based agents: A survey. Science China Information Sci-ences , 68(2):121101. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Jun-tao Tan, and Yongfeng Zhang. 2025. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110 .An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388 .An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayi-heng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Ji-axi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024. Qwen2.5 technical report. CoRR ,abs/2412.15115. Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, and 1 others. 2025b. Learning on the job: An experience-driven self-evolving agent for long-horizon tasks. arXiv preprint arXiv:2510.08002 .Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Be-rant. 2024. Assistantbench: Can web agents solve realistic and time-consuming tasks? arXiv preprint arXiv:2407.15711 .J Zheng, C Shi, X Cai, Q Li, D Zhang, C Li, D Yu, and Q Ma. 2025a. Lifelong learning of large language model based agents: A roadmap. arxiv 2025. arXiv preprint arXiv:2501.07278 .Junhao Zheng, Xidi Cai, Qiuke Li, Duzhen Zhang, ZhongZhi Li, Yingying Zhang, Le Song, and Qianli Ma. 2025b. Lifelongagentbench: Evaluat-ing llm agents as lifelong learners. arXiv preprint arXiv:2505.11942 .Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, and 1 others. 2023. Codegeex: A pre-trained model for code generation with multilingual bench-marking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 5673–5684. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory. In Pro-ceedings of the AAAI Conference on Artificial Intelli-gence , volume 38, pages 19724–19731. A Appendix 

We visualize the contents of representative cases and prompt templates as follows. Table 4 presents a complete dialogue example of an agent based on the Llama 3.1-8B-Instruct model for a database task, while Table 5 displays that of the same agent for an operating system task. Figures 7 and 8 illus-trate the contrastive prompt templates designed for database tasks and operating system tasks, respec-tively. Figures 9, 10 and 11 showcase the success knowledge extraction prompt templates tailored for tasks across three distinct domains, respectively. Finally, Figure 12 presents several knowledge ex-amples extracted from knowledge graph tasks. User: I will ask you a question, then you should help me operate a MySQL database with SQL to answer the question. You have to explain the problem and your solution to me and write down your thoughts. After thinking and explaining thoroughly, every round you can choose one of the two actions: Operation or Answer. To do operation, the format should be like this: Action: Operation “‘sql SELECT * FROM table WHERE condition; “‘ You MUST put SQL in markdown format without any other comments. Your SQL should be in one line. I will use Äction: Operation¨ literally to match your SQL. Every time you can only execute one SQL statement. I will only execute the statement in the first SQL code block. Every time you write a SQL, I will execute it for you and give you the output. If the SQL is not executed successfully, the response will be the error message. Otherwise, the response will be the raw MySQL response. For SELECT queries, the response will be the result of the query, such as [(1, ’John Doe’, ’HR’), (2, ’Jane Smith’, ’IT’), ...], where each tuple represents a row and the elements are the values of the columns in the row. For SQL such as INSERT, UPDATE, and DELETE, the response will be an empty list [] indicating that the SQL was executed successfully. If you have obtain the answer by interacting with the database, and you MUST commit your final answer using the format like this: Action: Answer Final Answer: [(1, ’John Doe’, ’HR’), (2, ’Jane Smith’, ’IT’), ...] DO NOT write this pattern unless you are sure about your answer. I expect an accurate and correct answer. Your answer should be accurate. Your answer must be exactly the same as the correct answer. If the question is about modifying the database, then after done operation, your answer field can be anything. If the question is about querying the database, then after done operation, your answer field should be the result of the query. We note that the column names will not be displayed in the result, and you need to ensure both the orders of the columns and rows are correct. If your response cannot match any pattern I mentioned earlier, you will be judged as FAIL immediately. Once you commit your answer or the number of rounds reaches 3, the task will be finished and the system will judge whether you pass the task or not. Error-prone experience Successful experience Past trajectories 

Agent: ok. 

User: Delete all cancelled bookings for customers who have more than 5 total bookings and a total spending of less than 1000 across all their bookings. The name of this table is customer_bookings, and the headers of this table are booking_id, customer_id, status, price, destination, booking_date. 

Agent: Action: Operation ”’sql DELETE FROM customer_bookings WHERE status = ’cancelled’ AND customer_id IN ( SELECT customer_id FROM ( SELECT customer_id, COUNT(*) AS total_bookings, SUM(price) AS total_spending FROM customer_bookings GROUP BY customer_id ) AS subquery WHERE total_bookings > 5 AND total_spending < 1000 ); ”’ 

User: [] 

Agent: Action: Answer Final Answer: [] 

Table 4: A case on the database task. User: 

System prompt Error-prone experience Successful experience Past trajectories 

Agent: ok. 

User: Create a group ’devteam’, add users ’user1’ and ’user2’ to it using gpasswd, then remove ’user2’ and add ’user3’. Additionally, create a directory ’/devteam’ accessible only to the group. 

Agent: To complete this task, we need to follow these steps: 1. Create a new group ’devteam’ using the ‘groupadd‘ command. 2. Add users ’user1’ and ’user2’ to the ’devteam’ group using the ‘gpasswd‘ command. 3. Remove ’user2’ from the ’devteam’ group using the ‘gpasswd‘ command. 4. Add ’user3’ to the ’devteam’ group using the ‘gpasswd‘ command. 5. Create a new directory ’/devteam’ with permissions that allow only the ’devteam’ group to access it. Here’s the thought process: - We’ll use the ‘groupadd‘ command to create a new group ’devteam’. - We’ll use the ‘gpasswd‘ command to add users ’user1’ and ’user2’ to the ’devteam’ group. - We’ll use the ‘gpasswd‘ command to remove ’user2’ from the ’devteam’ group. - We’ll use the ‘gpasswd‘ command to add ’user3’ to the ’devteam’ group. - We’ll use the ‘mkdir‘ command to create a new directory ’/devteam’. - We’ll use the ‘chmod‘ command to set the permissions of the ’/devteam’ directory to allow only the ’devteam’ group to access it. Now, let’s execute the commands: Act: bash “‘bash groupadd devteam gpasswd -a user1 devteam gpasswd -a user2 devteam gpasswd -d user2 devteam gpasswd -a user3 devteam mkdir -p /devteam chmod 770 /devteam chown :devteam /devteam “‘ This command will create the ’devteam’ group, add users ’user1’ and ’user3’ to it, remove ’user2’ from it, and create a new directory ’/devteam’ with permissions that allow only the ’devteam’ group to access it. 

User: The output of the OS: Adding user user1 to group devteam Adding user user2 to group devteam Removing user user2 from group devteam Adding user user3 to group devteam 

Agent: The output of the OS indicates that the commands were executed successfully. The output shows the actual output of the ‘gpasswd‘ command, which confirms that the users were added and removed from the ’devteam’ group as expected. Since the task has been completed successfully, I will conclude the task. Act: finish 

Table 5: A case on the operating system task. Contrastive Prompt design for database 

You are an expert database administrator and educator specializing in SQL query optimization and database problem-solving. 

Your role is to analyze failed database task attempts and successful task completions to create a comprehensive error prevention 

guide. 

You will be given: 

1. Successful interaction dialogues where database tasks were completed correctly. 

2. Failed interaction dialogues where database tasks encountered errors or exceeded the interaction limit 

Your mission is to identify specific mistakes made in failed attempts, understand why they occurred, and formulate clear 

guidelines to prevent similar errors in future database tasks. Think of this as creating a “common database mistakes handbook” 

that helps avoid repeating the same errors, especially focusing on SQL syntax errors, query logic mistakes, and inefficient 

approaches that lead to exceeding interaction limits. 

· Successful Interactions: (success_text) 

· Failed Interactions: (failed_text) 

By comparing successful and failed interactions, identify specific errors in the SQL statements that may have caused the failure. 

Create a concise error prevention guide, formatted similarly to a “Common Database Errors Handbook,” clearly explaining where 

problems might have occurred and how to avoid them. 

Important Context: 

- Pay special attention to SQL statement correctness, query efficiency, and problem-solving strategy Figure 7: Contrastive prompt designed for DB dataset. Contrastive Prompt design for operating system 

You are an expert Linux system administrator and educator. Your role is to analyze failed task attempts and successful task 

completions to create a comprehensive error prevention guide. 

You will be given: 

1. Successful interaction dialogues where tasks were completed correctly 

2. Failed interaction dialogues where tasks encountered errors 

Your mission is to identify the specific mistakes made in failed attempts, understand why they occurred, and formulate clear 

guidelines to prevent similar errors in future tasks. Think of this as creating a "common mistakes handbook" that helps avoid 

repeating the same errors. 

Reference: Successful Interactions (Study these to understand correct approaches): {success_text} 

Failed Interactions (Analyze these to identify mistakes): {failed_text} 

By comparing the successful and failed interactions, identify the specific errors and mistakes that led to failure. Create a concise 

error prevention guide in the format of a "common mistakes handbook" that clearly states what went wrong and how to avoid it. 

Output Format: 

Each rule should follow this structure: 

- Mistake: [Brief description of what went wrong] 

- Solution: [How to avoid this mistake / Correct approach] 

Figure 8: Contrastive prompt designed for OS dataset. Successful Knowledge Extraction Prompt design for database 

You are an expert SQL analyst and educator. Your role is to study successful database interaction examples, identify the problems 

they solved, analyze the correct SQL statements used, and extract the key important points that made these SQL statements 

effective. 

Your mission is to learn from successful examples by understanding: what problem was solved, what SQL was used correctly, and 

what are the critical aspects in these correct SQL statements that led to success. 

· Successful Interactions: {success_text} 

By studying these successful interactions, extract key learning points. For each successful example, identify: 

1. The specific problem/question that was solved 

2. The correct SQL statement that was used 

3. The important aspects in this correct SQL statement that made it work effectively 

Figure 9: Successful knowledge extraction prompt designed for DB dataset. Successful Knowledge Extraction Prompt design for operation system 

You are analyzing successful Linux system interaction logs. These interactions completed their tasks correctly within 5 interaction 

rounds. Your job is to extract concise Linux command knowledge from them. 

Extract 3-6 concise, actionable command rules. Each rule should be ONE line, showing what command to use for what purpose. 

Focus on specific command patterns that help complete task. 

Here are successful Linux system interactions that completed their tasks correctly within 5 interaction rounds: 

· {success_text} 

Your task: 

Extract 3-6 concise command rules from these interactions. Each rule should be one line, showing what command to use for 

what purpose. 

Format (Keep it concise—one rule per line): 

- Use `command` to execute [purpose]. Figure 10: Successful knowledge extraction prompt designed for OS dataset. Successful Knowledge Extraction Prompt design for knowledge graph 

You are a knowledge graph expert analyzing successful knowledge graph interaction processes. Your task is to extract concise, 

actionable practices from successful interactions. 

Extract 3-6 concise, actionable command rules. Each rule should be ONE line, showing what command to use for what purpose. 

Focus on specific command patterns that help complete task. 

Analyze the following successful knowledge graph interaction processes: 

· {success_text} 

For each successful interaction, extract: 

1. The specific question and entities 

2. Concise step-by-step practices that led to success 

For each interaction, provide: 

specific_question: "Question: [exact question text], Entities: [entity list]" 

successful_practices: 

- [Step 1: What to do first and which API command to use] 

- [Step 2: What to do next and which API command to use] 

- [Step 3: Additional important notes about traversal patterns or relations] 

Figure 11: Successful knowledge extraction prompt designed for KG dataset. Successful knowledge example for knowledge graph 

specific_question: "Question: unsteadiness can be a side effect in how many medical treatments?, Entities: ['Unsteadiness']" 

successful_practices: 

- To find the number of medical treatments that can cause unsteadiness, start by getting the relations connected to Unsteadiness 

using get_relations(Unsteadiness). 

- Then, traverse the relations to find the ones relevant to medical treatments, specifically'medicine.symptom.side_effect_of'. 

- Next, use get_neighbors(Unsteadiness, medicine.symptom.side_effect_of) to find the medical treatments that can cause 

unsteadiness. 

- Finally, count the number of medical treatments using count(#0) to determine the answer. 

Figure 12: Successful knowledge example for KG dataset.