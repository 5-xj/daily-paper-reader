Title: EvoMU: Evolutionary Machine Unlearning

URL Source: https://arxiv.org/pdf/2602.02139v1

Published Time: Tue, 03 Feb 2026 03:32:10 GMT

Number of Pages: 26

Markdown Content:
# EvoMU: Evolutionary Machine Unlearning 

Paweł Batorski 1 Paul Swoboda 1

## Abstract 

Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted ma-terial). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an op-timal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and over-lap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is there-fore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the po-tential of AI co-scientists with limited compu-tational resources. Our experimental evaluation shows that we surpass previous loss-based un-learning formulations on TOFU-5% , TOFU-10% ,MUSE and WMDP by synthesizing novel un-learning losses. Our code is available at https: //github.com/Batorskq/EvoMU 

## 1. Introduction 

As large language models (LLMs) grow in capability, they also increasingly memorize and reproduce sensitive, harm-ful, or copyrighted content (Wang et al., 2023a; Huang et al., 2024b; Barbulescu and Triantafillou, 2024; Ousid-houm et al., 2021; Wei et al., 2024a; Carlini et al., 2022;    

> 1Heinrich Heine Universit ¨at D ¨usseldorf, Germany. Correspon-dence to: Pawel Batorski <pawel.batorski@hhu.de >.
> Preprint. February 3, 2026.

Dou et al., 2025a). In many real deployments, this cre-ates a practical need for machine unlearning : removing the influence of specified training data from a trained model without retraining from scratch (Bourtoule et al., 2021; Gi-nart et al., 2019; Cao and Yang, 2015;?). Beyond its origins in data deletion and privacy regulations (e.g., the “right to be forgotten” (Hoofnagle et al., 2019; Rosen, 2011)), un-learning is now motivated by content compliance, safety, and intellectual property concerns (Liu et al., 2025a). A dominant approach for unlearning in modern LLMs is post-training fine-tuning with unlearning losses that sup-press unlearning data while preserving general utility. They include gradient-ascent or gradient-difference formulations (Liu et al., 2022; Maini et al., 2024b; Yao et al., 2024b), preference-optimization objectives that discourage unde-sired completions (Zhang et al., 2024a; Fan et al., 2024a; Mekala et al., 2025a), and distillation or logit/representation-regularization schemes (Li et al., 2024; Wang et al., 2024a; Dong et al., 2025a). However, loss design remains a ma-jor bottleneck. Hand-designed losses may be brittle: small changes in coefficients, margins, or reference signals can shift the trade-off between effective forgetting and over-unlearning. Additionally, unlearning losses are rarely univer-sal. The difficulty of selective forgetting depends strongly on the structure of the unlearning instance: how concen-trated or diffuse the target content is, how much the forget and retain distributions overlap, and whether the forget tar-gets are isolated facts, stylistic patterns, or long-form pas-sages. This structure changes the dominant failure mode. When forget and retain share entities, phrasing, or under-lying facts, a naive loss may suppress too broad regions of behavior (e.g., refusing or becoming uncertain in-domain) rather than selectively removing the intended information. What matters is not only “how much forgetting” is applied, but how the loss balances forget vs. retain and how it pre-vents degenerate solutions. These considerations suggest that loss design should adapt to the data geometry of the unlearning problem rather than rely on a single fixed loss. The above observation, namely large and problem-dependant, yet well-specified search space with clear eval-uation protocol, makes machine unlearning a good target for automated scientific discovery (ASD). Modern ASD has seen great success (i) when using LLMs for code generation, experimentation and iterative refinement (Schmidgall et al., 1

> arXiv:2602.02139v1 [cs.LG] 2 Feb 2026

EvoMU: Evolutionary Machine Unlearning Train & Eval Evolution           

> Training Evaluation
> 0246810
> 0.2
> 0.6
> 1
> 1.4
> 1.8
> 2
> Epochs
> Loss
> 00.20.40.60.81
> 0.45
> 0.5
> 0.55
> 0.6
> 0.65
> 0.7
> 1-Extraction Strength
> Model Utility

loss_1 

loss_2 

loss_3 

LLM Proposer / Mutate 

Losses Info 

Top-K                                                                            

> EvoMU Extraction Strength: 0.03 Model Utility: 0.65 Epochs: 2
> ℓEvmoMU (θ) = 1 .2E(xf,y f)∼Df
> [
> log πθ(yf|xf)−log πref (yf|xf)
> ]
> +E(xr,y r)∼Dr
> [
> log πref (yr|xr)−log πθ(yr|xr)
> ]
> ,
> SimNPO Extraction Strength: 0.08 Model Utility: 0.58 Epochs: 10
> ℓSimNPO (θ) = E(xf,y f)∼Df
> [
> −2
> βlog σ
> (
> −β
> |yf|log πθ(yf|xf)−γ
> )]
> −λE(xr,y r)∼Dr
> [
> log πθ(yr|xr)
> ]
> .
> NPO Extraction Strength: 0.10 Model Utility: 0.57 Epochs: 10
> ℓNPO (θ) = −2
> βE(xf,y f)∼Df
> [
> log σ
> (
> −β(
> log πθ(yf|xf)
> −log πref (yf|xf)))]
> −λE(xr,y r)∼Dr
> [
> log πθ(yr|xr)
> ]
> .

Figure 1. High level overview of EvoMU. A LLM proposes candidate unlearning loss functions along with their training budget. For each candidate, we fine-tune the base model with LoRA adapters on forget and retain data, then evaluate the resulting checkpoint using standard forgetting and utility metrics. We select the top-K objectives and summarize their full feedback (loss code, training curves, and evaluation scores). Conditioned on this feedback, the LLM mutates the objectives by adjusting coefficients, modifying structure, and optionally changing the training budget, to produce the next generation. Repeating this verify–select–mutate loop yields task-specific unlearning losses that match or outperform human-designed ones. 

2025a; Jansen et al., 2025a; Yamada et al., 2025a; Lu et al., 2024a; Wang et al., 2023b) and (ii) when candidates are executable and progress is measured by automatic metrics, enable efficient, auditable search loops (e.g., algorithm or reward-function discovery) (Fawzi et al., 2022; Ma et al., 2023; Richardson et al., 2024a; Cheng et al., 2025b). In this work, we study unlearning loss design as an ASD problem and introduce EvoMU, an evolutionary objective-discovery framework. EvoMU uses a code-generating LLM to pro-pose candidate unlearning loss, fine-tunes lightweight LoRA adapters for each candidate, evaluates with standardized un-learning metrics, and iteratively refines the best loss using an evolutionary search. Crucially, we show that this dis-covery loop does not require frontier-scale or closed-weight models: a small open model (Qwen3-4B-Thinking) is suf-ficient to synthesize novel, effective losses that match or outperform strong human-designed baselines across multi-ple benchmarks. Our contributions are: • ASD for unlearning. We formulate unlearning loss de-sign as an automated discovery problem and present an end-to-end pipeline that proposes, trains, evaluates, and refines executable loss functions. • Small-model effectiveness. We demonstrate that a 4B-parameter open LLM can effectively drive loss discovery, contrasting with prior ASD systems that rely on very large or closed models. • Task-specific losses matter. Across TOFU, MUSE, and WMDP, the best-performing losses differ substantially, indicating that unlearning losses should be tailored to the forget/retain data structure rather than treated as universal. • LLM-sampled losses are a strong baseline. Even with-out refinement, randomly sampled candidate losses can be competitive with established losses (e.g., NPO), high-lighting that loss choice is a major lever. • Empirical structure of effective losses. Across bench-marks, many top-performing discovered losses are simple and often do not require explicit reference-model terms, suggesting underexplored regions of the unlearning loss space. 

## 2. Related Work 

2.1. Machine Unlearning for LLMs. 

Recent surveys and position papers argue that unlearning for LLMs should be viewed as a continuum of intervention surfaces, spanning inference-time suppression, parameter editing, and weight-level updates, while emphasizing that reliable evaluation and meaningful guarantees remain open problems (Thaker et al., 2024a; Ren et al., 2025a; Liu et al., 2025a). Broadly, existing approaches fall into three (over-lapping) families: (i) mitigation mechanisms that restrict access to unwanted content without directly optimizing a forgetting loss, (ii) localized editing or model-merging meth-ods that overwrite specific associations, and (iii) training-time unlearning that updates model parameters (often via parameter-efficient fine-tuning) under a designed loss. 2EvoMU: Evolutionary Machine Unlearning 

Mitigation without optimizing a forget loss. This ap-proach limits access to knowledge to be unlearned without updating the base model. This includes inference-time con-trols such as decoding- or logit-level interventions and re-lated approximate-unlearning techniques (Eldan and Russi-novich, 2023; Ji et al., 2024a; Huang et al., 2024a; Yu et al., 2021a), as well as prompt- and context-based defenses that steer generation away from target content (Thaker et al., 2024b; Liu et al., 2024a; Pawelczyk et al., 2023). These approaches are often lightweight and deployment-friendly, but they may deflect rather than remove information, and auxiliary mechanisms can themselves become new loci of leakage if they encode or expose the content slated for dele-tion. Currently, such techniques seem to be less capable than unlearning loss based ones. 

Weight-level interventions and parameter editing. This approach performs localized parameter editing or model merging to overwrite specific associations (Chen and Yang, 2023; Ilharco et al., 2022; Barbulescu and Triantafillou, 2024; Meng et al., 2022; 2023). Such methods can pro-vide fine-grained control and fast updates, and they are increasingly used as primitives for targeted behavioral ed-its. However, editing typically targets a narrow mechanism (e.g., a small set of facts/associations) and may not scale cleanly to distributional “forget sets,” where the loss is to suppress a broader region of behaviors rather than a single mapping. Parameter editing techniques perform worse than unlearning loss approaches. 

Training-time unlearning via parameter updates and loss design. The currently best performing unlearning ap-proaches perform unlearning by updating model parameters via additional training, frequently with parameter-efficient fine-tuning, to reduce the likelihood of producing forget-set targets while preserving performance on retain data. This family includes early privacy-motivated formulations (Jang et al., 2023a) and many loss designs: KL-regularized or distillation-based variants (Wang et al., 2024a; Dong et al., 2025a; Vasilev et al., 2025a), “I don’t know” targets on forget prompts (Maini et al., 2024b), counterfactual fine-tuning that selects alternative answers using memorization signals (Gu et al., 2024a), and loss shaping schemes that operate even with forget-only data (Wang et al., 2024b). Classic likelihood-based primitives such as gradient ascent on the forget set (and related gradient-difference formula-tions) remain strong baselines and are frequently used as building blocks (Liu et al., 2022; Maini et al., 2024b; Bar-bulescu and Triantafillou, 2024; Thudi et al., 2022; Yao et al., 2024b). More recent work explores preference-style losses inspired by alignment (DPO/NPO and simplifica-tions) (Rafailov et al., 2023; Zhang et al., 2024a; Fan et al., 2024a; Mekala et al., 2025a), stronger optimization proce-dures (Jia et al., 2024a), and principled analyses/unifications of loss families (Wang et al., 2025b) as well as methods that explicitly manage the retention/forgetting trade-off (Wang et al., 2025d). In parallel, unlearning updates inherit the optimization sensitivity of modern fine-tuning. Recent stud-ies and methods further investigate optimization choices and stabilization strategies for unlearning-style losses (Choi et al., 2025a; Foret et al., 2021; Bhaila et al., 2025a). 

Exact/certified unlearning and formal perspectives. Be-yond empirical post-training updates, this line of work stud-ies conditions under which deletion can be made exact or otherwise formally controlled, and when forgetting may be fundamentally difficult. Work on exact/fast deletion proce-dures and on limits of learnability/unlearnability provides complementary perspectives on what can be guaranteed (and what cannot) under practical constraints (Muresanu et al., 2024a; 2025). 

Robustness, reversibility, relearning, and evaluation brit-tleness. Across training-time methods, a recurring chal-lenge is the tension between strong forgetting and over-unlearning, where utility degrades as forgetting pressure increases (Yang et al., 2025a; Tian et al., 2024a). Multi-ple works further highlight that post-hoc unlearning can be reversible: benign fine-tuning may restore seemingly forgotten data, and unlearning can behave more like obfus-cation than deletion under realistic threat models (Xu et al., 2025b; Hu et al., 2024a). Closely related, recent work stud-ies relearning dynamics and how quickly forgotten data can return under subsequent training (Xu et al., 2025c). Robust-ness can also fail unexpectedly. For instance, downstream quantization can significantly undermine unlearning effects (Zhang et al., 2025a). These concerns motivate practical/ro-bust procedures and primitives (Yu et al., 2025a; Xu et al., 2025a) and continued scrutiny of evaluation protocols and privacy-leakage measurements (Rashid et al., 2025; Chun-dawat et al., 2024a). More broadly, several recent efforts aim to better characterize what unlearning changes internally and how evaluation should be interpreted across regimes (Yuan et al., 2025a; Wei et al., 2025; Wang et al., 2025a; Fan et al., 2025a; Cha et al., 2024a; Yuan et al., 2024a; Jin et al., 2024a; Zhuang et al., 2025a; Di et al., 2024a). Our method is motivated by the practical implication of these findings: loss choice is often the dominant lever shaping fail-ure modes (under-forgetting vs. over-unlearning), and thus loss design should adapt to the structure of the unlearning instance. 

2.2. AI in Scientific Discovery and verification-first search. 

The idea of automating parts of the scientific process long predates modern deep learning, with early systems focusing on symbolic search and heuristic-driven hypothesis forma-3EvoMU: Evolutionary Machine Unlearning loss 1

> Candidate losses
> ...
> loss N
> LLM Proposer
> generate candidates
> Training
> for specified steps
> Top-K
> select best losses
> Loss functions definitions
> Training history
> Forget/Retain Performance
> LLM mutation
> training steps / parameters / concept
> loss 1
> Mutated losses
> ...
> loss M
> ×R

Figure 2. EvoMU overview. We use an LLM Proposer to generate an initial set of unlearning losses. Each loss is evaluated by training an LLM with it. The top-K candidates are retained and mutated based on the obtained results. The mutation loop is repeated R times. 

tion and validation (Langley et al., 1987). More recently, AI-assisted discovery has broadened substantially, spanning literature-driven hypothesis generation, experimental plan-ning, and closed-loop optimization pipelines (Wang et al., 2023b; Liu et al., 2024b). With the emergence of strong code-capable LLMs, several works have explored agentic or end-to-end discovery loops in which models propose ideas, implement experiments, and iteratively refine based on results (Schmidgall et al., 2025a; Jansen et al., 2025a; Yamada et al., 2025a; Lu et al., 2024a). A recurring limita-tion of open-ended automated science is that losses can be underspecified and evaluations difficult to verify automati-cally, which makes progress heavily dependent on expensive human feedback or brittle proxies. ASD is easier to apply for well-specified discovery problems with automatic, reproducible evaluation, such as algorithm or loss design. For example, AlphaTensor demonstrates that reinforcement-learning-driven search can discover improved matrix multiplication algorithms under a precise scoring rule (Fawzi et al., 2022), while Eureka shows that LLMs can syn-thesize reward functions that effectively drive reinforcement-learning agents, again under clear downstream evaluation (Ma et al., 2023). Recent works include LLMs (Cheng et al., 2025b) for neural network design and (Gottweis et al., 2025a) for various scientific computing tasks. Our work falls in this category, but focuses on unlearning loss de-sign. In contrast to prior ASD work that primarily relies on closed-source or very large LLMs, we show that a small 4B-parameter model, when paired with evolutionary search, can already perform effective automated scientific discovery. 

## 3. Method 

We study machine unlearning as a loss design problem: given a forget set Df and a retain set Dr , we seek a train-ing loss that reduces the model’s likelihood of producing forbidden completions from Df while preserving utility on 

Dr . Our approach EvoMU performs an evolutionary search over executable loss functions. At each iteration, a code-generating LLM proposes candidate losses, we optimize each loss by fine-tuning with LoRA adapters, evaluate the resulting checkpoints using standardized unlearning metrics, and use the best-performing losses to seed the next round of refinements, mutating the best K losses. 

Problem Setup We study machine unlearning for an au-toregressive language model pθ . We are given a forget dataset (content that should be forgotten) Df = {(xfi , y fi ) : 

i = 1 , . . . , N f } and a retain dataset (content that should be preserved) Dr = {(xri , y ri ) : i = 1 , . . . , N r }, where xf and 

xr are the forget and retain prompts and yf and yr are the respective responses. Our goal is to update model parame-ters θ such that the model’s behavior on Df becomes less probable while utility on Dr is maintained. For an input sequence s = ( x, y ) (prompt x and response 

y), we compute the average log-probability: 

log pθ (y | x) ≜ 1

|y|

> |y|

X

> t=1

log pθ (yt | x, y <t ) . (1) We use the following average log-probs at each optimiza-tion step: zf ∈ RB come from the forget batch and 

zr ∈ RB from a retain batch evaluated by the current model. Correspondingly zref denote reference-model log-probabilities. A candidate unlearning loss is a scalar func-tion L(zf , zr , zref  

> f

, zref  

> r

) .

Candidate loss space and constraints. We constrain our search space to be all python functions that take the four input parameters as in (3) and return a single scalar value, implementing a differentiable pytorch method. Addition-ally, in the doc-string we require the number of epochs the loss is optimized to be specified. To keep the discovery set-ting verifiable and efficient, candidates may only use these statistics (no hidden states, gradients, sampling, or external calls). Invalid candidates (exceptions, non-finite outputs, or missing required signature/docstring fields) are discarded. 

LLM for Proposing and Mutating Losses We use an LLM to initially propose candidate loss functions and num-ber of iterations to optimize and then, in the evolutionary search phase, to mutate these. For greater performance we use a thinking scratchpad, while the final output will contain python code for the produced loss function. 4EvoMU: Evolutionary Machine Unlearning 

Table 1. Results on TOFU-5% (LLaMA2-7B-Chat). Colors indicate rank (red: best, orange: second, yellow: third).                                                                                                                          

> Unlearning Efficacy Utility Preservation Method Forget Set Real Authors World Facts Retain Set MU ( ↑)1-Rouge-L ↑1-Prob. ↑1-Extr. Strength ↑Rouge-L ↑Prob. ↑Truth ratio ↑Rouge-L ↑Prob. ↑Truth ratio ↑Rouge-L ↑Prob. ↑Truth ratio ↑
> Original 0.04 0.01 0.05 0.93 0.44 0.58 0.91 0.43 0.55 0.98 0.99 0.48 0.62 Retain 0.61 0.85 0.93 0.92 0.44 0.57 0.90 0.43 0.54 0.97 0.99 0.48 0.62 GradDiff 1.00 1.00 0.96 0.59 0.59 0.81 0.88 0.46 0.59 0.42 0.49 0.48 0.56 IDKDPO 0.98 0.40 0.85 0.65 0.48 0.63 0.82 0.44 0.55 0.55 0.86 0.57 0.57 RKLD 0.69 0.96 0.92 0.92 0.47 0.61 0.87 0.47 0.58 0.58 0.52 0.56 0.56 NPO 0.73 0.94 0.90 0.91 0.50 0.62 0.90 0.50 0.61 0.47 0.51 0.57 0.57 SimNPO 0.74 0.97 0.92 0.90 0.50 0.64 0.90 0.48 0.60 0.54 0.56 0.58 0.58 EvoMU 1.00 0.99 0.97 0.89 0.50 0.65 0.89 0.47 0.60 0.90 0.95 0.46 0.65

In order to encourage more diversity, we use a higher tem-perature for the thinking scratchpad. To avoid errors in distilling the loss function from the scratchpad we use a lower temperature in this part. We discard duplicates that were proposed by different invocations. We also remove function candidates that are not executable. Also, we try to repair functions that were not generated fully correctly, e.g. if a function returns multiple return values we average them to make the repaired function scalar-valued and we replace numpy function calls by their pytorch equivalents. The LLM Proposer prompt is included in Appendix J and exemplary initial losses can be found in Appendix E. 

Training and Evaluation For each candidate loss L, we fine-tune the base model using LoRA adapters (Hu et al., 2022a), keeping the backbone weights frozen. Each loss function specifies its own training budget via an epochs: iter docstring, allowing the search to explore both losses and training duration. After training, we merge the learned LoRA weights into the base model to obtain a standalone checkpoint, and evaluate it with an external unlearning benchmark suite. This evaluation returns a number of met-rics m(L) that captures both forgetting efficacy on Df and utility preservation on Dr (and additional held-out slices when available). To rank candidates during search, we compute a scalar se-lection score 

s(L) = 12 Utility( m(L)) + 12 Forget( m(L)) , (2) where Forget( ·) averages the available normalized forget-ting terms (e.g., 1−ROUGE f and 1−Prob f ) and Utility( ·)

is a normalized utility summary returned by the evaluator. Any candidate that fails at generation, training, or evaluation is assigned zero score and is excluded from top-K selection. 

Mutation At each mutation iteration, we select the top-

K candidates under s(·) as parents. For each parent, we provide the LLM with (i) the loss code, (ii) the per-epoch training loss history, (iii) the chosen epoch budget, and (iv) the full evaluation metrics. Conditioned on this feed-back, the LLM proposes a set of mutated children losses, which may (a) adjust coefficients, margins, or nonlineari-ties to better trade off forgetting and utility, (b) introduce or remove reference-model terms, and/or (c) modify the recommended number of epochs. We then repeat the same train/merge/evaluate procedure for the children and con-tinue the evolutionary loop. The prompt for loss function refinement is included in Appendix K. 

## 4. Experiments 

We fine-tune each base model using LoRA adapters with rank r = 8 and scaling α = 16 . For each candidate loss proposed by EvoMU, the training budget is read directly from the loss function metadata: we use a variable number of epochs for TOFU and MUSE, and a variable number of optimization steps for WMDP. We evaluate EvoMU against established unlearning base-lines on TOFU-5% and TOFU-10% (Maini et al., 2024b), MUSE (Shi et al., 2024a) and WMDP (Li et al., 2024). Across all experiments, the LLM Proposer and Mutator is Qwen3-4B-Thinking (Yang et al., 2025b), while the un-learned (target) model depends on the benchmark. All checkpoints are evaluated using the Open-Unlearning evalu-ation protocol (Dorna et al., 2025a). The specific forgetting and utility metrics vary by benchmark (as is standard in machine unlearning). A summary of the target models, tasks, and metrics for unlearning effectiveness and utility preservation is provided in Appendix F. All training runs are performed on two NVIDIA A100 GPUs (40GB). For the evolutionary search, we sample N = 10 

initial candidate losses, select the top-5 according to the selection score, and generate 5 refinements per parent (yield-ing 25 candidates). We then select the top-3 among these and generate 10 additional refinements per parent. In total, our evolutionary search tests 65 losses in each run. EvoMU runs for 10 to 20 hours depending on the benchmark. We perform thinking stage decoding with temperature 0.6 and 5EvoMU: Evolutionary Machine Unlearning 

implementation with 0.2. 

4.1. Baselines 

In this section we briefly describe the algorithms we com-pare EvoMU against. • GA (Maini et al., 2024b; Thudi et al., 2022): Gradient Ascent fine-tunes the model to decrease likelihood on the forget set, pushing the model away from forbidden completions. • RMU (Li et al., 2024): Representation Misdirection for Unlearning steers latent representations of forget tokens toward a fixed random vector while regularizing retain tokens to match the original model’s representations. • RKLD (Wang et al., 2024a): Reverse KL-Divergence-based Knowledge Distillation performs unlearning by training a student model with a reverse-KL loss on for-get queries so that it imitates a privacy-sanitized teacher distribution. • AltPO (Mekala et al., 2025a): Alternate Preference Opti-mization applies a DPO-style preference loss over pairs of original and alternate answers on forget prompts . • GradDiff (Liu et al., 2022; Maini et al., 2024b; Yao et al., 2024a): Gradient Difference minimizes a loss equal to the retain loss minus a scaled forget loss so that gradi-ent descent corresponds to ascent on forget samples and descent on retain samples. • TaskVector (Ilharco et al., 2022): Task arithmetic con-structs a task vector as the parameter difference between a fine-tuned model and the base model, and applies or subtracts this vector to edit behavior. • IDKDPO / IDKNLL (Maini et al., 2024b): Train the model to answer forget prompts with “I don’t know”, us-ing a DPO-style preference loss (IDKDPO) or a standard NLL loss on the IDK target (IDKNLL). • UNDIAL (Dong et al., 2025a): Performs self-distillation with adjusted logits by subtracting a one-hot vector for target tokens from the teacher logits on forget data and training a student model to match these modified logits. • NPO (Zhang et al., 2024a): Negative Preference Opti-mization uses a DPO-like negative-preference loss that contrasting log-probabilities of undesirable forget re-sponses under the unlearned and reference models. • SimNPO (Fan et al., 2024a): SimNPO simplifies NPO by removing explicit dependence on a reference model and reweighting gradients in a simpler preference-optimization loss applied to forget data. 

4.2. Results TOFU. TOFU (Task of Fictitious Unlearning for LLMs) (Maini et al., 2024b) is a biographical question– answering benchmark designed to test selective forgetting. TOFU consists of a forget split containing prompts whose underlying biographical facts should be removed, and a re-tain split containing similar fictitious-author questions that should remain answerable. The TOFU-x% setting specifies that we unlearn x% of the authors while preserving perfor-mance on the remaining (1 − x)% . Additionally, TOFU evaluates collateral damage using two out-of-distribution general-knowledge subsets, Real Authors and World Facts. We report unlearning efficacy on the forget split using ROUGE-L (Lin, 2004), Prob, and extraction strength (Car-lini et al., 2021), and utility preservation on the retain split and the two general-knowledge subsets. Prob denotes the average teacher-forced probability assigned to the ground-truth answer. We report 1 − Prob on the forget split and Prob on the utility splits. Following prior work, we summa-rize the overall forgetting-utility trade-off using the TOFU model-utility (MU) score. We evaluate two settings, TOFU-5% and TOFU-10%, following (Fan et al., 2024a). As shown in Table 1, on TOFU-5% EvoMU achieves near-saturated forgetting while maintaining strong utility on re-tain, Real Authors, and World Facts, yielding the best MU among the compared methods. In Table 2 we see that TOFU-10% is substantially more challenging: several baselines over-unlearn and incur large utility drops (e.g., IDKDPO and UNDIAL), while others preserve utility but under-forget (e.g., RMU), while EvoMU provides highest MU. Addi-tional experimental details are provided in Appendix G. 

MUSE. MUSE (Shi et al., 2024a) targets long-form un-learning where models may leak verbatim passages and underlying facts. MUSE includes two corpora (News and Books) with qualitatively different forgetting scenarios. MUSE News consists of BBC news articles that are split into disjoint forget, retain, and holdout sets. This setting evaluates whether an unlearning method can remove specific articles at scale while preserving performance on other news articles. MUSE Books uses the Harry Potter book series as the forget set, while the retain set is constructed from Harry Potter FanWiki pages. This setting evaluates a harder, entangled scenario: the model should stop reproducing copy-righted book text and facts learned from it, but should still answer questions about closely related in-domain material that remains permissible. We report three unlearning metrics on Dforget : VerbMem, KnowMem, and PrivLeak. We use KnowMem on Dretain 

as the utility measure. VerbMem measures verbatim regur-gitation of the forget text, KnowMem measures whether the model still reproduces the knowledge contained in the forget corpus, and PrivLeak measures membership leakage using the holdout set as a non-member reference distribution (holdout is separate from the retain set). Additional metric details are provided in Appendix H. 6EvoMU: Evolutionary Machine Unlearning 

Table 2. Results on TOFU-10%. Colors indicate rank (red: best, orange: second, yellow: third). We unlearn Llama3.2-1B-Instruct.                                                                                                                                                                              

> Unlearning Efficacy Utility Preservation Method Forget Set Real Authors World Facts Retain Set MU ( ↑)1-Rouge-L ↑1-Prob. ↑1-Extr. Strength ↑Rouge-L ↑Prob. ↑Truth ratio ↑Rouge-L ↑Prob. ↑Truth ratio ↑Rouge-L ↑Prob. ↑Truth ratio ↑
> Original 0.18 0.12 0.29 0.80 0.41 0.53 0.83 0.44 0.62 0.79 0.87 0.52 0.60 Retain 0.62 0.88 0.94 0.83 0.39 0.50 0.80 0.43 0.62 0.83 0.88 0.51 0.59 RMU 0.50 0.39 0.72 0.75 0.42 0.52 0.83 0.43 0.60 0.61 0.74 0.51 0.57 AltPO 0.66 0.93 0.95 0.78 0.42 0.54 0.80 0.43 0.61 0.61 0.75 0.47 0.57 GradDiff 0.42 0.35 0.67 0.73 0.40 0.53 0.84 0.42 0.62 0.79 0.88 0.53 0.59 IDKDPO 0.87 1.00 0.96 0.41 0.42 0.53 0.68 0.43 0.58 0.62 0.75 0.50 0.52 IDKNLL 0.98 0.45 0.74 0.70 0.39 0.49 0.73 0.43 0.57 0.66 0.78 0.49 0.55 UNDIAL 0.69 0.82 0.96 0.50 0.38 0.48 0.78 0.41 0.56 0.56 0.61 0.46 0.51 NPO 0.61 0.71 0.91 0.76 0.41 0.52 0.79 0.43 0.60 0.66 0.78 0.50 0.57 SimNPO 0.65 0.94 0.94 0.78 0.42 0.53 0.83 0.45 0.61 0.56 0.71 0.48 0.56 EvoMU 0.96 1.00 0.97 0.60 0.44 0.58 0.81 0.44 0.62 0.60 0.77 0.52 0.58 EvoMU (TOFU-5%) 0.77 1.00 0.92 0.60 0.43 0.56 0.80 0.43 0.63 0.67 0.81 0.52 0.58

Results are shown in Table 3. On MUSE News, EvoMU sub-stantially reduces privacy leakage while matching the best retain-set utility On MUSE Books, several baselines achieve low memorization on the forget set but severely degrade retain utility, whereas EvoMU preserves much higher re-tain performance while reducing memorization of the forget corpus. 

WMDP. WMDP (Li et al., 2024) focuses on suppressing hazardous knowledge. Forgetting is measured as a drop in performance on a safety-critical question set. We follow the evaluation protocol of (Fan et al., 2024a). As the forgetting metric, we measure accuracy on WMDP-Bio and report 1−AccBio , where larger values indicate stronger suppression of the hazardous slice. As the utility metric we report accuracy on MMLU (Hendrycks et al., 2020) to as-sess whether broad general knowledge is retained. Results are shown in Table 4. Several baselines achieve substan-tial suppression with degraded utility. EvoMU improves upon NPO in utility while matching or improving forget-ting. EvoMU substantially improves over the best baseline SimNPO in utility at the same level of forgetting. All proposed loss functions can be found in Appendix B. We also include discussions on why we found those losses relevant per each dataset in Appendix C. 

Hyperparameter search of existing methods. To show that EvoMU does not merely tune existing baselines but syn-thesizes new loss functions, we take the strongest existing loss (SimNPO) and fit hyperparameters via an extensive grid search on TOFU-10%. Results are reported in Appendix N. Across the searched configurations, SimNPO spans a range of forgetting-utility trade-offs, confirming that careful tun-ing can shift the operating point. However, even the best tuned SimNPO runs remain substantially below EvoMU in overall performance. This suggests that the gains of EvoMU are not primarily attributable to better hyperparameters of 

Table 3. Performance of unlearning methods on MUSE News (LLaMA2-7B) and MUSE Books (ICLM-7B). 

Unlearning Efficacy Utility Method VerbMem 

Df (↓)KnowMem 

Df (↓)PrivLeak (→ 0)KnowMem 

Dr (↑)

MUSE News 

Original 58.29 62.93 -98.71 54.31 Retain 20.75 33.32 0.00 53.79 GA 0.00 0.00 20.14 0.00 GradDiff 4.85 31.29 108.12 28.21 Task Vector 77.42 58.76 -100.00 47.94 NPO 2.53 56.93 108.91 37.58 SimNPO 2.34 44.84 72.93 39.65 EvoMU 18.56 51.24 4.38 47.98 EvoMU (TOFU-5%) 42.24 64.19 -99.78 50.51 

MUSE Books 

Original 99.56 58.32 -56.32 67.01 Retain 14.30 28.90 0.00 74.50 GA 0.00 0.00 -24.07 0.00 GradDiff 0.00 0.00 -24.59 0.13 Task Vector 99.31 35.55 -83.78 62.55 NPO 0.00 0.00 -31.17 23.71 SimNPO 0.00 0.00 -19.82 48.27 EvoMU 0.00 7.33 -27.34 53.58 EvoMU (TOFU-5%) 40.01 28.22 -52.68 46.83 

an existing loss, but to the loss structure itself. 

Relearning. Recent work suggests that many unlearning procedures are reversible: fine-tuning on a small amount of the forgotten data can rapidly restore suppressed capabil-ities or memorized content (Wang et al., 2025a; Xu et al., 2025b; Hu et al., 2024a; Fan et al., 2025a; Xu et al., 2025c). Following the relearning protocol of (Fan et al., 2024a), we test how quickly hidden WMDP-Bio knowledge returns after unlearning. Concretely, starting from the unlearned checkpoints produced by SimNPO, NPO, and EvoMU, we sample 20% of the forget set and fine-tune each model for 1000 steps on this subset with learning rate 10 −5. We report the forgetting score 1 − Acc WMDP-Bio throughout training in 7EvoMU: Evolutionary Machine Unlearning 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1         

> 0.54
> 0.56
> 0.58
> 0.6
> 0.62
> 0.64
> 0.66
> N=1
> N=50
> SimNPO
> NPO
> EvoMU
> IDK
> (1 - extraction strength)
> Model utility 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
> 0.25
> 0.3
> 0.35
> 0.4
> 0.45
> 0.5
> 0.55
> 0.6
> 0.65
> N=1 N=50
> SimNPO UnDIAL
> NPO EvoMU
> GradDiff
> (1 - WMDP-Bio acc.)
> MMLU acc.

Figure 3 (a) Left : TOFU-5% random-loss sampling ablation and Right : WMDP random-loss sampling ablation. In both plots, points denote baseline unlearning methods. The curve connects the best-performing randomly sampled losses from the LLM Proposer as the number of sampled loss functions N increases (shown for N ∈ { 1, 10 , 15 , 25 , 50 },with endpoints annotated). Better performance lies in the top-right region (higher utility and stronger forgetting). 100 200 300 400 500 600 700 800 900 1,000    

> 0.25
> 0.35
> 0.45
> 0.55
> 0.65
> 0.75
> Steps
> 1 − WMDP -Bio  Acc.
> Original SimNPO EvoMU NPO

Figure 3 (b) Relearning on WMDP. We com-pare EvoMU to SimNPO and NPO by fine-tuning each unlearned model. The y-axis reports forgetting as 1 − Acc WMDP-Bio 

Figure 3b. Consistent with prior observations (Fan et al., 2024a), both SimNPO and NPO lose forgetting rapidly as fine-tuning proceeds. In contrast, EvoMU remains nearly flat for roughly the first 800 steps, suggesting substantially stronger resistance to relearning. After sufficient updates, all methods eventually converge to similar levels. This ex-periment indicates that EvoMU forgets unlearned data more thoroughly. 

4.3. Ablations Generalization of loss functions. We might suspect that EvoMU discovers benchmark-specific losses and may there-fore overfit to a particular evaluation protocol. To probe cross-benchmark generalization, we take the loss discov-ered on TOFU-5% and apply it unchanged to the other settings: TOFU-10%, MUSE News, MUSE Books, and WMDP. We report the resulting performance in Tables 2, 3, and 4 under the label EvoMU (TOFU-5%). We do not modify hyperparameters nor functional form of the TOFU-discovered loss. Overall, the TOFU-5% loss transfers well to TOFU-10% and WMDP, which argues against simple benchmark overfitting. In contrast, performance degrades on MUSE News and MUSE Books. We attribute this gap to the different nature of MUSE (long-form memorization, membership leakage) that makes different losses necessary. 

Without Evolutionary Search. To assess importance of the evolutionary search, we compare against sampling only the initial N candidates from the LLM Proposer for TOFU-5% and WMDP. In Figure 3a we plot best results achieved with different N (from N =1 to N =50 ). Overall MU is significantly better after evolutionary search. Interestingly, random sampling seems to already match previous SotA of our baselines on TOFU-5%. Interestingly, some sampled losses are not sensible, for ex-ample, they increase the log-likelihood of the forget targets, 

Table 4. Results on the WMDP (Llama-3-8B-Instruct). Forgetting is measured by 1 − AccBio on WMDP-Bio and utility by accuracy on MMLU. 

Unlearning Efficacy Utility Preservation Method 1 − AccBio ( ↑) MMLU ( ↑)

Original 0.27 0.65 UnDIAL 0.65 0.45 GradDiff 0.73 0.26 IDKNLL 0.66 0.47 IDKDPO 0.67 0.44 NPO 0.73 0.51 SimNPO 0.75 0.44 EvoMU 0.75 0.53 EvoMU (TOFU-5%) 0.67 0.62 

in contradiction with the unlearning goal. We provide rep-resentative examples in Appendix L. Notably, such patho-logical losses do not appear among the final-generation candidates: the evolutionary selection process consistently filters them out. Additional ablation experiments ( # thinking tokens, impact of proposing # of epochs to train, robustness w.r.t. seeds) can be found in Appendix M. 

## 5. Conclusions & Limitations 

We have proposed an evolutionary search procedure that automatically synthesizes dataset-specific unlearning losses using LLMs as co-scientists. We use comparatively little compute to come up with losses, yet outperform existing ones proposed by deep learning experts. More broadly, we argue that it might be more advantageous to synthesize specific components of deep learning systems that can be evaluated than to design them by hand. We conjecture that in many applications similar to machine unlearning the design space might be large enough so that a search procedure such as ours will be able to discover new and superior solutions. 8EvoMU: Evolutionary Machine Unlearning 

## Impact Statement 

Our work EvoMU makes machine unlearning more practi-cal by automatically discovering task-specific fine-tuning losses that better balance targeted forgetting with retained capability, reducing the need for brittle hand-tuning and potentially lowering the cost of responding to data-deletion requests or mitigating unwanted memorization (e.g., sensi-tive, harmful, or copyrighted content) without full retraining; additionally, because the discovered loss code and evalua-tion results are explicit artifacts, the process can be more auditable and reproducible than ad hoc loss design. At the same time, responsible use still requires careful eval-uation and access controls, since unlearning methods can be dual-use (e.g., repurposed to remove safety behaviors) and benchmark-driven optimization can give misleading confidence if metrics do not capture real deployment threat models, so we view EvoMU as a tool that strengthens the unlearning workflow when paired with rigorous, transparent evaluation. 

## Acknowledgments 

The authors gratefully acknowledge the funding of this project by computing time provided by the Paderborn Center for Parallel Computing (PC2). 

## References 

Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, and oth-ers. The WMDP benchmark: Measuring and reduc-ing malicious use with unlearning. arXiv preprint arXiv:2403.03218 , 2024. Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine un-learning. In Proceedings of the IEEE Symposium on Security and Privacy (IEEE S&P 2021) , 2021. Antonio Ginart, Melody Y. Guan, Gregory Valiant, and James Zou. Making AI forget you: Data deletion in machine learning. In Advances in Neural Information Processing Systems (NeurIPS 2019) , 2019. Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In Proceedings of the IEEE Symposium on Security and Privacy (IEEE S&P 2015) , 2015. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Er-mon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Advances in Neural Information Pro-cessing Systems (NeurIPS 2023) , 2023. Chongyang Gao, Lixu Wang, Kaize Ding, Chenkai Weng, Xiao Wang, and Qi Zhu. On large language model contin-ual unlearning. arXiv preprint arXiv:2407.10223 , 2024. Md Rafi Ur Rashid, Jing Liu, Toshiaki Koike-Akino, Ye Wang, and Shagufta Mehnaz. Forget to flourish: Leverag-ing machine-unlearning on pretrained language models for privacy leakage. In Proceedings of the AAAI Con-ference on Artificial Intelligence , 39(19):20139–20147, 2025. Huu-Tien Dang, Tin Pham, Hoang Thanh-Tung, and Naoya Inoue. On effects of steering latent representation for large language model unlearning. In Proceedings of the AAAI Conference on Artificial Intelligence , 39(22):23733– 23742, 2025. Hongbang Yuan, Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. Towards robust knowledge unlearning: An adversarial framework for assessing and improving unlearning robustness in large language mod-els. In Proceedings of the AAAI Conference on Artificial Intelligence , 39(24):25769–25777, 2025. Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, and others. Do LLMs really forget? Evaluating unlearning with knowledge correlation and confidence awareness. arXiv preprint arXiv:2506.05735 , 2025. Yize Sui, Jing Ren, Wenjing Yang, Ruochun Jin, Liyang Xu, Xiyao Liu, and Ji Wang. Elastic robust unlearning of specific knowledge in large language models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. Abudukelimu Wuerkaixi, Qizhou Wang, Sen Cui, Wutong Xu, Bo Han, Gang Niu, Masashi Sugiyama, and Chang-shui Zhang. Adaptive localization of knowledge nega-tion for continual LLM unlearning. In Proceedings of the Forty-second International Conference on Machine Learning , 2025. Puning Yang, Qizhou Wang, Zhuo Huang, Tongliang Liu, Chengqi Zhang, and Bo Han. Exploring criteria of loss reweighting to enhance LLM unlearning. arXiv preprint arXiv:2505.11953 , 2025. Changsheng Wang, Yihua Zhang, Jinghan Jia, Parikshit Ram, Dennis Wei, Yuguang Yao, Soumyadeep Pal, Nathalie Baracaldo, and Sijia Liu. Invariance makes LLM unlearning resilient even to unanticipated down-stream fine-tuning. arXiv preprint arXiv:2506.01339 ,2025. 9EvoMU: Evolutionary Machine Unlearning 

Jiali Cheng and Hadi Amiri. Tool unlearning for tool-augmented LLMs. arXiv preprint arXiv:2502.01083 ,2025. Swanand Ravindra Kadhe, Farhan Ahmed, Dennis Wei, Nathalie Baracaldo, and Inkit Padhi. Split, unlearn, merge: Leveraging data attributes for more effective un-learning in LLMs. arXiv preprint arXiv:2406.11780 ,2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-suring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020. Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, and Sijia Liu. SOUL: Unlocking the power of second-order optimization for LLM unlearning. arXiv preprint arXiv:2404.18239 , 2024. Pratyush Maini, Hengrui Jia, Nicolas Papernot, and Adam Dziedzic. LLM dataset inference: Did you train on my dataset? Advances in Neural Information Processing Systems , 37:124069–124092, 2024. Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Se-won Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on large language models? arXiv preprint arXiv:2402.07841 , 2024. Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke Zettle-moyer, Noah A. Smith, and Chiyuan Zhang. MUSE: Ma-chine unlearning six-way evaluation for language models. 

arXiv preprint arXiv:2407.06460 , 2024. Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, and oth-ers. The WMDP benchmark: Measuring and reduc-ing malicious use with unlearning. arXiv preprint arXiv:2403.03218 , 2024. Qizhou Wang, Jin Peng, Zhanke Zhou, Saebyeol Shin, Bo Han, and Kilian Q. Weinberger. Rethinking LLM unlearn-ing objectives: A gradient perspective and go beyond. 

arXiv preprint arXiv:2502.19301 , 2025. Miao Yu, Liang Lin, Guibin Zhang, Xinfeng Li, Jun-feng Fang, Ningyu Zhang, Kun Wang, and Yang Wang. UniErase: Unlearning token as a universal era-sure primitive for language models. arXiv preprint arXiv:2505.15674 , 2025. Xiaoyu Xu, Minxin Du, Qingqing Ye, and Haibo Hu. OBLIVIATE: Robust and practical machine un-learning for large language models. arXiv preprint arXiv:2505.04416 , 2025. Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Haibo Hu, and Minxin Du. Unlearning isn’t deletion: Investigat-ing reversibility of machine unlearning in LLMs. arXiv preprint arXiv:2505.16831 , 2025. Pratiksha Thaker, Yash Maurya, Shengyuan Hu, Zhiwei Steven Wu, and Virginia Smith. Position: LLM unlearn-ing benchmarks are weak measures of progress. arXiv preprint arXiv:2410.02879 (SaTML 2025), 2024. Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu, and Virginia Smith. Unlearning or obfuscating? Jogging the memory of unlearned LLMs via benign relearning. arXiv preprint arXiv:2406.13356 (ICLR 2025), 2024. Jie Ren, Yue Xing, Yingqian Cui, Charu C. Aggarwal, and Hui Liu. SoK: Machine unlearning for large language models. arXiv preprint arXiv:2506.09227 , 2025. Qizhou Wang, Jin Peng Zhou, Zhanke Zhou, Saebyeol Shin, Bo Han, and Kilian Q. Weinberger. Rethinking LLM unlearning objectives: A gradient perspective and go beyond. arXiv preprint arXiv:2502.19301 , 2025. Minseok Choi, Daniel Rim, Dohyun Lee, and Jaegul Choo. Opt-out: Investigating entity-level unlearning for large language models via optimal transport. In Proceedings of the 63rd Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers) , pages 28280–28297, 2025. Yinzhi Cao and Junfeng Yang. Towards making systems for-get with machine unlearning. In 2015 IEEE Symposium on Security and Privacy , pages 463–480. IEEE, 2015. Pratiksha Thaker, Yash Maurya, Shengyuan Hu, Zhiwei Steven Wu, and Virginia Smith. Guardrail baselines for unlearning in LLMs. arXiv preprint arXiv:2403.03329 ,2024. Tianle Gu, Kexin Huang, Ruilin Luo, Yuanqi Yao, Yujiu Yang, Yan Teng, and Yingchun Wang. MEOW: Memory supervised LLM unlearning via inverted facts. arXiv preprint arXiv:2409.11844 , 2024. Bo Liu, Qiang Liu, and Peter Stone. Continual learning and private unlearning. In Conference on Lifelong Learning Agents , pages 243–254. PMLR, 2022. Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. TOFU: A task of fictitious unlearning for LLMs. arXiv preprint arXiv:2401.06121 ,2024. 10 EvoMU: Evolutionary Machine Unlearning 

Ronen Eldan and Mark Russinovich. Who’s Harry Potter? Approximate unlearning for LLMs. 2023. Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for LLMs. arXiv preprint arXiv:2310.20150 , 2023. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Ha-jishirzi, and Ali Farhadi. Editing models with task arith-metic. arXiv preprint arXiv:2212.04089 , 2022. Jiabao Ji, Yujian Liu, Yang Zhang, Gaowen Liu, Ramana R. Kompella, Sijia Liu, and Shiyu Chang. Reversing the forget-retain objectives: An efficient LLM unlearning framework from logit difference. Advances in Neural Information Processing Systems , 37:12581–12611, 2024. James Y. Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, and Muhao Chen. Offset unlearning for large language models. arXiv preprint arXiv:2404.11045 , 2024. Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, and oth-ers. Differentially private fine-tuning of language models. 

arXiv preprint arXiv:2110.06500 , 2021. Bichen Wang, Yuzhe Zi, Yixin Sun, Yanyan Zhao, and Bing Qin. RKLD: Reverse KL-divergence-based knowl-edge distillation for unlearning personal information in large language models. arXiv preprint arXiv:2406.01983 ,2024. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, and others. DecodingTrust: A comprehensive assessment of trustworthiness in GPT models. In Advances in Neural Information Processing Systems (NeurIPS) , 2023. Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, and others. TrustLLM: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561 ,2024. Andrei Muresanu, Anvith Thudi, Michael R. Zhang, and Nicolas Papernot. Unlearnable algorithms for in-context learning. arXiv preprint arXiv:2402.00751 , 2024. Yaxuan Wang, Jiaheng Wei, Chris Yuhao Liu, Jinlong Pang, Quan Liu, Ankit Parag Shah, Yujia Bao, Yang Liu, and Wei Wei. LLM unlearning via loss adjustment with only forget data. arXiv preprint arXiv:2410.11143 , 2024. Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations , 2021. Chongyu Fan, Jinghan Jia, Yihua Zhang, Anil Ramakrishna, Mingyi Hong, and Sijia Liu. Towards LLM unlearn-ing resilient to relearning attacks: A sharpness-aware minimization perspective and beyond. arXiv preprint arXiv:2502.05374 , 2025. Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang, Dianbo Sui, Xi Chen, Huajun Chen, and Ningyu Zhang. To forget or not? Towards practical knowledge unlearning for large language models. arXiv preprint arXiv:2407.01920 , 2024. George-Octavian Barbulescu and Peter Triantafillou. To each (textual sequence) its own: Improving memorized-data unlearning in large language models. arXiv preprint arXiv:2405.03097 , 2024. Sungmin Cha, Sungjun Cho, Dasol Hwang, and Moontae Lee. Towards robust and parameter-efficient knowledge unlearning for LLMs. arXiv preprint arXiv:2408.06621 ,2024. Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, and Min Lin. A closer look at machine unlearning for large language models. arXiv preprint arXiv:2410.08109 , 2024. Chris Liu, Yaxuan Wang, Jeffrey Flanigan, and Yang Liu. Large language model unlearning via embedding-corrupted prompts. Advances in Neural Information Pro-cessing Systems , 37:118198–118266, 2024. Kevin Meng, David Bau, Alex J. Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems , 2022. Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. In International Conference on Learning Representations , 2023. Chin-Yew Lin. ROUGE: A package for automatic evalua-tion of summaries. In Text Summarization Branches Out ,pages 74–81, 2004. Vineeth Dorna, Anmol Mekala, Wenlong Zhao, Andrew Mc-Callum, Zachary C. Lipton, J. Zico Kolter, and Pratyush Maini. OpenUnlearning: Accelerating LLM unlearning via unified benchmarking of methods and metrics. arXiv preprint arXiv:2506.12618 , 2025. 11 EvoMU: Evolutionary Machine Unlearning 

Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) , 2021. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand `es, and Tatsunori B. Hashimoto. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing , pages 20286–20332, 2025. Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling SGD: Understanding factors influencing machine unlearning. In 2022 IEEE 7th Eu-ropean Symposium on Security and Privacy (EuroS&P) ,pages 303–319. IEEE, 2022. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and others. LoRA: Low-rank adaptation of large lan-guage models. In International Conference on Learning Representations , 2022. Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. Advances in Neural Information Pro-cessing Systems , 37:105425–105475, 2024. Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. In Advances in Neural Information Processing Systems , 2024. NeurIPS, arXiv:2310.10683. Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse to effective unlearning. In Proceedings of the Conference on Language Modeling (COLM) , 2024. arXiv:2404.05868. Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language models as few shot un-learners. arXiv preprint arXiv:2310.07579 , 2023. Chris Jay Hoofnagle, Bart Van Der Sloot, and Frederik Zuiderveen Borgesius. The European Union general data protection regulation: What it is and what it means. Infor-mation & Communications Technology Law , 28(1):65–98, 2019. Jeffrey Rosen. The right to be forgotten. Stan. L. Rev. Online , 64:88, 2011. Andrei Ioan Muresanu, Anvith Thudi, Michael R. Zhang, and Nicolas Papernot. Fast exact unlearning for in-context learning data for LLMs. In Proceedings of the 42nd International Conference on Machine Learning , volume 267 of Proceedings of Machine Learning Research , pages 45272–45288. PMLR, 13–19 Jul 2025. Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao Liu, Xiaojun Xu, Hang Li, and others. Rethinking machine unlearning for large language models. Nature Machine Intelligence , pages 1–14, 2025. Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, and Sijia Liu. Simplicity prevails: Rethinking negative preference optimization for LLM unlearning. arXiv preprint arXiv:2410.07163 , 2024. Alhussein Fawzi, Matej Balog, A. Huang, Thomas Hubert, and others. Discovering faster matrix multiplication algo-rithms with reinforcement learning. Nature , 610(7930), 2022. doi:10.1038/s41586-022-05172-4. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. 

arXiv preprint arXiv:2310.12931 , 2023. Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using LLM agents as research assistants. arXiv preprint arXiv:2501.04227 ,2025. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Sian-gliulue, Tom Hope, Bhavana Dalvi, Bodhisattwa Prasad Majumder, Daniel S. Weld, and Peter Clark. CodeScien-tist: End-to-end semi-automated scientific discovery with code-based experimentation. In Findings of the Associ-ation for Computational Linguistics: ACL 2025 , pages 13370–13467, 2025. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The AI scientist-v2: Workshop-level automated scien-tific discovery via agentic tree search. arXiv preprint arXiv:2504.08066 , 2025. Kyle Richardson, Vivek Srikumar, and Ashish Sabharwal. Understanding the logic of direct preference alignment through logic. arXiv preprint arXiv:2412.17696 , 2024. Junyan Cheng, Peter Clark, and Kyle Richardson. Lan-guage modeling by language models. arXiv preprint arXiv:2506.20249 , 2025. Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion from machine learning models. In International Conference on Artifi-cial Intelligence and Statistics , pages 2008–2016. PMLR, 2021. 12 EvoMU: Evolutionary Machine Unlearning 

Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292 , 2024. Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal from machine learning models. arXiv preprint arXiv:1911.03030 , 2019. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Pe-ter Van Katwyk, Andreea Deac, and others. Scientific discovery in the age of artificial intelligence. Nature ,620(7972):47–60, 2023. Pat Langley, Herbert A. Simon, Gary L. Bradshaw, and Jan M. Zytkow. Scientific Discovery: Computational Explorations of the Creative Processes . The MIT Press, Cambridge, MA, 1987. Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and Dit-Yan Yeung. Probing toxic content in large pre-trained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) ,pages 4262–4274, 2021. Zijun Liu, Kaiming Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, and Yang Liu. AIGS: Gen-erating science from AI-powered automated falsification. 

arXiv preprint arXiv:2411.11910 , 2024. Boyi Wei, Weijia Shi, Yangsibo Huang, Noah A. Smith, Chiyuan Zhang, Luke Zettlemoyer, Kai Li, and Peter Henderson. Evaluating copyright takedown methods for language models. Advances in Neural Information Pro-cessing Systems , 37:139114–139150, 2024. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations , 2022. Abhiraj R. Mekala and others. Alternate preference opti-mization for unlearning factual knowledge in large lan-guage models. In Proceedings of the 31st International Conference on Computational Linguistics (COLING) ,2025. arXiv:2409.13474. Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, and Ivan Vuli ´c. UNDIAL: Self-distillation with adjusted logits for robust unlearning in large language models. In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies , 2025. arXiv:2402.10052. Zhuoran Jin, Peng Cao, Chen Wang, Zhexuan He, Hao Yuan, Jun Li, Yaliang Chen, Kang Liu, and Jun Zhao. RWKU: Benchmarking real-world knowledge unlearning for large language models. In Advances in Neural Information Processing Systems , 2024. Vikram S. Chundawat, Varun Chandrasekaran, and Nicolas Papernot. Verification of machine unlearning is fragile. In Proceedings of the 41st International Conference on Machine Learning , 2024. Yue Wang, Qizhou Wang, Feng Liu, Wei Huang, Yali Du, Xiaojiang Du, and Bo Han. GRU: Mitigating the trade-off between unlearning and retention for large language mod-els. In Proceedings of the 42nd International Conference on Machine Learning , 2025. Zhexuan Zhang, Feng Wang, Xinyang Li, Zhewei Wu, Xi-anfeng Tang, Hui Liu, Qi He, Wenpeng Yin, and Suhang Wang. Catastrophic failure of LLM unlearning via quan-tization. In International Conference on Learning Repre-sentations , 2025. Karuna Bhaila, Minh-Hao Van, and Xintao Wu. Soft prompt-ing for unlearning in large language models. In Pro-ceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies , 2025. Stefan Vasilev, Christian Herold, Baohao Liao, Seyyed Hadi Hashemi, Shahram Khadivi, and Christof Monz. UniLogit: Robust machine unlearning for LLMs us-ing uniform-target self-distillation. arXiv preprint arXiv:2505.06027 , 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chen-gen Huang, Chenxu Lv, and others. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025. Guangyao Dou, Zheyuan Liu, Qing Lyu, Kaize Ding, and Eric Wong. Avoiding copyright infringement via large language model unlearning. In Findings of the Associa-tion for Computational Linguistics: NAACL 2025 , 2025. Hongteng Xu and others. ReLearn: Unlearning via learning for large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics , 2025. Haomin Zhuang, Yihua Zhang, Kehan Guo, Jinghan Jia, Gaowen Liu, Sijia Liu, and Xiangliang Zhang. SEUF: Is unlearning one expert enough for mixture-of-experts LLMs? In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics , 2025. 13 EvoMU: Evolutionary Machine Unlearning 

Junha Jang and others. Knowledge unlearning for mitigat-ing privacy risks in language models. In Proceedings of the 61st Annual Meeting of the Association for Computa-tional Linguistics , 2023. X. Di and others. Dissecting fine-tuning unlearning in large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing ,2024. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, and others. Towards an AI co-scientist. arXiv preprint arXiv:2502.18864 , 2025. 14 EvoMU: Evolutionary Machine Unlearning 

## A. Pseudocode 

Algorithm 1 EvoMU 

Require: Base model pθ0 ; forget/retain datasets Df , Dr ; generator G; population size N ; iterations R; top-K parents; children per parent C

1: (Optional) Precompute reference caches zref  

> f

, zref  

> r

using pθ0

2: S0 ← Generate( {G (∅)}Ni=1 (initial losses) 

3: B ← ∅ (best-so-far record) 

4: for t = 0 , . . . , R − 1 do 

5: for each candidate loss L ∈ S t do 

6: Parse training budget (e.g., epochs/steps) from L’s metadata 

7: Train LoRA adapters from pθ0 on Df , Dr minimizing L

8: Merge LoRA weights into the base model to obtain checkpoint pθL

9: Evaluate pθL with the unlearning benchmark; extract metrics m(L) and training history H(L)

10: Compute selection score s(L) ← Score( m(L)) 

11: Update best-so-far B ← arg max L′∈B∪{L} s(L′)

12: end for 

13: Pt ← TopK( St; s) (parent selection) 

14: St+1 ← Refine 

S 

> L∈P t

{G (L, m (L), H (L)) }Cj=1 



(refine top-K selection) 

15: end for 

16: return best loss L⋆ and its checkpoint pθL⋆ from B

## B. Obtained loss functions 

In this section we present the loss functions obtained for each benchmark.                                                                                                                                                  

> TOFU5% (7 epochs)
> ℓ(θ) = βE(xf ,y f)∼Df
> h
> log πθ(yf|xf)−log πref (yf|xf)
> i
> +E(xr ,yr )∼Dr
> h
> log πref (yr|xr)−log πθ(yr|xr)
> i
> ,β= 1 .2.
> TOFU10% (8 epochs)
> ℓ(θ) = E(xf ,y f)∼Df ,(xr ,yr )∼Dr
> h
> exp
> 
> log πθ(yf|xf)−log πref (yf|xf)
> 
> −2ρexp
> 
> log πθ(yr|xr)−log πref (yr|xr)
> i
> ,ρ= 0 .3.
> MUSE-Books (8 epochs)
> ℓ(θ) = E(xf ,y f)∼Df ,(xr ,yr )∼Dr
> h
> 0.7 log πθ(yf|xf)−log πθ(yr|xr) + αmax
> 
> log πref (yr|xr)−log πθ(yr|xr),0
> i
> ,α= 0 .3.
> MUSE-News (8 epochs)
> ℓ(θ) = 0 .35 E(xf ,y f)∼Df ,(xr ,yr )∼Dr
> h
> min
> 
> log πθ(yf|xf)−log πθ(yr|xr),1
> i
> .
> WMDP (300 steps)
> ℓ(θ) = E(xf ,y f)∼Df ,(xr ,yr )∼Dr
> h
> 1.5 log πθ(yf|xf)−log πref (yf|xf)− log πθ(yr|xr)−log πref (yr|xr)i
> .

## C. Proposed Loss Functions Discussion 

This section explains why the discovered losses in Appendix B are well matched to each benchmark and why they can yield better forgetting–utility trade-offs than NPO/SimNPO-style losses. A common pattern across all discovered objectives is that they (i) impose explicit pressure to reduce the model’s likelihood on the forget targets, and (ii) simultaneously include an explicit retention term that either increases likelihood on retain targets or prevents retain likelihood from falling below 15 EvoMU: Evolutionary Machine Unlearning 

a reference. This explicit two-sided control is important because most failures in unlearning are not due to insufficient forgetting, but due to collateral utility loss from overly aggressive updates. 

TOFU-5% objective. Proposed loss function: 

ℓ(θ) = β E(xf ,y f )∼Df

h

log πθ (yf | xf )−log πref (yf | xf )

i

+E(xr ,y r )∼Dr

h

log πref (yr | xr )−log πθ (yr | xr )

i

, β = 1 .2.

The TOFU-5% loss is a reference-anchored, sign-asymmetric delta objective: it penalizes the model when it assigns higher log-probability than the reference to the forget answers, while rewarding the model for assigning higher log-probability than the reference to the retain answers. Concretely, the forget term has the form log πθ (yf | xf ) − log πref (yf | xf ) with a positive weight, so minimizing the loss drives this delta negative, i.e., pushes πθ (yf | xf ) below the reference. This directly targets the teacher-forced probability metric used by TOFU and, by reducing the model’s likelihood of the ground-truth answer, also reduces extraction strength, which is computed from length-normalized likelihood under extraction prompts. The retain term has the opposite sign, log πref (yr | xr ) − log πθ (yr | xr ), so minimizing it pushes the retain delta positive (increase retain likelihood above the reference). This is a useful inductive bias for TOFU-5%, where the forget set is relatively small and the primary risk is over-unlearning: the objective encourages the model to compensate for the forgetting pressure by explicitly improving retain likelihood rather than merely trying to stay close in KL. The single scalar β sets the forgetting–utility balance without requiring paired preferences or synthetic negative answers. Compared to NPO/SimNPO, this objective provides tighter control over retention. NPO/SimNPO-style losses primarily emphasize lowering undesired likelihood on forget responses; retention is typically handled indirectly (e.g., via auxiliary retain NLL, regularization, or reference comparisons that are symmetric in form). The discovered TOFU-5% objective is intentionally asymmetric: it uses the reference model as a stable anchor for forget suppression, but it actively pushes retain likelihood upward, which helps preserve (and sometimes improve) the MU components based on retain/Real-Authors/World-Facts probabilities. 

TOFU-10% objective. Proposed loss function: 

ℓ(θ) = E(xf ,y f )∼Df , (xr ,y r )∼Dr

h

exp 



log πθ (yf | xf )−log πref (yf | xf )



−2·0.3 exp 



log πθ (yr | xr )−log πref (yr | xr )

i 

.

TOFU-10% is harder because many more entities are removed, so simple linear penalties can under-focus on the remaining hard-to-forget examples or can require large coefficients that increase collateral damage. The discovered objective replaces linear deltas with exponentiated deltas: exp(log πθ − log πref ) on both forget and retain, with opposite signs. Minimization drives the forget delta negative (reducing exp(∆ f )) and encourages the retain delta positive (increasing exp(∆ r ) because it is subtracted). The exponential transform acts as an adaptive reweighting mechanism: samples with positive deltas (cases where the model is still more confident than the reference) receive disproportionately larger gradients. On TOFU-10%, this concentrates optimization on stubborn memorized facts without requiring manual per-sample weighting or brittle margins. At the same time, the retain exponential term creates strong counter-pressure that discourages global degradation of likelihood on the retain distribution, which is exactly the failure mode observed in many over-unlearning baselines at higher deletion rates. Relative to NPO/SimNPO, the key difference is that the objective shapes gradient allocation across examples through the exponential rather than through preference comparisons. NPO/SimNPO is effective when the preference signal (undesirable vs. alternate) is well specified, but in TOFU the evaluation is teacher-forced likelihood and extraction prompts; exponentiated deltas provide a direct, automatically scaled mechanism aligned with these metrics while maintaining a strong, explicit retain reward. 

MUSE News objective. Proposed loss function: 

ℓ(θ) = 0 .35 E(xf ,y f )∼Df , (xr ,y r )∼Dr

h

min 



log πθ (yf | xf ) − log πθ (yr | xr ), 1

i 

.

16 EvoMU: Evolutionary Machine Unlearning 

MUSE News is a deletion-at-scale scenario within a relatively coherent domain distribution: the forget and retain documents are similar in style and topic (same source distribution), and the benchmark additionally evaluates membership leakage using a separate holdout set. In this setting, a good objective should avoid large distribution shifts while still making the forget documents behave like non-members. The discovered objective is a capped relative-ranking loss based on the difference between forget and retain log-probabilities: 

min(log πθ (yf | xf ) − log πθ (yr | xr ), 1) (up to a scaling constant). Minimizing encourages log πθ (yf | xf ) to be smaller than log πθ (yr | xr ), i.e., it pushes the model to treat forget targets as less likely than retain targets, rather than enforcing an absolute decrease relative to a reference. This relative formulation is well matched to MUSE News because it implicitly normalizes for domain-wide calibration shifts: even if the model’s overall likelihood scale changes slightly during fine-tuning, the loss keeps forget likelihood suppressed relative to retain likelihood, which helps reduce membership signals without collapsing general in-domain competence. The cap (the min( ·, 1) ) prevents extreme gradients when log π(yf ) − log π(yr ) becomes very large, which improves stability and reduces the chance of runaway updates that damage utility. This is a concrete way to mitigate over-unlearning in a setting where the model is trained on long-form data and where large updates can easily cause broad degradation. Compared to NPO/SimNPO, this objective does not require a reference model or preference pairs and instead uses the retain batch as an implicit anchor. In MUSE News, that anchor is particularly appropriate because retain is drawn from the same distribution as forget. NPO/SimNPO can drift (SimNPO) or overly constrain to the reference (NPO) in ways that do not directly target membership leakage; the discovered ranking-and-cap structure more directly targets the relative separability that underlies membership inference while preserving in-domain behavior. 

MUSE Books objective. Proposed loss function: 

ℓ(θ) = E(xf ,y f )∼Df , (xr ,y r )∼Dr

h

0.7 log πθ (yf | xf ) − log πθ (yr | xr ) + 0 .3 max 



log πref (yr | xr ) − log πθ (yr | xr ), 0

i 

MUSE Books is qualitatively different from MUSE News: the forget set is copyrighted book text, while the retain set is constructed from closely related material (FanWiki pages) that overlaps heavily in entities, relationships, and surface cues. This creates an entangled unlearning problem: the model must stop reproducing verbatim passages and book-derived facts, but still answer questions about the same universe using permissible retain sources. In such a setting, objectives that simply push down likelihood on forget targets can easily harm retain performance because the representations and lexical triggers are shared. The discovered Books objective combines three ingredients: (1) a weighted forget penalty ( 0.7 log πθ (yf | xf )) that reduces book memorization pressure without being excessively aggressive, (2) a strong retain reward ( − log πθ (yr | xr )) that explicitly increases likelihood on the permissible in-domain retain corpus, and (3) a one-sided hinge barrier on retain relative to the reference, α max(log πref (yr | xr ) − log πθ (yr | xr ), 0) . The hinge activates only when retain likelihood drops below the reference, acting like a safety constraint that prevents the model from sacrificing retain knowledge to achieve lower forget likelihood. This structure makes sense for Books because the main risk is not insufficient forgetting, but damaging closely related retain competence. The hinge term is a targeted guardrail: it does not force the model to stay identical to the reference on retain, but it prevents regressions. That asymmetry is valuable in an overlapping-domain scenario because it allows the optimizer to reshape behavior where needed (reduce book memorization) while preserving or improving behavior on the permissible corpus. Relative to NPO/SimNPO, the hinge barrier provides a more direct mechanism for retain protection. Preference-style losses can be effective for discouraging particular outputs, but when forget and retain are highly entangled, suppressing the undesired outputs often suppresses the desired ones as well unless the objective includes an explicit constraint. The discovered loss encodes such a constraint directly at the level of retain likelihood, which is closely aligned with the KnowMem utility metric on Dretain .

WMDP objective. Proposed loss function: 17 EvoMU: Evolutionary Machine Unlearning 

ℓ(θ) = E(xf ,y f )∼Df , (xr ,y r )∼Dr

h

1.5  log πθ (yf | xf ) − log πref (yf | xf ) −   log πθ (yr | xr ) − log πref (yr | xr )i

.

WMDP measures forgetting as reduced accuracy on a hazardous knowledge slice (WMDP-Bio), with utility measured on a broad capability benchmark (MMLU). Unlike TOFU or MUSE, the target is not primarily verbatim memorization; it is a transferable capability. This setting benefits from objectives that are (i) explicitly anchored to a reference model for stability, and (ii) able to apply stronger pressure on the forget slice without causing widespread degradation. The discovered WMDP objective is a weighted reference-delta difference: 1.5(log πθ − log πref ) on forget minus (log πθ −

log πref ) on retain. Minimizing drives the forget delta negative (reduce hazardous likelihood relative to the original model) while driving the retain delta positive (preserve or improve likelihood on retained targets relative to the original model). The heavier forget weight reflects the fact that meaningful suppression of hazardous accuracy often requires stronger targeted pressure than biography-style forgetting, while the explicit retain delta term counterbalances this to maintain general performance. Compared to NPO/SimNPO, this objective is closer in spirit but differs in two practical ways that matter for WMDP. First, it keeps an explicit reference anchor, which reduces drift under strong forgetting pressure; SimNPO-style objectives without a reference can move the model broadly, harming MMLU. Second, it includes an explicit retain-improvement term rather than relying on indirect regularization, which helps maintain general competence when the forget objective targets a capability that shares features with general knowledge. 

Summary of why these objectives can outperform NPO/SimNPO. Across benchmarks, the discovered objectives share two properties that are not guaranteed in standard NPO/SimNPO formulations: (i) explicit, often asymmetric retain protection (sometimes with a hard one-sided barrier), and (ii) loss shaping that adapts to the benchmark’s failure mode (exponential reweighting for TOFU-10%, relative ranking and gradient capping for MUSE News, hinge-based retain constraints for MUSE Books, and reference-anchored weighted deltas for WMDP). These design choices better match the evaluation metrics used in each benchmark and directly target over-unlearning, which is the dominant reason strong baselines lose MU or general utility in our experiments. 

## D. Robustness-run objective definitions 

We list the exact objectives used in the five-seed robustness experiment. Each loss is written in terms of per-example average log-probabilities on forget/retain batches, zf , zr ∈ RB , and (optionally) reference-model values zref  

> f

, zref  

> r

.                                                                              

> Loss 17 (7 epochs)
> ℓ17 (θ) = 1
> B
> PBi=1
> h
> max  z(i)
> f,−10 −0.4z(i)
> r+ 0 .6 max  z(i)
> f−z(i)
> f, ref ,0i
> .
> Loss 10 (5 epochs)
> ℓ10 (θ) = 1
> B
> PBi=1
> h
> z(i)
> f−min  z(i)
> r, α i
> ,α= 0 .4.
> Loss 2 (2 epochs)
> ℓ2(θ) = β·1
> B
> PBi=1
> h
> log  exp( z(i)
> f) + ε−log  exp( z(i)
> f, ref ) + εi
> +1
> B
> PBi=1
> h
> log  exp( z(i)
> r, ref ) + ε−log  exp( z(i)
> r) + εi
> ,β=1.2, ε = 10 −6.
> Loss 5 (5 epochs)
> ℓ5(θ) = 1
> B
> PBi=1
> h
> −z(i)
> r+δmin  z(i)
> f,0−0.2 max  z(i)
> r−z(i)
> r, ref ,0i
> ,δ= 0 .6.
> Loss 9 (3 epochs)
> ℓ9(θ) = γ·1
> B
> PBi=1
> h
> log  exp( z(i)
> f) + ε−log  exp( z(i)
> f, ref ) + εi
> +1
> B
> PBi=1
> h
> log  exp( z(i)
> r, ref ) + ε−log  exp( z(i)
> r) + εi
> ,γ=1.5, ε = 10 −6.

18 EvoMU: Evolutionary Machine Unlearning 

## E. Initial functions generated 

In this subsection we list the N =10 initial candidate loss functions sampled from the proposer at iteration 0. Each objective is expressed in terms of per-example average log-probabilities on forget/retain batches, zf , zr ∈ RB , and (optionally) reference-model values zref 

f , zref 

r (Eq. 3). We denote the i-th elements by z(i)

f , z(i)

r , and z(i)

f, ref .

Initial Loss 1 (1 epoch) 

ℓ1(θ) = 1

> B

PBi=1 

h

− z(i) 

> r

+ α z (i)

> f

i

, α = 0 .7.

Initial Loss 2 (2 epochs) 

ℓ2(θ) = 1

> B

PBi=1 max 



− z(i) 

> r

+ α z (i) 

> f

, 0



, α = 0 .5.

Initial Loss 3 (3 epochs) 

ℓ3(θ) = 1

> B

PBi=1 

h

− z(i) 

> r

+ α min  z(i) 

> f

, 0i

, α = 0 .8.

Initial Loss 4 (4 epochs) 

ℓ4(θ) = 1

> B

PBi=1 

h

− z(i) 

> r

+ α max  z(i) 

> f

− z(i) 

> f, ref

, 0i

, α = 0 .6.

Initial Loss 5 (5 epochs) 

ℓ5(θ) = 1

> B

PBi=1 

h

− z(i) 

> r

+ α exp  z(i)

> f

i

, α = 0 .9.

Initial Loss 6 (6 epochs) 

ℓ6(θ) = 1

> B

PBi=1 

h

− z(i) 

> r

+ α σ  z(i) 

> f

− z(i)

> f, ref

i

, α = 0 .4, σ(u) = 11+ e−u .

Initial Loss 7 (7 epochs) 

ℓ7(θ) = 1

> B

PBi=1 

h

− z(i) 

> r

+ α z(i) 

> f

− z(i)

> f, ref

i

, α = 0 .3.

Initial Loss 8 (8 epochs) 

ℓ8(θ) = 1

> B

PBi=1 

h

− z(i) 

> r

+ α  z(i) 

> f

− z(i)

> f, ref

2i

, α = 0 .2.

Initial Loss 9 (9 epochs) 

ℓ9(θ) = 1

> B

PBi=1 

h

− z(i) 

> r

+ α log  1 + ( z(i) 

> f

− z(i) 

> f, ref

)i

, α = 0 .1.

Initial Loss 10 (10 epochs) 

ℓ10 (θ) = 1

> B

PBi=1 

h

− z(i) 

> r

+ α exp  z(i) 

> f

− z(i)

> f, ref

i

, α = 0 .5.

## F. Metrics & Model Benchmark summary 

n this section, we present Table 5, which summarizes the base model and the evaluation metrics used for unlearning effectiveness and utility preservation in each benchmark. 

## G. Unlearning on TOFU 

In this section we showcase which unlearning metrics did we use for TOFU 5% and 10%. We run experiments on TOFU 5% and 10% with learning rate equal to 5e-4 with batch size 8. 19 EvoMU: Evolutionary Machine Unlearning 

Table 5. Benchmarks, base models, and evaluation metrics used for unlearning effectiveness and utility preservation. 

Benchmark LLM to be used Unlearning Effectiveness Utility Preservation 

TOFU LLaMA-2-chat 7B Probability on Df ↓

ROUGE-L on Df ↓

Extraction Strength on Df ↓

Model utility (harmonic mean of 9 metrics) ↑

Prob. on Dr /Dreal author /Dworld facts ↑

ROUGE-L on Dr /Dreal author /Dworld facts ↑

Truth ratio on Dr /Dreal author /Dworld facts ↑

MUSE ICLM-7B LLaMA-2 7B KnowMem on Df ↓

VerbMem on Df ↓

PrivLeak → 0

KnowMem on Dr ↑

WMDP Llama3-8B-Instruct Accuracy on WMDP-Bio ↓ Accuracy on MMLU ↑

Probability. For each example in the retain or forget split, we compute the model’s length-normalized conditional likelihood of the answer given the question, P (a | q)1/|a|, where q is the question, a is the answer, and |a| is the number of tokens in a. For the real authors and world facts subsets, each prompt is paired with five candidate answers 

{a0, ˜a1, ˜a2, ˜a3, ˜a4}: one correct answer a0 and four perturbed (incorrect) alternatives. We summarize preference for the correct answer via the ratio 

P (a0 | q)1/|a0|

P4 

> i=1

P (˜ ai | q)1/|˜ai| .

Truth ratio. The truth ratio compares the model’s preference for a paraphrased correct answer ˆa against a set of perturbed incorrect answers A = {˜a1, ˜a2, . . . }. Specifically, we take the geometric mean of the perturbed answers’ length-normalized likelihoods and divide by the length-normalized likelihood of ˆa:

Rtruth =

Q|A| 

> i=1

P (˜ ai | q)1/|˜ai|

1/|A|

P (ˆ a | q)1/|ˆa| .

For the real authors and world facts subsets, no paraphrase is provided; in that case we use the original correct answer a in the denominator. 

ROUGE-L. Across all TOFU subsets, we report ROUGE-L recall (Lin, 2004) between the ground-truth responses (from the forget split) and the model outputs produced after unlearning. 

Extraction strength. To quantify how easily a model can be induced to reveal information that should be forgotten, we measure extraction strength as the model’s length-normalized likelihood of the target (ground-truth) answer under a set of extraction-oriented prompts. Let E = {e1, . . . , e K } denote K extraction templates (e.g., paraphrases or adversarially phrased requests), and let q(k) = Compose( q, e k) be the resulting query for template ek. We compute 

Sext (q, a ) = max  

> k∈{ 1,...,K }

P



a | q(k)1/|a|

,

where the maximization reflects a strong (best-of-K) attacker that tries multiple prompts and uses the most effective one. Higher extraction strength indicates that the target answer is easier to extract (i.e., weaker unlearning), while lower values suggest better resistance to extraction. 

Model utility. We aggregate overall model utility as the harmonic mean of nine quantities: the answer probability, truth ratio, and ROUGE-L recall computed on each of the retain, real authors, and world facts subsets. Higher utility indicates better overall performance. 

## H. Unlearning on MUSE 

MUSE (Shi et al., 2024a) evaluates unlearning in long-form domains using memorization and privacy-leakage metrics computed on a forget split Dforget and a retain split Dretain . Below we briefly summarize the metrics used in our experiments. 20 EvoMU: Evolutionary Machine Unlearning 

We run experiments on MUSE News with learning rate 5e − 4 and batch size 16, and on MUSE Books with learning rate 1e-3 and batch size 4. 

VerbMem. VerbMem (verbatim memorization) measures how much the model reproduces the forget text word-for-word. For each forget example, the model is prompted to generate a continuation/answer, and VerbMem scores the extent of exact-string or near-exact overlap between the generated text and the reference forget span (e.g., using longest common subsequence or high-threshold n-gram overlap as defined in (Shi et al., 2024a)). Lower VerbMem indicates less verbatim regurgitation of the forget content, and is therefore preferred. 

KnowMem. KnowMem (knowledge memorization) measures whether the model still expresses the underlying facts contained in the forget data, even if it does not reproduce them verbatim. Concretely, MUSE evaluates the model on question-style or probe prompts derived from the forget documents and checks whether the generated responses contain the target facts (using the benchmark’s automatic matching/verification procedure from (Shi et al., 2024a)). We report KnowMem on both splits: on Dforget , lower KnowMem indicates better forgetting; on Dretain , higher KnowMem indicates better preservation of non-forget knowledge. 

PrivLeak. PrivLeak is a privacy-leakage proxy derived from the Min-K% Prob membership-inference attack. It compares how well an attacker can distinguish forget examples from a holdout set Dholdout using model likelihood statistics. Importantly, Dholdout is not the retain set: it is a disjoint set used as a “non-member” reference distribution for the membership test. PrivLeak is defined relative to a retraining baseline as 

PrivLeak = AUC  funlearn ; Dforget , Dholdout 

 − AUC  fretrain ; Dforget , Dholdout 



AUC  fretrain ; Dforget , Dholdout 

 , (3) where AUC( ·) is the standard AUC-ROC for separating samples from Dforget and Dholdout using Min-K% Prob features. PrivLeak closer to 0 is better, indicating that the unlearned model approaches the retraining baseline in terms of membership distinguishability. As discussed in the main text, Min-K% Prob can be sensitive to the evaluation split and may exhibit high variance across random dataset samples (Maini et al., 2024a; Duan et al., 2024a). 

## I. Unlearning on WMDP 

WMDP (Li et al., 2024) evaluates targeted capability suppression rather than memorization removal. In our experiments, the forget set Df consists of the WMDP-Bio subset (biosecurity-related multiple-choice questions), and the utility evaluation uses MMLU (Hendrycks et al., 2020) as a broad general-knowledge proxy. We follow the same evaluation protocol as prior unlearning work on WMDP (Fan et al., 2024a). On WMDP we run the experiments with learning rate 5e-4 and batch size 1. Each WMDP example is a multiple-choice question with a fixed set of answer options. We convert each item into a standard instruction-format prompt that includes the question and options, and we train/evaluate the model to produce the correct option token(s) (or option letter, depending on the benchmark’s official formatting). We use the official WMDP-Bio split provided by the benchmark. We measure forgetting as the drop in WMDP-Bio accuracy after unlearning. Let AccBio denote the fraction of WMDP-Bio questions answered correctly by the model. Following the main paper, we report 1 − AccBio as the forgetting score, where larger values indicate stronger suppression of the hazardous slice. To quantify retained general capability, we report accuracy on MMLU (Hendrycks et al., 2020). We evaluate using the same decoding rule as for WMDP (selecting the highest-likelihood option among the multiple-choice candidates), and we report overall MMLU accuracy as AccMMLU 

(higher is better). For WMDP, each candidate objective specifies a training budget in steps (rather than epochs), since the dataset is smaller and we aim for a controlled, comparable update size across objectives. 21 EvoMU: Evolutionary Machine Unlearning 

## J. LLM Proposer Prompt 

In this section we include prompt for LLM Proposer. 

ProposerPrompt 

SYSTEM 

You are an expert ML researcher specializing in machine unlearning for large language models. Your task is to design novel loss functions for fine-tuning a model to forget specific data while preserving performance on a retain set. 

CRITICAL OUTPUT FORMAT 

(1) First output a <think> block (private reasoning). (2) Then output a <answer> block containing only Python code : exactly N function definitions. No prose. No Markdown fences. 

Inside <answer> , you must output exactly: loss fn 1, loss fn 2, ..., loss fn N 

Each function must satisfy: • Name: loss fn K for K ∈ { 1, . . . , N }.• Signature (exact): 

def loss fn K(log probs forget, log probs retain, ref log probs forget=None, ref log probs retain=None): 

• First line of the function body must be a one-line docstring: 

"""epochs: K""" where K is an integer in [1 , 10] .• Return a single scalar loss (use .mean() or another reduction). • Use only PyTorch tensor ops ( torch ); do not use external libraries. • Include at least one fixed numeric tradeoff constant inside the function body (e.g., alpha = 0.7 ). 

OBJECTIVE DIRECTION (CRITICAL) 

Training minimizes the loss. • Higher log probs forget should increase the loss (push the model to lower forget likelihood). • Higher log probs retain should decrease the loss (push the model to raise retain likelihood). 

REFERENCE USAGE (OPTIONAL, ENCOURAGED) 

If you use reference log-probs, use them non-trivially, e.g., penalize positive deltas (log probs forget - ref log probs forget) and/or penalize negative deltas (log probs retain - ref log probs retain) .

DIVERSITY & STABILITY 

Vary mechanisms across losses (margin/hinge/softplus, squared/absolute, logistic/exponential, normalization/ratios, ref-deltas). Use numerically stable transforms (e.g., torch.nn.functional.softplus , torch.clamp , small eps for division). Avoid Python max/min on tensors; use torch.clamp or torch.relu .

USER 

We are doing machine unlearning with a forget set Df (must be forgotten) and a retain set Dr (must be preserved). Propose N diverse candidate unlearning losses that satisfy all rules above. 

IMPORTANT: In <answer> , output only the function definitions loss fn 1..loss fn N back-to-back. After </answer> , output nothing. 

22 EvoMU: Evolutionary Machine Unlearning 

## K. LLM Refinement Prompt 

In this section we include prompt for LLM that performs loss refinement. 

Refinement prompt 

SYSTEM 

You are an expert ML researcher specializing in machine unlearning for large language models. You will improve a given parent loss by proposing refined variants that better trade off forgetting vs. utility. 

CRITICAL OUTPUT FORMAT 

(1) First output a <think> block (private reasoning). (2) Then output a <answer> block containing only Python code : exactly C function definitions. No prose. No Markdown fences. 

Inside <answer> , you must output exactly: loss fn 1, loss fn 2, ..., loss fn C 

Each function must satisfy: • Name: loss fn K for K ∈ { 1, . . . , C }.• Signature (exact): 

def loss fn K(log probs forget, log probs retain, ref log probs forget=None, ref log probs retain=None): 

• First line of the function body must be a one-line docstring: 

"""epochs: K""" where K is an integer in [1 , 10] .• Return a single scalar loss (use .mean() or another reduction). • Use only PyTorch tensor ops ( torch ); do not use external libraries. • Include at least one fixed numeric tradeoff constant inside the function body (e.g., alpha = 0.7 ). 

OBJECTIVE DIRECTION (CRITICAL) 

Training minimizes the loss. • Higher log probs forget should increase the loss (push the model to lower forget likelihood). • Higher log probs retain should decrease the loss (push the model to raise retain likelihood). 

REFERENCE USAGE (OPTIONAL, ENCOURAGED) 

If you use reference log-probs, use them non-trivially, e.g., penalize positive deltas (log probs forget - ref log probs forget) and/or penalize negative deltas (log probs retain - ref log probs retain) .

METRIC-GUIDED REFINEMENT (IMPORTANT) 

You will be given the parent loss code, its training loss history, and its final evaluation metrics. • If forgetting is too weak (e.g., forget metrics like forget Q A Prob or forget Q A ROUGE are high), increase forgetting pressure : raise weights/margins on log probs forget , add softplus/hinge margins, or add ref-delta penalties when above reference. • If utility is too low (e.g., model utility is low or retain metrics degrade), protect retention : increase the magnitude of retain reward terms (more negative dependence on log probs retain ) or add ref-based penalties for retain drops. • Avoid extreme updates: prefer smooth robust penalties (softplus, Huber-like) and balanced coefficients. 

DIVERSITY & STABILITY 

Generate diverse refinements (don’t just tweak one constant). Use stable transforms (softplus/clamp, eps for division). Avoid Python max/min on tensors; use 

torch.clamp or torch.relu .

USER 

Here is the PARENT loss (Python), the parent training loss history, and parent evaluation metrics: 

<PARENT LOSS CODE> <PARENT LOSS HISTORY> <PARENT METRICS JSON> 

Now produce C refined candidate loss functions that address weaknesses implied by the history/metrics. 

IMPORTANT: In <answer> , output only the function definitions loss fn 1..loss fn C back-to-back. After </answer> , output nothing. 

## L. Non-Sensical loss function 

Intruingly LLM initially happens to propose non-sentical loss functions for example increasing logits on the forget set. 

Loss #10 (epochs: 10) — Exponential inverse-delta 

Let zf ∈ RB be per-sample forget log-probabilities and zref  

> f

∈ RB the reference values. Define the per-sample delta ∆(i) 

> f

≜ z(i) 

> f

− zref 

> f
> (i)

.

L10 = α10 

1

B

> B

X

> i=1

exp  −∆(i)

> f

, α10 = 0 .95 .

Implementation (PyTorch): 0.95 * torch.exp(-(log_probs_forget - ref_log_probs_forget)).mean() 

Loss #20 (epochs: 10) — Softplus of negative delta 

Using the same notation ∆(i) 

> f

≜ z(i) 

> f

− zref  

> f
> (i)

and softplus( u) ≜ log(1 + eu),

L20 = α20 

1

B

> B

X

> i=1

log 



1 + exp  −∆(i)

> f



= α20 

1

B

> B

X

> i=1

softplus  −∆(i)

> f

, α20 = 0 .50 .

Implementation (PyTorch): 0.5 * torch.log(1 + torch.exp(-(log_probs_forget - ref_log_probs_forget))).mean() 

23 EvoMU: Evolutionary Machine Unlearning 

## M. Additional Ablations 

Robustness of EvoMU. We analyze the robustness of EvoMU. Since the discovery loop samples objectives from an LLM, different random seeds can yield different candidate losses and thus different outcomes. To quantify this variability, we run EvoMU five times independently with different seeds on TOFU-5% task and report the resulting trade-off between forgetting and utility. Table 6 shows that EvoMU is stable across runs: 1 − Extr . Strength remains near-saturated and MU varies only mildly, indicating that the method reliably finds strong objectives despite stochastic proposal and training dynamics. We list the exact discovered objectives used in the robustness runs in Appendix D. 

Table 6. Robustness of EvoMU on TOFU-5% across five random seeds. We report 1 − Extr . Strength (higher is better) and model utility (MU). Mean and standard deviation are computed across seeds. 

Seed 1 − Extr . Strength ( ↑) MU ( ↑)

1 0.95 0.65 2 0.97 0.63 3 0.97 0.63 4 0.97 0.63 5 0.97 0.61 

Mean ± Std 0.966 ± 0.009 0.630 ± 0.014 

Fixed training budget (no epoch proposal). To isolate the effect of jointly searching over the objective form and the training budget, we run the evolutionary loop unchanged but without suggesting the number of epochs. We train every candidate for 10 epochs, following common practice in prior work (e.g., (Zhang et al., 2024a; Fan et al., 2024a)). Results are shown in Table 7 (“EvoMU (10 Epochs)”). Overall, using a fixed budget yields over-unlearns : We obtain worse overall utility, while keeping extraction strength constant due to optimizing for too long. Extraction strength stays essentially unchanged at 0.03 , and 1−Prob . on the forget set remains near-saturated, while retain-set ROUGE-L drops from 0.90 to 0.66 and retain-set probability from 0.95 to 0.83 .Importantly, the MU score decreases only modestly (from 0.65 to 0.63 ), indicating that the primary gains of EvoMU come from discovering better loss shapes rather than merely tuning training length. Nevertheless, permitting the proposer to adjust epochs acts as a lightweight regularizer/early-stopping mechanism that improves the Pareto frontier by preventing excessive utility loss once forgetting has been attained. 

Number of tokens to think. Recent work suggests that increasing the number of reasoning tokens at inference time can improve the quality of LLM outputs by enabling more deliberate search and self-correction (Muennighoff et al., 2025). We test whether this “test-time compute” effect also applies to objective discovery by varying the proposer’s Thinking-Stage budget (TT), while keeping the Implementation Stage and all downstream training/evaluation settings fixed. As shown in Table 7, performance is relatively insensitive to TT within the range we test: even with 512 TT, the discovered objectives already outperform or match strong hand-designed baselines in terms of overall MU. Increasing TT to 1024–2048 yields comparable MU and maintains saturated forgetting efficacy, with modest shifts in the utility metrics (e.g., on Real 

Table 7. Results on TOFU-5% (LLaMA2-7B-Chat) for # of training epochs and # of thinking tokens ablations. EvoMU (10 epochs) trains every candidate loss for 10 epochs, and EvoMU (X TT) uses X thinking tokens during the Thinking Stage.                                                                                                              

> Unlearning Efficacy Utility Preservation MU ( ↑)Method Forget Set Real Authors World Facts Retain Set MU ( ↑)1-Rouge-L ↑1-Prob. ↑1-Extr. Strength ↑Rouge-L ↑Prob. ↑Truth ratio ↑Rouge-L ↑Prob. ↑Truth ratio ↑Rouge-L ↑Prob. ↑Truth ratio ↑
> Original 0.04 0.01 0.05 0.93 0.44 0.58 0.91 0.43 0.55 0.98 0.99 0.48 0.62 Retain 0.61 0.85 0.93 0.92 0.44 0.57 0.90 0.43 0.54 0.97 0.99 0.48 0.62 EvoMU 1.00 0.99 0.97 0.89 0.50 0.65 0.89 0.47 0.60 0.90 0.95 0.46 0.65 EvoMU (10 Epochs) 0.98 1.00 0.97 0.90 0.49 0.64 0.88 0.51 0.65 0.66 0.83 0.45 0.63 EvoMU (512 TT) 0.97 1.00 0.96 0.82 0.48 0.66 0.86 0.49 0.63 0.87 0.86 0.41 0.63 EvoMU (1024 TT) 1.00 0.97 0.97 0.86 0.45 0.58 0.88 0.49 0.58 0.90 0.94 0.44 0.62 EvoMU (2048 TT) 1.00 0.99 0.97 0.72 0.49 0.67 0.89 0.49 0.63 0.83 0.91 0.46 0.63

24 EvoMU: Evolutionary Machine Unlearning 

Authors / World Facts). Finally, EvoMU (4096 TT) achieves the best overall trade-off, indicating that additional deliberation can help the proposer find slightly better-balanced objectives, but is not essential for reaching state-of-the-art performance. Overall, these results suggest that our evolutionary search loop is the dominant driver of performance, while increased Thinking Tokens primarily provide incremental gains and improved stability rather than a qualitative jump. 

## N. Additional Experimental Results on TOFU 10% 

To provide a strong tuned baseline, we perform an exhaustive grid search of key SimNPO hyperparameters on TOFU-10% For each configuration, we run the full unlearning pipeline and report the standard TOFU forgetting/utility metrics in Table 8. The hyperparameters are: • lr : the optimizer learning rate used for LoRA fine-tuning. • b (β): the SimNPO preference-loss scaling (inverse temperature). Larger β typically increases the strength/sharpness of the forget-side update. • d: a binary switch indicating whether the retain-side term/regularizer is enabled in our SimNPO implementation ( d=0 

disables it; d=1 enables joint optimization that explicitly protects retain behavior). • g (γ): the weight applied to the retain-side term/regularizer when it is enabled; larger values put more emphasis on utility preservation. • ep : the number of training epochs. We include the full metric bundle (forget split, Real Authors, World Facts, retain split, and MU) to make the forgetting–utility trade-off of each setting explicit. 25 EvoMU: Evolutionary Machine Unlearning  

> Table 8. TOFU results with hyperparameters (all runs: ModelType=SimNPO, forget pct=10, a=1.0). Abbreviations: R F=RougeL forget, P F=Prob forget, ES=ExtractionStrength, R RA=RougeL RA, P RA=Prob RA norm, TR RA=TruthRatio RA, R WF=RougeL WF, P WF=Prob WF norm, TR WF=TruthRatio WF, R ret=RougeL retain, P ret=Prob retain, TR ret=TruthRatio retain, MU=ModelUtility.

lr b d g ep (1-R F) (1-P F) (1-ES) R RA P RA TR RA R WF P WF TR WF R ret P ret TR ret MU 1e-05 3.5 0 0.125 5 0.26 0.16 0.43 0.76 0.40 0.53 0.83 0.42 0.62 0.81 0.07 0.53 0.59 1e-05 3.5 0 0.125 10 0.28 0.17 0.46 0.78 0.41 0.53 0.84 0.42 0.62 0.82 0.07 0.53 0.60 1e-05 3.5 0 0.25 5 0.33 0.21 0.55 0.74 0.41 0.53 0.84 0.42 0.62 0.80 0.06 0.53 0.59 1e-05 3.5 0 0.25 10 0.36 0.25 0.60 0.74 0.40 0.53 0.83 0.42 0.62 0.83 0.06 0.53 0.59 1e-05 3.5 1 0.125 5 0.38 0.31 0.63 0.73 0.40 0.53 0.84 0.42 0.62 0.78 0.06 0.53 0.59 1e-05 3.5 1 0.125 10 0.43 0.38 0.69 0.70 0.40 0.53 0.85 0.42 0.62 0.79 0.05 0.53 0.59 1e-05 3.5 1 0.25 5 0.49 0.53 0.79 0.68 0.39 0.52 0.85 0.41 0.62 0.72 0.04 0.53 0.58 1e-05 3.5 1 0.25 10 0.52 0.63 0.82 0.73 0.39 0.52 0.87 0.41 0.62 0.75 0.04 0.53 0.58 1e-05 4.5 0 0.125 5 0.26 0.15 0.43 0.77 0.41 0.53 0.83 0.43 0.62 0.81 0.07 0.53 0.60 1e-05 4.5 0 0.125 10 0.27 0.16 0.45 0.76 0.41 0.53 0.83 0.42 0.62 0.82 0.07 0.53 0.60 1e-05 4.5 0 0.25 5 0.32 0.20 0.53 0.77 0.40 0.53 0.83 0.42 0.62 0.81 0.07 0.53 0.60 1e-05 4.5 0 0.25 10 0.34 0.22 0.58 0.76 0.40 0.53 0.83 0.42 0.62 0.83 0.07 0.53 0.60 1e-05 4.5 1 0.125 5 0.39 0.31 0.63 0.74 0.40 0.53 0.84 0.42 0.62 0.77 0.06 0.53 0.59 1e-05 4.5 1 0.125 10 0.44 0.39 0.69 0.70 0.40 0.53 0.84 0.42 0.62 0.78 0.05 0.53 0.59 1e-05 4.5 1 0.25 5 0.49 0.54 0.79 0.72 0.39 0.52 0.86 0.41 0.62 0.71 0.04 0.53 0.58 1e-05 4.5 1 0.25 10 0.52 0.63 0.82 0.70 0.39 0.52 0.85 0.41 0.62 0.75 0.05 0.53 0.58 2e-05 3.5 0 0.125 5 0.40 0.29 0.65 0.80 0.40 0.52 0.83 0.43 0.62 0.78 0.07 0.52 0.60 2e-05 3.5 0 0.125 10 0.45 0.35 0.73 0.78 0.40 0.53 0.84 0.42 0.62 0.82 0.07 0.53 0.60 2e-05 3.5 0 0.25 5 0.49 0.47 0.79 0.77 0.40 0.52 0.84 0.43 0.62 0.79 0.07 0.52 0.59 2e-05 3.5 0 0.25 10 0.51 0.56 0.82 0.74 0.40 0.53 0.84 0.43 0.62 0.81 0.06 0.53 0.59 2e-05 3.5 1 0.125 5 0.57 0.73 0.86 0.73 0.40 0.52 0.84 0.43 0.63 0.78 0.06 0.52 0.59 2e-05 3.5 1 0.125 10 0.59 0.84 0.88 0.73 0.40 0.53 0.85 0.43 0.63 0.81 0.06 0.53 0.60 2e-05 3.5 1 0.25 5 0.60 0.88 0.89 0.78 0.40 0.52 0.84 0.43 0.63 0.77 0.07 0.52 0.59 2e-05 3.5 1 0.25 10 0.62 0.91 0.90 0.76 0.40 0.52 0.82 0.43 0.63 0.81 0.07 0.53 0.59 2e-05 4.5 0 0.125 5 0.39 0.26 0.63 0.81 0.41 0.53 0.84 0.43 0.62 0.79 0.07 0.52 0.60 2e-05 4.5 0 0.125 10 0.42 0.31 0.70 0.82 0.40 0.53 0.84 0.42 0.62 0.83 0.07 0.52 0.60 2e-05 4.5 0 0.25 5 0.48 0.42 0.77 0.76 0.40 0.52 0.83 0.43 0.62 0.78 0.07 0.52 0.59 2e-05 4.5 0 0.25 10 0.50 0.49 0.81 0.75 0.40 0.52 0.86 0.42 0.62 0.82 0.07 0.53 0.60 2e-05 4.5 1 0.125 5 0.56 0.72 0.86 0.79 0.40 0.52 0.85 0.43 0.63 0.78 0.07 0.52 0.59 2e-05 4.5 1 0.125 10 0.58 0.82 0.88 0.74 0.40 0.52 0.84 0.42 0.63 0.81 0.06 0.53 0.59 2e-05 4.5 1 0.25 5 0.60 0.86 0.88 0.80 0.40 0.51 0.85 0.43 0.63 0.78 0.07 0.52 0.59 2e-05 4.5 1 0.25 10 0.61 0.89 0.89 0.76 0.40 0.52 0.82 0.43 0.63 0.81 0.07 0.53 0.59 5e-05 3.5 0 0.125 5 0.56 0.53 0.86 0.77 0.40 0.49 0.85 0.44 0.60 0.57 0.10 0.47 0.55 5e-05 3.5 0 0.125 10 0.56 0.57 0.88 0.75 0.40 0.50 0.83 0.44 0.60 0.69 0.09 0.49 0.57 5e-05 3.5 0 0.25 5 0.59 0.65 0.89 0.74 0.41 0.50 0.85 0.44 0.59 0.57 0.10 0.48 0.55 5e-05 3.5 0 0.25 10 0.58 0.69 0.90 0.76 0.41 0.50 0.81 0.44 0.60 0.67 0.09 0.49 0.57 5e-05 3.5 1 0.125 5 0.62 0.88 0.92 0.76 0.41 0.51 0.84 0.44 0.60 0.56 0.10 0.48 0.56 5e-05 3.5 1 0.125 10 0.63 0.90 0.93 0.77 0.41 0.52 0.80 0.44 0.60 0.67 0.09 0.50 0.57 5e-05 3.5 1 0.25 5 0.65 0.94 0.94 0.78 0.42 0.53 0.83 0.45 0.61 0.56 0.10 0.48 0.56 5e-05 3.5 1 0.25 10 0.66 0.95 0.94 0.77 0.42 0.53 0.83 0.45 0.62 0.66 0.09 0.49 0.58 5e-05 4.5 0 0.125 5 0.56 0.47 0.84 0.78 0.40 0.49 0.85 0.44 0.60 0.56 0.10 0.47 0.56 5e-05 4.5 0 0.125 10 0.56 0.49 0.86 0.79 0.40 0.50 0.83 0.44 0.60 0.69 0.09 0.49 0.57 5e-05 4.5 0 0.25 5 0.58 0.58 0.88 0.77 0.40 0.50 0.84 0.44 0.59 0.57 0.10 0.47 0.55 5e-05 4.5 0 0.25 10 0.57 0.61 0.89 0.74 0.40 0.50 0.80 0.43 0.60 0.69 0.09 0.49 0.57 5e-05 4.5 1 0.125 5 0.61 0.86 0.92 0.76 0.41 0.51 0.83 0.44 0.60 0.55 0.10 0.48 0.56 5e-05 4.5 1 0.125 10 0.62 0.88 0.92 0.79 0.41 0.51 0.80 0.44 0.60 0.67 0.09 0.50 0.58 5e-05 4.5 1 0.25 5 0.66 0.92 0.93 0.78 0.42 0.53 0.84 0.45 0.61 0.56 0.10 0.48 0.56 5e-05 4.5 1 0.25 10 0.66 0.93 0.93 0.80 0.42 0.53 0.83 0.44 0.61 0.66 0.09 0.49 0.58 

26