# RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents
# RE-TRAC：面向深度搜索智能体的递归轨迹压缩

**Authors**: Jialiang Zhu, Gongrui Zhang, Xiaolong Ma, Lin Xu, Miaosen Zhang, Ruiqi Yang, Song Wang, Kai Qiu, Zhirong Wu, Qi Dai, Ruichun Ma, Bei Liu, Yifan Yang, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Xin Geng, Baining Guo \\
**Date**: 2026-02-02 \\
**PDF**: https://arxiv.org/pdf/2602.02486v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 6.0 \\
**Evidence**: Agentic framework for efficient search and iterative reflection \\

---

## Abstract
LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.

## 摘要
基于大语言模型（LLM）的深度

---

## 速览摘要（自动生成）

**问题**：传统ReAct框架因线性设计难以回溯或全局规划，导致搜索冗余且易陷入局部最优。

**方法**：提出RE-TRAC框架，通过在每条轨迹后生成结构化状态表示（总结证据、不确定性及计划），实现递归轨迹压缩与