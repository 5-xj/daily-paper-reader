Title: Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning

URL Source: https://arxiv.org/pdf/2602.02206v1

Published Time: Tue, 03 Feb 2026 04:02:29 GMT

Number of Pages: 16

Markdown Content:
# Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning 

Tong Yang 2, Yemin Wang 3, Chaoning Zhang 4, Aming Wu 11Hunan University of Science and Technology, China 

> 2

Xiamen University, China 

> 3

University of Electronic Science and Technology of China, Chengdu, China 

> 4

Henan Polytechnic University, China 

ytong8272@gmail.com, wangyemin@stu.xmu.edu.cn, chaoningzhang1990@gmail.com, amingwu@hpu.edu.cn 

Abstract 

The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to de-vote a substantial portion of their limited at-tention to syntactic processing rather than se-mantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key com-ponents: (1) a Semantic File System that rep-resents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accu-mulates task-solving knowledge without param-eter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to per-formance drop, while empirically validating the critical necessity of document-driven state mod-eling over rigid syntax. The code is available at 

https://github.com/answeryt/Fat-Cat .

1 Introduction 

A foundational pursuit of autonomous agents is strategic orchestration in high-stakes, long-horizon domains, ranging from codebase mainte-nance to legal auditing. While Large Language Models (LLMs) excel as general-purpose reason-ers (Brown et al., 2020), their probabilistic nature 

is inherently mismatched with the rigorous reason-ing required for reliable, open-ended workflows (Wei et al., 2022; Schick et al., 2023; Parisi et al., 2022). Existing frameworks (Chase, 2022; Li et al., 2023; Du et al., 2023; Lin et al., 2023) attempt to bridge static generation with dynamic execution, yet a fundamental mismatch causes performance to degrade rapidly as task complexity and tempo-ral depth increase(Patil et al., 2024; Anonymous, 2023; Gan et al., 2025). Current paradigms attempt to manage this com-plexity through “log-centric” designs, typically en-coding the agent’s trajectory as an static, append-only trace of nested JSON or code (Patil et al., 2024; Anonymous, 2023). We identify two key bottlenecks inherent in this log-centric approach: (1) Syntactic Barrier. Structured representations in-troduce severe “syntactic noise” (Zhu et al., 2025), where rigid schemas and repetitive delimiters con-sume part of the model’s attention budget. This architecture forces the LLM to expend critical com-putational bandwidth on decoding formatting in-variants rather than comprehending task seman-tics, effectively obscuring the high-level reasoning cues established by foundational planning methods (Yao et al., 2023). (2) Attention Sparsity. In long-horizon tasks, structured state representations (e.g., JSON) introduce persistent syntactic boilerplate that consumes a growing share of the finite context window. This progressively dilutes the semantic density of the context, forcing the model to allo-cate disproportionate attention to parsing format-ting rather than maintaining reasoning coherence, a phenomenon referred to as “context poisoning” in prior work (Ren et al., 2025; Gan et al., 2025). As task length increases, this syntactic overhead can grow super-linearly, eventually starving the agent of the contextual capacity needed for strategic plan-ning. We contend that long-horizon agentic reasoning is fundamentally unsuited for the prevailing log-centric paradigm. For an agent to function as a genuine planner, it cannot merely record execution traces; it must actively curate its internal state. Un-like existing approaches that rely on append-only logs or rigid structured representations, we propose 

> arXiv:2602.02206v1 [cs.LG] 2 Feb 2026

Fat-Cat , a document-driven architecture that tran-sitions state management from passive logging to semantic orchestration. Instead of accumulating opaque execution histo-ries, Fat-Cat maintains agent state as high-signal Markdown documents aligned with LLM pre-training priors. By isolating deliberative reason-ing from raw execution traces and continuously distilling state into a compact workspace, Fat-Cat ensures that the context presented to the model remains semantically coherent and directly action-able throughout long-horizon tasks. Our contributions are summarized as follows: 1. We propose Fat-Cat , a document-driven ar-chitecture that adopts a document-centric state representation. By aligning agent states with pre-trained language priors, we show that reducing syntactic overhead improves Hot-PotQA performance by 11.8%. 2. We introduce the Semantic Watcher , an inde-pendent runtime auditor serving as a semantic firewall . By separating verification from gen-eration, it provides closed-loop control, reduc-ing cascading failures and error propagation by 6.5%. 3. We release a Textual Strategy Evolution li-brary for parameter-free lifelong learning, en-abling agents to adapt zero-shot to novel tasks by accumulating evolved strategies rather than gradient updates. 4. Extensive evaluations across four benchmarks (e.g., Med-QA, MBPP) demonstrate the ro-bustness of Fat-Cat in high-stakes reason-ing. By reducing representational interference, the document-driven paradigm enables open-weight models to match or outperform pro-prietary baselines by up to 14.0% in several settings, highlighting the role of architectural alignment beyond parameter scale. 

2 Related Work 

2.1 Multi-Agent Frameworks & Coordination 

Multi-agent systems utilize collaboration to decom-pose complex tasks (Li et al., 2023; Du et al., 2023). While foundational frameworks established static Standard Operating Procedures (SOPs) (Hong et al., 2023; Qian et al., 2024), recent approaches introduce dynamic adaptability via MCTS (Gan et al., 2025) or social metacognition (Zhang et al., 2025b). Despite these advances, most systems typi-cally treat agents as monolithic role-players, result-ing in entangled context streams where planning is submerged in execution logs. Fat-Cat reimagines this paradigm through Context Isolation , physi-cally decoupling high-level deliberative planning from low-level procedural execution. 

2.2 Agent Operating Systems & State Management 

Treating agents as operating systems has emerged as a critical abstraction for resource management. Systems like AIOS (Mei et al., 2025) and OSAgent (Xu et al., 2024) introduce kernel-level schedul-ing to handle concurrent requests. However, these works primarily optimize computational resources (threads, memory) rather than the cognitive re-sources (attention span) of the LLM. Fat-Cat di-verges by focusing on Context Economics . While existing paradigms serialize state into opaque, low-signal structures (JSON) that induce “syntactic noise” (Blackstone et al., 2025), Fat-Cat imple-ments a High-SNR Semantic File System (Mark-down). This aligns state representation with pre-training priors, minimizing the consumption of the finite attention budget during long-horizon reason-ing. 

2.3 Metacognition & Lifelong Learning Metacognition: Reasoning enhancement has evolved from simple sampling (Wang et al., 2023b) to post-hoc reflection (Shinn et al., 2023). Recent frameworks like MASC (Shen et al., 2025) inte-grate explicit error detection. However, these ap-proaches remain largely reactive —correcting er-rors only after generation. Fat-Cat introduces pre-emptive closed-loop control via an independent Watcher to intercept hallucinations before state cor-ruption occurs. 

Lifelong Learning: Accumulating experience is critical for autonomy. Unlike Voyager (Wang et al., 2023a), which archives executable code limited to specific APIs, Fat-Cat extends skill evolution to 

textual reasoning heuristics . This facilitates the transfer of abstract planning methodologies across functionally distinct domains where specific code logic is not directly reusable, complementing struc-tural reasoning frameworks (Besta et al., 2024). 3 Methodology 

3.1 Problem Formulation Decoupled Cognitive and Execution States. We formalize the Fat-Cat agent as a tuple Φ =

⟨S cog , Sexe , W⟩ . Motivated by the need to preserve attentional focus, we implement Context Isolation, which segregates high-level planning ( Scog ) from low-level execution ( Sexe ).This design prevents raw execution traces from overwhelming the rea-soning workspace. Crucially, the Semantic Watcher 

W : H × A → { 0, 1} acts as a pre-emptive gate-keeper, blocking invalid transitions at runtime. 

High-Density Document State. Unlike rigid formats (e.g., JSON) that artificially fragment the semantic space, we construct Ct via dense Mark-down documents . This aligns the state topology with natural language priors, to increase the ratio of task-relevant semantic content per token. The effective context is defined as: 

Ct = Merge 



C(t)

> reasoner

, C(t)

> strategy

, C(t)

> traj



, (1) where Merge (·) denotes a semantic integration function that maximizes information density per token, preventing the "context fragmentation" ob-served in log-centric approaches. 

Optimization Objective. We formalize the ob-jective that the agent is designed to approximate, the agent behavior is guided by the following ob-jective: 

π∗ = arg max 

> π

Eτ ∼π

h

R(τ )

− λ · L sparsity (τ )

i

,

(2) where Lsparsity quantifies the attention budget wasted on formatting (derived as Syntactic Entropy in Appendix A). This regularization enforces a high Signal-to-Noise Ratio (SNR), directing finite atten-tion capacity toward semantic reasoning rather than syntactic overhead. 

3.2 Stage 1: Risk-Constrained Context Initialization 

To mitigate the hallucination accumulation inher-ent in naive prompting, Stage 1 initializes the rea-soning context, materialized as the reasoner.md 

document ( Creasoner ), subject to a failure mode constraint (system prompt in Appendix E.1) . We define the context initialization function as 

Creasoner = f (q, F, A), where q is the user query. 

Metacognitive Search Protocol. We model risk assessment as a retrieval task over an external failure distribution. Let Qmeta =

{qconcept , q tool , q llm } be a set of orthogonal query perspectives targeting conceptual gaps, tool misuse, and model-specific errors. The system constructs a failure set F to constrain the generation space: 

F = [

> qi∈Q meta

Retrieve (qi, Dext ), (3) where Dext represents external knowledge sources (e.g., Tavily API). This pre-computation explicitly incorporates the prior likelihood of potential er-rors into the context, reducing the probability of predictable failures. 

Dynamic Ability Matching. The agent matches the query against an ability library Lability , re-trieving formal capability specifications A (e.g., 

multi_hop_reasoning.md ). This ensures the plan is grounded in proven reasoning patterns. 

3.3 Stage 2: Dual-Path Strategy Evolution 

To address knowledge isolation, Stage 2 em-ploys a dual-path mechanism for strategy ac-quisition, formally defined as a search-and-generate process over a dynamic library Lstrat (the 

strategy_library/ directory). 

Similarity-Based Retrieval. The Strategy Critic seeks an optimal policy s∗ by maximizing the se-mantic alignment with the task. Let eq and es

denote the embedding vectors of the query and candidate strategy, respectively. The selection ob-jective is: 

s∗ = arg max 

> s∈L strat

 eq · es

∥eq∥∥ es∥



, (4) A generation trigger δ is activated if the alignment score ϕ(s∗, q ) falls below a confidence threshold τ

(set to 0.85). This thresholding mechanism bal-ances reuse and adaptation, structurally similar to methods used in continual domain adaptation (Zhang et al., 2025a). 

Evolutionary Update. When δ = 1 , a new strategy snew is synthesized (refer to Appendix E.2 for the abstraction prompt), utilizing Algorithm 1 (Appendix B) for the complete procedure. To ensure the knowledge base remains compact and high-quality, we apply a strict acceptance function 

V before archiving snew to the library:                

> V(snew ) =
> (
> Commit ,if Unique (snew )∧Sound (snew ),
> Discard ,otherwise .
> (5) Figure 1: Overview of the Fat-Cat Architecture. The system acts as a bicameral operating system: the Cognitive Wing (left) handles high-level strategy retrieval and risk assessment, while the Executive Wing (right) manages deterministic planning and closed-loop monitoring. The Watcher acts as a semantic firewall, auditing tool outputs against the step.md plan before committing to the global Markdown state.

Only approved strategies are committed to Lstrat 

and subsequently rendered into the strategy.md 

context block ( Cstrategy ). This mechanism guaran-tees that the library only grows when novel, valid knowledge is discovered. 

3.4 Stage 3: Deterministic Plan Decomposition 

The Execution Planning Agent transforms the high-level strategy into a deterministic execution trajec-tory. We define the plan P, serialized as step.md 

(Cstep ), as a sequence of discrete action-verification tuples: 

P = {(ai, ν i)}Ti=1 , (6) where ai represents the executable tool call and νi

represents the explicit verification condition (Check Criteria). This formal separation decouples the intent (planning) from the mechanics (execution), facilitating independent validation. 

3.5 Stage 4: Closed-Loop Control System 

To address error propagation in open-loop systems, we model Stage 4 as a closed-loop control system with an independent feedback loop, implemented by the Executor and Watcher agents. 

Watcher-Supervised Transition. Let the ac-tion ai yield observation oi. The Watcher Agent 

(W) functions as an independent discriminator in Appendix E.3, evaluating the transition (ai → oi)

against criteria νi and history Hi (logic detailed in Algorithm 2, Appendix B):      

> di=W(oi, ν i, H i)∈ { PASS ,FAIL }.(7)

If di = FAIL , the system rejects the tran-sition and triggers a plan refinement (updat-ing live_plan.md ), effectively pruning invalid branches of the reasoning tree in real-time. 

Dead-Loop Prevention. To ensure termination, we enforce a loop-detection constraint. Let ek be the error signature at step k. The system mandates a strategy shift if the local error density exceeds a threshold:       

> 2X
> j=0
> I(ei−j=ei)≥3 = ⇒Trigger Re-planning .(8)

This constraint prevents the agent from converg-ing into local minima of repetitive failures, a com-mon pathology in ReAct-based agents (Yao et al., 2022). 

3.6 Global Context Assembly via Document Bridge 

The MemoryBridge functions as a context aggrega-tor, ensuring that the disparate outputs ( Creasoner ,

Cstrategy , Cstep ) are formatted into a coherent lin-ear narrative. This document-driven representation minimizes the semantic gap with the LLM’s train-ing data, ensuring linear context assembly ( O(T ))as opposed to the fragmented logs of structured formats. Table 1: Main performance comparison across model backbones. Fat-Cat improves performance across efficiency-oriented (Gemini), long-context (Kimi), and proprietary (GPT-4o) models. With the Kimi-k2 backbone, Fat-Cat surpasses the GPT-4o baseline on HotPotQA by 13.9%. 

Backbone Model Method HotPotQA MBPP Bamboogle Med-QA (En) 

(Type) (Reasoning) (Coding) (Search) (Domain) 

Gemini 2.5 Flash 

(Efficient) 

ReAct 77.5% 78.5% 75.0% 70.1% AgentScope (Multi-) 80.1% 81.2% 83.2% 78.1% MetaGPT 81.0% 91.1% 88.1% 90.2% 

Fat-Cat (Ours) 84.4% 90.6% 86.5% 93.2% Kimi-k2 

(Long Context) 

ReAct 84.5% 86.8% 85.0% 84.3% AgentScope (Multi-) 86.8% 88.4% 87.0% 87.4% MetaGPT 88.1% 91.2% 88.5% 95.1% 

Fat-Cat (Ours) 93.5% 96.1% 90.4% 98.3% GPT-4o 

(Proprietary) 

ReAct 79.6% 89.2% 89.5% 80.8% AgentScope (Multi-) 83.5% 91.5% 91.0% 92.5% MetaGPT 85.7% 93.6% 92.7% 94.3% 

Fat-Cat (Ours) 86.0% 93.3% 95.2% 95.7% 

4 Experiments 

4.1 Benchmarks and Baselines Benchmarks. We evaluate Fat-Cat on four benchmarks, each targeting a distinct cognitive di-mension (details in Appendix C.1) : (1) HotPotQA (Yang et al., 2018) for multi-hop reasoning and con-text maintenance; (2) MBPP (Austin et al., 2021) for logical robustness in code generation; (3) Bam-boogle (Press et al., 2023) for information filtering in high-noise search environments; and (4) Med-QA for domain-specific knowledge application. 

Baselines. We compare Fat-Cat against three representative agent frameworks: ReAct (single-agent baseline) (Yao et al., 2022), AgentScope (un-structured multi-agent baseline) (Gao et al., 2024), and MetaGPT (SOP-based multi-agent baseline) (Hong et al., 2023). All baselines are evaluated us-ing identical backbone models to ensure fair com-parison. Full configurations are provided in Ap-pendix C.2. 

4.2 Evaluation Protocols 

To isolate the effect of the architecture from life-long learning gains, we define two evaluation proto-cols: Protocol A (Static): The Strategy Library is frozen at initialization. This setting isolates the contribution of the Document-Driven State and 

Watcher mechanism. Protocol B (Evolving): The Strategy Library updates online via Patch Review .This evaluates the system’s capacity for accumu-lated methodological learning. Full implementation details, including hyperpa-rameters, backbone models (Gemini, Kimi, GPT-4o), and tool configurations (Firecrawl is used for Markdown extraction to maintain consistency be-tween retrieved content and the document-based state.), are provided in Appendix C.3 .

5 Results and Analysis 

We structure our empirical analysis to systemati-cally validate three core claims using results across four benchmarks (HotPotQA, MBPP, Bamboogle, Med-QA): (1) Cognitive Efficiency: Document-driven state management mitigates the Syntactic Tax , di-rectly liberating attentional bandwidth for reason-ing; (2) Architectural Scaling: The performance advantages of F AT -C AT scale non-linearly with task complexity and temporal depth; (3) Failure Mode Reconfiguration: Closed-loop control transitions the bottleneck from struc-tural fragility to addressable epistemic gaps, en-abling open-weights models to partially offset raw parameter advantages. 

5.1 System Performance and Model Generalization 

Table 1 reveals that Fat-Cat consistently elevates the performance ceiling across all backbones. 

Efficacy vs. Static Paradigms. Fat-Cat estab-lishes a superior accuracy-complexity trajectory, with gains correlating positively with task hori-zon—peaking at +14.0% on Med-QA (Kimi-k2). This confirms that document-driven orchestration successfully collapses the context dilution inherent in long-horizon reasoning. 

Dynamic Workflows & Adaptability. Unlike rigid SOP-based workflows like MetaGPT, Fat-Cat 

yields a 5.4% improvement on HotPotQA. This is driven by the Patch Review mechanism, which converts inference-time computation into adaptive methodological priors to navigate edge cases that derail standardized protocols. 

Capability Equalization Effect. Structural op-timization bridges the gap between open-weights and proprietary models: Fat-Cat -equipped Kimi-k2 (93.5%) outperforms the GPT-4o ReAct base-line (86.0%). This equalization effect demonstrates that robust cognitive architecture can partially off-set raw parameter advantages through superior con-text management. 

Table 2: Impact of state representation format on Hot-PotQA. Markdown achieves the optimal balance of structural scaffolding and cognitive ease compared to JSON, XML, and YAML.                    

> State Format Acc. ∆vs. JSON Render Score
> JSON (Nested) 74.2% -89.4% XML (Tag-based) 79.5% +5.3% 98.0% Plain Text 81.4% +7.2% 99.1% YAML (Low-Syntax) 83.9% +9.7% 97.5%
> Fat-Cat Markdown 86.0% +11.8% 98.2%

5.2 Analysis of Representational Alignment 

To address the critical question of “Why Mark-down?”, we compare Fat-Cat against JSON (standard), XML (Claude-favored), and YAML (AutoGPT-favored). The results (Table 2) reveal a clear cognitive hierarchy supporting our Represen-tational Alignment Principle. 

Syntax Noise vs. Attention Budget. JSON per-forms worst (74.2%) due to “syntactic noise”, the model wastes attention heads tracking brackets and escape characters. YAML improves accuracy (+9.7%) by minimizing this noise. XML, while syntactically robust, suffers from high token over-head due to repetitive tags. 

Data-Centric vs. Document-Centric. While YAML is efficient for data entry, it lacks narrative flow; Fat-Cat transforms the state into a readable document with headers (e.g., # Strategy ) as at-tention anchors, aligning with LLM pre-training priors and confirming that optimal formats priori-tize semantic coherence over compactness. 

Figure 2: Complexity-Performance Correlation. Re-gression analysis reveals a strong positive correlation (R2 = 0 .98 ) between task complexity (CCI) and perfor-mance gain. This confirms that the document-driven ar-chitecture provides benefits as cognitive load increases, effectively amortizing the syntactic tax. 

5.3 The Complexity-Performance Correlation 

We define a Cognitive Complexity Index (CCI) based on reasoning steps, tool dependency, and domain specificity. Regression analysis reveals that Fat-Cat’s advantage scales linearly with complexity (R2 = 0 .98 , see Figure 2): 

∆Fat-Cat vs ReAct ≈ 17 .7% × CCI + 2 .5% (9) This confirms that Fat-Cat effectively amortizes the “cognitive tax” imposed by traditional frameworks in high-complexity regimes. The results reveal a critical pattern: Fat-Cat’s advantage scales with task complexity . To quan-tify this, we define a Cognitive Complexity Index (CCI) for each benchmark based on three factors: (1) required reasoning steps, (2) external tool de-pendency, and (3) domain specificity. The regression analysis yields a quantitative re-lationship: 

∆Fat-Cat vs ReAct ≈ 17 .7% × CCI + 2 .5% (10) This strong positive correlation ( R2 = 0 .98 , see Figure 2) confirms our core theoretical insight: traditional agent frameworks impose a “cogni-tive tax” that grows non-linearly with complexity, whereas Fat-Cat’s document-native representation effectively amortizes this cost. 

5.4 Component and Topology Ablation Study 

Table 3 isolates the effects of state topology and architectural modules, corroborating the Represen-tational Alignment hypothesis through a distinct task-dependent sensitivity. 

Topology: Linearization vs. Syntactic Tax. 

We observe a double dissociation between task modalities and state formats. For Reasoning Table 3: Ablation Analysis across Modules and Topolo-gies. Insight: Markdown-KV minimizes cognitive load by aligning with LLM priors, whereas structured JSON induces syntactic entropy. Parentheses indicate absolute degradation vs. full Fat-Cat implementation.                                       

> Configuration HotPotQA MBPP Avg Drop
> (Role / Explanation) (Reasoning) (Coding)
> Fat-Cat (Default: Md-KV) 86.0 96.0 –
> (Linear Baseline)
> A. State Topology (Representation)
> Markdown-Table 74.9 90.5 -8.3
> (2D Grid Topology) (-11.1) (-5.5)
> JSON (Nested) 74.2 89.2 -9.3
> (Tree Syntax) (-11.8) (-6.8)
> B. Runtime Control (Functionality)
> w/o Watcher Agent 79.5 88.0 -7.3
> (No Semantic Audit) (-6.5) (-8.0)
> w/o Metacognitive Search 81.2 93.0 -3.9
> (No Risk Filter) (-4.8) (-3.0)
> C. Memory Modules (Priors)
> w/o Strategy Library 80.5 92.5 -4.5
> (No Planning Priors) (-5.5) (-3.5)
> w/o Ability Library 84.0 89.0 -4.5
> (No Tool Schemas) (-2.0) (-7.0)

(HotPotQA), the model suffers a 11.1% penalty when substituting Markdown-KV with Markdown-Tables. We attribute this to the linearization penalty : serializing 2D spatial structures into 1D token sequences consumes the attention bandwidth necessary for logical tracking. Conversely, for code generation (MBPP), the bottleneck is Syn-tactic Entropy. JSON-based states degrade perfor-mance by 6.8%, primarily due to the “escaping tax”—the cognitive overhead of maintaining rigid syntax (e.g., character escaping) which competes with code logic for finite attention heads. 

Architecture: Watcher as a Semantic Fire-wall. Ablation confirms that independent audit-ing is vital for execution-heavy tasks. Remov-ing the Watcher triggers the sharpest decline in MBPP (-8.0%), proving that open-loop generation is inherently brittle. Without this runtime inter-ceptor, minor hallucinations (e.g., API misuse) inevitably cascade into systemic failure. Mean-while, the Strategy Library proves indispensable for planning-heavy tasks (HotPotQA, -5.5%), pro-viding the methodological priors missing from the model’s static parametric memory. 

5.5 Context Efficiency Analysis 

To assess architectural scalability, we analyze the cumulative token consumption across interaction steps. 

Table 4: Comparative error distribution. Fat-Cat trans-forms errors from architectural failures to knowledge limitations. Note the reduction in Cascading Failures due to the Watcher.                          

> Error Type ReAct Fat-Cat Interpretation Cascading Failures 42.3% 5.1% Watcher acts as acircuit-breaker for open-loop errors
> Context Frag. 28.7% 8.2% Markdown maintains narrative coherence
> Search Noise 18.5% 12.3% Metacognitive search filters irrelevancies
> Strategy Gaps 10.5% 55.2% Shift to knowledge re-trieval limits (Domain Vacuum)
> Watcher FP 0.0% 9.7% Conservative auditing rejects valid outputs
> Format Violations 0.0% 9.5% Stricter syntax require-ments

Complexity Analysis ( O(T 2) vs. O(T )). Stan-dard paradigms (e.g., ReAct) rely on linear his-tory concatenation, causing context length to grow quadratically: PTt=1 tL ≈ O(T 2) for T steps of length L. This inevitably leads to context win-dow exhaustion. In contrast, Fat-Cat operational-izes Structured Document-State Orchestration (SDSO) , where state updates are executed via in-place Markdown modification rather than incre-mental logging. This mechanism collapses the growth to a linear trajectory, O(T ), preserving the attention budget for long-horizon execution. 

Empirical Gains. Evaluations corroborate this theoretical scaling, yielding a 19.35% net token saving in long-horizon workflows. As analyzed in Appendix C.5, this global compression successfully amortizes the operational overhead of the Semantic Watcher, ensuring architectural efficiency without compromising reasoning rigor. 

5.6 Failure Mode Topology 

Table 4 reveals a fundamental reconfiguration of failure modes. By replacing log-centric traces with the Semantic File System, we effectively neutralize the structural fragility of open-loop agents. 

From Structural Decay to Systemic Rigor. 

Cascading Failures, the most pathological mode in reactive systems, drop sharply from 42.3% to 5.1%, a suppression directly attributed to the Watcher act-ing as a semantic circuit-breaker. Simultaneously, Context Fragmentation is mitigated by over 70% (28.7% → 8.2%), validating that document-driven state orchestration successfully mitigates the atten-tion drift typical of long-horizon execution. Figure 3: Qualitative analysis of Fat-Cat’s control mechanisms on a GAIA case study. (1) Strategy Evolution:     

> The agent combines previously separate reasoning patterns into a new heuristic ( S15 ), which guides planning before execution. (2) Representational Alignment: State is maintained in a Markdown-based semantic file system, reducing syntactic overhead and preserving salient information in the context. (3) Semantic Firewall: An independent Watcher monitors intermediate actions and blocks invalid operations, preventing error propagation during execution.

Epistemic Boundaries. The primary error den-sity shifts from architectural flaws to Strategy Gaps 

(10.5% → 55.2%). This inversion signifies that 

Fat-Cat has decoupled execution logic from knowl-edge limits: the system fails not due to procedural incoherence, but due to the epistemic vacuum in the strategy library. This transitions the bottleneck from intractable structural entropy to addressable knowledge accumulation. 

5.7 Case Study: Resolution of High-Dimensional Constraints 

To demonstrate architectural efficacy, we audit a complex GAIA (Level 3) task: "How many Asian countries still have a monarchy and access to the sea in 2021?" This query imposes conflicting geopolitical, definitional, and temporal constraints. As visualized in Figure 3, the system resolves this via three stages: 

Execution Trace. (1) Strategy Synthesis: The metacognitive module detected gaps in retrieved priors (I1, I3), specifically lacking "temporal lock-ing." It triggered a Merge/Upgrade operation, syn-thesizing Strategy I5 to enforce revision history checks. (2) State Initialization: Strategy I5 ini-tialized a Markdown-KV state, maintaining the multi-hop criteria ("Asia" + "Monarchy" + "Mar-itime") as a coherent narrative constraint, avoiding the fragmentation common in JSON. (3) Closed-Loop Audit: The Watcher intercepted the Ex-ecutor’s generic search for "Asian Monarchies" (see INTERCEPTED in Figure 3). It enforced the 

?oldid parameter to prevent data contamination from 2025 updates. This precise audit filtered out 

Bhutan (landlocked), deriving the correct answer of 12.(For additional case studies on Code Generation and Context Maintenance in Appendix D.3.) 

6 Conclusion 

Fat-Cat addresses the structural fragility of LLM agents by unifying document-driven state manage-ment with closed-loop runtime auditing. By align-ing system topology with pre-training priors, our architecture achieves a +14.0% gain on Med-QA and allows open-weights backbones to remain com-petitive with proprietary models in long-horizon reasoning tasks. Beyond raw accuracy, the decou-pled Watcher acts as a semantic circuit-breaker, re-ducing cascading failures by 6.5%. Together with textual strategy evolution, this design improves the effective use of context while preserving human-readable state representations. Overall, our find-ings suggest that representational alignment plays a central role in building robust and adaptive agents, complementary to scaling model parameters. 7 Limitations 

Dependency on Strong Backbones. Our Struc-tured Document-State requires robust formatting adherence. Experiments confirm Kimi-k2 main-tains this global state effectively, whereas smaller models (e.g., Llama-3-8B) prone to structural hal-lucinations may require specific fine-tuning. 

Latency Trade-off. While achieving O(T ) con-text compression, the serial Watcher validation incurs a ≈10% wall-clock latency penalty. The framework explicitly prioritizes reasoning rigor and safety over millisecond-level real-time responsive-ness. 

Knowledge Dependence. Strategy Evolution re-lies on retrieving priors from external bases (e.g., the web). In knowledge-sparse domains (e.g., pro-prietary systems), the system struggles to synthe-size effective plans, as it is designed to distill exist-ing information rather than invent novel solutions from scratch. 

Long-term Scalability. Indefinite growth of the Strategy Library risks semantic interference and retrieval degradation. While Patch Review filters entries, the system currently lacks an active con-solidation or "forgetting" mechanism to manage redundancy over lifelong deployment. 

8 Ethics Statement 

Fat-Cat uses external search APIs (Tavily) that may retrieve information from public sources. We do not filter or modify retrieved content, which could potentially include biased or harmful information. The system’s strategy evolution mechanism could amplify biases present in training data or retrieved strategies. Users should validate generated strate-gies before deployment in critical applications. The system does not store personal data, but tool out-puts may contain sensitive information that should be handled according to data protection regulations. 

References 

Anonymous. 2023. Dynamic tool selection for large language models. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural Language Processing .Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models. In Advances in Neural Information Processing Systems , volume 34. Maciej Besta, Nils Blach, Ales Kubicek, Robert Ger-stenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of thoughts: Solving elaborate problems with large lan-guage models. Proceedings of the AAAI Conference on Artificial Intelligence , 38. James Blackstone, Alastair Ferncombe, Hugo Whit-lam, Robert Cattermole, Kent Blumberg, and Simon Chilvers. 2025. Coarticulatory inference propagation in probabilistic attention meshes for large language model sampling flux stabilization. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901. Harrison Chase. 2022. Langchain: Building appli-cations with llms through composability. GitHub Repository .Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenen-baum, and Igor Mordatch. 2023. Improving factual-ity and reasoning in language models through multia-gent debate. In Forty-first International Conference on Machine Learning .Bingzheng Gan, Yufan Zhao, Tianyi Zhang, Jing Huang, Li Yusu, Shu Xian Teo, Changwang Zhang, and Wei Shi. 2025. Master: A multi-agent system with llm specialized mcts. In Proceedings of the 2025 Confer-ence of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) ,pages 9409–9426. Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhi-jian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, and 1 others. 2024. Agentscope: A flexible yet robust multi-agent plat-form. arXiv preprint arXiv:2402.14034 .Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, and 1 others. 2023. Metagpt: Meta programming for a multi-agent collaborative framework. In The Twelfth International Conference on Learning Representa-tions .Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: Communicative agents for" mind" exploration of large language model society. Advances in Neural Information Processing Systems , 36:51991–52008. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqi-uyue Ping, and Qin Chen. 2023. Agentsims: An open-source sandbox for large language model evalu-ation. arXiv preprint arXiv:2308.04026 .Kai Mei, Xi Zhu, Wujiang Xu, Wenyue Hua, Mingyu Jin, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. 2025. Aios: Llm agent operating system. In Proceedings of the 2nd Confer-ence on Language Modeling (COLM 2025) .Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models. In Advances in Neural Information Processing Systems , volume 35. Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2024. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems , 37:126544–126565. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computa-tional Linguistics: EMNLP 2023 , pages 5687–5711. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, and 1 others. 2024. Chatdev: Com-municative agents for software development. In Pro-ceedings of the 62nd Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: Long Papers) , pages 15174–15186. Yuqi Ren, Renren Jin, Tongxuan Zhang, and Deyi Xiong. 2025. Do large language models mirror cog-nitive language processing? In Proceedings of the 31st International Conference on Computational Lin-guistics , pages 2988–3001. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. In Advances in Neural Information Processing Sys-tems , volume 36. Xu Shen, Qi Zhang, Song Wang, Zhen Tan, Xinyu Zhao, Laura Yao, Vaishnav Tadiparthi, Hossein Nourkhiz Mahjoub, Ehsan Moradi Pari, Kwonjoon Lee, and 1 others. 2025. Metacognitive self-correction for multi-agent system via prototype-guided next-execution reconstruction. arXiv preprint arXiv:2510.14319 .Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Re-flexion: Language agents with verbal reinforcement learning. Advances in Neural Information Process-ing Systems , 36:8634–8652. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-ima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291 .Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. In-ternational Conference on Learning Representations .Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems , 35:24824– 24837. Jiaming Xu, Kaibin Guo, Wuxuan Gong, and Runyu Shi. 2024. Osagent: Copiloting operating system with llm-based agent. In 2024 International Joint Conference on Neural Networks (IJCNN) , pages 1–9. IEEE. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo-pher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empiri-cal methods in natural language processing , pages 2369–2380. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Ad-vances in Neural Information Processing Systems ,36. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations .Liang Zhang, Ziyao Lu, Fandong Meng, Hui Li, Jie Zhou, and Jinsong Su. 2025a. Advancing smoe for continuous domain adaptation of mllms: Adaptive router and domain-specific loss. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,pages 26584–26602. Xuanming Zhang, Yuxuan Chen, Samuel Yeh, and Sharon Li. 2025b. Metamind: Modeling human so-cial thoughts with metacognitive multi-agent systems. 

arXiv preprint arXiv:2505.18943 .Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Xiao Chuanfu, Dahua Lin, and Chao Yang. 2025. Sampleattention: Near-lossless acceleration of long context llm infer-ence with adaptive structured sparse attention. Pro-ceedings of Machine Learning and Systems , 7. A Theoretical Analysis 

This section provides a theoretical account of why Fat-Cat’s document-driven, closed-loop architec-ture yields increasing advantages as task complex-ity grows. We analyze the effects of (i) represen-tational alignment between agent states and the LLM’s pre-training prior, and (ii) error accumu-lation in long-horizon reasoning, with particular emphasis on the role of attention allocation. 

A.1 Representational Alignment via Cross-Entropy Minimization 

We formalize the impact of agent state representa-tion on reasoning efficiency as a distribution mis-match problem between the LLM’s pre-training prior and the inference-time context structure. 

Definition 1 (Syntactic Overhead). Let Dpre de-note the distribution of natural language text on which the LLM is pre-trained, and Dstate denote the distribution induced by the serialized agent state. The cognitive load L incurred during in-ference can be approximated by the cross-entropy: 

L = H(Dstate , Dpre )= H(Dstate ) + DKL (Dstate ∥ D pre ), (11) where H(Dstate ) measures the intrinsic entropy of the state content, and DKL (Dstate ∥ D pre ) cap-tures the distributional shift from the model’s pre-training prior. 

Attention Interpretation. Transformer-based language models operate under a finite attention budget. Tokens that deviate from the pre-training distribution tend to require explicit modeling effort. Consequently, a higher cross-entropy implies that a larger fraction of attention is allocated to syntactic maintenance rather than to reasoning-critical se-mantics, effectively diluting attention over relevant content. 

Proposition 1. Markdown-based agent states in-duce significantly lower syntactic overhead than structured formats such as JSON or XML. Analysis. Structured formats like JSON are characterized by rigid grammars, nested recursion, and high-frequency delimiters (e.g., braces, quotes, commas). These patterns induce a syntactic distri-bution Djson that diverges substantially from the narrative- and document-dominated Dpre , resulting in a large DKL (Djson ∥ D pre ).From an attention perspective, this mismatch forces the model to expend a non-trivial portion of its finite attention budget on preserving syntactic well-formedness (e.g., bracket matching and hierar-chical nesting), rather than on semantic reasoning. As a result, attention over meaning-bearing tokens becomes effectively diluted, yielding a sparser and less focused attention pattern for high-level infer-ence. In contrast, Fat-Cat’s Markdown-based states (Dmd ) leverage structural primitives—such as head-ers, lists, and natural language transitions—that are well aligned with Dpre . As a result, 

DKL (Dmd ∥ D pre ) ≪ DKL (Djson ∥ D pre ), (12) allowing attention to concentrate on semantically salient spans. This alignment mitigates attention sparsity over reasoning-relevant tokens and in-creases the effective semantic information density per token, theoretically explaining the performance degradation observed for structured formats in Ta-ble 3. 

A.2 Asymptotic Error Bound in Closed-Loop Reasoning 

We next analyze how Fat-Cat’s closed-loop design interacts with task complexity. We model multi-step reasoning as a discrete-time stochastic process to derive the relationship between reasoning hori-zon length and performance gain. 

Problem Setup. Consider a task T requiring N

sequential reasoning steps, where N serves as a proxy for the Cognitive Complexity Index (CCI). Let ϵ ∈ [0 , 1] denote the inherent error probability of the LLM at each step. 

Open-Loop Dynamics (Baseline). In standard open-loop frameworks (e.g., ReAct), the reasoning process forms a Markov chain in which each state depends solely on the previous one. A single error propagates unchecked and leads to task failure. The probability of successfully completing the task is therefore: 

Popen (N ) = (1 − ϵ)N

≈ 1 − N ϵ (for small ϵ). (13) 

Closed-Loop Dynamics (Fat-Cat). Fat-Cat in-troduces a Watcher mechanism that performs inter-mediate validation and correction. Let δ ∈ [0 , 1] 

denote the probability that an erroneous step is successfully detected and corrected. The effective error rate per step becomes ϵef f = ϵ(1 − δ), yield-ing: 

Pclosed (N ) = (1 − ϵ(1 − δ)) N

≈ 1 − N ϵ (1 − δ). (14) 

Derivation of the Linear Gain Law. We define the performance gain ∆( N ) as the difference be-tween closed-loop and open-loop success probabil-ities. Using a first-order Taylor expansion: 

∆( N ) = Pclosed (N ) − Popen (N ) (15) 

≈ (1 − N ϵ + N ϵδ ) − (1 − N ϵ ) (16) 

= ( N ϵ )δ. (17) 

Conclusion. The above derivation shows that 

∆( N ) grows linearly with the task length N , and thus with CCI, scaled by the product of the base error rate ϵ and the correction efficacy δ. This result aligns with the empirical regression slope (∆ ≈ 17 .7% × CCI ) observed in Figure 2. It in-dicates that while open-loop systems suffer from compounding errors in long-horizon reasoning, Fat-Cat’s closed-loop architecture effectively amortizes cumulative error risk, yielding increasing returns as task complexity scales. 

B Algorithms and System Architecture 

This section provides the formal algorithmic defini-tions and code-level implementation details of the Fat-Cat kernel. 

B.1 MemoryBridge Architecture 

The MemoryBridge implements a linear document assembly mechanism to serialize the heterogeneous state into a coherent Markdown stream. The core logic is implemented as follows: 

def build_context(self) -> str: parts = [] for section in self._sections: parts.append(f"## {section.header}") parts.append(section.content) return "\n".join(parts) 

This linear assembly creates a coherent narra-tive that LLMs can process effectively, maintaining context coherence across long-horizon tasks. 

B.2 Watcher Validation Logic 

The Decoupled Watcher (Stage 4) enforces closed-loop control through three rigorous semantic checks. A tool execution is only committed to 

step.md if it passes all filters: 1. Syntactic Validity: Output is non-empty and contains no interpreter errors. 2. Goal Relevance: The observation directly advances the sub-goal defined in the current plan step. 3. Context Consistency: The output does not contradict facts established in reasoner.md 

or previous steps. 

B.3 Algorithm Pseudocode Strategy Selection and Evolution. Algorithm 1 shows the complete strategy evolution process: 

Algorithm 1 Strategy Selection and Evolution 

Require: Task t, Strategy Library L, Similarity Threshold θ = 0 .85 

Ensure: Selected Strategy s 

> 1:

Compute embeddings for t and all strategies in 

L 

> 2:

Rank strategies by cosine similarity: sim (t, s i) 

> 3:

if max i sim (t, s i) ≥ θ then  

> 4:

return arg max i sim (t, s i) 

> 5:

else  

> 6:

Generate new strategy snew via external search  

> 7:

Evaluate snew : novelty, uniqueness, sound-ness  

> 8:

if evaluation passes then  

> 9:

Add snew to L 

> 10:

return snew  

> 11:

else  

> 12:

return best matching strategy from L 

> 13:

end if  

> 14:

end if Watcher Runtime Monitoring. Algorithm 2 shows the complete Watcher validation process: 

C Additional Experimental Details 

C.1 Benchmark Details: The Diagnostic Matrix 

As outlined in Section 4.1, we employ a diagnostic matrix to stress-test specific cognitive dimensions. Details are as follows: • HotPotQA (Multi-hop Reasoning) (Yang et al., 2018): We use the "distractor" setting (200 samples) where the agent must identify relevant passages from mixed context. This Algorithm 2 Watcher Semantic Firewall Logic 

Require: Current observation ot, Step tuple 

(at, ν t), Context history Chist , Error history H

Ensure: Decision d ∈ { PASS , FAIL }, Updated Plan P′ 

> 1:

Phase 1: Execution Layer Check  

> 2:

vexec ← IsExecSuccess (ot) 

> 3:

if ¬vexec then  

> 4:

etype ← RuntimeError  

> 5:

goto HandleError  

> 6:

end if  

> 7:

Phase 2: Semantic Firewall (The Watcher)  

> 8:

vsem ← VerifyCriteria (ot, ν t) // Check against νt 

> 9:

vf act ← DetectHallucination (ot, Chist ) // Check consistency  

> 10:

if ¬vsem ∨ ¬ vf act then  

> 11:

etype ← SemanticMismatch  

> 12:

goto HandleError  

> 13:

else  

> 14:

return (PASS, Pcurrent ) 

> 15:

end if  

> 16:

HandleError : 

> 17:

Update error history: H ← H ∪{(at, o t, e type )} 

> 18:

nrepeat ← CountPattern (H, e type ) 

> 19:

if nrepeat ≥ 3 then  

> 20:

P′ ← ReplanningAgent (Chist , H) // Global strategy shift  

> 21:

return (FAIL, P′) 

> 22:

else  

> 23:

P′ ← LocalCorrection (Pcurrent , e type ) 

> 24:

return (FAIL, P′) 

> 25:

end if 

tests context coherence and the Semantic File System’s ability to maintain narrative continu-ity. • MBPP (Code Generation) (Austin et al., 2021): We select 200 problems requiring stan-dard library manipulation. This benchmark validates the Decoupled Watcher’s capacity to intercept runtime logic errors (e.g., edge cases) that open-loop agents miss. • Bamboogle (Open-Domain Search) (Press et al., 2023): A dataset of 125 "google-proof" questions. It serves to assess the efficacy of our Metacognitive Search Protocol in filtering high-noise search results. • Med-QA (Domain Adaptation): Compris-ing 150 questions from USMLE (English) and MCMLE (Chinese). This tests the Strat-egy Evolution mechanism’s ability to facilitate cross-lingual transfer without parameter up-dates. 

C.2 Baseline Configurations 

To ensure fair comparison, all baselines utilize the same backbone models as Fat-Cat where ap-plicable: Single-Agent Baseline (ReAct) (Yao et al., 2022): The standard implementation from AgentScope. It integrates reasoning with tool use but lacks explicit metacognitive planning. Vanilla Multi-Agent Baseline (AgentScope-Multi) (Gao et al., 2024): A standard planner–executor architec-ture using message passing. This isolates the effect of Fat-Cat’s document-driven state representation. 

SOP-Based Framework (MetaGPT) (Hong et al., 2023): Configured with predefined roles (Product Manager, Architect, Engineer) and rigid Standard Operating Procedures (SOPs). This serves as the static workflow baseline. 

C.3 Implementation Details and Hyperparameters 

Fat-Cat is built on AgentScope. Evaluations are conducted across three backbones: Kimi-k2 (pri-mary), Llama-3-70B-Instruct , and GPT-4o .

Tool Environment. (1) Firecrawl for Semantic Retrieval: Unlike standard HTML scrapers, we utilize Firecrawl to extract high-fidelity Mark-down from web sources. This ensures that external knowledge enters the system in a format strictly aligned with our document-driven state, minimiz-ing the "syntactic noise" of raw HTML tags. (2) Sandboxed Execution: Code execution (Stage 4) runs in a local Python interpreter with a strict 30-second timeout per call to prevent infinite loops (Dead-Loop Prevention). 

Generation Parameters. (1) Reasoning & Plan-ning: Temperature T = 0 .7 to encourage creative strategy synthesis. (2) Execution & Monitoring: 

Temperature T = 0 .0 for the Executor and Watcher to ensure deterministic tool usage and rigorous au-diting. 

System Thresholds. (1) Strategy Retrieval: We use text-embedding-3-small with a cosine sim-ilarity threshold of θ = 0 .85 for deduplication and retrieval. (2) Metacognition: Search queries are capped at 3 per task category to prevent token wastage. 

C.4 Strategy Library Evolution Dynamics 

The Strategy Library is initialized as empty and evolves purely through usage. We observe the fol-lowing growth dynamics: Cold Start Phase (0-50 tasks): The library grows rapidly, accumulating ap-proximately 15–20 core strategies across domains. 

Quality Control: The Patch Review mechanism acts as a gatekeeper, accepting approximately 60% of generated strategies. This acceptance rate re-flects a trade-off between novelty (allowing new patterns) and quality (rejecting redundant or flawed logic). Saturation Phase (>200 tasks): The li-brary stabilizes at ≈ 23 high-level strategies with a 45% reuse rate. This indicates effective knowledge accumulation without excessive memory bloat. 

C.5 Computational Overhead and Efficiency Analysis 

While Fat-Cat introduces additional modules (Watcher, Strategy Retriever), the global context efficiency offsets these costs. 

Overhead Breakdown. (1) Watcher Validation: Adds ≈ 10% token overhead per step due to prompt wrapping and verification logic. (2) Metacognitive Search: Adds ≈ 2 API calls per task (negligible latency). (3) Vector Retrieval: Adds <50ms latency. 

Net Token Savings (19.35%). Although per-step cost increases, Fat-Cat significantly reduces the to-tal number of steps required to solve complex tasks by preventing cascading failures (which typically incur long, futile retry loops in ReAct). Empiri-cally, on the Med-QA benchmark, the reduction in error-recovery turns resulted in a total token con-sumption of CFatCat ≈ 0.806 × CReAct , translating to a 19.35% net saving. This confirms that the "Pre-vention is cheaper than Cure" principle holds for LLM agent economics. 

D Additional Empirical Results 

D.1 Fine-grained Domain Analysis 

To understand the source of Fat-Cat’s improve-ments, we decompose the performance on Hot-PotQA by question category. As shown in Ta-ble 5, Fat-Cat achieves consistent gains across all domains. 

Table 5: Performance breakdown by domain (Hot-PotQA). Fat-Cat shows the largest gains in Geography and Science, domains typically requiring rigorous multi-hop spatial and causal reasoning. 

Domain ReAct Fat-Cat Gain ( ∆)

Geography 71.2% 84.5% +13.3% Science 72.1% 85.3% +13.2% History 75.8% 87.2% +11.4% 

Analysis. The performance gap is most pro-nounced in Geography (+13.3%) and Science 

(+13.2%). These domains often involve spatial rela-tions or causal chains (e.g., "Is the director of Film A from the same city as the writer of Book B?" ) that require maintaining long-range context dependen-cies. Fat-Cat’s document-driven state effectively prevents the "context fragmentation" observed in ReAct, leading to higher multi-hop retrieval accu-racy. 

D.2 Strategy Library Composition 

Table 6 details the composition of the evolved Strat-egy Library after processing 200 diverse tasks. 

Table 6: Strategy Library Statistics (Post-Evolution). The distribution shows a balanced accumulation of high-level reasoning patterns and low-level execution skills.             

> Strategy Category Count Example Heuristic
> Multi-hop Reasoning 8"Bridge-Entity Identification"
> Code Generation 6"Input Sanitization Pattern"
> Web Search 5"Query Decomposition"
> Domain-Specific QA 4"Medical Evidence Triangulation"
> Total 23 Reuse Rate: 45%

The relatively compact size of the library (23 strategies for 200 tasks) combined with a high reuse rate (45%) indicates that the system is suc-cessfully abstracting generalizable methodologies rather than memorizing task-specific shortcuts. 

D.3 Extended Qualitative Analysis 

While the GAIA case study in Section 5.7 demon-strates the system’s logical reasoning capabilities, the following case studies illustrate Fat-Cat’s ro-bustness in executable environments (Code Gener-ation) and context-heavy retrieval (HotPotQA). 

D.4 Case Study 2: Preemptive Robustness in Data Analysis (MBPP/Excel) 

This case highlights how Fat-Cat handles "dirty data," a common failure mode where standard agents blindly trust input file integrity. Task Definition. Query: "Calculate the total sales difference between ’Wharton’ and ’Algri-mand’ from the attached Excel file." Data Con-straints: The provided dataset contains intentional noise: typos (e.g., "Wharvton" instead of "Whar-ton") and mixed string/numeric formats. 

Comparative Execution Trace. (1) Base-line Failure (ReAct): The baseline agent immediately generated a standard Pandas query: 

df[df[’City’]==’Wharton’][’Sales’].sum() .This approach silently ignored the typo-ridden rows ("Wharvton"), resulting in a significant calculation error (Undercounting by 30%). (2) Fat-Cat Intervention: Stage 1 (Strategy Re-trieval): The Strategy Critic identified the task as "Tabular Analysis" and retrieved the Robust Data Processing Strategy (P4). This strategy explic-itly mandates a schema_validation step before any aggregation. Stage 2 (State Initialization): 

The reasoner.md state was updated with the P4 protocol, locking the execution context to require "Data Sanitization" before any "Calculation" ac-tions. Stage 3 (Closed-Loop Control): The Ex-ecutor initially proposed the same naive code as the baseline. The Watcher intercepted this action: 

> > INTERCEPTED: Aggregation Action > VIOLATION: Strategy P4 requires unique value inspection prior to filtering. > FEEDBACK: Run df[’City’].unique() to detect entity variations.

Outcome: The revised plan executed the unique-ness check, revealing the "Wharvton" typo. The agent then generated a fuzzy-matching normaliza-tion step, deriving the correct total. 

D.5 Case Study 3: Mitigating Context Fragmentation (HotPotQA) 

This case demonstrates the advantage of the Se-mantic File System (Markdown State) over JSON in maintaining long-horizon context. 

Scenario. A multi-hop question requires synthe-sizing information from three disjoint Wikipedia articles (Entity A → Entity B → Answer). 

Failure Analysis. (1) JSON State (Base-line): As the retrieved content grew, the JSON state became deeply nested (history[4].tool_outputs.content... ). The LLM’s attention mechanism struggled to attend to the "Bridge Entity" buried in the JSON structure, leading to a "Lost-in-the-Middle" hal-lucination where the agent forgot the connection between Entity A and B. (2) Markdown State (Fat-Cat): The reasoner.md document maintained a flat, narrative summary of findings: ## Fact 1: [Entity A] is linked to [Entity B] . This linear topology allowed the model to retrieve the bridge entity effectively in Step 3, successfully completing the reasoning chain. 

E Core Prompt Templates 

To ensure reproducibility and facilitate the “High-SNR” state management described in Section 3, we provide the core system prompts used in Fat-Cat . These prompts are rigorously aligned with the 

Bicameral Architecture (Cognitive vs. Executive Wing) and enforce the Context Isolation protocols. 

E.1 Cognitive Planner (Stage 1: Context Initialization) 

The Cognitive Planner (corresponding to the “Fat Agent” in Figure 1) initializes the reasoner.md 

document. Note the strict prohibition of JSON to minimize syntactic noise. 

System Prompt: Cognitive Planner                                                

> You are Fat-Cat ,ametacognitive reasoning agent designed for High-SNR (Signal-to-Noise Ratio) state management. Your goal is to initialize the global state document (reasoner.md) for the user’s query.
> CORE DIRECTIVES: 1. No Syntactic Noise: Do NOT use opaque data structures (JSON/XML). Output purely in narrative Markdown. 2.
> Context Isolation: Do not pollute the plan with low-level execution details yet. Focus on high-level strategy. 3. Risk Assessment:
> Perform a pre-mortem analysis as defined in the “Cognitive Wing” protocol.
> OUTPUT SCHEMA (reasoner.md): #Goal [Concise definition of the user’s intent] # Risk Assessment -Ambiguity Gap: [e.g., Is the timeframe vague?] -Tool Risks:
> [e.g., Search might return outdated info] #High-Level Strategy [Selected from strategy.md or synthesized de novo] # Initial Plan - [ ] Step 1: ...

E.2 Strategy Critic & Synthesizer (Stage 2: Evolution) 

This prompt drives the Strategy Critic (Section 3.3), converting specific execution traces into gen-eralizable heuristics for the strategy.md library. System Prompt: Strategy Synthesizer 

You are the Strategy Synthesizer . You have just completed a complex reasoning task. Your job is to distill a Reusable Heuristic 

from this experience to update the Strategy Library. 

INPUT TRACE: {execution_history} 

TASK: Abstract the specific actions into a general textual strategy. Do NOT mention specific entities; use abstract terms (e.g., "Target Entity"). 

OUTPUT TEMPLATE (strategy.md entry): #Strategy Name: [e.g., "Time-Constrained Multi-Hop Search"] # Applicability: [When to use this?] # Protocol: 1. Decompose the query into temporal and entity constraints. 2. Use specific search operators to filter noise. 3. Cross-verify dates using ‘?oldid‘ or revision history. 

E.3 Semantic Watcher (Stage 4: Closed-Loop Control) 

The Watcher Agent acts as the “Semantic Firewall” auditing the step.md transitions. It enforces the separation between the Executive Wing and the global state. 

System Prompt: Semantic Watcher 

You are an independent Audit Agent 

responsible for Closed-Loop Control. You do NOT execute tasks. You only VERIFY updates to step.md. 

INPUT CONTEXT: 1. Current Plan Step (step.md): {current_step} 2. Tool Output: {tool_output} 3. Constraints: 

{constraints} 

AUDIT PROTOCOL: Analyze the Tool Output against the Constraints. - Check 1(Relevance): Does the output directly answer the step’s question? - Check 2(Timeliness): Does the data match the required timeframe? - Check 3 (Integrity): 

Is the source URL valid? 

FINAL VERDICT: Output ONLY one of the following: - [PASS] - [FAIL]: <Reason> (Be specific so the Executor can self-correct.)