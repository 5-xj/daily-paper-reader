Title: Hippasus: Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data

URL Source: https://arxiv.org/pdf/2602.02025v1

Published Time: Tue, 03 Feb 2026 03:59:28 GMT

Number of Pages: 13

Markdown Content:
# Hippasus : Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data 

# Serafeim Papadias 

Athena Research Center serafeim.papadias@athenarc.gr 

# Kostas Patroumpas 

Athena Research Center kpatro@athenarc.gr 

# Dimitrios Skoutas 

Athena Research Center dskoutas@athenarc.gr 

## ABSTRACT 

Machine learning models depend critically on feature quality, yet useful features are often scattered across multiple relational tables. Feature augmentation enriches a base table by discovering and integrating features from related tables through join operations. However, scaling this process to complex schemas with many tables and multi-hop paths remains challenging. Feature augmentation must address three core tasks: identify promising join paths that connect the base table to candidate tables, execute these joins to ma-terialize augmented data, and select the most informative features from the results. Existing approaches face a fundamental trade-off between effectiveness and efficiency: achieving high accuracy requires exploring many candidate paths, but exhaustive explo-ration is computationally prohibitive. Some methods compromise by considering only immediate neighbors, limiting their effective-ness, while others employ neural models that require expensive training data and introduce scalability limitations. We present Hip-pasus , a modular framework that achieves both goals through three key contributions. First, we combine lightweight statistical signals with semantic reasoning from Large Language Models to prune unpromising join paths before execution, focusing computational resources on high-quality candidates. Second, we employ optimized multi-way join algorithms and consolidate features from multiple paths, substantially reducing execution time. Third, we integrate LLM-based semantic understanding with statistical measures to select features that are both semantically meaningful and empiri-cally predictive. Our experimental evaluation on publicly available datasets shows that Hippasus substantially improves feature aug-mentation accuracy by up to 26.8% over state-of-the-art baselines while also offering high runtime performance. 

PVLDB Reference Format: 

Serafeim Papadias, Kostas Patroumpas, and Dimitrios Skoutas. Hippasus :Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data. PVLDB, 16(2): 356-368, 2022. doi:10.14778/3565816.3565835 

PVLDB Artifact Availability: 

The source code, data, and/or other artifacts have been made available at https://github.com/spapadias/hippasus. 

> This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 16, No. 2 ISSN 2150-8097. doi:10.14778/3565816.3565835

## 1 INTRODUCTION 

The performance of a Machine Learning (ML) model depends not only on the learning algorithm but also on the quality of its input features [ 4]. In practice, useful features are often distributed across several relational tables rather than in a single base table. Prior work has shown that additional features improve generalization and reduce overfitting [ 3, 14 , 15 , 25 , 26 ]. To capture these features, researchers and practitioners turn to feature augmentation , which enriches a base table with features from related sources. Specifically, given a base table with labeled instances for a pre-diction ML task, such as classification or regression, and a set of candidate tables, feature augmentation aims to enrich the base table with additional features from the candidate tables. As in previous works [ 3, 15 , 25 , 26 ], we assume that the candidate tables are con-nected to the base table and to each other through primary‚Äìforeign key relationships, forming a join graph . This process involves iden-tifying useful columns from the candidate tables and determining the join paths that connect them to the base table. By executing these one-hop or multi-hop joins, the base table is augmented with new features, producing an enriched training dataset that combines original and external features. Feature augmentation faces several challenges that impact both efficiency and effectiveness, which existing systems address only partially [ 3, 15 , 25 , 26 ]. On the efficiency side, search space explo-sion occurs as the number of possible paths grows exponentially with join path length, making exhaustive exploration infeasible and requiring careful path selection for tractability. Execution cost is substantial because evaluating candidate join paths that span mul-tiple tables is computationally expensive and memory-intensive, demanding efficient join execution strategies. On the effectiveness side, incorporating semantic reasoning remains an open challenge: relying solely on statistical signals (e.g., correlations or null ratios) risks overlooking features that are semantically meaningful for the prediction task. Moreover, noisy and redundant features degrade pre-dictive performance even when joins are valid, requiring effective filtering of uninformative features. Finally, path quality variabil-ity arises when a feature is reachable through multiple join paths, making it critical to identify the most reliable path: some paths yield dense join results with a few null values, while others suffer from sparsity or redundancy. Proper feature augmentation requires addressing both efficiency bottlenecks and effectiveness concerns simultaneously. Existing approaches face fundamental tradeoffs between effi-ciency and effectiveness when addressing the challenges of explo-ration, execution cost, and feature quality. Some methods [ 3, 14 ]restrict exploration to star-shaped schemas or directly joinable ta-bles, limiting search scope to avoid computational costs but missing 

> arXiv:2602.02025v1 [cs.DB] 2 Feb 2026

features reachable through multi-hop paths. Reinforcement learn-ing approaches [ 26 ], which employ multi-armed bandits and deep Q-networks, balance exploration and exploitation across candidate tables but require extensive model training to evaluate join path quality. Moreover, they assess join quality indirectly through model feedback rather than explicitly modeling join characteristics, which prevents identifying high-quality paths before execution. Ranking-based methods [ 15 ] avoid repeated model training during search by using relevance and redundancy metrics, improving efficiency, but still face high costs from executing binary joins across numerous candidate paths. Most recently, neural model-based approaches [ 25 ]have leveraged LSTMs to explicitly predict the integration qual-ity of join paths and utilize clustering to reduce redundancy, but this approach introduces substantial training overhead. Overall, existing methods either compromise effectiveness by restricting the scope of augmentation or sacrifice efficiency through costly join execution or model training, thus leaving unresolved the need for solutions that achieve both. Recent work has shown that Large Language Models (LLMs) can effectively support database tasks such as entity resolution [ 9], column type annotation [ 12 , 31 ], dataset description [ 34 ], and table understanding [ 19 , 24 ]. Inspired by these advances, and to over-come the limitations of existing augmentation methods, we present 

Hippasus , a modular framework for feature augmentation that combines semantic reasoning from LLMs with statistical signals and efficient join execution. Unlike prior works that tightly cou-ple join path exploration with execution or require costly training to assess path quality, Hippasus explicitly decouples these stages, enabling early pruning without materializing joins. Hippasus de-composes the augmentation pipeline into four components. First, a Feature Description Generator enhances feature names that are not very informative with semantically-rich feature descriptions. Second, a Path Explorer discovers promising join paths before any join execution by combining lightweight statistical indicators with semantic cues from LLMs. Then, a Join Executor executes the se-lected paths using multi-way join algorithms and introduces an explicit consolidation step that resolves competing feature versions arising from different paths to the same attribute, while preserving base-table information. Finally, a Feature Selector employs LLM-based semantic reasoning alongside statistical measures to identify features that are both semantically meaningful and empirically pre-dictive. By reorganizing the augmentation process to perform path pruning before join execution, followed by explicit consolidation and feature selection, Hippasus enables each stage to be optimized independently, avoiding execution-heavy exploration and training phases. This design bridges efficiency and effectiveness: it scales to large search spaces by avoiding unnecessary joins, yet captures high-quality features that purely statistical approaches may miss. To the best of our knowledge, Hippasus is the first work to leverage LLMs for feature augmentation on relational data. Our main contributions can be summarized as follows: 

‚Ä¢ We propose a framework that decouples path selection, join ex-ecution, and feature selection. This design enables early path pruning without join execution, introduces an explicit consoli-dation stage, and defers feature selection until after all selected paths are materialized. 

‚Ä¢ We introduce a Feature Description Generator that enhances un-informative feature names with automatically generated descrip-tions to improve semantic reasoning in subsequent stages of the pipeline. 

‚Ä¢ We design a Path Explorer to prune unpromising join paths be-fore execution, thus avoiding search-space explosion in large join graphs. We combine lightweight statistical indicators with LLM-based semantic reasoning to assess path quality without materializing joins. 

‚Ä¢ We propose a Join Executor that reduces execution cost through two techniques: (i) a multi-way join algorithm adapted from Yannakakis [ 32 ] with left-join semantics tailored for feature aug-mentation, and (ii) a consolidation strategy that resolves compet-ing feature versions (from different paths to the same column) by preserving the most informative variant while maintaining base-table integrity. 

‚Ä¢ We devise a Feature Selector that drives the final selection of external features by combining statistical measures with LLM-based semantic understanding to select features that are both empirically predictive and semantically relevant to the prediction task. 

‚Ä¢ We conduct a comprehensive experimental evaluation against real-world datasets demonstrating that Hippasus substantially improves downstream model accuracy by up to 26.8% over state-of-the-art baselines while offering high runtime performance. The rest of the paper is organized as follows: Section 2 discusses related work, Section 3 formally defines the problem, Sections 4‚Äì8 introduce Hippasus and its main components, Section 9 presents the experimental evaluation, and Section 10 concludes the paper. 

## 2 RELATED WORK 

Feature Augmentation. Given a base table, with a specified target column, and a collection of candidate tables, feature augmenta-tion aims to enrich the base table with additional features that improve the predictive performance of downstream models. Some earlier works [ 21 , 28 ] focused on identifying when key-foreign key joins can be avoided without significantly harming model accuracy, which is orthogonal to our setting. Traditional feature augmen-tation approaches relied on exhaustive exploration or predefined heuristics. Deep Feature Synthesis (DFS) [ 18 ] operates in a brute-force manner, navigating all joinable relationships from the base table to candidate tables and applying mathematical functions along discovered paths. ARDA [ 3 ] introduced a more structured approach by leveraging data discovery tools like Aurum [ 10 ] to rank can-didate tables and applying heuristics for feature selection. Still, it relies on model-agnostic scores, and it is limited to single-hop joins. Reinforcement learning (RL) has been employed to balance the exploration of promising features with the exploitation of underuti-lized ones. AutoFeature [ 26 ] employs multi-armed bandits (MAB) or deep Q-networks (DQN) to guide the augmentation process, using sampling techniques for efficiency while evaluating each join path through model execution. METAM [ 14 ] introduces a goal-driven framework that leverages MAB for feature discovery and augmenta-tion, guided by downstream utility metrics and data characteristics to form a feedback loop that steers the search process. However, RL-based approaches incur high computational costs: evaluating each join path requires expensive join execution and repeated model training to obtain the reward signals necessary for the RL agent. More recent methods have focused on improving efficiency by avoiding full model training during feature evaluation. Aut-oFeat [ 15 ] explores multi-hop, transitive join paths and ranks them using relevance and redundancy metrics to assess the predictive power of features without training ML models. FeatPilot [ 25 ] evalu-ates candidate features from two complementary perspectives: the efficacy of join paths connecting features to the base table (estimated using clustering techniques and LSTM models) and the intrinsic value of features toward the ML task. FeatAug [ 27 ] conducts feature augmentation in a two-table setting with one-to-many relationships by automatically extracting predicate-aware SQL queries with ag-gregation functions (e.g., average, count) to preserve information; however, extending it to multi-hop join paths across table corpora is non-trivial. Still, these methods couple path exploration with join execution, incurring high computational cost due to expensive join operations, LSTM model training, and exhaustive search. In contrast, Hippasus decouples path exploration from join execution, discovering promising join paths without materializing them. 

Data and Feature Discovery with LLMs. LLMs have been applied to data discovery and annotation tasks that involve understand-ing data semantics [ 13 ]. ArcheType [ 12 ] introduces a zero-shot approach for semantic column type annotation (CTA) using LLMs, addressing limitations of deep learning methods that require fixed types at training time and large numbers of training samples. Au-toDDG [ 34 ] automatically generates descriptions for tabular data, combining data-driven summarization with LLM-based enrichment. However, these works focus on metadata generation and schema matching rather than feature augmentation. Inspired by these, Hip-pasus introduces a Feature Description Generator component that creates semantic descriptions which provide context for its Path Explorer and Feature Selector components. LLMs have also been applied to feature selection in single-table settings. LLM-Select [ 16 ] prompts an LLM with feature names and task descriptions, achieving performance comparable to traditional methods like LASSO without requiring training data. Li et al. [22] compare data-driven methods, which utilize actual data samples, with text-based methods, which rely on semantic descriptions. LLM-Lasso [ 33 ] incorporates LLM-informed domain knowledge into LASSO regression by assigning feature-specific penalties based on LLM outputs. LLM4FS [ 23 ] combines LLM reasoning with classical techniques like random forests and sequential search. AltFS [ 17 ]refines LLM-based semantic rankings using lightweight models such as decision trees. In contrast to existing feature augmentation approaches, Hippasus decouples feature selection from join path exploration, which allows it to benefit from LLM-driven feature se-lection techniques. Specifically, we employ a hybrid LLM‚Äìstatistical approach for feature ranking by extending LLM-Rank [ 22 ] with statistical signals computed on the augmented table. Overall, to the best of our knowledge, Hippasus is the first work to leverage LLMs for feature augmentation. 

Joinable and Unionable Table Discovery. Joinable table dis-covery methods identify tables that can be joined based on col-umn overlap or semantic similarity. Traditional approaches like Aurum [ 10 ] build enterprise knowledge graphs to capture relation-ships between datasets, while methods like Lazo [ 11 ] and Josie [ 35 ]use locality-sensitive hashing and overlap set similarity to effi-ciently find joinable columns in massive data lakes. More recent methods like PEXESO [ 5 ] and DeepJoin [ 6 ] leverage embeddings and deep learning to discover semantic joins that go beyond ex-act value matching, tolerating misspellings and format differences. Unionable table discovery aims to find tables that can be vertically combined to increase the number of rows. SANTOS [ 20 ] introduces semantic relationships between column pairs to improve union search accuracy, using both external knowledge bases and synthe-sized knowledge from the data lake itself. These approaches are orthogonal and complementary to Hippasus : joinable table discov-ery methods help construct the join graph that Hippasus takes as input, while unionable methods serve a different purpose, i.e., increasing the number of rows rather than the number of features. 

## 3 PROBLEM DEFINITION 

Join Graph. Consider a collection of relational tables D =

{ùëá 0,ùëá 1, . . . ,ùëá ùëõ } connected via a join graph ùê∫ = (ùëâ , ùê∏ ), where each node ùë£ ùëñ ‚àà ùëâ corresponds to a table ùëá ùëñ ‚àà D , and each directed edge 

ùëí ùëñ ùëó ‚àà ùê∏ indicates that table ùëá ùëñ contains a foreign key referencing the primary key of table ùëá ùëó . We assume that primary-key/foreign-key (PK-FK) relationships are given as input, either via schema meta-data or dataset discovery tools [2, 10]. A join path ùúã is a path in ùê∫ 

that represents an ordered sequence of join operations, each one referring to a valid relational join between two tables that share a joinable attribute (i.e., a PK-FK pair). 

ML Task. A machine learning (ML) task seeks to learn a predictive function ùëì : X ‚Üí Y that maps an input feature space X to an output space Y. Each input xùëñ ‚àà X represents a feature vector (corresponding to a record in a relational table), and each output 

ùë¶ ùëñ ‚àà Y represents the target value for prediction. We consider two types of tasks: classification , where Y is a discrete label space (binary or multiclass), and regression , where Y ‚äÜ R represents a continuous target domain. 

Base Table, Candidate Tables, and Augmented Table. We de-note by ùëá ùëèùëéùë†ùëí a table in ùê∫ that contains a set of attributes (i.e., features) and a target variable ùë¶ for prediction. The rest of the ta-bles in ùê∫ are referred to as candidate tables . These tables can be potentially joined with ùëá ùëèùëéùë†ùëí via appropriate join paths to augment it with additional features that may be helpful for predicting ùë¶ . This augmented table, denoted as ùëá aug , is a table that includes all original attributes from ùëá base along with additional attributes obtained by joining candidate tables via the join paths. The augmented table represents an enriched version of the base table whose feature space has been extended with attributes from the candidate tables. With-out loss of generality, we assume that ùëá 0 corresponds to the base table ùëá ùëèùëéùë†ùëí while {ùëá 1, . . . ,ùëá ùëõ } correspond to the candidate tables. 

Problem Statement. Assume (a) a base table ùëá base and a collection of candidate tables T connected via a join graph ùê∫ , and (b) an ML task with fixed model ùëì and performance metric. The goal of feature augmentation is to select features from T and identify appropriate join paths to augment ùëá base such that the performance of ùëì on the augmented table ùëá aug is maximized. Feature Descriptions  œÄ Ranked Paths  Consolidated Table Feature Description        

> Generator 1
> Feature Descriptions DAugmented Table
> Path Explorer
> Join Graph
> 2
> Target y
> Orig. Features
> D
> Feature Selector 4
> Target y
> Orig. Features
> Base Table
> D
> ML Trainer
> Table Corpus
> kAug. Features
> 3
> Materialization Consolidation
> œÄJoined Tables
> Join Executor

Figure 1: Overview of Hippasus .

## 4 HIPPASUS OVERVIEW 

Hippasus has a modular architecture consisting of four main com-ponents, as shown in Figure 1 and described below. 

(1) Feature Description Generator (FDG). FDG receives as input the names of the tables T , the names of their features, and any available dataset descriptions, and utilizes an LLM to produce a short, semantically-rich description for each feature. The aim is to enrich feature names that are not very informative (e.g., abbre-viations) with descriptions that can enable or improve semantic reasoning in subsequent steps in the pipeline. Indicatively, in the datasets used in our experiments, pupilDiamMax is enriched with the description ‚ÄúMaximum pupil diameter during fixation‚Äù, V9 with ‚ÄúMinimum luminosity value‚Äù, etc. 

(2) Path Explorer (PEX). PEX takes as input the base table 

ùëá base , the join graph ùê∫ , the target variable ùë¶ , and a path bud-get ùúã , and produces as output a ranked list of top-ùúã join paths 

P = {ùëù 1, ùëù 2, . . . , ùëù ùúã }. The goal is to prioritize paths that are most likely to yield high-quality features for the prediction task instead of exhaustively exploring all paths. We consider as candidate join paths all acyclic paths in ùê∫ starting from ùëá base with length up to 

‚Ñì (‚Ñì ‚àà [ 2, 7] in our experiments). PEX ranks paths by combining lightweight statistics with semantic cues derived from LLMs, utiliz-ing the feature descriptions produced by FDG. No join operations are executed at this stage. 

(3) Join Executor (JEX). JEX takes as input the selected paths P

and the tables in the corpus, and produces ùúã augmented tables, one per path. To materialize join paths efficiently while preserving base table rows, JEX adapts the Yannakakis algorithm [ 32 ] with left-join semantics, using multi-way joins to reduce intermediate result sizes and prevent blowup. JEX then consolidates these ùúã tables into a compact table ùëá cons by reconciling features that appear in multiple paths while maintaining the rows of the base table. 

(4) Feature Selector (FS). FS takes as input ùëá cons and produces as output the final augmented table ùëá aug containing ùëò selected features. The rationale is that the consolidated table may contain a large number of features, some of which may be redundant or noisy and risk overfitting. To address this, Hippasus employs hybrid strategies that integrate LLM-based reasoning with statistical measures. Eventually, the produced augmented table ùëá aug is provided to an ML Trainer, which trains an ML model for the given task and calculates model performance metrics. These metrics serve as a means to assess the effectiveness of ùëá aug . Similar to previous work, we employ an ML Trainer that supports various standard models (e.g., Random Forests, Gradient Boosted Trees, Extra Trees) using AutoML backends such as AutoGluon [8]. Our decoupled architecture allows us to avoid redundant opera-tions, such as excessive join executions, while instead benefiting from LLMs to integrate statistical with semantic reasoning when navigating the search space of join paths and features. As indicated by our experiments, this leads to superior effectiveness compared to previous approaches, while still maintaining high efficiency. 

## 5 FEATURE DESCRIPTION GENERATOR 

Real-world datasets often contain uninformative feature names (e.g., pupilDiamMax , V9 ) that prevent reasoning over feature rel-evance and relationships across tables. Hippasus uses an LLM to automatically generate concise, semantically-rich descriptions for each feature in the dataset domain-specific context, enabling ef-fective semantic reasoning in both path exploration and feature selection. As shown in Figure 2, the LLM is prompted to generate concise descriptions that focus on semantic meaning rather than technical data types. To this end, the LLM leverages any available dataset description (e.g., accompanying documentation in the form of a readme file) to extract feature descriptions from the raw text. In the absence of such input, we can alternatively use data samples and statistics to provide context to the LLM. Since FDG operates independently of the target variable ùë¶ and the prediction task, these descriptions can be generated offline. 

## 6 PATH EXPLORER 

Hippasus conducts path exploration in two phases, which combine semantic understanding with statistical grounding: (1) semantic table scoring through a single batch LLM call that evaluates all candidate tables simultaneously for predictive relevance to the target variable, and (2) hybrid path scoring that fuses LLM-derived semantic scores with statistical connection quality metrics during breadth-first search exploration. The rationale is that semantic reasoning from LLMs can identify domain-relevant features based on world knowledge, but lacks the data-specific signals needed to assess join feasibility. For instance, a table with perfect join coverage may contain irrelevant features (e.g., random_id , timestamp ), while one with lower coverage may provide semantically crucial features (e.g., weather.temperature for predicting delivery_delay ). In the first phase, Hippasus performs semantic table scoring through a single batch LLM call. As shown in Figure 3, we con-struct a prompt that encodes the prediction task, the base table schema, all candidate table schemas, the feature descriptions gen-erated by FDG, and foreign key relationships. The LLM is queried System: You are a data science expert analyzing database schemas. Given table names, feature names, and optional dataset descriptions, generate concise, semantically-rich descriptions for each feature.          

> Requirements: (1) Generate descriptions for all features provided; (2) Each description should be 3-10 words; (3) Focus on semantic meaning, not technical data types; (4) Transform abbreviations and codes into clear, domain-relevant descriptions; (5) Return a JSON object:
> {table_1.feature1: description_1, table_1.feature_2: description_2,...} .
> User: Dataset context: {dataset_description} (if available). Tables and features:
> {table_1}: [{feature_1} ({type_1}) ,...] , {table_2}: [{feature_2} ({type_2}) ,...] ,...

Figure 2: Prompt template for Feature Description Generator.                 

> System: You are a data science expert performing semantic table relevance assessment for {task_description} . Given table schemas and foreign key relationships, feature names and descriptions, and the target variable, score each candidate table‚Äôs relevance to the prediction target.
> Requirements: (1) Score all candidate tables‚Äîno exceptions; (2) Score range [0, 100]: 100 = highly relevant, 0 = irrelevant; (3) Return a JSON object:
> {table_1: score_1, table_2: score_2,...} .
> User: Task: {task_description} . Target: {target_column} . Base table:
> {base_table_name}: [{feature_0} ({type_0}, {description_0}) ,...]
> Candidate tables:
> {table_1}: [{feature_1} ({type_1}, {description_1}) ,...] , {table_2}: [{feature_2} ({type_2}, {description_2}) ,...] ,...

Figure 3: Prompt template for Path Explorer (semantic table scoring). once to obtain semantic relevance scores for all candidate tables in the range [0,100], which are then normalized to [0,1] for subse-quent computation. This design evaluates all tables within a unified prompt context, promoting consistent scoring across candidates. Note that when the number of tables and features exceeds the LLM‚Äôs context window, we employ embedding-based prefiltering: we compute semantic embeddings of table schemas (using table and feature names along with feature descriptions) and the target variable, then retain only the top-ùëá tables with the highest cosine similarity for LLM scoring. Tables not selected receive a semantic score of zero, ensuring that Phase 1 always requires a single LLM call regardless of dataset size. In the second phase, Hippasus employs a bidirectional breadth-first search to explore join paths up to length ‚Ñì starting from the base table. Crucially, while in the first phase we score individual tables, in the second phase we score complete paths using a hybrid scoring function that combines the LLM-derived table scores with statistical connection quality metrics (scoring formula detailed below). The algorithm maintains a min-heap of the top-ùëò highest-scoring paths discovered during exploration. As paths are discovered, we score them immediately using the hybrid scoring function and update the top-ùëò heap incrementally, avoiding materialization of all possible paths. To prevent cycles, the algorithm checks whether each neigh-bor already appears in the current path before extension, ensuring that paths form valid trees over the join graph. 

Hybrid Path Scoring. We score each join path by combining semantic and statistical signals. For a path ùúã = [ùëá base ,ùëá 1, . . . ,ùëá ‚Ñì ‚àí1],this hybrid score is computed as: Score (ùúã ) = ùëÜ sem + ùëÜ stat 

2(‚Ñì ‚àí 1) (1) where ùëÜ sem is the cumulative semantic score, ùëÜ stat is the cumulative statistical score, and 2(‚Ñì ‚àí 1) corresponds to the number of score components (two per hop: one semantic, one statistical). The se-mantic score sums the LLM-assigned scores for all tables beyond the base: 

ùëÜ sem = 

> ‚Ñì‚àí1

‚àëÔ∏Å 

> ùëñ =1

LLMScore (ùëá ùëñ ) (2) The statistical score aggregates connection quality for consecutive table pairs: 

ùëÜ stat = 

> ‚Ñì‚àí2

‚àëÔ∏Å 

> ùëñ =0

[ùõº ¬∑ Cov (ùëá ùëñ ,ùëá ùëñ +1) + ùõΩ ¬∑ Uniq (ùëá ùëñ ,ùëá ùëñ +1) + ùõæ ¬∑ SRatio (ùëá ùëñ ,ùëá ùëñ +1)] 

(3) where ùõº + ùõΩ +ùõæ = 1 are weights (we use ùõº = ùõΩ = ùõæ = 1/3 by default). Normalizing by 2(‚Ñì ‚àí 1) ensures scores lie in [0, 1] regardless of the path length ‚Ñì, preventing bias toward longer or shorter paths. The functions Cov , Uniq and SRatio are defined below. 

Statistical Metrics. We evaluate join feasibility between con-secutive tables using three metrics computed from table metadata (requiring no join execution). For each hop from ùëá ùëñ to ùëá ùëñ +1, where 

ùëá ùëñ is the anchor and ùëá ùëñ +1 is the lookup table, let FK ùëá ùëñ denote the foreign key column in ùëá ùëñ and JK ùëá ùëñ +1 the join key column in ùëá ùëñ +1 that it references. Since traversal is bidirectional, JK ùëá ùëñ +1 may be either a primary key or a foreign key. 

‚Ä¢ Coverage measures the fraction of anchor rows that will receive features: Cov (ùëá ùëñ ,ùëá ùëñ +1) = 1 ‚àí NullRate (FK ùëá ùëñ ) (4) Non-null foreign keys indicate successful matches, while null values result in missing features after the join. Low coverage leads to sparse feature columns in the augmented table. 

‚Ä¢ Uniqueness measures whether the join will cause row explosion: Uniq (ùëá ùëñ ,ùëá ùëñ +1) = |{ distinct values in JK ùëá ùëñ +1 }| |ùëá ùëñ +1 | (5) System: You are a data science expert performing feature selection for {task_description} . Given feature names and descriptions, the target variable, and statistical measures, rank features by importance for predicting the target. You have access to two statistical metrics for each feature: (1) mutual_info :Non-linear predictive power (mutual information measures dependency with target); (2) pearson_corr : Measures linear relationship strength.                 

> Integration Strategy:
> ‚Ä¢Features with high statistical scores across multiple metrics should be ranked high.
> ‚Ä¢Features with consistently low statistical scores should be ranked low unless they are semantically critical for the task.
> ‚Ä¢When statistical evidence and semantic reasoning agree, rank with high confidence accordingly.
> ‚Ä¢When statistical evidence conflicts with semantic intuition, prioritize statistical evidence as it reflects actual data patterns.
> ‚Ä¢For cryptic or uninformative feature names, with no comprehensive descriptions, rely primarily on statistical evidence.
> Requirements: (1) Rank all features provided‚Äîno exceptions; (2) Return a JSON array: ["most_important",...,"least_important"]
> User: Task: {task_description} . Target: {target_column} . Features with statistical context:
> [{ name :{feature_1}, desc :{description_1}, mutual_info :{mutual_info_1}, pearson_corr :{pearson_corr_1}}, { name :{feature_2}, desc :{description_2}, mutual_info :{mutual_info_2}, pearson_corr :{pearson_corr_2}} ,...]

Figure 4: Prompt template for Feature Selector. Values near 1 indicate that each key appears once in ùëá ùëñ +1, ensur-ing no fan-out. Lower values signal that multiple rows in ùëá ùëñ +1

share the same key, causing row explosion that can degrade both efficiency and model quality. 

‚Ä¢ Size Ratio reflects relative table sizes: SRatio (ùëá ùëñ ,ùëá ùëñ +1) = min (| ùëá ùëñ |, |ùëá ùëñ +1 |) 

max (| ùëá ùëñ |, |ùëá ùëñ +1 |) (6) This heuristic captures information diversity: very small lookup tables typically yield low-cardinality categorical features, while similarly-sized tables tend to provide richer feature sets. 

## 7 JOIN EXECUTOR 

The Join Executor (JEX) is responsible for materializing the join paths selected by the Path Explorer. Feature augmentation for su-pervised learning must satisfy two fundamental invariants: (1) row preservation ‚Äîthe augmented table contains exactly |ùëá base | rows, ensuring no training examples are duplicated or removed, and (2) 

distribution preservation ‚Äîthe distribution of the target variable ùë¶ 

remains unchanged, preserving class frequencies (classification) or the empirical distribution (regression). Violating these invari-ants would alter the ML task itself, making the augmented dataset incompatible with the original supervised learning objective. To satisfy these invariants, existing systems [ 3, 15 , 25 ] employ sequential binary left outer joins when materializing join paths. Given a join path ùëù = [ùëá base ,ùëá 1,ùëá 2, . . . ,ùëá ‚Ñì ‚àí1] with join keys (ùëê ùêø ùëñ , ùëê ùëÖ ùëñ )

for each edge (ùëá ùëñ ‚àí1,ùëá ùëñ ), the binary approach sequentially materi-alizes the path via left outer joins, i.e., ùëÖ ùëñ ‚Üê ùëÖ ùëñ ‚àí1 ‚ãâ ùëá ‚Ä≤ 

> ùëñ

, where ùëá ‚Ä≤

> ùëñ

is the deduplicated version of ùëá ùëñ on join key ùëê ùëÖ ùëñ . Deduplication en-sures unique key values in ùëá ‚Ä≤ 

> ùëñ

, preventing row explosion, while left outer joins preserve all base table rows, thereby maintaining both invariants by construction. While correct, this sequential approach materializes all ‚Ñì ‚àí 2 intermediate results at full size, incurring potential computational overhead for long paths. To address this inefficiency, we propose a strategy that leverages the classical Yannakakis algorithm [ 32 ], which efficiently computes acyclic joins through semi-join reductions. The key insight is to partition the join path into a prefix (the base table ùëá base ) and a suffix ( [ùëá 1,ùëá 2, . . . ,ùëá ‚Ñì ‚àí1]). We apply the Yannakakis algorithm with inner joins to the suffix tables, which performs bottom-up semi-join reductions to eliminate non-contributing tuples, followed by top-down joins to produce an intermediate result ùëÜ . We then left outer join this suffix result to the base table: ùëá aug ‚Üê ùëá base ‚ãâ ùëÜ ‚Ä≤, where ùëÜ ‚Ä≤

is the deduplicated suffix on join key ùëê ùëÖ  

> 1

. We refer to this approach as suffix-Yannakakis. To maintain the invariants when dealing with one-to-many or many-to-many join relationships, we perform deduplication, ensur-ing that each base table row joins with at most one tuple from each foreign table. Following prior work [ 3, 15 , 25 ], we deduplicate by selecting the first occurrence of a tuple among multiple tuples that share the same join key, converting relationships to one-to-one. Al-ternatively, an aggregation function (e.g., average, sum, count) can be applied. The selection of an appropriate aggregation function has been investigated in FeatAug [ 27 ], which, however, operates in a different setting, involving a fixed pair of tables rather than a table corpus. Incorporating such a mechanism in JEX can be a future extension. Under deterministic table ordering and the same deduplication policy, our suffix-Yannakakis approach produces equivalent results with the binary join strategy. However, it performs in ùëÇ (√ç‚Ñì ‚àí1 

> ùëñ =1

ùëõ ‚àó 

> ùëñ

+

ùëõ ¬∑ | ùëÜ ‚Ä≤ |) time, where ùëõ = |ùëá base |, ùëõ ùëñ = |ùëá ùëñ |, ùëõ ‚àó 

> ùëñ

denotes table sizes after the semi-join reductions, and |ùëÜ ‚Ä≤ | is the deduplicated suffix size, which is asymptotically lower compared to ùëÇ (√ç‚Ñì ‚àí1 

> ùëñ =1

(ùëõ + ùëõ ùëñ )) 

for binary joins. In practice, as shown in previous work [ 1 , 29 ,32 ] and in our experiments, semi-join reductions often achieve 

ùëõ ‚àó 

> ùëñ

‚â™ ùëõ ùëñ , particularly for acyclic join paths, yielding considerable computational savings. After executing all ùúã selected join paths, ùúã augmented tables are produced, each satisfying the invariants. When multiple paths reach the same table, the same feature may appear in multiple augmented tables with different ratio of null values, depending on join effectiveness. In that case, we select the version with the lowest null ratio. Eventually, the consolidated table ùëá cons is constructed by combining the base table with all selected features, maintaining 

|ùëá cons | = |ùëá base | as consolidation performs only column operations. 8 FEATURE SELECTOR 

After consolidation, the output table ùëá cons contains all base features plus additional features discovered through join path materializa-tion. However, a large feature space may even degrade model per-formance, as it may lead to overfitting or it may contain redundant (e.g., correlated) or noisy features. Hippasus combines statistical evidence with LLM-based semantic reasoning to select a subset of the discovered features to be included in the final augmented table. Existing feature augmentation approaches [ 3, 15 , 25 ] rely on statistical metrics to rank features based on their observed relation-ships with the target variable in training data. Hippasus computes two complementary statistical measures to ground semantic reason-ing in empirical evidence. First, we employ Mutual Information (MI), which measures the dependency between a feature ùëì and target 

ùë¶ , quantifying how much knowing ùëì reduces uncertainty about 

ùë¶ . Second, we utilize Pearson Correlation (ùúå ), which measures the linear association between numeric features and targets, capturing both positive and negative relationships via |ùúå (ùëì , ùë¶ )| . These two metrics are complementary‚ÄîMI captures general dependencies in-cluding nonlinear relationships, while Pearson explicitly captures linear trends‚Äîand provide interpretable measurements without overwhelming the LLM with excessive statistical context. To leverage the complementary strengths of empirical measure-ments and semantic reasoning, Hippasus provides the statistical evidence computed earlier directly to the LLM alongside feature names and their descriptions generated by the Feature Description Generator (FDG). We construct a prompt (Figure 4) consisting of a system message that establishes the task context and a user mes-sage that provides the feature information. The prompt includes the task type (classification or regression), task description, and target column to establish context. For each feature in the con-solidated table, we provide its name, the description generated by FDG, and the two statistical measures computed earlier: mutual information and Pearson correlation. The prompt includes an inte-gration strategy with five rules that guide the LLM in combining statistical evidence with semantic reasoning. These rules establish priorities such as trusting features with high statistical scores, de-prioritizing those with consistently low scores unless semantically critical, and resolving conflicts between statistical evidence and semantic intuition by prioritizing the former as it reflects actual data patterns. The LLM is prompted to rank all provided features with explicit constraints‚Äîeach feature must appear exactly once in the output‚Äîand return the ranking as a JSON array ordered from most to least important. From the LLM‚Äôs ranked output, we select the top-ùúÖ features to produce the final augmented table ùëá aug , which is then provided to the ML Trainer for model training. When the dataset contains too many features to fit within the context window, we employ a statistical prefiltering strategy to en-sure a single LLM call. Specifically, we merge the statistical rankings based on the mutual information and Pearson correlation already computed for all features using Borda count [ 30 ] to produce a uni-fied ranking. We then select the top-ùêæ features from this unified statistical ranking, where ùêæ ‚â´ ùúÖ (e.g., ùêæ = 100 ), ensuring that ùêæ 

is large enough to preserve strong candidates while fitting within the LLM‚Äôs context window. These ùêæ features are then forwarded 

Table 1: Dataset Statistics. 

Dataset #Tables #Features Task 

School 24 1104 CCredit 5 18 CSteel 15 28 CEyemove 6 19 CJannis 12 50 CMiniboone 15 55 CCovertype 30 55 CFraud 16 25 CDiabetes 19 19 CPoverty 25 58 RAir 23 118 RNorthwind 11 66 Rto the LLM for hybrid semantic ranking using the prompt mech-anism described earlier. Finally, we select the top-ùúÖ features from the LLM‚Äôs ranking to produce ùëá aug , ensuring that the two-stage approach‚Äîstatistical prefiltering followed by LLM-based seman-tic ranking‚Äîachieves feature selection through a single LLM call regardless of the original feature space size. 

## 9 EXPERIMENTAL EVALUATION 

We evaluate Hippasus using a variety of publicly available real-world datasets and investigate: how effective and efficient Hippasus 

is compared to state-of-the-art approaches; how the usage of LLMs in feature description generation, in path exploration, and in feature selection affects performance; how our join execution strategy im-pacts runtime; and how sensitive Hippasus is to parameter choices. 

## 9.1 Experimental Setup 

Datasets. We evaluate Hippasus over 12 real-world datasets that have also been used by recent approaches [ 15 , 25 ]. Table 1 shows for each dataset: the total number of tables, the total number of features, and the type of task, i.e., classification (‚ÄúC‚Äù) or regression (‚ÄúR‚Äù). These datasets span various domains and prediction tasks, enabling a comprehensive evaluation of Hippasus across different scenarios. Specifically, we use Credit (predicting whether an indi-vidual is a good or bad credit risk based on financial, demographic, and employment-related attributes); Steel (classifying fault type in steel plates using geometric, spatial, and luminosity-related fea-tures); Eyemove (predicting sentence relevance using eye-tracking features); Jannis (classifying high-dimensional features from the ChaLearn AutoML benchmark); Miniboone (distinguishing elec-tron neutrino events from muon neutrino background using particle identification features); Covertype (predicting the presence of a specific forest cover type using normalized numerical and binary environmental features); School (predicting school performance based on student attributes on standardized tests); Fraud (predict-ing whether online transactions are fraudulent based on transaction and identity characteristics); Diabetes (predicting diabetes using features such as BMI and blood pressure); Poverty (predicting poverty levels using socioeconomic features such as unemployment and education rates across U.S. states); Air (predicting air quality in cities on given dates using features such as temperature and sulfur Table 2: Effectiveness of Hippasus against baselines. Parameters: LLM= gpt-4o-mini , max ‚Ñì = 7, ùúã = 10 , ùúÖ = 10 features.                                                                             

> Variant School Credit Eyemove Steel Jannis Miniboone Covertype Diabetes Fraud Poverty Air Northwind
> Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 MAE RMSE MAE Base 0.689 0.696 0.501 0.653 0.563 0.697 0.503 0.519 0.033 12027 1.095 14.037 ARDA 0.806 0.695 0.512 0.711 0.541 0.865 0.551 0.506 0.408 9220 0.929 13.914 AutoFeat 0.692 0.734 0.532 0.801 0.706 0.816 0.799 0.725 0.315 3611 0.920 13.361 FeatPilot 0.756 0.701 0.596 0.774 0.561 0.895 0.641 0.734 0.575 12981 1.045 12.962
> Hippasus 0.713 0.744 0.685 0.971 0.721 0.910 0.698 0.723 0.626 3587 0.875 11.925

Table 3: Efficiency of Hippasus against baselines (time in seconds). Parameters: LLM= gpt-4o-mini , max ‚Ñì = 7, ùúã = 10 , ùúÖ = 10 features.                                                    

> Variant School Credit Eyemove Steel Jannis Miniboone Covertype Diabetes Fraud Poverty Air Northwind
> ARDA 480 764 19 88 49 55 48 51 329 249 38 AutoFeat 11 13312 17 66 912 10 10 6FeatPilot 378 89 192 115 628 512 341 196 905 622 1150 113
> Hippasus 35 712 931 54 138 15 15 19 37 12

dioxide); and Northwind [7 ] (predicting order quantities using features such as product categories, and supplier information). 

LLM Models. We test Hippasus with various LLMs including three commercial (gpt-4o-mini , gpt-4o , claude-3.5-sonnet )and four open models (llama-3.1-8B , llama-3.3-70B ,

mistral-nemo-12B , qwen-2.5-72B ). Unless stated otherwise, all LLMs are queried with a temperature 0.1 to reduce output variance, and prompts are identical across models to ensure a fair comparison. All LLM calls are issued through OpenRouter 1 using fixed model versions and identical prompts across runs. 

Evaluation Metrics. For classification tasks, we report accuracy 

for balanced datasets and F1-score for imbalanced datasets. For regression tasks, we report Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE), following prior work. Efficiency is measured as the total feature augmentation time (in seconds), in-cluding path exploration, join execution, and feature selection. We exclude downstream model training time, as it is orthogonal to fea-ture augmentation. All reported results are averages over five runs with different random seeds. We use AutoGluon [ 8 ] to automati-cally train multiple models (e.g., tree-based models, kNN, neural networks) and ensemble them to create the final predictor. In the results, values in bold indicate the best performance per dataset; underlined values indicate second-best performance. 

Baselines. We compare Hippasus with three state-of-the-art fea-ture augmentation methods: ARDA [ 3 ], AutoFeat [15 ], and Feat-Pilot [ 25 ] (see Section 2 for details). For AutoFeat and FeatPilot, we use the publicly released implementations from the respective authors. We have implemented ARDA following the algorithmic details in [ 3], as the original code was not available. For all methods, we use the default parameters from the respective papers. We sup-ply ARDA with star schemata, since it is limited to this setting. For FeatPilot, we train the LSTM on the same exploration data used by 

Hippasus . We do not include AutoFeature [ 26 ] in our comparison, as recent work [ 15 , 25 ] has shown it is outperformed by both Aut-oFeat and FeatPilot in effectiveness and efficiency. Moreover, we denote by Base a method that uses the original base table without any augmented features. This serves as a reference point to evaluate 

> 1https://openrouter.ai/

the impact of feature augmentation. All baselines use the same ML training and evaluation pipeline through AutoGluon [ 8 ] to ensure a fair comparison. 

## 9.2 Comparison with Baselines 

We evaluate end-to-end performance, which reflects the quality of the final downstream model after the complete feature augmenta-tion pipeline. Unless stated otherwise, we use gpt-4o-mini as the LLM backend, search for paths with maximum path length ‚Ñì = 7,select ùúã = 10 join paths, and extract ùúÖ = 10 features per dataset. 

Effectiveness. As listed in Table 2, Hippasus achieves the best performance on 9 out of 12 datasets, with average improvements of 26.8% over ARDA, 14.5% over AutoFeat, and 18.6% over FeatPilot. We attribute this to several limitations or weaknesses of existing approaches, which Hippasus overcomes via its hybrid (statistics-and semantics-driven) method. ARDA‚Äôs 1-hop exploration strategy prevents it from discovering distant features, leading to poor perfor-mance on multi-hop schemas such as Eyemove , Fraud , and Poverty ,while excelling on School ‚Äôs star schema, where informative features are within one hop. AutoFeat ranks join paths using cheaper sta-tistical metrics (relevancy and redundancy scores), which fails to effectively address search space complexity, resulting in suboptimal path selection that misses semantically meaningful relationships. FeatPilot trains LSTM models to predict join path quality and em-ploys embeddings for feature clustering, but this added complexity yields marginal benefits, suggesting that embeddings alone are in-sufficient for capturing task-specific semantic relevance in path exploration. Finally, the Base method, which uses only the initial ta-ble without augmentation, shows poor performance, demonstrating the clear value of feature augmentation. 

Efficiency. From the results shown in Table 3, it is clear that Hip-pasus maintains competitive efficiency, with substantial speedups over complex baselines (ARDA and FeatPilot). Note that the execu-tion time of the Base method is zero, since it does not perform any feature augmentation. Hippasus is up to 60 √ó faster than FeatPilot, as FeatPilot trains LSTM models during the augmentation process to predict join path quality. Against ARDA, Hippasus achieves an average speedup of 5 √ó, since ARDA exhaustively materializes all Table 4: Effect of Feature Description Generation (FDG). Parameters: LLM= GPT-4o-mini , ‚Ñì = 7, ùúã =10, ùúÖ = 10 .                                                           

> Variant School Credit Eyemove Steel Jannis Miniboone Covertype Diabetes Fraud Poverty Air Northwind
> Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 MAE RMSE MAE
> Hippasus w/o FDG 0.711 0.729 0.618 0.713 0.565 0.878 0.703 0.718 0.611 3710 1.006 11.925
> Hippasus w/ FDG 0.713 0.744 0.685 0.971 0.721 0.910 0.698 0.723 0.626 3587 0.875 12.310 Improvement +0.28% +2.05% +10.84% +36.08% +21.6% +3.64% -0.71% +0.69% +2.45% +3.33% +13.02% +2.41%

Table 5: Effect of Feature Selector (FS) strategies. Parameters: LLM= GPT-4o-mini , ‚Ñì = 7, ùúã = 10 , ùúÖ = 10 features except for w/o FS.                                                                        

> Variant School Credit Eyemove Steel Jannis Miniboone Covertype Diabetes Fraud Poverty Air Northwind
> Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 MAE RMSE MAE
> Hippasus w/o FS 0.802 0.740 0.668 0.960 0.597 0.879 0.651 0.717 0.595 3658 0.758 12.088
> Hippasus stats-only 0.709 0.735 0.669 0.965 0.660 0.879 0.646 0.719 0.609 3667 0.930 11.935
> Hippasus LLM-only 0.825 0.715 0.551 0.927 0.565 0.880 0.618 0.721 0.556 4535 1.006 12.095
> Hippasus 0.713 0.744 0.685 0.971 0.721 0.910 0.698 0.723 0.626 3587 0.875 11.925

1-hop joins and incorporates the ML model into its feature discov-ery process. Relative to AutoFeat, Hippasus incurs modest LLM inference overhead, running on average 2.9 √ó slower, as AutoFeat uses lightweight statistical heuristics that avoid LLM costs, though this comes at the expense of effectiveness as discussed earlier. Hip-pasus ‚Äôs efficiency stems from its decoupled architecture: the path exploration stage ranks candidate paths using LLM semantic scor-ing without materializing joins, then the join execution stage selec-tively executes only top-ranked paths using the suffix-Yannakakis algorithm. 

To conclude, Hippasus ‚Äôs combination of LLM-based semantic rea-soning with feature-level statistics achieves superior effectiveness across diverse datasets and prediction tasks, while maintaining com-petitive efficiency via decoupled exploration and execution. 

## 9.3 Ablation Study 

To quantify the contribution of Hippasus ‚Äôs key design choices, we perform an ablation study that isolates and evaluates the effect of each core component on final prediction accuracy. Specifically, we study the role of feature descriptions in providing semantic signals for path exploration, the impact of feature selection on controlling noise and overfitting, and the efficiency of the suffix-Yannakakis join algorithm compared to binary joins. 

Feature Description Generation. To assess the impact of se-mantic context on Hippasus effectiveness, we ablate the Feature Description Generator (FDG) component. This component uses dataset descriptions provided in text form, and the LLM‚Äôs training knowledge to generate brief one-sentence semantic descriptions for each column across all tables. When enabled, these descriptions are provided to both the Path Explorer to guide join path ranking and the Feature Selector to inform feature selection. We compare two configurations: Hippasus w/ FDG with generated descriptions and Hippasus w/o FDG without them. Table 4 presents the results across all datasets. Feature descrip-tions provide substantial performance gains when comprehensive semantic information can be generated, with improvements rang-ing from modest to substantial depending on the informativeness of the original column names. On datasets where original column names lack semantic context, generated descriptions yield substan-tial gains: Steel improves by 36.2% as the LLM can now understand, for instance, that V14 represents ‚Äústeel plate thickness‚Äù, while Jan-nis gains 21.6%, Air gains 13%, Eyemove 10.8%, and Miniboone 3.6%. Conversely, modest benefits are observed in datasets having se-mantic column names, as generated descriptions largely reinforce information already present: datasets such as Credit and Diabetes 

exhibit smaller gains since features like credit_amount, BMI, and age are self-explanatory. 

Feature Selector Module. To assess the contribution of dif-ferent feature selection strategies, we ablate the Feature Selec-tor module by comparing four variants: (1) no feature selection (Hippasus w/o FS ), which keeps all features from materialized join paths; (2) statistics-only selection ( Hippasus stats-only ), which ranks features using mutual information and correlation with the tar-get variable; (3) LLM-only selection ( Hippasus LLM-only ), which uses LLM semantic reasoning to rank features; it essentially corresponds to LLM-Rank [ 16 ]; and (4) hybrid selection ( Hippasus ), which com-bines statistical signals with LLM semantic reasoning. All variants select ùúÖ = 10 features except Hippasus w/o FS .From the results listed in Table 5, the hybrid feature selector achieves the best performance on 10 out of 12 datasets, demonstrat-ing that combining statistical evidence with semantic reasoning is superior to using either signal in isolation. Hippasus LLM-only ,which relies purely on semantic reasoning, consistently underper-forms compared to the hybrid approach, degrading performance by 20% on Eyemove , 11% on Fraud , and 26% on Poverty (note that higher MAE is worse), as it selects features that appear contextu-ally relevant but lack statistical predictive power. Hippasus stats-only 

performs competitively on datasets with limited semantic context, nearly matching the hybrid approach on Steel , where feature de-scriptions provide minimal additional value, but underperforms on other datasets where semantic reasoning is beneficial, such as 

Miniboone , Covertype , and Air , as statistical signals alone cannot capture contextual relationships that feature descriptions reveal. The variant without feature selection occasionally achieves compet-itive results as on School and Air , but generally introduces noise and risks overfitting by retaining all available features, underperforming on datasets such as Jannis , Steel , and Fraud .

Join Execution Strategy. To evaluate the efficiency gains of our suffix-Yannakakis join strategy compared to the traditional binary approach, we compare the two strategies across representative Table 6: Effect of LLM model. Parameters: ‚Ñì = 7, ùúã =10, ùúÖ =10. 

LLM Model School Credit Eyemove Steel Jannis Miniboone Covertype Diabetes Fraud Poverty Air Northwind 

Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 MAE RMSE MAE Llama-3.1-8B 0.704 0.704 0.605 0.857 0.592 0.897 0.699 0.731 0.615 4013 0.988 11.981 Llama-3.3-70B 0.715 0.733 0.617 0.790 0.680 0.879 0.849 0.713 0.591 3832 0.957 12.195 Mistral-Nemo-12B 0.699 0.700 0.624 0.906 0.603 0.905 0.614 0.709 0.615 3996 0.982 11.908 

Qwen-2.5-72b 0.705 0.728 0.586 0.972 0.741 0.879 0.931 0.720 0.563 3837 0.988 11.925 GPT-4o-mini 0.713 0.744 0.685 0.971 0.721 0.910 0.698 0.723 0.626 3587 0.875 11.925 GPT-4o 0.703 0.741 0.583 0.710 0.596 0.877 0.680 0.723 0.587 3688 0.971 11.925 Claude-sonnet 3.5 0.706 0.750 0.593 0.968 0.564 0.906 0.930 0.721 0.619 3635 0.987 12.402 Credit Diabetes Poverty Fraud Northwind        

> Dataset
> 0
> 1
> 2
> Speedup
> 1.13√ó 1.13√ó 1.31√ó 1.40√ó
> 1.95√ó
> (a) All paths
> Baseline (All Paths)
> 23456
> Path Length ( )
> 0
> 1
> 2
> 3
> Speedup
> 0.98√ó
> 10
> 1.81√ó
> 25
> 1.91√ó
> 5
> 2.57√ó
> 5
> 2.73√ó
> 5
> (b) Northwind
> Baseline (Binary)

Figure 5: Performance comparison of Suffix-Yannakakis approach. (a) Speedup over all paths across different datasets. (b) Speedup over binary join by path length on the Northwind dataset. datasets while fixing ùúã = 10 paths, maximum length ‚Ñì = 7, and ùúÖ =

10 features. Figure 5a presents the average speedup across datasets, showing that suffix-Yannakakis achieves consistent gains ranging from 1.13 √ó on Credit to 1.95 √ó on Northwind , demonstrating that multi-way join execution with semi-join reductions outperforms binary joins even on moderately complex schemas. Figure 5b shows speedup per path length on Northwind . At path length ‚Ñì = 2, both strategies perform identically since suffix-Yannakakis reduces to binary joins for single-hop paths, but from path length ‚Ñì = 3 onward, semi-join reductions eliminate non-result tuples early, with speedup growing to 2.73 √ó at ‚Ñì = 6. This pattern is particularly pronounced on schemas with 1-to-many relationships, where suffix-Yannakakis prunes large intermediate results before materialization. 

To conclude, semantic enrichment of feature descriptions signifi-cantly boosts performance, especially when original feature names are not informative. The synergy of statistical evidence with semantic rea-soning is essential for robust feature selection. The suffix-Yannakakis join execution strategy provides consistent speedups compared to bi-nary joins, especially for longer join paths. 

## 9.4 Effect of LLM Model 

To assess the impact of LLM model choice on Hippasus effective-ness, we evaluate seven models ranging from 8B to 72B parameters, including both open (Llama-3.1, Llama-3.3, Mistral-Nemo, Qwen-2.5) and commercial ones (GPT-4o-mini, GPT-4o, Claude-sonnet 3.5), while fixing all other parameters to the default values. Table 6 lists the effectiveness results across all datasets. GPT-4o-mini ranks among the two highest in effectiveness across most datasets, demonstrating the most consistent performance. Counter-intuitively, it outperforms its larger sibling GPT-4o substantially, suggesting that architectural refinements and training methodology may matter more than parameter count for feature augmentation tasks. However, no single model dominates universally: Qwen-2.5-72b excels on datasets with complex relational schemas such as 

Covertype , Claude-sonnet 3.5 achieves strong balanced performance across diverse tasks, while GPT-4o-mini provides the most consis-tent results across all dataset types. Open models demonstrate competitive performance with commercial alternatives, as the effec-tiveness scores of Llama-3.3-70B and Qwen-2.5-72b are comparable to those of Claude-sonnet 3.5, offering viable privacy-preserving options for organizations with data sensitivity constraints. 

To conclude, GPT-4o-mini demonstrates consistently high perfor-mance across diverse datasets, making it our default LLM backend for 

Hippasus . Nevertheless, there is no single model that performs best in all datasets. 

## 9.5 Effect of Parameters 

To understand how Hippasus ‚Äôs key hyperparameters affect effec-tiveness and efficiency, we conduct sensitivity analysis following the natural dependency of Hippasus ‚Äôs pipeline: (i) maximum join path length ‚Ñì, (ii) number ùúã of join paths to materialize, and (iii) number ùúÖ of features to finally select. 

Maximum path length. We evaluate the impact of maximum join path length ‚Ñì by varying it from 2 to 7 while fixing ùúã = 10 paths and ùúÖ = 10 features. We assess the impact of ‚Ñì on both effectiveness and efficiency, with detailed runtime breakdowns for the latter. Table 7 presents effectiveness results, revealing that shallow ex-ploration at ‚Ñì = 2 universally underperforms by restricting access to informative features located deeper in relational schemas. Fur-thermore, we observe that most datasets benefit monotonically from deeper exploration, with performance improving or stabiliz-ing as length increases: Jannis improves progressively from ‚Ñì = 2

to ‚Ñì = 7, Steel reaches optimal performance at ‚Ñì = 7, and Poverty 

achieves best results at maximum path length. A few datasets ex-hibit mild performance variations across intermediate lengths, such as Diabetes peaking at ‚Ñì = 3 and Fraud at ‚Ñì = 4, yet crucially, increasing length does not cause severe degradation, confirming the robustness of deeper exploration. Setting ‚Ñì = 7 as the default provides flexibility to discover distant features when beneficial, while the LLM-based path ranker naturally prioritizes shorter paths even when deeper exploration is permitted, with average selected path lengths ranging from 2.0 to 5.8 across datasets (bottom row in Table 7). Figure 6 shows the total time (in seconds; on top of each bar) of the feature augmentation, as well as the breakdown across Hip-pasus ‚Äôs three core components: Path Explorer, Join Executor, and Table 7: Effect of the maximum join path length. Parameters: LLM= gpt-4o-mini , ùúã =10, ùúÖ =10. The bottom row shows the min/max/avg selected path length for ‚Ñì = 7.

Max ‚Ñì School Credit Eyemove Steel Jannis Miniboone Covertype Diabetes Fraud Poverty Air Northwind 

Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 MAE RMSE MAE 2 0.709 0.672 0.588 0.722 0.565 0.879 0.641 0.645 0.444 9256 0.942 11.990 3 0.710 0.737 0.662 0.735 0.628 0.883 0.638 0.732 0.576 3594 0.943 12.397 4 0.703 0.743 0.668 0.965 0.596 0.878 0.642 0.719 0.653 3649 0.980 12.393 5 0.703 0.733 0.667 0.954 0.689 0.878 0.642 0.717 0.590 3594 0.990 12.397 6 0.705 0.744 0.667 0.965 0.696 0.879 0.673 0.715 0.611 3785 0.995 12.313 7 0.713 0.744 0.685 0.971 0.721 0.910 0.698 0.723 0.626 3587 0.875 11.925 

Length ‚Ñì =7 2/2/2 2/5/3.5 2/5/3.5 2.4/6.8/5.8 2.2/5/3.8 2/5/3.5 2/5.2/4 2/7/4.5 3/6/5 2/7/3.6 2/5/3.3 2/6/3.42 3 4 5 6 7                                                                                                      

> 0
> 20
> 40
> 60
> 80
> 100
> Percentage (%)
> 57
> 35 28 31 35 33
> 42
> 63 69 65 62 64
> 577777
> (a) Credit
> 234567
> 0
> 20
> 40
> 60
> 80
> 100
> 69 63
> 49 58 59 60
> 31 35
> 48 38 36 35
> 7810 910 9
> (b) Steel
> 234567
> 0
> 20
> 40
> 60
> 80
> 100
> 52
> 28 27 27 29 29
> 23 29 29 29 29
> 42 49 44 44 42 42
> 16 24 25 26 24 24
> (c) Jannis
> 234567
> 0
> 20
> 40
> 60
> 80
> 100
> 54
> 38 38 38 40 38
> 24 34 33 32 33
> 40 38 28 29 28 29
> 17 24 24 25 25 24
> (d) Miniboone
> 234567
> 0
> 20
> 40
> 60
> 80
> 100
> 34
> 21 20 20 19 19
> 10 34 47 47 44 44
> 56 45
> 33 33 37 37
> 65 108 112 112 118 118
> (e) Covertype
> Maximum Join Path Length ( )
> Path Explorer (PEX) Join Executor (JEX) Feature Selector (FS)

Figure 6: Time breakdown per component across five datasets, with maximum path length varying from ‚Ñì=2 to ‚Ñì=7. We show the percentage of total runtime spent in Path Explorer, Join Executor, and Feature Selector. Total runtime is displayed above each bar. Feature Selector. Note that the Feature Description Generator oper-ates during preprocessing as it does not require the target column or downstream task, thus its time is excluded from runtime measure-ments. Path Explorer time remains stable across all path lengths, as it performs a single LLM call to score candidate tables regardless of maximum length, with runtime dominated by LLM inference latency. Join Executor time grows from ‚Ñì = 2 to ‚Ñì = 4 as deeper paths are initially explored, but plateaus for ‚Ñì ‚â• 4 because: (a) Path Explorer‚Äôs LLM prioritizes shorter paths, and (b) paths with length greater than 5 constitute a minority of the selected set. Feature Selector time grows modestly ( 1.2 ‚àí 2.3√ó from ‚Ñì = 2 to ‚Ñì = 7) as deeper exploration discovers more features, increasing both the cost of statistical computations such as Pearson correlation and the LLM ranking overhead for larger feature sets. These component-level behaviors result in moderate total runtime growth from ‚Ñì = 2

to ‚Ñì = 7, with increases ranging from 23% to 49% across datasets, demonstrating that maximum path length has a bounded impact on overall efficiency. 

Number of Join Paths. We assess the impact of the number ùúã of join paths on both effectiveness and efficiency. We vary ùúã from 5, 10, 20, to all discoverable paths, while fixing maximum path length 

‚Ñì and ùúÖ = 10 features selected. Table 8 presents effectiveness results, revealing that ùúã = 10 paths achieve the best performance on most datasets. On datasets such as 

Steel and Eyemove , accuracy deteriorates when materializing 20 or all paths, demonstrating that excessive path exploration introduces noise rather than signal on simpler relational schemas. On the con-trary, materializing only 5 paths underperforms on most datasets by providing insufficient feature coverage, limiting the discovery of high-utility features necessary for improving prediction accuracy. Conversely, materializing all paths can achieve best performance on datasets with complex schemas such as Diabetes and Fraud , which benefit from extensive path exploration, discovering feature sets that provide richer signal. Figure 7 shows the runtime analysis (total time and breakdown) when varying the number of paths. FDG is again omitted as it is executed offline. Path exploration cost remains stable across all path configurations (5-15 seconds), as the Path Explorer performs a single LLM call to score candidate tables regardless of how many paths are ultimately materialized, with runtime dominated by LLM inference latency. The cost of the Feature Selector grows modestly as more paths discover additional features, increasing both the cost of statistical computations, such as Pearson correlation, and the LLM ranking overhead for larger feature sets. In contrast, the Join Executor dominates runtime on complex schemas and grows substantially with the number of paths materialized, for instance, increasing 142 √ó on Jannis , and 189 √ó on Miniboone when materializ-ing all paths compared to 5 paths. This growth escalates further on datasets with complex schemas and large tables such as Covertype 

(‚âà 0.5M rows), where attempting to materialize all paths exhausts available memory and prevents completion, demonstrating the computational infeasibility of exhaustive path exploration. Table 8: Effect of the number of materialized join paths. Parameters: ùúÖ = 10 features selected, maximum length ‚Ñì = 7, LLM= GPT-4o-mini .

Paths ùúã School Credit Eyemove Steel Jannis Miniboone Covertype Diabetes Fraud Poverty Air Northwind 

Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 MAE RMSE MAE 5 0.714 0.737 0.628 0.967 0.571 0.878 0.639 0.714 0.591 4052 0.986 12.216 10 0.713 0.744 0.685 0.971 0.721 0.910 0.698 0.723 0.626 3587 0.875 11.925 

20 0.697 0.743 0.678 0.934 0.726 0.898 0.611 0.721 0.608 3671 0.930 12.397 

all 0.700 0.744 0.667 0.911 0.760 0.906 OOM 0.744 0.665 3667 0.914 12.401 5 10 20 all                                                       

> 0
> 20
> 40
> 60
> 80
> 100
> Percentage (%)
> 34 26 29 29
> 64 71 67 67
> 6777
> (a) Credit
> 510 20 all
> 0
> 20
> 40
> 60
> 80
> 100
> 60 55 53
> 11
> 10
> 68
> 37 38 37
> 21
> 8810 42
> (b) Steel
> 510 20 all
> 0
> 20
> 40
> 60
> 80
> 100
> 43 30 21
> 17 38 47 92
> 40 32 32
> 16 23 34 453
> (c) Jannis
> 510 20 all
> 0
> 20
> 40
> 60
> 80
> 100
> 60
> 44
> 23
> 14
> 32
> 44 94
> 26 24 33
> 29 31 41 838
> (d) Miniboone
> 510 20 all
> 0
> 20
> 40
> 60
> 80
> 100
> 47
> 18 10
> 19
> 51 56
> 34 31 34
> 110 123 226 OOM
> (e) Covertype
> Number of Join Paths ( )
> Path Explorer (PEX) Join Executor (JEX) Feature Selector (FS)

Figure 7: Time breakdown per component across five datasets, with the number of materialized paths varying from 5 to all discoverable paths. We show the total time percentages of Path Explorer, Join Executor, and Feature Selector. Total runtime is displayed above each bar. 

Table 9: Effect of the number of features selected. Parameters: ‚Ñì = 7, ùúÖ = 10 , LLM= gpt-4o-mini .

Features ùúÖ School Credit Eyemove Steel Jannis Miniboone Covertype Diabetes Fraud Poverty Air Northwind 

Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 MAE MAE MAE 5 0.694 0.733 0.619 0.894 0.592 0.859 0.627 0.719 0.567 3549 1.043 12.382 10 0.713 0.744 0.685 0.971 0.721 0.910 0.698 0.723 0.626 3587 0.875 11.925 

15 0.709 0.731 0.675 0.926 0.685 0.879 0.696 0.719 0.602 3647 0.877 11.986 20 0.814 0.743 0.669 0.927 0.690 0.879 0.642 0.719 0.612 3682 0.849 11.964 

Number of Features Selected. We vary the number ùúÖ of selected features from 5, 10, 15, to 20, while fixing ùúã = 10 paths materialized and maximum length ‚Ñì = 7. We assess the impact of ùúÖ on effec-tiveness only, as efficiency remains unaffected since the Feature Selector‚Äôs LLM ranks all features in the consolidated table regardless of how many are ultimately retained. Table 9 presents effectiveness results, revealing that ùúÖ = 10 fea-tures achieve the best performance in most datasets. Selecting too few features ( ùúÖ = 5) underperforms by limiting signal retention, while selecting more than 10 provides diminishing returns without improving accuracy on most datasets. In some cases, such as in 

School , 20 features perform best since the star schema places in-formative features within one hop, increasing the likelihood that additional features provide a beneficial signal. 

To conclude, allowing Hippasus to explore longer paths typically improves effectiveness, as it enables the discovery of some distant features that may be valuable in certain cases. Still, the LLM mostly prioritizes shorter paths, hence preventing this to become a bottleneck in execution time. Materializing a modest number of paths (around 10) offers a good trade-off between effectiveness and efficiency, achieving the best performance in many cases while maintaining reasonable runtime. Similarly, selecting around 10 features achieves optimal effectiveness in general, balancing signal retention with noise control. 

## 10 CONCLUSION 

We presented Hippasus , a modular framework for effective and ef-ficient feature augmentation in relational databases. Our approach addressed the fundamental tradeoff between computational effi-ciency and feature quality by decoupling the augmentation pipeline into three specialized parts: path exploration, join execution, and feature selection. We combined lightweight statistical signals with LLM-based semantic reasoning to prune unpromising join paths early, employed optimized multi-way join algorithms to efficiently consolidate features from multiple paths, and integrated semantic understanding with statistical measures to identify the most in-formative features. Experiments on publicly available benchmarks demonstrate that Hippasus achieves the best overall performance across both effectiveness and efficiency. For accuracy, Hippasus 

achieves substantial improvements with an average gain of 26.8% over ARDA, 18.6% over FeatPilot, and 14.5% over AutoFeat. For run-time performance, Hippasus delivers up to 60 √ó speedup compared to FeatPilot and up to 5 √ó speedup over ARDA, while maintaining competitive efficiency with AutoFeat. Future work includes extend-ing Hippasus to support data lake settings, where the join graph is not precomputed and may also involve similarity joins. REFERENCES 

[1] Liese Bekkers, Frank Neven, Stijn Vansummeren, and Yisu Remy Wang. 2025. Instance-Optimal Acyclic Join Processing Without Regret: Engineering the Yan-nakakis Algorithm in Column Stores. Proc. VLDB Endow. 18, 8 (2025), 2413‚Äì2426. [2] Sonia Castelo, R√©mi Rampin, A√©cio S. R. Santos, Aline Bessa, Fernando Chirigati, and Juliana Freire. 2021. Auctus: A Dataset Search Engine for Data Discovery and Augmentation. Proc. VLDB Endow. 14, 12 (2021), 2791‚Äì2794. [3] Nadiia Chepurko, Ryan Marcus, Emanuel Zgraggen, Raul Castro Fernandez, Tim Kraska, and David R. Karger. 2020. ARDA: Automatic Relational Data Augmentation for Machine Learning. Proc. VLDB Endow. 13, 9 (2020), 1373‚Äì1387. [4] Xin Luna Dong and Theodoros Rekatsinas. 2018. Data Integration and Machine Learning: A Natural Synergy. In Proceedings of the 2018 International Conference on Management of Data (SIGMOD) . 1645‚Äì1650. [5] Yuyang Dong, Kunihiro Takeoka, Chuan Xiao, and Masafumi Oyamada. 2021. Efficient Joinable Table Discovery in Data Lakes: A High-Dimensional Similarity-Based Approach. In 37th IEEE International Conference on Data Engineering (ICDE) . 456‚Äì467. [6] Yuyang Dong, Chuan Xiao, Takuma Nozawa, Masafumi Enomoto, and Masafumi Oyamada. 2023. DeepJoin: Joinable Table Discovery with Pre-trained Language Models. Proc. VLDB Endow. 16, 10 (2023), 2458‚Äì2470. [7] John N. Dyer and Camille F. Rogers. 2015. Teaching Case : Adapting the Access Northwind Database to Support a Database Course. J. Inf. Syst. Educ. 26, 2 (2015), 85‚Äì102. [8] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander J. Smola. 2020. AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. CoRR abs/2003.06505 (2020). [9] Meihao Fan, Xiaoyue Han, Ju Fan, Chengliang Chai, Nan Tang, Guoliang Li, and Xiaoyong Du. 2024. Cost-effective in-context learning for entity resolution: A design space exploration. In 40th International Conference on Data Engineering (ICDE) . 3696‚Äì3709. [10] Raul Castro Fernandez, Ziawasch Abedjan, Famien Koko, Gina Yuan, Samuel Madden, and Michael Stonebraker. 2018. Aurum: A Data Discovery System. In 

34th IEEE International Conference on Data Engineering (ICDE) . 1001‚Äì1012. [11] Raul Castro Fernandez, Jisoo Min, Demitri Nava, and Samuel Madden. 2019. Lazo: A Cardinality-Based Method for Coupled Estimation of Jaccard Similarity and Containment. In 35th IEEE International Conference on Data Engineering (ICDE) .1190‚Äì1201. [12] Benjamin Feuer, Yurong Liu, Chinmay Hegde, and Juliana Freire. 2024. ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. Proc. VLDB Endow. 17, 9 (2024), 2279‚Äì2292. [13] Juliana Freire, Grace Fan, Benjamin Feuer, Christos Koutras, Yurong Liu, Eduardo Pe√±a, A√©cio S. R. Santos, Cl√°udio T. Silva, and Eden Wu. 2025. Large Language Models for Data Discovery and Integration: Challenges and Opportunities. IEEE Data Eng. Bull. 49, 1 (2025), 3‚Äì31. [14] Sainyam Galhotra, Yue Gong, and Raul Castro Fernandez. 2023. Metam: Goal-Oriented Data Discovery. In 39th IEEE International Conference on Data Engineer-ing (ICDE) . 2780‚Äì2793. [15] Andra Ionescu, Kiril Vasilev, Florena Buse, Rihan Hai, and Asterios Katsifodimos. 2024. AutoFeat: Transitive Feature Discovery over Join Paths. In 40th IEEE International Conference on Data Engineering (ICDE) . 1861‚Äì1873. [16] Daniel P. Jeong, Zachary Chase Lipton, and Pradeep Kumar Ravikumar. 2025. LLM-Select: Feature Selection with Large Language Models. Trans. Mach. Learn. Res. 2025 (2025). [17] Pengyue Jia, Zhaocheng Du, Yichao Wang, Xiangyu Zhao, Xiaopeng Li, Yuhao Wang, Qidong Liu, Huifeng Guo, and Ruiming Tang. 2024. AltFS: Agency-light Feature Selection with Large Language Models in Deep Recommender Systems. 

CoRR abs/2412.08516 (2024). [18] James Max Kanter and Kalyan Veeramachaneni. 2015. Deep feature synthesis: Towards automating data science endeavors. In IEEE International Conference on Data Science and Advanced Analytics (DSAA) . 1‚Äì10. [19] Moe Kayali, Anton Lykov, Ilias Fountalis, Nikolaos Vasiloglou, Dan Olteanu, and Dan Suciu. 2024. CHORUS: Foundation Models for Unified Data Discovery and Exploration. Proc. VLDB Endow. 17, 8 (2024), 2104‚Äì2114. [20] Aamod Khatiwada, Grace Fan, Roee Shraga, Zixuan Chen, Wolfgang Gatter-bauer, Ren√©e J. Miller, and Mirek Riedewald. 2023. SANTOS: Relationship-based Semantic Table Union Search. Proc. ACM Manag. Data 1, 1 (2023), 9:1‚Äì9:25. [21] Arun Kumar, Jeffrey F. Naughton, Jignesh M. Patel, and Xiaojin Zhu. 2016. To Join or Not to Join?: Thinking Twice about Joins before Feature Selection. In 

Proceedings of the 2016 International Conference on Management of Data (SIGMOD) .19‚Äì34. [22] Dawei Li, Zhen Tan, and Huan Liu. 2024. Exploring Large Language Models for Feature Selection: A Data-centric Perspective. SIGKDD Explor. 26, 2 (2024), 44‚Äì53. [23] Jianhao Li and Xianchao Xiu. 2025. LLM4FS: Leveraging Large Language Models for Feature Selection and How to Improve It. CoRR abs/2503.24157 (2025). [24] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. 2024. Table-GPT: Table Fine-tuned GPT for Diverse Table Tasks. Proceedings of the ACM on Management of Data 2, 3 (2024), 1‚Äì28. [25] Jiaming Liang, Chuan Lei, Xiao Qin, Jiani Zhang, Asterios Katsifodimos, Christos Faloutsos, and Huzefa Rangwala. 2025. FeatPilot: Automatic Feature Augmenta-tion on Tabular Data. In 41st IEEE International Conference on Data Engineering (ICDE) . 2148‚Äì2160. [26] Jiabin Liu, Chengliang Chai, Yuyu Luo, Yin Lou, Jianhua Feng, and Nan Tang. 2022. Feature Augmentation with Reinforcement Learning. In 38th IEEE International Conference on Data Engineering (ICDE) . 3360‚Äì3372. [27] Danrui Qi, Weiling Zheng, and Jiannan Wang. 2024. FeatAug: Automatic Feature Augmentation From One-to-Many Relationship Tables. IEEE 40th International Conference on Data Engineering (ICDE) (2024), 1805‚Äì1818. [28] Vraj Shah, Arun Kumar, and Xiaojin Zhu. 2017. Are Key-Foreign Key Joins Safe to Avoid when Learning High-Capacity Classifiers? Proc. VLDB Endow. 11, 3 (2017), 366‚Äì379. [29] Qichen Wang, Bingnan Chen, Binyang Dai, Ke Yi, Feifei Li, and Liang Lin. 2025. Yannakakis+: Practical Acyclic Query Evaluation with Theoretical Guarantees. 

Proc. ACM Manag. Data 3, 3 (2025), 235:1‚Äì235:28. [30] Siyi Wang, Qi Deng, Shiwei Feng, Hong Zhang, and Chao Liang. 2024. A sur-vey on rank aggregation. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI-24) . 8281‚Äì8289. [31] Lindsey Linxi Wei, Guorui Xiao, and Magdalena Balazinska. 2024. RACOON: An LLM-based Framework for Retrieval-Augmented Column Type Annotation with a Knowledge Graph. In NeurIPS 2024 Third Table Representation Learning Workshop .[32] Mihalis Yannakakis. 1981. Algorithms for Acyclic Database Schemes. In VLDB .82‚Äì94. [33] Erica Zhang, Ryunosuke Goto, Naomi Sagan, Jurik Mutter, Nick Phillips, Ash A. Alizadeh, Kangwook Lee, Jose Blanchet, Mert Pilanci, and Robert Tibshirani. 2025. LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization. CoRR abs/2502.10648 (2025). [34] Haoxiang Zhang, Yurong Liu, Wei-Lun Hung, A√©cio S. R. Santos, and Juliana Freire. 2025. AutoDDG: Automated Dataset Description Generation using Large Language Models. CoRR abs/2502.01050 (2025). [35] Erkang Zhu, Dong Deng, Fatemeh Nargesian, and Ren√©e J. Miller. 2019. JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes. In 

Proceedings of the 2019 International Conference on Management of Data (SIGMOD) .847‚Äì864.