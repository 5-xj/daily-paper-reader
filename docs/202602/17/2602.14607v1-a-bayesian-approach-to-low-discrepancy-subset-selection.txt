Title: A Bayesian Approach to Low-Discrepancy Subset Selection

URL Source: https://arxiv.org/pdf/2602.14607v1

Published Time: Tue, 17 Feb 2026 02:45:35 GMT

Number of Pages: 13

Markdown Content:
# A Bayesian Approach to Low-Discrepancy Subset Selection 

## Nathan Kirk 1* 

> 1*

School of Mathematics and Statistics, University of St Andrews, Mathematical Institute, St Andrews, KY16 9SS, United Kingdom. Corresponding author(s). E-mail(s): nmk7@st-andrews.ac.uk ;

Abstract 

Low-discrepancy designs play a central role in quasi–Monte Carlo methods and are increasingly influential in other domains such as machine learning, robotics and computer graphics, to name a few. In recent years, one such low-discrepancy construction method called subset selection has received a lot of attention. Given a large population, one optimally selects a small low-discrepancy subset with respect to a discrepancy-based objective. Versions of this problem are known to be NP–hard. In this text, we establish, for the first time, that the subset selection problem with respect to kernel discrepancies is also NP–hard. Motivated by this intractability, we propose a Bayesian Optimization procedure for the subset selection problem utilizing the recent notion of deep embedding kernels. We demonstrate the performance of the BO algorithm to minimize discrepancy measures and note that the framework is broadly applicable any design criteria. 

Keywords: Quasi-Monte Carlo, Low-Discrepancy, Bayesian Optimization 

MSC Classification: 65C05 , 60G15 

# 1 Introduction 

An important problem in statistics and numerical computation is to approximate a probability distribution of interest by a finite set of representative points. This idea underlies a widely used variant of the Monte Carlo (MC) method known as quasi– Monte Carlo (QMC) [ 1, 2], which replaces the naive random sampling of MC with so-called low-discrepancy (LD) design sets whose elements are more evenly spread over the domain. Such designs find applications in numerical approximation, uncertainty 1

> arXiv:2602.14607v1 [stat.ME] 16 Feb 2026

quantification, Bayesian inference, computer vision, robotics, and scientific machine learning [ 3–9]. Nowadays, many “off-the-shelf” LD designs are available and can typ-ically be divided into two prominent families: digital nets and sequences [ 10 ] and lattice rules [ 11 ]. Both rely heavily on ideas from number theory and abstract algebra. Complementary to these classical constructions, LD designs have also been gener-ated computationally in recent years using modern optimization techniques, including neural network-based approaches [ 12 ] and nonlinear programming methods [ 13 ]. In the classical QMC setting, discrepancy measures the irregularity of distribution of a point set PN := {Xi}Ni=1 ⊂ [0 , 1] d with respect to the uniform distribution on the unit hypercube. Perhaps the most popular discrepancy in the literature is the so-called 

L∞ star discrepancy .                       

> Definition 1 (L∞Star Discrepancy) Given a point set PN={Xi}Ni=1 ⊂[0 ,1] d, the L∞
> star discrepancy of PNis defined as
> D∗∞(PN) := sup
> x∈[0 ,1] d
> ∣∣∣∣#( PN∩[0 ,x))
> N−λ([0 ,x))
> ∣∣∣∣.(1) where λ(·) denotes the usual Lebesgue measure.

This notion has been studied extensively over several decades and because of its nature—testing the uniformity of subintervals of the unit hypercube—it is often referred to as the geometric interpretation of discrepancy [ 14 , 15 ]. On the other hand, discrepancy can also be viewed from a kernel methods perspective, an approach com-monly attributed to [ 16 ]. In this paper, we consider a more general notion of kernel discrepancy than that introduced in [ 16 ], namely the maximum mean discrepancy [17 ], which encompasses several classical and modern kernel discrepancy settings within a unified framework.                                         

> Definition 2 (Maximum Mean Discrepancy) Let Pand Qbe Borel probability measures on a measurable space X. Let Hbe a reproducing kernel Hilbert space (RKHS) with reproducing kernel k(·,·). The squared maximum mean discrepancy between Pand Qis defined as (D k
> 2(P, Q )) 2:= sup
> ‖f‖H≤1
> (∫
> fdP−
> ∫
> fdQ
> )2
> ,(2) which can be written equivalently as
> E[k(X,X′) + k(Y,Y′)−2k(X,Y)],
> where X,X′∼Pand Y,Y′∼Qare independent copies.

The maximum mean discrepancy specializes to two cases that will be of particu-lar interest in this text. The first corresponds to classical L2 discrepancies in QMC, obtained by choosing a positive definite kernel k : [0 , 1] d × [0 , 1] d → R. Without loss of generality, let P denote the empirical distribution induced by a design set PN ⊂ [0 , 1] d,2and let Q denote the uniform distribution on the unit hypercube. Then, with a slight abuse of notation, the L2 discrepancy of PN is defined as (D k 

> 2

(PN )) 2 := 

∫ ∫ 

> [0 ,1] d

k(x, y) d x dy − 2

N

> N

∑

> i=1

∫

> [0 ,1] d

k(Xi, y) d y + 1

N 2

> N

∑

> i,j =1

k(Xi, Xj ),

(3) for specific choices of k yielding, for example, the star D∗ 

> 2

, extreme Dext 2 , and periodic 

Dper 2 discrepancies. For the second case, when both Borel measures P and Q are empirical, the max-imum mean discrepancy reduces to a kernel-based discrepancy between two finite design sets, enabling direct comparison of samples drawn either from the same or from different underlying distributions. In this setting, again with a slight abuse of notation, for PN = {Xi}Ni=1 and Pm = {Yi}mi=1 ⊂ [0 , 1] d, one may write ( 2) as (D k 

> 2

(PN , P m)) 2 := 1

N 2

> N

∑

> i,j =1

k(Xi, Xj ) − 2

mN 

> N

∑

> i=1
> m

∑

> j=1

k(Xi, Yj ) + 1

m2

> m

∑

> i,j =1

k(Yi, Yj ).

(4) Together, ( 1), ( 3), and ( 4) define the objective functions that we seek to minimize in this work. 

## 1.1 The Subset Selection Problem 

One particular computational LD construction strategy that has received considerable attention in recent years is the subset selection procedure. It was originally formulated for the L∞ star discrepancy ( 1) in [ 18 , 19 ] and later generalized to kernel discrepancy measures in [ 20 ]. The idea is simple: given a large population of N design points, one seeks a subset of size m ≪ N that is, in some sense, most representative of the full set. Represen-tativeness will be measured in our setting by one of the discrepancy-based objectives introduced above. 

Given a population set PN ⊂ [0 , 1] d of size N , the goal is to select a subset Pm ⊂ PN of size m ≪ N that minimizes a discrepancy function, D(·). That is, we wish to solve the following optimization problem: 

argmin    

> Pm⊂PN,|Pm|=m

D(Pm). (5) Clearly, even for moderate values of N and m, exhaustive search or exact methods for finding an optimum of this combinatorial problem are computationally infeasible. In fact, it was established in [ 18 , Theorem 3.1] that ( 5) is NP-hard when the dis-crepancy measure is the L∞ star discrepancy ( 1). As a result, a range of heuristic approaches have been proposed for solving ( 5), most of which are based on swap-based local search strategies. We refer to [ 21 ] for further reading on subset selection. 31.2 Our Contribution 

In this work, we propose to address ( 5) using Bayesian optimization. First, we gener-alize the hardness result and show in Section 2 that subset selection with respect to a wide class of so-called kernel discrepancy measures is also, in fact, NP–hard. Next, we discuss the recently introduced framework of deep kernel embeddings [22 ] to con-struct strictly positive definite covariance kernels for Gaussian Process surrogates over set-valued inputs which is key for our subset selection problem. Our full Bayesian opti-mization method is detailed in Section 3.2 , and finally, numerical results illustrating its performance are presented in Section 4.

# 2 Kernel Subset Selection is NP–Hard 

As discussed in the Introduction, subset selection is known to be computationally intractable in the classical L∞ star discrepancy setting [ 18 , Theorem 3.1]. In particular, computing the L∞ star discrepancy itself is NP–hard [ 23 ], which strongly suggests that optimizing it over subsets is likewise intractable. This result provided theoretical justification for the widespread use of heuristic and approximate algorithms in that context. We now extend this perspective to the kernel-based setting and show that, over the class of positive definite kernels, subset selection with respect to the maximum mean discrepancy is NP–hard in general. Specifically, we reduce from a known NP–hard setting, the constrained quadratic 0-1 problem [ 24 ]. 

Theorem 1 For the inputs PN = {Xi}Ni=1 ⊂ [0 , 1] d, an integer m ≤ N , a threshold τ > 0

and a positive definite kernel k : [0 , 1] d × [0 , 1] d → R, the decision problem 

∃ Pm ⊂ PN , |Pm| = m such that Dk 

> 2

(Pm) ≤ τ

is NP–hard. Proof From a population set PN = {Xi}Ni=1 ⊂ [0 , 1] d and an index subset S ⊂ { 1, . . . , N }

with |S| = m, let Pm := {Xi : i ∈ S} and for a positive definite kernel k : [0 , 1] d ×[0 , 1] d → R,define 

Dk 

> 2

(Pm) = 

√√√√

∫ ∫ 

k(x, y) dx dy − 2

m

∑

> i∈S

∫

k(Xi, y) dy + 1

m2

∑

> i,j ∈S

k(Xi, Xj ).

For such k, write 

C := 

∫ ∫ 

k(x, y) dx dy, bi := 

∫

k(Xi, y) dy, Kij := k(Xi, Xj ).

Let z ∈ { 0, 1}N be the indicator of S, i.e., zi = 1{i∈S}. Then 

Dk 

> 2

(Pm)2 = C − 2

m

> N

∑

> i=1

bizi + 1

m2

> N

∑

> i,j =1

Kij zizj .

4Multiplying by m2 gives 

m2Dk 

> 2

(Pm)2 = m2C − 2m b⊤z + z⊤Kz. (6) Hence, minimizing Dk 

> 2

(Pm) over all m–element subsets is equivalent to minimizing aquadratic polynomial in z under the constraint ∑ 

> i

zi = m.We prove the hardness result by reducing from the NP–hard decision problem known as the constrained quadratic 0-1 problem [ 24 , 25 ]: given a symmetric positive semidefinite matrix Q ∈ QN ×N , vector c ∈ QN , integer m, and threshold T , decide whether there exists 

z ∈ { 0, 1}N with ∑ 

> i

zi = m such that 

z⊤Qz + c⊤z ≤ T. (7) Given an instance ( Q, c, m, T ), define 

K := Q + IN , b := − 1

2m c.

Since Q  0, we have K ≻ 0. For any feasible z with ∑ 

> i

zi = m,

z⊤Kz = z⊤Qz + z⊤z = z⊤Qz + m. 

Substituting into ( 6) gives 

m2Dk 

> 2

(Pm)2 = m2C + z⊤Qz + c⊤z + m. 

Thus 

z⊤Qz + c⊤z ≤ T ⇐⇒ m2Dk 

> 2

(Pm)2 ≤ T + m + m2C. 

and by setting τ 2 := ( T + m + m2C)/m 2, we obtain equivalence of the two decision problems. It now remains to construct an positive definite kernel on [0 , 1] d realizing the quantities (K, b, C ). Since K ≻ 0, let A be an invertible matrix with A⊤A = K. Define 

μf := A−T b, C := ‖μf ‖22 = b⊤K−1b.

We now construct a measurable feature map φ : [0 , 1] d → RN such that 

φ(Xi) = Aei for i = 1 , . . . , N, 

∫

> [0 ,1] d

φ(x) dx = μf .

This can be done by defining φ to be piecewise constant on a partition of [0 , 1] d with the desired integral, and redefining its values at the finitely many points Xi (which does not affect the integral). Next, define 

k(x, y) := φ(x)⊤φ(y).

Then k is positive definite on [0 , 1] d, and: 

k(Xi, Xj ) = Kij ,

∫

k(Xi, y) dy = bi,

∫ ∫ 

k(x, y) dx dy = C. 

Thus the discrepancy computed from this k satisfies ( 6). Finally, the NP–hard instance (7) has a feasible solution if and only if there exists a subset Pm ⊂ PN with Dk 

> 2

(Pm) ≤ τ . 

This resolves an open question posed by F. Cl´ ement [ 21 ]. 53 Subset Selection via Bayesian Optimization 

We now describe our Bayesian optimization approach for solving the subset selection problem ( 5). The key technical ingredient is a class of strictly positive definite kernels defined on finite subsets, known as deep embedding kernels. We first review their construction and then describe the full optimization pipeline. 

## 3.1 Deep Embedding Kernels 

Many learning and optimization problems involve set-valued inputs. Examples arise in natural language processing [ 26 ], clustering [ 27 ], and in general experimental design. In our design setting, for a fixed population PN = {Xi}Ni=1 ⊂ [0 , 1] d, let Sm(PN )denote the set of all m-element subsets of PN . Then the optimization variable is a subset Pm ∈ S m(PN ), i.e., a set-valued input. Standard Gaussian process (GP) methodology is designed for Euclidean inputs and does not directly provide kernels on finite subsets. A classical construction for sets is the double-sum (DS) kernel [28 , 29 ], defined for subsets Pm, ˜Pm ∈ S m(PN ) by 

k0(Pm, ˜Pm) = 1

|Pm| | ˜Pm|

∑

> X∈Pm

∑  

> X′∈˜Pm

kX (X, X′), (8) where kX : [0 , 1] d × [0 , 1] d → R is a positive definite kernel acting on the base space. While k0 is positive definite, it is generally not strictly positive definite, which may lead to problems downstream like singular Gram matrices in GP regression and optimization. To address this, [ 22 ] introduced deep embedding (DE) kernels . The construction begins by embedding each subset into the reproducing kernel Hilbert space (RKHS) 

HkX via the empirical kernel mean embedding 

E(Pm) = 1

|Pm|

∑

> X∈Pm

kX (·, X).

The canonical RKHS distance between two subsets is then 

dE (Pm, ˜Pm) = ‖E (Pm) − E ( ˜Pm)‖HkX =

√

k0(Pm, P m) + k0( ˜Pm, ˜Pm) − 2k0(Pm, ˜Pm).

(9) and a DE kernel is obtained by composing this distance with a radial positive definite kernel on Hilbert space: 

kDE (Pm, ˜Pm) = kH

(

dE (Pm, ˜Pm)

)

, (10) where kH : [0 , ∞) → R is such that ( h, h ′) 7 → kH (‖h − h′‖H) is positive definite for any Hilbert space H. Under mild conditions, kDE is in fact strictly positive definite acting on the set of all subsets [ 22 , Propositions 1–3]. 63.2 Bayesian Optimization for Subset Selection 

We now describe our BO framework over the discrete domain Sm(PN ). Let k be a positive definite kernel on [0 , 1] d and define the objective 

f (Pm) = Dk 

> 2

(Pm), Pm ∈ S m(PN ).

In practice, to improve numerical stability, we model the logarithm of the discrepancy, 

f (Pm) = log Dk 

> 2

(Pm).

We place a zero-mean Gaussian process prior on f with covariance kernel given by a DE kernel of the form 

kDE (Pm, ˜Pm) = exp 

(

− dE (Pm, ˜Pm)2

2θ2

> H

)

,

where dE is defined in ( 9). At the point level we use a radial basis function (RBF) kernel kX (x, x′) = exp (−‖ x − x′‖2/2σ2

> X

). The hyperparameters ( σX , θ H ) are selected at each BO iteration by maximizing the GP log marginal likelihood over log-spaced grids, after standardizing the observed targets to zero mean and unit variance. Let {P (i) 

> m

}ti=1 denote the subsets evaluated up to iteration t, and define 

fmin = min  

> 1≤i≤t

f (P (i) 

> m

).

For minimization, we employ the Expected Improvement acquisition function EI( Pm) = E[( fmin − f (Pm)) + | D t] ,

which admits the standard closed-form expression in terms of the GP posterior mean 

μ(Pm) and variance σ2(Pm): EI( Pm) = ( fmin − μ(Pm))Φ 

( fmin − μ(Pm)

σ(Pm)

)

+ σ(Pm)φ

( fmin − μ(Pm)

σ(Pm)

)

,

where Φ and φ denote the standard normal CDF and PDF. Since direct maximization of EI over the combinatorial domain Sm(PN ) is, in itself, infeasible, we approximate the maximization via local search over 1-swap neighborhoods . Given a subset Pm, a 1-swap neighbor is any subset of the form ¯Pm = Pm \ { Xi} ∪ { Xj }, Xi ∈ Pm, Xj ∈ PN \ Pm.

Starting from an initial subset, we sample a finite collection of 1-swap neighbors and move to a neighbor with strictly larger EI whenever one exists. The procedure terminates at a local maximizer of EI. To encourage exploration, each BO iteration performs one hill-climb initialized at the current best subset and several additional 7Algorithm 1 BO-DE for Low-Discrepancy Subset Selection  

> 1:

Input: PN = {Xi}Ni=1 , subset size m, discrepancy D(·), ninit , iterations T , grids ΣX and Θ H , hill-climb steps Smax , neighbors per step M , restarts R 

> 2:

Output: Best subset P ⋆m ∈ S m(PN ) found  

> 3:

Sample P (1)  

> m

, . . . , P (ninit ) 

> m

∼ Unif( Sm(PN ))  

> 4:

Evaluate yi ← D(P (i) 

> m

) and set ˜ yi ← log(max {yi, ε }) (or ˜yi ← yi) 

> 5:

D ← { (P (i) 

> m

, ˜yi)}ninit  

> i=1
> 6:

for t = 1 to T do  

> 7:

Choose ( σX , θ H ) ∈ ΣX × ΘH maximizing GP log marginal likelihood on standardized targets  

> 8:

Fit GP surrogate on D with k(σX ,θ H )DE ; 

> 9:

let fmin = min (Pm ,˜y)∈D ˜y and P best  

> m

= arg min (Pm ,˜y)∈D ˜y 

> 10:

Run EI hill-climb from P best  

> m

and from R random starts;  

> 11:

each step samples ≤ M 1-swap neighbors and moves if EI increases; collect endpoints C 

> 12:

Select P next  

> m

= arg max Pm ∈C EI( Pm) 

> 13:

Evaluate ynext ← D(P next  

> m

) and transform ˜ ynext ; 

> 14:

append ( P next  

> m

, ˜ynext ) to D 

> 15:

end for  

> 16:

return P ⋆m = arg min (Pm ,˜y)∈D ˜y 

> 17:

(equivalently arg min D(Pm) when ˜y = log D(Pm)). 

hill-climbs initialized from randomly sampled subsets. The next evaluation point is selected as the local maximizer with the largest EI among these runs. 

# 4 Numerical Experiments 

We now evaluate the proposed Bayesian optimization framework for subset selection. Following the Introduction, we consider three objectives which cover several related settings selecting an m–point design Pm from a larger population PN ⊂ [0 , 1] d to approximate a reference measure, which may be discrete or continuous. When the reference measure is discrete, ( 4) will be used as the objective function and ( 1) or ( 3)otherwise. In the continuous case, we are interested in optimizing the classical L∞

star discrepancy, on which the subset selection problem was originally framed in the QMC literature, and also the L2-type kernel discrepancies. 

## 4.1 Experimental Setup 

Our goal is to achieve substantial discrepancy minimization using a limited number of true objective evaluations. For this reason, in all experiments we restrict the eval-uation budget to 100 discrepancy evaluations: 50 initial evaluations used to fit the Gaussian Process surrogate, followed by 50 additional evaluations selected sequentially by maximizing the EI acquisition function. We focus on two-dimensional settings throughout and compare BO-DE against three baseline methods: 8• Random: Subsets are sampled uniformly at random from Sm(PN ) and evaluated using the true discrepancy. 

• BO-DS: This method replaces the DE kernel with the double-sum (DS) kernel, with the same fixed RBF base kernel. Acquisition optimization is performed using the same EI hill-climbing procedure over sampled 1-swap neighborhoods as in the DE method. 

• Greedy Local Swap (GLS): This method directly optimizes the discrepancy without a surrogate model. Starting from a collection of randomly sampled subsets, the best initial subset is selected and iteratively improved via 1-swap moves. At each step, a sampled collection of 1-swap neighbors is evaluated using the true discrep-ancy, and the algorithm moves to the best improving neighbor. If no improvement is found, a random restart is performed until the evaluation budget is exhausted. All methods are compared under equal discrepancy evaluations budgets rather than wall-clock time. 

## 4.2 Results 

Experiment 1. We minimize the symmetric L2 discrepancy ( 3) induced by the prod-uct kernel k(x, x′) = ∏dj=1 (1 − 2|xj − x′ 

> j

|)/4. This choice corresponds to a classical 

L2 discrepancy widely used in quasi-Monte Carlo methods, though other L2 variants could equally be considered within our framework. A population PN of size N = 1000 is sampled uniformly from [0 , 1] 2, and subsets of size m = 25 are selected. Figure 1 reports the best discrepancy value observed as a function of the evaluation count. BO-DE achieves the smallest final discrepancy and produces the largest reduc-tion relative to the initial random design. GLS yields the second-best performance, improving upon the initial subsets but plateauing once no improving swaps are found. In contrast, both BO-DS and random sampling show no improvement beyond the ini-tial 50 random evaluations, indicating limited effectiveness in this budget-restricting setting.      

> Fig. 1 Symmetric discrepancy minimization for N= 1000 and m= 25.

9Fig. 2 Maximum mean discrepancy minimization for two-component Gaussian mixture for random, GLS, BO-DS and BO-DE methods for N = 1000 and m = 25. The resulting subset for GLS ( Middle )and BO-DE ( Right ). 

Experiment 2. We next consider the maximum mean discrepancy as defined in ( 4). A population of size N = 1000 is sampled from a two-component Gaussian mixture, and a subset of size m = 25 is selected under the same evaluation budget as in the previous experiments. The results are shown in Figure 2. Consistent with the earlier discrepancy exper-iments, BO-DE attains the smallest final discrepancy value within the fixed budget. GLS yields moderate improvement, while both random sampling and BO with the DS kernel show limited reduction beyond the initial designs. For visual comparison, the final subsets obtained by GLS and BO-DE are dis-played as black points over the original population (shown in grey). The BO-DE subset exhibits more uniform coverage of the mixture components, with fewer visible clusters and sparse regions than the GLS solution.       

> Fig. 3 L∞star discrepancy minimization for N= 1000 and m= 25.

Experiment 3. Lastly, we consider minimization of the L∞ star discrepancy ( 1)under the same experimental protocol. The L∞ star discrepancy is well known to be computationally demanding—even the fastest exact algorithms scale on the order of 

O(N d/ 2+1 ); see [ 30 ]. Consequently, evaluating the true objective becomes increasingly 10 expensive as N grows, making surrogate-based approaches particularly appealing in this setting. Figure 3 shows the best star discrepancy values. As in both previous experiments, BO-DE achieves the smallest final discrepancy. GLS provides moderate improvement but both BO-DS and uniform random sampling exhibit little to no improvement once again beyond the initial 50 random evaluations. These results suggest that the DE surrogate is especially beneficial when the objective function is expensive to compute, as is the case for the L∞ star discrepancy. 

# 5 Discussion 

In this work, we introduced a Bayesian Optimization framework for selecting low-discrepancy subsets based on several variants of the discrepancy, using deep embedding kernels to construct Gaussian Process surrogates directly over the combinatorial space of subsets. Analysis first established that the subset selection problem with respect to kernel discrepancies is NP-hard in general and we demonstrated the efficacy of the method empirically illustrating that the DE-based surrogate can substantially improve discrepancy reduction with small numbers of true objective evaluations. Although our experiments focused on standard geometric discrepancies, the methodology is considerably more general. This paper further emphasizes that deep embedding kernels provide a principled way to perform black-box optimization over structured combinatorial domains, an important setting in many modern statistical and scientific domains. 

Acknowledgments. 

This work was partially supported by the U.S. National Science Foundation (NSF DMS–2316011). The author thanks Fred J. Hickernell for the invitation to speak at the 2025 IMSI Workshop “Kernel Methods in Uncertainty Quantification and Exper-imental Design”, where the initial idea for this paper emerged in conversation with David Ginsbourger. The author also thanks Illinois Institute of Technology student Sanjana Waghray for assistance with exploratory numerical experiments. 

# References 

[1] Hickernell, F.J., Kirk, N., Sorokin, A.G.: Quasi-Monte Carlo Methods: What, Why, and How? Preprint, https://arxiv.org/abs/2502.03644 (2025) [2] Lemieux, C.: Monte Carlo and Quasi-Monte Carlo Sampling. Springer, New York (2009) [3] Paulin, L., Bonneel, N., Coeurjolly, D., Iehl, J.-C., Keller, A., Ostromoukhov, V.: Matbuilder: Mastering sampling uniformity over projections. ACM Transactions on Graphics (Proceedings of SIGGRAPH) 41 (4) (2022) [4] Fang, K.T., Lin, D.K.J., Winker, P., Zhang, Y.: Uniform design: Theory and applications. Technometrics 42 , 237–248 (2000) 11 [5] Keller, A.: Quasi-Monte Carlo image synthesis in a nutshell. In: Monte Carlo and Quasi-Monte Carlo Methods 2012, pp. 213–249 (2013) [6] Herrmann, L., Schwab, C.: Multilevel quasi-Monte Carlo uncertainty quantifi-cation for advection-diffusion-reaction. In: Monte Carlo and Quasi-Monte Carlo Methods: MCQMC, Rennes, France, July 2018, pp. 31–67 (2020) [7] Mishra, S., Rusch, T.K.: Enhancing accuracy of deep learning algorithms by train-ing with low-discrepancy sequences. SIAM Journal on Numerical Analysis 59 (3), 1811–1834 (2021) [8] Longo, M., Mishra, S., Rusch, T.K., Schwab, C.: Higher-order quasi-Monte Carlo training of deep neural networks. SIAM Journal on Scientific Computing 43 (6), 3938–3966 (2021) [9] Chahine, M., Rusch, T.K., Patterson, Z.J., Rus, D.: Improving efficiency of sampling-based motion planning via Message-Passing Monte Carlo. In: 9th Annual Conference on Robot Learning, vol. 305 (2025) [10] Dick, J., Pillichshammer, F.: Digital Nets and Sequences: Discrepancy Theory and Quasi-Monte Carlo Integration. Cambridge University Press, Cambridge (2010) [11] Dick, J., Kritzer, P., Pillichshammer, F.: Lattice Rules: Numerical Integration, Approximation, and Discrepancy. Springer Series in Computational Mathematics. Springer, Cham (2022) [12] Rusch, T.K., Kirk, N., Bronstein, M.M., Lemieux, C., Rus, D.: Message-Passing Monte Carlo: Generating low-discrepancy point sets via graph neural networks. Proceedings of the National Academy of Sciences 121 (40), 2409913121 (2024) [13] Cl´ ement, F., Doerr, C., Klamroth, K., Paquete, L.: Constructing optimal star discrepancy sets. Proceedings of the American Mathematical Society, Series B 

12 (7), 78–90 (2025) [14] Kuipers, L., Niederreiter, H.: Uniform Distribution of Sequences. John Wiley, New York (1974) [15] Matousek, J.: Geometric Discrepancy: An Illustrated Guide. Algorithms and Combinatorics. Springer, Berlin Heidelberg (1999) [16] Hickernell, F.J.: A generalized discrepancy and quadrature error bound. Math. Comput. 67 (221), 299–322 (1998) [17] Gretton, A., Borgwardt, K., Rasch, M., Sch¨ olkopf, B., Smola, A.: A kernel method for the two-sample problem. In: Advances in Neural Information Processing Systems (2006) [18] Cl´ ement, F., Doerr, C., Paquete, L.: Star discrepancy subset selection: Problem 12 formulation and efficient approaches for low dimensions. Journal of Complexity 

70 , 101645 (2022) [19] Cl´ ement, F., Doerr, C., Paquete, L.: Heuristic approaches to obtain low-discrepancy point sets via subset selection. Journal of Complexity 83 , 101852 (2024) [20] Chen, D., Cl´ ement, F., Doerr, C., Kirk, N.: Optimizing Kernel Discrepancies via Subset Selection. Preprint, https://arxiv.org/abs/2508.04926 (2025) [21] Cl´ ement, F.: An optimization perspective on the construction of low-discrepancy point sets. PhD thesis, Sorbonne Universit´ e (2024) [22] Buathong, P., Ginsbourger, D., Krityakierne, T.: Kernels over Sets of Finite Sets using RKHS Embeddings, with Application to Bayesian (Combinatorial) Opti-mization. In: Chiappa, S., Calandra, R. (eds.) Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research, vol. 108, pp. 2731–2741. PMLR, Online (2020) [23] Gnewuch, M., Srivastav, A., Winzen, C.: Finding optimal volume subintervals with k points and calculating the star discrepancy are np-hard problems. Journal of Complexity 25 (2), 115–127 (2009) [24] Murty, K.G., Kabadi, S.N.: Some NP-complete problems in quadratic and nonlinear programming. Mathematical Programming 39 (2), 117–129 (1987) [25] Gao, J., Li, D.: A polynomial case of the cardinality-constrained quadratic optimization problem. Journal of Global Optimization 56 (4), 1441–1455 (2013) [26] Pappas, N., Popescu-Belis, A.: Explicit document modeling through weighted multiple-instance learning. Journal of Articial Intelligence Research 58 (2017) [27] Kim, J., McCourt, M., You, T., Kim, S., Choi, S.: Bayesian optimization over sets. In: 6th ICML Workshop on Automated Machine Learning (2019) [28] Haussler, D.: Convolution kernels on discrete structures. Technical Report, University of California at Santa Cruz, Department of Computer Science (1999) [29] Gartner, T., Flach, P.A., Kowalczyk, A., Smola, A.J.: Multi-instance kernels. In: Proceedings of the International Conference on Machine Learning, pp. 179–186 (2002) [30] Dobkin, D.P., Eppstein, D., Mitchell, D.P.: Computing the discrepancy with applications to supersampling patterns. ACM Trans. Graph 15 , 354–376 (1996) 13