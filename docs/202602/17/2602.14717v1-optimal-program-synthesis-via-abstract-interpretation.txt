Title: Optimal Program Synthesis via Abstract Interpretation

URL Source: https://arxiv.org/pdf/2602.14717v1

Published Time: Tue, 17 Feb 2026 02:55:00 GMT

Number of Pages: 25

Markdown Content:
# Optimal Program Synthesis via Abstract Interpretation 

STEPHEN MELL, University of Pennsylvania, USA 

STEVE ZDANCEWIC, University of Pennsylvania, USA 

OSBERT BASTANI, University of Pennsylvania, USA We consider the problem of synthesizing programs with numerical constants that optimize a quantitative objective, such as accuracy, over a set of input-output examples. We propose a general framework for optimal synthesis of such programs in a given domain specific language (DSL), with provable optimality guarantees. Our framework enumerates programs in a general search graph, where nodes represent subsets of concrete programs. To improve scalability, it uses ğ´ âˆ— search in conjunction with a search heuristic based on abstract interpretation; intuitively, this heuristic establishes upper bounds on the value of subtrees in the search graph, enabling the synthesizer to identify and prune subtrees that are provably suboptimal. In addition, we propose a natural strategy for constructing abstract transformers for monotonic semantics, which is a common property for components in DSLs for data classification. Finally, we implement our approach in the context of two such existing DSLs, demonstrating that our algorithm is more scalable than existing optimal synthesizers. CCS Concepts: â€¢ Theory of computation â†’ Abstraction ; Program analysis .Additional Key Words and Phrases: program synthesis, optimal synthesis, abstract interpretation 

ACM Reference Format: 

Stephen Mell, Steve Zdancewic, and Osbert Bastani. 2024. Optimal Program Synthesis via Abstract Interpreta-tion. Proc. ACM Program. Lang. 8, POPL, Article 16 (January 2024), 25 pages. https://doi.org/10.1145/3632858 

1 INTRODUCTION 

Due to their interpretability, robustness, and data-efficiency, there is recent interest in synthesizing programs to solve data processing and querying tasks, including handling semi-structured and unstructured data such as images and natural language text. Examples include neurosymbolic programs that incorporate deep neural network (DNN) components to extract semantic information from raw data [Chen et al . 2021; Shah et al . 2020], as well as fuzzy matching programs that use predicates with quantitative semantics to approximately match real-valued data [Mell et al . 2023]. For instance, Shah et al . [2020] synthesizes programs that label sequence data, Mell et al . [2023] synthesizes queries over trajectories output by an object tracker, and Chen et al . [2021] synthesizes web question answering programs. Most work focuses on programming by example (PBE), where the user provides a set of input-output (IO) examples, and the goal is to synthesize a program that generates the correct output for each input. There are two key properties that distinguish synthesis of such programs from traditional PBE: 

â€¢ Quantitative objectives: The goal in neurosymbolic synthesis is typically to optimize a quantitative objective such as accuracy or ğ¹ 1 score rather than to identify a program that is correct on all examples (which may be impossible). 

Authorsâ€™ addresses: Stephen Mell, University of Pennsylvania, USA, sm1@cis.upenn.edu; Steve Zdancewic, University of Pennsylvania, USA, stevez@cis.upenn.edu; Osbert Bastani, University of Pennsylvania, USA, obastani@seas.upenn.edu. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). 

Â© 2024 Copyright held by the owner/author(s). ACM 2475-1421/2024/1-ART16 https://doi.org/10.1145/3632858 Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024.  

> arXiv:2602.14717v1 [cs.PL] 16 Feb 2026 16:2 Stephen Mell, Steve Zdancewic, and Osbert Bastani

â€¢ Numerical constants: Programs operating on fuzzy real-world data or the outputs of DNN components typically include real-valued constants that serve as thresholds; for example, when querying video trajectories, one constant may be a threshold on the maximum velocity of the object. While these properties occasionally arise in traditional PBE settings (e.g., minimizing resource consumption), they are fundamental issues in neurosymbolic synthesis. Furthermore, in the neu-rosymbolic setting, there is often additional structure that can be exploited to improve synthesis performanceâ€”for instance, some of the numerical components might be monotone in their inputs. Most existing systems focus on synthesizing examples in a particular domain-specific language (DSL). In these settings, prior work has leveraged monotonicity of the semantics to prune the search space [Chen et al . 2021; Mell et al . 2023]. One general framework is Shah et al . [2020], which uses neural relaxations to guide search over a general DSL. At a high level, they use ğ´ âˆ— search to enumerate partial programs in the DSL, which are represented as a directed acyclic graph (DAG). In general, ğ´ âˆ— search prioritizes the order in which to enumerate partial programs based on a score function (called a heuristic ) that maps each partial program to a real-valued score. When the heuristic is admissible â€”i.e., its output is an upper bound on the objective value for any completion of that partial program (assuming the goal is to maximize the objective)â€”then ğ´ âˆ— search is guaranteed to find the optimal program if it terminates. 1

Shah et al . [2020] proposes the following heuristic: fill each hole in the partial program with an untrained DNN, and then maximize the quantitative objective as a function of the DNN parameters using resulting objective value as the heuristic. However, this score function is only guaranteed to be admissible under assumptions that typically do not hold in practice: (i) the neural relaxations are sufficiently expressive to represent any program in the DSL, which requires very large DNNs, and (ii) maximization of the DNN parameters converges to the global optimum, which does not hold for typical strategies such as stochastic gradient descent (SGD). Furthermore, SGD cannot handle non-differentiable objectives, which include common objectives such as accuracy and ğ¹ 1 score. Thus, a natural question is whether we can construct practical heuristics that are guaranteed to be admissible. In this work, we take inspiration from deduction-guided synthesis , which uses automated reasoning techniques such as SMT solvers [Bornholt et al . 2016; Gulwani et al . 2011; Solar-Lezama et al . 2006] or abstract interpretation [Cousot and Cousot 1977] to prune partial programs from the search spaceâ€”i.e., prove that no completion of a given partial program can satisfy the given IO examples. In particular, we propose using abstract interpretation to construct heuristics for synthesis for quantitative objectives. Traditionally, abstract interpretation can be used to prune partial programs by replacing each hole with an abstract value overapproximating all possible concrete values that can be taken by that hole in the context of a given input. Then, if the abstract output does not include the corresponding concrete output, that partial program cannot possibly be completed into a program that satisfies that IO example, so it can be pruned. Our key insight is that abstract interpretation can similarly be used to construct an admissible heuristic for a quantitative objective. Essentially, we can use abstract interpretation to overapproxi-mate the possible objective values obtained by any completion of a given partial program; then, the supremum of concrete values represented by the abstract output serves as an upper bound of the objective, so it can be used as an admissible heuristic. Thus, given abstract transformers for the DSL components and for the quantitative objective, our framework can synthesize optimal programs. In addition, we propose general strategies for constructing abstract domains and transformers for common DSLs and objectives. As discussed above, many DSLs have monotone components. In    

> 1We mean optimal on the given IO examples. This notion ignores suboptimality due to generalization error, which can be handled using standard techniques from learning theory [Kearns and Vazirani 1994]. Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:3
> Fig. 1. A frame from a video of two mice interacting [Sun et al .2021]; the mice are very close together, and are exhibiting the â€œsniffâ€ behavior. The video has been processed using deep neural networks to produce certain keypoints, which are shown.

these settings, a natural choice of abstract domain is to use intervals for the real-valued constants; then, a natural abstract transformer is to evaluate the concrete semantics on the upper and lower bounds of the intervals. This strategy can straightforwardly be shown to correctly overapproximate the concrete semantics. We implement our approach in the context of two DSLsâ€”namely, the NEAR [Shah et al . 2020] DSL for the CRIM13 [Burgos-Artizzu et al . 2012] dataset, and the Quivr [Mell et al . 2023] DSL and benchmark. In our experiments, we demonstrate that our approach significantly outperforms an adaptation of Metasketches [Bornholt et al . 2016]â€”an existing optimal synthesis framework based on SMT solvingâ€”to our setting, as well as an ablation that uses breadth first search instead of ğ´ âˆ—

search. Our approach significantly outperforms both of these baselines in terms of running time. In summary, our contributions are: 

â€¢ We propose a novel algorithm for optimal synthesis which performs enumerative search over a space of generalized partial programs . To prioritize search, it uses the ğ´ âˆ— algorithm with a search heuristic based on abstract interpretation. If it returns a program, then that program is guaranteed to be optimal (Section 3). 

â€¢ In practice, many DSLs have types that are equipped with a partial order, such as the real numbers or Booleans. For these types, we propose to use intervals as the abstract domains. For monotone DSL componentsâ€”i.e., the concrete semantics respects the partial ordersâ€”a natural choice of abstract transformer is to simply apply the concrete semantics to the lower and upper bounds of the interval (Section 4). 

â€¢ We implement our framework in the context of two existing DSLs (Section 5) and show in our experiments that it outperforms Metasketches [Bornholt et al . 2016], a state-of-the-art optimal synthesis technique based on SMT solvers, and a baseline that uses breadth-first search instead of our search heuristic (Section 6). 

2 MOTIVATING EXAMPLE 

We consider a task where the goal is to synthesize a program for predicting the behavior of mice based on a video of them interacting [Shah et al . 2020], motivated by a data analysis problem in biology. In particular, biologists use mice as model animals to investigate both basic biological processes and to develop new therapeutic interventions, which sometimes requires determining the effect of an intervention on mouse behavioral patterns, such as the nature and duration of interactions with other mice. For example, Figure 1 depicts two mice in an enclosure, engaging in the â€œsniffâ€ behavior.  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:4 Stephen Mell, Steve Zdancewic, and Osbert Bastani

Doing this behavior analysis typically involves researchers viewing and manually annotating these behaviors in hours of video, which is very labor intensive. As a result, program synthesis has been applied to automating this task [Shah et al . 2020]. Their approach first uses an object tracker to track each mouse across frames in the video, producing trajectories represented as a sequence of 2D positions for each mouse. Then, they featurize each step in the trajectoryâ€”for instance, if there are two mice in the video, then one feature might be the distance between them in each frame. Based on this sequence of features, the goal is to predict a label for the behavior (if any) that the mice are engaged in during each frame (producing a sequence of labels, one for each frame). Shah et al . [2020] solves this problem by synthesizing a program in a functional DSL for processing trajectories (the NEAR DSL). In summary, the goal is to synthesize a program that takes as input a sequence of feature vectors, and outputs a sequence of labels. We consider the programming by example (PBE) setting, where we are given a number of human annotated examples, and the goal is to synthesize a program that maximizes some objective, such as accuracy or ğ¹ 1 score. For example, consider synthesizing a program that, given a featurized trajectory representing two mice in a video, outputs the behavior of the mice at each time step. In particular, the input is a trajectory ğ‘¥ âˆˆ Râˆ— (where ğ‘‡ âˆ— denotes lists of ğ‘‡ s), where the feature ğ‘¥ [ğ‘¡ ] âˆˆ R on time step ğ‘¡ 

encodes the distance between the two mice at that time step, and the output is ğ‘¦ âˆˆ { f, t}âˆ—, where 

ğ‘¦ [ğ‘¡ ] encodes whether the mice are engaging in the â€œsniffâ€ behavior ( ğ‘¦ = t if so, and ğ‘¦ = f if not) at time step ğ‘¡ .We consider synthesizing such a program based on a single training example ğ‘¥ 1 = (101 , 65 ) and 

ğ‘¦ 1 = (f, t) (i.e., the first frame has mouse distance 101 and is labeled â€œnot sniffâ€ and the second frame has mouse distance 65 and is labeled â€œsniffâ€). Our goal is to find some program that classifies a dataset of videos wellâ€”i.e., if we evaluate the program on the videos to get predicted labels, and then compute a classification metric (e.g. accuracy) between the predicted and true labels, the discrepancy should be small. Given a program ğ‘ , its accuracy is 1 if âŸ¦ğ‘ âŸ§( ğ‘¥ 1) = ğ‘¦ 1 and 1/2 if 

âŸ¦ğ‘ âŸ§( ğ‘¥ 1) [ 0] = ğ‘¦ 1 [0] but âŸ¦ğ‘ âŸ§( ğ‘¥ 1) [ 1] â‰  ğ‘¦ 1 [1] (or vice versa), and 0 otherwise (where ğ‘¥ [ğ‘– ] is the ğ‘– -th item in a sequence ğ‘¥ ). Consider the candidate program â€œ map (ğ‘‘ â‰¤ 50 )â€. In traditional syntax, this program would be â€œmap (ğœ†ğ‘‘ . ğ‘‘ â‰¤ 50 ) ğ‘¥ â€, but here the input is specified separately and the ğœ† omitted (in combinator-style, similar to the NEAR DSL). This program performs a map over the sequence of frames, and for each one, it would output whether the mouse distance in that frame is less than or equal to 50 .For example, when evaluated on ğ‘¥ 1, it outputs 

âŸ¦map (ğ‘‘ â‰¤ 50 )âŸ§( ğ‘¥ 1) = (f, f).

Thus, its accuracy is 1/2, since it correctly labels the first frame but not the second. One strategy for computing the optimal program (i.e. the one that maximizes the objective) is to enumerate partial programs (i.e. programs with holes representing pieces that need to filled to obtain a concrete program) in the DSL, evaluate the objective on every concrete program, and then choose the best one. There are several challenges to this approach: 

â€¢ Unlike traditional synthesis, where we can stop enumerating when we reach a concrete program that satisfies the given specification, for optimal synthesis, we need to enumerate all programs or risk returning a suboptimal program. 

â€¢ Traditional synthesizers use a variety of techniques to prune the search space to improve scalability. For instance, they might use deduction to prove that no completion of a partial program can satisfy the specificationâ€”i.e., no matter how the holes in the partial program are filled, the specification will not hold. However, these techniques are not directly applicable to optimal synthesis.  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:5

â€¢ Synthesizing real-valued constants poses a problem: one approach is to discretize the con-stants, enumerate all of them, and choose the best, but this approach can be prohibitively slow. For example, suppose we are enumerating completions of the partial program 

map (ğ‘‘ â‰¤ ?? ),

where ?? is a hole that needs to be filled with a real value ğœƒ âˆˆ [ 0, 100 ] to obtain a concrete pro-gram. If we discretize ğœƒ âˆˆ { 0, 1, ..., 100 }, then we would enumerate map (ğ‘‘ â‰¤ 0), ..., map (ğ‘‘ â‰¤

100 ), evaluate each of these on (ğ‘¥ 1, ğ‘¦ 1) to measure its accuracy, and choose the program with the highest accuracy. Our framework uses two key innovations to address these challenges: 

â€¢ Generalized partial programs: Our framework takes the traditional notion of partial programs, representing sets of concrete programs as completions of syntax with holes, and extends it to more general sets of programs, equipped with a directed acyclic graph (DAG) structure. This allows us to avoid discretization of real-valued constants and reduces the branching-factor of the search space. 

â€¢ Aâˆ— search: Rather than enumerate programs in an arbitrary order (e.g., breadth first search), our framework uses ğ´ âˆ— search to enumerate programs. This allows us to avoid considering all programs in the search space, similar to deductive pruning, while still returning the optimal program. We describe each of these techniques in more detail below. 

Generalized Partial Programs. Traditionally, the search space over partial programs is a DAG, where the nodes are partial programs, and there is an edge Ë†ğ‘ â†’ Ë†ğ‘ â€² if Ë†ğ‘ â€² can be obtained by filling a hole in Ë†ğ‘ using some production in the DSL. For instance, there is an edge 

map (ğ‘‘ â‰¤ ?? ) â†’ map (ğ‘‘ â‰¤ 50 )

in this DAG, since we have filled the hole with the value 50 . However, even if we discretize the search space, there are 101 ways to fill this hole. As a consequence, if even a single completion of 

map (ğ‘‘ â‰¤ ?? ) is valid, then we cannot prune it from the search space (even ignoring that we want the optimal program rather than just any valid program). Instead, in our framework, we allow search DAGs beyond just programs with holes, so long as each node represents a set of concrete programs and the nodeâ€™s children collectively represent that set. As a practical instantiation of the general framework, we consider partial programs where holes for real-valued constants may be annotated with constraints on the value that can be used to fill them. For instance, the generalized partial program 

map (ğ‘‘ â‰¤ ?? [50 ,100 ] )

represents the set of concrete programs 

{map (ğ‘‘ â‰¤ ğœƒ ) | ğœƒ âˆˆ [ 50 , 100 ]} 

Then, the children of this generalized partial program in the search DAG should split the constraint in a way that covers the search spaceâ€”e.g., 

children (map (ğ‘‘ â‰¤ ?? [50 ,100 ] )) = {map (ğ‘‘ â‰¤ ?? [50 ,75 ] ), map (ğ‘‘ â‰¤ ?? [75 ,100 ] )} .

This strategy presents more opportunities for pruning the search space. For instance, even if we cannot prune the program map (ğ‘‘ â‰¤ ?? [50 ,100 ] ), we may be able to prune the program map (ğ‘‘ â‰¤

?? [50 ,75 ] ). Then, rather than needing to enumerate 51 programs (i.e., one for each ğœƒ âˆˆ { 50 , ..., 100 }), we would only need to prune map (ğ‘‘ â‰¤ ?? [50 ,75 ] ) and evaluate 26 programs (i.e., one for each ğœƒ âˆˆ{75 , ..., 100 }). Of course, we can further subdivide the search space to further reduce enumeration.  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:6 Stephen Mell, Steve Zdancewic, and Osbert Bastani

ğ´ âˆ— Search. Next, we describe how we achieve the optimal synthesis analogue of pruning by using 

ğ´ âˆ— search. At a high level, ğ´ âˆ— search enumerates nodes in a search graph according to a heuristic ;for our purposes, a heuristic is function that maps a partial program Ë†ğ‘ to a real value ğœ‡ , and a heuristic is said to be admissible if it is an upper bound on the best possible objective value for any completion of Ë†ğ‘ â€”i.e., 

ğ‘ âˆˆ completions ( Ë†ğ‘ ) â‡’ ğœ‡ â‰¥ objective (ğ‘ ).

The heuristic adapts deductive reasoning to optimal synthesis: whereas deductive reasoning guar-antees that no completion of Ë†ğ‘ can satisfy the given specification, the heuristic guarantees that no completion of Ë†ğ‘ can achieve objective value greater than ğœ‡ â€”e.g., if we find a concrete program with objective value â‰¥ ğœ‡ , we can safely prune completions of Ë†ğ‘ from the search DAG. While ğ´ âˆ— search has previously been used in synthesis for quantitative objectives [Shah et al .2020], the heuristics used are not admissible and so do not provide theoretical guarantees. Our key contribution is showing that abstract interpretation can naturally be adapted to design admissible heuristics. Abstract interpretation can be used for traditional deductive reasoning as follows: fill each hole in the current partial program with an abstract value âŠ¤, evaluate the partial program using abstract semantics, and check if the abstract output is consistent with the specification. In our example, a natural choice of abstract domain for constants is the interval domain. In addition, rather than fill each hole with âŠ¤, if a hole has a constraint ?? [ğ‘,ğ‘ ] , we can instead fill it with the interval [ğ‘, ğ‘ ]. For instance, for our example program map (ğ‘‘ â‰¤ ?? [50 ,75 ] ), we can fill the hole with the interval [50 , 75 ] to obtain map (ğ‘‘ â‰¤ [ 50 , 75 ]) . Then, the abstract semantics âŸ¦Â·âŸ§ #

evaluate as follows: 

âŸ¦map (ğ‘‘ â‰¤ ?? [50 ,75 ] )âŸ§ # (ğ‘¥ 1) = (âŸ¦ 101 â‰¤ ?? [50 ,75 ] âŸ§#, âŸ¦65 â‰¤ ?? [50 ,75 ] âŸ§#) = (f, âŠ¤) . (1) In other words, the first element is f since ğ‘¥ 1 = (101 , 65 ), and we know 101 Ì¸ â‰¤ ğœƒ for any ğœƒ âˆˆ [ 50 , 75 ],and the second element is âŠ¤ since the relationship between 65 and ğœƒ âˆˆ [ 50 , 75 ] can be either f or t.Importantly, here the abstract values are over holes in the program rather than over the input to it. Traditionally, we would then check whether this abstract output is consistent with the specifica-tion. For optimal synthesis, we observe that we can define an abstract semantics for the objective function. In our example, we can compute an â€œabstract accuracyâ€ as follows (where 1 is a indicator function, and â‰¤, 1, +, Â·, and = are abstracted in the obvious way): accuracy # ( Ë†ğ‘ ) = 12

> 2

âˆ‘ï¸ 

> ğ‘¡ =1

1 âŸ¦map (ğ‘‘ â‰¤ ?? [50 ,75 ] )âŸ§ # (ğ‘¥ 1) [ ğ‘¡ ] = ğ‘¦ 1 [ğ‘¡ ]

= 12 (1[f = f] + 1[âŠ¤ = t]) 

= 12 ([ 1, 1] + [ 0, 1]) 

= [1/2, 1].

(2) In other words, for the first frame, the concrete programs represented by map (ğ‘‘ â‰¤ ?? [50 ,75 ] ) all predict f, which equals ğ‘¦ 1 [1], so this frame is always correctly classified. In contrast, for the second frame, concrete programs programs may output either f or t, so we are uncertain whether this frame is correctly classified. Thus, the true accuracy is in the interval [1/2, 1]. Since abstract interpretation is guaranteed to overapproximate the semantics, we can use the upper bound of the abstract objective value as our heuristicâ€”e.g., for map (ğ‘‘ â‰¤ [ 50 , 75 ]) , this heuristic computes ğœ‡ = 1. 

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:7

Finally, we summarize the full search procedure starting from the node map (ğ‘‘ â‰¤ ?? [0,100 ] ) in our search DAG, which has an abstract objective value of [1/2, 1]. First, we split it into 

Ë†ğ‘ 1 = map (ğ‘‘ â‰¤ ?? [0,50 ] ) and Ë†ğ‘ 2 = map (ğ‘‘ â‰¤ ?? [50 ,100 ] ).

The abstract accuracies of Ë†ğ‘ 1 and Ë†ğ‘ 2 are [1/2, 1/2] and [1/2, 1], so their heuristic values are ğœ‡ 1 = 1/2

and ğœ‡ 2 = 1, respectively. Since ğœ‡ 2 > ğœ‡ 1, our algorithm explores Ë†ğ‘ 2 next, splitting it into 

Ë†ğ‘ 3 = map (ğ‘‘ â‰¤ ?? [50 ,75 ] ) and Ë†ğ‘ 4 = map (ğ‘‘ â‰¤ ?? [75 ,100 ] ),

which have abstract accuracies of [1/2, 1] and [1, 1], respectively, and heuristic values of ğœ‡ 3 = 1

and ğœ‡ 4 = 1, respectively. In this example, the lower bound on the accuracy of Ë†ğ‘ 4 is also 1, so we know that any choice ğœƒ âˆˆ [ 75 , 100 ] for filling this hole is guaranteed to achieve an accuracy of 1;thus, any concrete program map (ğ‘‘ â‰¤ ğœƒ ) such that ğœƒ âˆˆ [ 75 , 100 ] is optimal, and our algorithm can terminate without ever considering Ë†ğ‘ 1 or Ë†ğ‘ 3.In general, our algorithm terminates once the range of possible optimal values is sufficiently small. For each node on the search frontier, we have an upper and lower bound on the objective value of all the programs it represents. The greatest of these lower bounds provides a lower bound on the best possible objective value, and the greatest of these upper bounds provides an upper bound on the best possible objective value. As a result, once the difference between the bounds is 

â‰¤ ğœ– , we know that we have a program within ğœ– of being optimal. 

3 OPTIMAL SYNTHESIS VIA ABSTRACT INTERPRETATION 

In this section, we consider the program synthesis problem where: (i) programs in the domain-specific language may have real-valued constants, and (ii) the synthesis objective is real-valued, where the goal is to return the optimal program (Section 3.1). Then, we describe our synthesis algorithm for solving this problem, which uses ğ´ âˆ— search in conjunction with a search heuristic based on abstract interpretation (Section 3.2). 

3.1 Problem Formulation 

Domain-Specific Language. For concreteness, consider a DSL whose syntax is given by a context-free grammar G = (ğ‘‰ , Î£, ğ‘…, ğ‘ƒ ), where ğ‘‰ is the set of nonterminals, Î£ is the set of terminals, ğ‘ƒ âˆˆ ğ‘‰ is the start symbol, and ğ‘… is the set of productions 

ğ‘ƒ F ğ‘‹ | ğ‘ | ğ‘“ (ğ‘ƒ, ..., ğ‘ƒ ) (ğ‘ âˆˆ C , ğ‘“ âˆˆ F ) 

where ğ‘‹ is a symbol representing the input, ğ‘ âˆˆ C is a constant (including real-valued constants, i.e., 

R âŠ† C ), and ğ‘“ âˆˆ F is a DSL component (i.e., function), but our framework extends straightforwardly to more general grammars. We let ğ‘ âˆˆ P = L(G) âŠ† Î£âˆ— denote the concrete programs in our DSL. Furthermore, we assume the DSL has denotational semantics âŸ¦Â·âŸ§ , where âŸ¦ğ‘ âŸ§ : X â†’ Y maps inputs 

ğ‘¥ âˆˆ X to outputs ğ‘¦ âˆˆ Y according to the following rules: 

âŸ¦ğ‘‹ âŸ§( ğ‘¥ ) = ğ‘¥, âŸ¦ğ‘ âŸ§( ğ‘¥ ) = ğ‘, âŸ¦ğ‘“ (ğ‘ 1, ..., ğ‘ ğ‘˜ )âŸ§( ğ‘¥ ) = ğ‘“ (âŸ¦ ğ‘ 1âŸ§( ğ‘¥ ), ..., âŸ¦ğ‘ ğ‘˜ âŸ§( ğ‘¥ )) ,

where we assume the functions ğ‘“ : X1 Ã— ... Ã— X ğ‘˜ â†’ Y are given. In Section 2, we considered programs like map (ğ‘‘ â‰¤ 50 ), which we will use as a running example in this section. They are simplified versions of programs from the NEAR DSL, and are generated by the grammar (where ğ‘‘ represents the input) 

ğ¸ F ğ‘‘ | ğ‘ | map (ğ¸ ) | ğ¸ â‰¤ ğ¸ (ğ‘ âˆˆ R) 

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:8 Stephen Mell, Steve Zdancewic, and Osbert Bastani

Task Specification. We consider programming by example (PBE), where each task is specified by a sequence ğ‘ âˆˆ Z B (X Ã— Y) âˆ— of input-output (IO) examples, and the goal is to compute a program 

ğ‘ âˆ— that maximizes a given quantitative objective ğœ™ : P Ã— Z â†’ R

ğ‘ âˆ— âˆˆ arg max  

> ğ‘ âˆˆ P

ğœ™ (ğ‘, ğ‘ ).ğœ™ is a function of the semantics applied to the examples (ğ‘¥, ğ‘¦ ) âˆˆ ğ‘ â€”i.e., there is a function 

ğœ™ 0 : (Y Ã— Y) âˆ— â†’ R such that 

ğœ™ (ğ‘, ğ‘ ) = ğœ™ 0

 {(âŸ¦ ğ‘ âŸ§( ğ‘¥ ), ğ‘¦ )} (ğ‘¥,ğ‘¦ ) âˆˆ ğ‘ 

 , (3) In our running example, we choose ğœ™ 0 as follows, so ğœ™ (ğ‘, ğ‘ ) is the accuracy of ğ‘ƒ on ğ‘ :

ğœ™ 0 (ğ‘Š ) = 1

|ğ‘Š |âˆ‘ï¸ (ğ‘¦ â€²,ğ‘¦ ) âˆˆ ğ‘Š 

1[ğ‘¦ â€² = ğ‘¦ ],

where 1[Â·] is the indicator function. 

Partial Programs. A common strategy for PBE is to enumerate partial programs â€”programs in the DSL that have holesâ€”to try and find a concrete program that satisfies the given IO examples. Intuitively, partial programs are partial derivations in the grammar G. To formalize this notion, given two sequences Ë†ğ‘, Ë†ğ‘ â€² âˆˆ ( Î£ âˆª ğ‘‰ )âˆ—, we write Ë†ğ‘ â†’ Ë†ğ‘ â€² if Ë†ğ‘ â€² can be obtained by replacing a nonterminal symbol ğ‘ ğ‘– âˆˆ ğ‘‰ in Ë†ğ‘ with the right-hand side of a production ğ‘Ÿ = ğ‘ ğ‘– â†’ ğ‘€ 1...ğ‘€ ğ‘˜ âˆˆ ğ‘… 

for that nonterminalâ€”i.e. Ë†ğ‘ = ğ‘ 1...ğ‘ ğ‘– ...ğ‘ â„ and Ë†ğ‘ â€² = ğ‘ 1...ğ‘€ 1...ğ‘€ ğ‘˜ ...ğ‘ â„ . We denote this relationship by Ë†ğ‘ â€² = fill ( Ë†ğ‘, ğ‘–, ğ‘Ÿ )â€”i.e. we obtain Ë†ğ‘ â€² by filling the ğ‘– th hole ğ‘ ğ‘– in Ë†ğ‘ using production ğ‘Ÿ . Next, we write Ë†ğ‘ âˆ—

âˆ’â†’ Ë†ğ‘ â€² if there exists a sequence Ë†ğ‘ = Ë†ğ‘ 1 â†’ ... â†’ Ë†ğ‘ ğ‘› = Ë†ğ‘ â€², and say Ë†ğ‘ â€² can be derived from Ë†ğ‘ .Note that concrete programs ğ‘ âˆˆ P are sequences ğ‘ âˆˆ Î£âˆ— that can be derived from the start symbol ğ‘ƒ (i.e. ğ‘ƒ âˆ—

âˆ’â†’ ğ‘ ). Similarly, a partial program is a sequence Ë†ğ‘ âˆˆ Ë†P B (Î£ âˆª ğ‘‰ )âˆ— that can be derived from ğ‘ƒ â€”i.e. ğ‘ƒ âˆ—

âˆ’â†’ Ë†ğ‘ . The only difference is that Ë†ğ‘ may contain nonterminals, which are called holes . The space of partial programs naturally forms a directed acyclic graph (DAG) via the relation Ë†ğ‘ â†’ Ë†ğ‘ â€²; note that concrete programs are leaf nodes in this DAG. Thus, we can perform synthesis by enumerating partial programs according to the structure of this DAG. Furthermore, given a partial program Ë†ğ‘ âˆˆ Ë†P and a concrete program ğ‘ âˆˆ P , we say ğ‘ is a completion of Ë†ğ‘ if 

Ë†ğ‘ âˆ—

âˆ’â†’ ğ‘ (i.e., ğ‘ can be obtained from Ë†ğ‘ by iteratively filling the holes of Ë†ğ‘ ). In our running example, traditional partial programs correspond to the grammar 

Â¯ğ¸ F ?? | ğ‘‘ | ğ‘ | map ( Â¯ğ¸ ) | Â¯ğ¸ â‰¤ Â¯ğ¸ (ğ‘ âˆˆ R).

Generalized Partial Programs. A key challenge is searching over real-valued constants ğ‘ âˆˆ R. Our grammar contains infinitely many productions of the form ğ‘ƒ â†’ ğœƒ for ğœƒ âˆˆ R, and even if we discretize this search space, the number of productions is still large in practice. To address this challenge, we propose a strategy where we enumerate generalized partial programs 

Ë†ğ‘ âˆˆ Ë†P, which generalize (i) the fact that partial programs correspond to sets of concrete programs (i.e. the set of their completions), and (ii) the DAG structure of partial programs. 

Definition 3.1. A space of generalized partial programs is a set Ë†P together with a concretization function ğ›¾ : Ë† P â†’ 2P and a DAG structure children : Ë† P â†’ 2 Ë†P , such that 

ğ›¾ ( Ë†ğ‘ ) =

Ã˜   

> Ë†ğ‘ â€²âˆˆchildren (Ë†ğ‘ )

ğ›¾ ( Ë†ğ‘ â€²). (4)  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:9

Intuitively, ğ›¾ ( Ë†ğ‘ ) âŠ† P is the set of concrete programs represented by the abstract program Ë†ğ‘ . In addition, children encodes a DAG structure on Ë†P that is compatible with ğ›¾ â€”i.e., the children Ë†ğ‘ â€² of 

Ë†ğ‘ must collectively contain all the concrete programs in Ë†ğ‘ .For example, to capture traditional partial programs, we let 

ğ›¾ ( Ë†ğ‘ ) = {ğ‘ âˆˆ P | Ë†ğ‘ âˆ—

âˆ’â†’ ğ‘ },

i.e. ğ‘ is a completion of Ë†ğ‘ , and children ( Ë†ğ‘ ) = { Ë†ğ‘ â€² âˆˆ Ë†P | Ë†ğ‘ â†’ Ë†ğ‘ â€² }.

In Section 4, we will propose generalized partial programs that can include constraints on real-valued holesâ€”e.g., ?? [0,1] is a partial program that can only be filled by a real value ğœƒ âˆˆ [ 0, 1].Finally, a simple way to satisfy Definition 3.1 is to define ğ›¾ based on the children functionâ€”i.e., we can define ğ‘ âˆˆ ğ›¾ ( Ë†ğ‘ ) if there exists a sequence Ë†ğ‘ = Ë†ğ‘ 1, ..., Ë†ğ‘ ğ‘› = ğ‘ of generalized partial programs such that Ë†ğ‘ ğ‘— +1 âˆˆ children ( Ë†ğ‘ ğ‘— ) for all 1 < ğ‘— < ğ‘› . In other words, we can reach the concrete program ğ‘ from the generalized partial program Ë†ğ‘ in the search DAG. This strategy straightforwardly guarantees (4), since by definition, every ğ‘ âˆˆ ğ›¾ ( Ë†ğ‘ ) must be the descendant of some child of Ë†ğ‘ .In our running example, the generalized partial programs correspond to the grammar 

Ë†ğ¸ F ?? [ğ‘,ğ‘ ] | ?? | ğ‘‘ | ğ‘ | map ( Ë†ğ¸ ) | Ë†ğ¸ â‰¤ Ë†ğ¸ (ğ‘, ğ‘ âˆˆ R),

while the concretization function satisfies 

ğ›¾ (map (?? )) = {map (ğ‘ ) | ğ‘ âˆˆ P} 

ğ›¾ (map (ğ‘‘ â‰¤ ?? [50 ,100 ] )) = {map (ğ‘‘ â‰¤ ğ‘ ) | ğ‘ â‰¤ ğ‘ â‰¤ ğ‘ }

and the children function satisfies 

children (map (?? )) = {map (?? [0,100 ] ), map (ğ‘‘ ), map (ğ‘ ), map (map (?? )) , map (?? â‰¤ ?? )} 

children (map (ğ‘‘ â‰¤ ?? [50 ,100 ] )) = {map (ğ‘‘ â‰¤ ?? [50 ,75 ] ), map (ğ‘‘ â‰¤ ?? [75 ,100 ] )} .

Abstract Objective. For now, we consider an abstract objective âŸ¦Â·âŸ§ # 

> ğœ™

that directly maps generalized partial programs to abstract real values; it is typically constructed compositionally by providing abstract transformers for each component ğ‘“ âˆˆ F as well as for the objective function ğœ™ 0, and then composing them together. In particular, the abstract objective has type âŸ¦ Ë†ğ‘ âŸ§# 

> ğœ™

: Z â†’ Ë†R, where Ë†R is an abstract domain for the reals representing the potential objective values ğœ™ (ğ‘, ğ‘ ) (e.g. the interval domain). Rather than require a concretization function for Ë†R, we only need an upper bound for this abstract domainâ€”i.e., a function ğœ‡ : Ë†R â†’ R, which encodes the intuition that â€œ ğœ‡ ( Ë†ğ‘Ÿ ) is larger than any real number ğ‘Ÿ contained in Ë†ğ‘Ÿ â€. 

Definition 3.2. Given objective ğœ™ and generalized partial programs ( Ë†P, ğ›¾, children ), an abstract objective (âŸ¦Â·âŸ§ # 

> ğœ™

, ğœ‡ ) is valid if 

 ğ‘ âˆˆ ğ›¾ ( Ë†ğ‘ ) â‡’  ğœ‡ (âŸ¦ Ë†ğ‘ âŸ§# 

> ğœ™

(ğ‘ )) â‰¥ ğœ™ (ğ‘, ğ‘ ) (âˆ€ ğ‘ âˆˆ P) , (5) and 

ğœ‡ (âŸ¦ ğ‘ âŸ§# 

> ğœ™

(ğ‘ )) = ğœ™ (ğ‘, ğ‘ ) (âˆ€ ğ‘ âˆˆ P) . (6) Intuitively, (5) says that ğœ‡ (âŸ¦ Ë†ğ‘ âŸ§# 

> ğœ™

(ğ‘ )) is an upper bound on the objective value ğœ™ (ğ‘, ğ‘ ) for concrete programs ğ‘ is contained in the abstract program Ë†ğ‘ . In addition, (6) says that for concrete programs, the abstract objective and concrete objective coincide. Finally, a typical choice of Ë†R is the space of intervals Ë†R, where (ğ‘Ÿ, ğ‘Ÿ â€²) âˆˆ Ë†R represents the set of real numbers {ğ‘Ÿ â€²â€² âˆˆ R | ğ‘Ÿ â‰¤ ğ‘Ÿ â€²â€² â‰¤ ğ‘Ÿ â€² } (see Definition 4.1 for details). Then, the upper bound is  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:10 Stephen Mell, Steve Zdancewic, and Osbert Bastani

Algorithm 1 Our algorithm takes as input a task specification ğ‘ , along with a DSL G, a space of generalized programs ( Ë†P, ğ›¾ ), abstract objective (âŸ¦Â·âŸ§ # 

> ğœ™

, ğœ‡ ), objective lower bound ğœˆ , objective error tolerance ğœ€ , and returns the optimal program ğ‘ âˆ— for task ğ‘ . To do so, it uses the abstract objective as a heuristic in ğ´ âˆ— search, starting from the initial generalized partial program Ë†ğ‘ 0.

procedure ğ´ âˆ—-Synthesis (ğ‘, G, Ë†P, ğ›¾, âŸ¦Â·âŸ§ # 

> ğœ™

, ğœ‡ )

â„ â† heap (sort _by = ğœ† Ë†ğ‘.ğœ‡ (âŸ¦ Ë†ğ‘ âŸ§# 

> ğœ™

(ğ‘ ))) 

â„. push ( Ë†ğ‘ 0)

while true do 

Ë†ğ‘ â† â„. pop_greatest () 

if max {ğœ‡ (âŸ¦ Ë†ğ‘ â€²âŸ§# 

> ğœ™

(ğ‘ )) | Ë†ğ‘ â€² âˆˆ â„} âˆ’ max {ğœˆ (âŸ¦ Ë†ğ‘ â€²âŸ§# 

> ğœ™

(ğ‘ )) | Ë†ğ‘ â€² âˆˆ â„} â‰¤ ğœ€ then return arg max {ğœˆ (âŸ¦ Ë†ğ‘ â€²âŸ§# 

> ğœ™

(ğ‘ )) | Ë†ğ‘ â€² âˆˆ â„}

end if for Ë†ğ‘ â€² âˆˆ children ( Ë†ğ‘ ) do 

â„. push ( Ë†ğ‘ â€²)

end for end while end procedure 

given by ğœ‡ (( ğ‘Ÿ, ğ‘Ÿ â€²)) = ğ‘Ÿ â€². The abstract objective âŸ¦Â·âŸ§ # 

> ğœ™

depends on the DSL; we describe a general construction in Section 4. In our running example, the objective ğœ™ decomposes according to Equation 3 into a semantics 

âŸ¦Â·âŸ§ and accuracy ğœ™ 0. We thus define the abstract objective âŸ¦Â·âŸ§ # 

> ğœ™

in terms of an abstract semantics 

âŸ¦Â·âŸ§ # : Ë† P â†’ Ë†R (Equation 1), and abstract accuracy (Equation 2). 

3.2 ğ´ âˆ— Synthesis via Abstract Interpretation 

Given a set of IO examples, Algorithm 1 uses ğ´ âˆ— search over generalized partial programs Ë†ğ‘ âˆˆ Ë†P in conjunction with the heuristic Ë†ğ‘ â†¦ â†’ ğœ‡ (âŸ¦ Ë†ğ‘ âŸ§# 

> ğœ™

(ğ‘ )) to compute the optimal program. In particular, it uses a heap â„ to keep track of the generalized partial program Ë†ğ‘ in the frontier of the search DAG; at each iteration, it pops the current best node Ë†ğ‘ , and then enumerates the children Ë†ğ‘ â€² of Ë†ğ‘ and adds them to â„ according to the heuristic. Termination occurs when the maximum over Ë†ğ‘ âˆˆ â„ of the objective lower bound ğœˆ : Ë†R â†’ R (analogous to ğœ‡ , but lower-bounding rather than upper-bounding the abstract objective) is within a tolerance ğœ€ â‰¥ 0 of the maximum over Ë†ğ‘ âˆˆ â„ of the upper bound ğœ‡ ,returning the generalized partial program with the highest lower-bound. When when the objective abstraction Ë†R is real intervals, ğœˆ (( ğ‘Ÿ, ğ‘Ÿ â€²)) = ğ‘Ÿ is a natural choice. For our running example, consider the following generalized partial programs and their abstract objective values: 

Ë†ğ‘ 0 B map (ğ‘‘ â‰¤ ?? [0,100 ] ) âŸ¦ Ë†ğ‘ 0âŸ§# 

> ğœ™

= [1/2, 1]

Ë†ğ‘ 1 B map (ğ‘‘ â‰¤ ?? [0,50 ] ) âŸ¦ Ë†ğ‘ 1âŸ§# 

> ğœ™

= [1/2, 1/2]

Ë†ğ‘ 2 B map (ğ‘‘ â‰¤ ?? [50 ,100 ] ) âŸ¦ Ë†ğ‘ 2âŸ§# 

> ğœ™

= [1/2, 1]

Ë†ğ‘ 3 B map (ğ‘‘ â‰¤ ?? [50 ,75 ] ) âŸ¦ Ë†ğ‘ 3âŸ§# 

> ğœ™

= [1/2, 1]

Ë†ğ‘ 4 B map (ğ‘‘ â‰¤ ?? [75 ,100 ] ) âŸ¦ Ë†ğ‘ 4âŸ§# 

> ğœ™

= [1, 1] 

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:11

The search process starts with â„0 = {( Ë†ğ‘ 0 : 1 )} . In the first iteration, Ë†ğ‘ 0 is popped, since it (trivially) has the highest heuristic value. Its children, Ë†ğ‘ 1 and Ë†ğ‘ 2 are pushed, resulting in â„1 = { Ë†ğ‘ 2 : 1 , Ë†ğ‘ 1 : 1 /2}.Next, we pop Ë†ğ‘ 2, since it has the highest heuristic value. Here we can see the â€œsoft pruningâ€ of 

ğ´ âˆ— at work: we know descendents of Ë†ğ‘ 1 cannot achieve objective values > 1/2, whileâ€”as far as we knowâ€”descendents of Ë†ğ‘ 2 might achieve objective values as high as 1. We canâ€™t prune Ë†ğ‘ 1 per se ,since the optimal objective value may be â‰¤ 1/2, but we can defer considering descendents of Ë†ğ‘ 1

until we know that the optimal objective value is â‰¤ 1/2. We continue by pushing the children of 

Ë†ğ‘ 2, which are Ë†ğ‘ 3 and Ë†ğ‘ 4, resulting in â„2 = { Ë†ğ‘ 3, : 1 , Ë†ğ‘ 4 : 1 , Ë†ğ‘ 1 : 1 /2}.Finally, we observe that, for the abstract objective intervals in the heap ( {[ 1/2, 1], [1, 1], [1/2, 1/2]} ), the distance between the greatest lower bound ( 1) and greatest upper bound ( 1) is 0, and so search terminates, returning Ë†ğ‘ 4.Let ğœ™ âˆ— 

> ğ‘

B max ğ‘ â€² âˆˆ P ğœ™ (ğ‘ â€², ğ‘ ) be the optimal objective value. We have the following optimality guarantee: 

Theorem 3.3. If Algorithm 1 returns an abstract program Ë†ğ‘ , then Ë†ğ‘ is ğœ– -optimalâ€”i.e. âˆ€ğ‘ âˆˆ ğ›¾ ( Ë†ğ‘ ),

ğœ™ âˆ— 

> ğ‘

âˆ’ ğœ™ (ğ‘, ğ‘ ) â‰¤ ğœ– .

Proof. By (4), Algorithm 1 preserves the invariant that every program ğ‘ âˆˆ P is contained in some generalized partial program Ë†ğ‘ âˆˆ â„â€”i.e., 

Ã˜ 

> Ë†ğ‘ âˆˆâ„

ğ›¾ ( Ë†ğ‘ ) = P.

This property can be checked by an easy induction argument on the while loop iteration. This implies that 

ğœ™ âˆ— 

> ğ‘

= max {max {ğœ™ (ğ‘ â€², ğ‘ ) | ğ‘ â€² âˆˆ ğ›¾ ( Ë†ğ‘ â€²)} | Ë†ğ‘ â€² âˆˆ â„}

and since ğœ‡ must overapproximate the concrete objective values, 

ğœ™ âˆ— 

> ğ‘

â‰¤ max {ğœ‡ (âŸ¦ Ë†ğ‘ â€²âŸ§# 

> ğœ™

(ğ‘ )) | Ë†ğ‘ â€² âˆˆ â„}.

Next, suppose Algorithm 1 terminates, returning Ë†ğ‘ âˆˆ Ë†P, and let ğ‘ be any concrete program in ğ›¾ ( Ë†ğ‘ ).The termination condition ensures that 

ğœ™ âˆ— 

> ğ‘

â‰¤ ğœˆ (âŸ¦ Ë†ğ‘ âŸ§# 

> ğœ™

(ğ‘ )) + ğœ–. 

Finally, since ğœˆ must underapproximate the concrete objective values, 

ğœ™ âˆ— 

> ğ‘

â‰¤ ğœ™ (ğ‘, ğ‘ ) + ğœ–. 

â–¡

We can ensure termination straightforwardly by using a finite DAG as the search space; for instance, we can do so by discretizing the real-valued constants. In general, we can guarantee convergence when the abstract losses of every infinite chain converge. For example, this property holds when the objective is Lipschitz continuous in the real-valued program parameters. (The gap between the upper and lower bounds of the objective value is bounded by the Lipschitz constant times the diameter of the box, which goes to zero as search proceeds and the boxes become smaller.) However, Lipschitz continuity often does not hold; we leave exploration of alternative ways to ensure convergence to future work.  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:12 Stephen Mell, Steve Zdancewic, and Osbert Bastani

4 INSTANTIATION FOR INTERVAL DOMAINS 

Section 3 described a general framework for optimal synthesis when given an abstract semantics and search DAG for the target DSL. In this section, we describe a natural strategy for constructing abstract semantics when the DSL types are partially ordered. We begin by showing how to con-struct abstract domains for partially ordered types (Section 4.1), abstract semantics for individual components with monotone semantics (Section 4.2), and abstract transformers for monotone ob-jectives (Section 4.3). Next, we describe a space of partial programs where holes corresponding to real-valued constants are optionally annotated with interval constraints (Section 4.4). Finally, we show how to combine abstract transformers and objectives to perform abstract interpretation for our interval-constrained partial programs (Section 4.5). 

4.1 Interval Domains from Partial Orders 

Many DSLs have the property that their types are equipped with a partial orderâ€”e.g. the usual order on the real numbers, or the order f â‰¤ t on the Booleans. 

Definition 4.1. Given a partially ordered set Z, let Â¯Z = Z âˆª {âˆ’âˆ , +âˆ} , where âˆ’âˆ â‰¤ ğ‘§ â‰¤ +âˆ 

for all ğ‘§ âˆˆ Z . Then, the interval domain is the set Ë†Z = {( ğ‘, ğ‘ ) âˆˆ Â¯Z2 | ğ‘ â‰¤ ğ‘ } âˆª {âŠ¥} , together with an abstraction function ğ›¼ : Z â†’ Ë†Z defined by ğ›¼ (ğ‘§ ) = (ğ‘§, ğ‘§ ), and a concretization function 

ğ›¾ : Ë†Z â†’ 2Z defined by 

ğ›¾ (( ğ‘§ 0, ğ‘§ 1)) = {ğ‘§ âˆˆ Z | ğ‘§ 0 â‰¤ ğ‘§ â‰¤ ğ‘§ 1 }.

In other words, ğ›¾ maps (ğ‘§ 0, ğ‘§ 1) to the interval [ğ‘§ 0, ğ‘§ 1]. It is clear that ğ‘§ âˆˆ ğ›¾ (ğ›¼ (ğ‘§ )) .

4.2 Interval Transformers for Monotone Functions 

Next, we define our notion of abstract transformer for the interval domain. DSLs often contain functions that are monotone, respecting the partial orders between their input and output types. 

Definition 4.2. Consider a function ğ‘“ : X1 Ã— ... Ã— X ğ‘˜ â†’ Y , and where X1, ..., Xğ‘˜ and Y are partially ordered sets. We say ğ‘“ is monotone if 

> ğ‘˜

Ã›

> ğ‘– =1

ğ‘¥ ğ‘– â‰¤ ğ‘¥ â€² 

> ğ‘–

â‡’ ğ‘“ (ğ‘¥ 1, ..., ğ‘¥ ğ‘˜ ) â‰¤ ğ‘“ (ğ‘¥ â€²

> 1

, ..., ğ‘¥ â€² 

> ğ‘˜

).

For example, the + operator is monotone in both of its inputs. 

Definition 4.3. Given a monotone function ğ‘“ : X1 Ã— ... Ã— X ğ‘˜ â†’ Y , let Ë†Xğ‘– be the interval domain for each Xğ‘– and Ë†Y be the interval domain for Y. The interval transformer for ğ‘“ is the function 

Ë†ğ‘“ : Ë† X1 Ã— ... Ã— Ë†Xğ‘˜ â†’ Ë†Y defined by 

Ë†ğ‘“ (( ğ‘¥ 1,0, ğ‘¥ 1,1), ..., (ğ‘¥ ğ‘˜, 0, ğ‘¥ ğ‘˜, 1)) = (ğ‘“ (ğ‘¥ 1,0, ..., ğ‘¥ ğ‘˜, 0), ğ‘“ (ğ‘¥ ğ‘˜, 1, ..., ğ‘¥ ğ‘˜, 1)) .

If ğ‘¥ ğ‘–, 0 = âˆ’âˆ for any ğ‘– âˆˆ [ ğ‘˜ ], then we let the lower bound be ğ‘“ (ğ‘¥ 1,0, ..., ğ‘¥ ğ‘˜, 0) = âˆ’âˆ , and if ğ‘¥ ğ‘–, 1 = +âˆ 

for any ğ‘– âˆˆ [ ğ‘˜ ], then we let the upper bound be ğ‘“ (ğ‘¥ ğ‘˜, 1, ..., ğ‘¥ ğ‘˜, 1) = +âˆ .2

For example, since + : R Ã— R â†’ R is monotone, then Ë†+ : Ë†R Ã— Ë†R â†’ Ë†R is given by 

(ğ‘, ğ‘ ) Ë†+ ( ğ‘, ğ‘‘ ) B (ğ‘ + ğ‘, ğ‘ + ğ‘‘ )

The following key result shows that Ë†ğ‘“ overapproximates the concrete semantics:  

> 2Here we took monotone to mean monotonically increasing. For functions that are monotonically decreasing, we can flip the endpoints of the interval transformer output. Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:13

Lemma 4.4. Let ğ‘“ : X1 Ã— ... Ã— X ğ‘˜ â†’ Y be monotone, and let Ë†ğ‘“ be its interval transformer. For any 

ğ‘¥ ğ‘– âˆˆ X ğ‘– and Ë†ğ‘¥ ğ‘– = (ğ‘¥ ğ‘–, 0, ğ‘¥ ğ‘–, 1) âˆˆ Ë†Xğ‘– such that ğ‘¥ ğ‘– âˆˆ ğ›¾ ( Ë†ğ‘¥ ğ‘– ) for all ğ‘– âˆˆ [ ğ‘˜ ], we have 

ğ‘“ (ğ‘¥ 1, ..., ğ‘¥ ğ‘˜ ) âˆˆ ğ›¾ 

 Ë†ğ‘“ ( Ë†ğ‘¥ 1, ..., Ë†ğ‘¥ ğ‘˜ )



.

Proof. For all ğ‘– âˆˆ [ ğ‘˜ ], by our assumption that ğ‘¥ ğ‘– âˆˆ ğ›¾ ( Ë†ğ‘¥ ğ‘– ), we have ğ‘¥ ğ‘–, 0 â‰¤ ğ‘¥ ğ‘– â‰¤ ğ‘¥ ğ‘–, 1. Thus, by monotonicity of ğ‘“ , we have 

ğ‘“ (ğ‘¥ 1,0, ..., ğ‘¥ ğ‘˜, 0) â‰¤ ğ‘“ (ğ‘¥ 1, ..., ğ‘¥ ğ‘˜ ) â‰¤ ğ‘“ (ğ‘¥ 1,1, ..., ğ‘¥ ğ‘˜, 1). (7) By definition of Ë†ğ‘“ , we have Ë†ğ‘“ ( Ë†ğ‘¥ 1, ..., Ë†ğ‘¥ ğ‘˜ ) = (ğ‘“ (ğ‘¥ 1,0, ..., ğ‘¥ ğ‘˜, 0), ğ‘“ (ğ‘¥ 1,1, ..., ğ‘¥ ğ‘˜, 1)) , so by (7) and definition of ğ›¾ , we have ğ‘“ (ğ‘¥ 1, ..., ğ‘¥ ğ‘˜ ) âˆˆ ğ›¾ ( Ë†ğ‘“ ( Ë†ğ‘¥ 1, ..., Ë†ğ‘¥ ğ‘˜ )) , as claimed. â–¡

In other words, if ğ‘¥ ğ‘– is contained in the interval Ë†ğ‘¥ ğ‘– for each ğ‘– , then ğ‘“ (ğ‘¥ 1, ..., ğ‘¥ ğ‘˜ ) is contained in the interval Ë†ğ‘“ ( Ë†ğ‘¥ 1, ..., Ë†ğ‘¥ ğ‘˜ ). Thus, Ë†ğ‘“ overapproximates the concrete semantics of ğ‘“ .

4.3 Interval Transfomers for Monotone Objectives 

Similarly, if ğœ™ 0 is monotoneâ€”i.e. for ğ‘Š ğ‘– = {( ğ‘¦ â€²

> ğ‘–, 1

, ğ‘¦ ğ‘–, 1), ..., (ğ‘¦ â€² 

> ğ‘–,ğ‘˜

, ğ‘¦ ğ‘–,ğ‘˜ )} if ğ‘¦ â€² 

> 0,ğ‘—

â‰¤ ğ‘¦ â€² 

> 1,ğ‘—

for all ğ‘— âˆˆ [ ğ‘˜ ], then we have 

ğœ™ 0 (ğ‘Š 0) â‰¤ ğœ™ 0 (ğ‘Š 1).

Note that we only require monotonicity in the labels ğ‘¦ â€² 

> ğ‘—,ğ‘–

output by a candidate program ğ‘ , not the ground truth labels ğ‘¦ ğ‘—,ğ‘– , since the latter are always concrete values. Then, we can construct the abstract transformer Ë†ğœ™ 0 : ( Ë†Y Ã— Y) âˆ— â†’ Ë†R by 

Ë†ğœ™ 0



{(( ğ‘¦ â€²

> 0,ğ‘–

, ğ‘¦ â€² 

> 1,ğ‘–

), ğ‘¦ ğ‘– )} ğ‘˜ ğ‘– =1



=



ğœ™ 0



{( ğ‘¦ â€²

> 0,ğ‘–

, ğ‘¦ ğ‘– )} ğ‘˜ ğ‘– =1



, ğœ™ 0



{( ğ‘¦ â€²

> 1,ğ‘–

, ğ‘¦ ğ‘– )} ğ‘˜ ğ‘– =1

 

.

4.4 Partial Programs with Interval Constraints 

Next, we describe a space of generalized partial programs which extend partial programs with hole annotations that constrain the values that can be used to fill those holes. 

Definition 4.5. Assume that the space of constants C is partially ordered, and let Ë†C be its interval domain. Then, an interval-constrained partial program Ëœğ‘ = ( Ë†ğ‘, ğœ… ) is a partial program Ë†ğ‘ together with a mapping ğœ… : holes ( Ë†ğ‘ ) â†’ Ë†C âˆª {âˆ…} , where holes ( Ë†ğ‘ ) âŠ† N are the indices of the holes in Ë†ğ‘ .For example, suppose that Ë†ğ‘ = ğ‘ 1...ğ‘ ğ‘– ...ğ‘ â„ is a partial program, where ğ‘ ğ‘– is a nonterminal (and therefore a hole), so ğ‘– âˆˆ holes ( Ë†ğ‘ ). Intuitively, an annotation ğœ… (ğ‘– ) = Ë†ğ‘ imposes the constraint that the value used to fill ğ‘ ğ‘– must be a constant ğ‘ âˆˆ C , and that ğ‘ must satisfy ğ‘ âˆˆ ğ›¾ ( Ë†ğ‘ ) (i.e., ğ‘ is contained in the interval [ğ‘ 0, ğ‘ 1]). Alternatively, if ğœ… (ğ‘– ) = âˆ…, then no such constraint is imposedâ€”i.e., ğ‘ ğ‘– may be filled with any constant or a different production in the grammar. (Note that âˆ… is not the same as providing the interval [âˆ’âˆ , +âˆ] because that constraint would require this hole to be filled by a 

constant , whereas the optimal program might need a non-constant expression.) We let ËœP denote the space of interval-constrained partial programs. These constraints are imposed by the structure of the search DAG, which is a extended version of the search DAG over partial programs. Below, we first describe the children function for interval-constrained partial programs; the concretization function is constructed from the children function. 

Children Function. Next, we describe the children of an interval constrained partial program children ( Ëœğ‘ ) âŠ† ËœP. Intuitively, if a hole ğ‘ in a partial program Ë†ğ‘ can be filled with a constant value 

ğ‘ âˆˆ C (i.e., there is a production ğ‘ â†’ ğ‘ ) to obtain Ë†ğ‘ â€², then Ë†ğ‘ â€² âˆˆ children ( Ë†ğ‘ ). In contrast, for an interval-constrained partial program Ëœğ‘ , we include a child annotating ğ‘ with [âˆ’âˆ , âˆ] . Then,  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:14 Stephen Mell, Steve Zdancewic, and Osbert Bastani

subsequent children can further split interval constraints to obtain finer-grained interval constraints (the concrete constant value ğ‘ can be represented by the constraint (ğ‘, ğ‘ ) âˆˆ Ë†C). To formalize this notion, we first separate out productions for constants. 

Definition 4.6. A production ğ‘Ÿ âˆˆ ğ‘… is constant if it has the form ğ‘Ÿ = ğ‘ â†’ ğ‘ for some ğ‘ âˆˆ C .Now, we can partition ğ‘… into the set ğ‘… C of constant productions and its complement Ë†ğ‘… = ğ‘… \ ğ‘… C ,which we call non-constant productions . Next, given an interval-constrained partial program Ëœğ‘ =

( Ë†ğ‘, ğœ… ), its holes are simply the holes of the underlying partial program Ë†ğ‘ : holes ( Ëœğ‘ ) = holes ( Ë†ğ‘ ). Then, we partition these holes into unannotated holes and annotated holes â€”i.e., holes âˆ… (( Ë†ğ‘, ğœ… )) = {ğ‘– âˆˆ holes ( Ë†ğ‘ ) | ğœ… (ğ‘– ) = âˆ…} 

holes C (( Ë†ğ‘, ğœ… )) = {ğ‘– âˆˆ holes ( Ë†ğ‘ ) | ğœ… (ğ‘– ) â‰  âˆ…} ,

respectively. Finally, we define three kinds of children for an interval-constrained partial program Ëœğ‘ .First, we include children obtained by filling an unannotated hole with a non-constant production: children âˆ… ( Ëœğ‘ ) = {( Ë†ğ‘ â€², ğœ… â€²) âˆˆ ËœP | âˆƒ ğ‘– âˆˆ holes âˆ… ( Ëœğ‘ ), ğ‘Ÿ âˆˆ Ë†ğ‘… . Ë†ğ‘ = fill ( Ë†ğ‘, ğ‘–, ğ‘Ÿ ) âˆ§ ğœ… â€² = repair (ğœ… ; Ë† ğ‘, ğ‘–, ğ‘Ÿ )} .

These children are the same as the children constructed in the original search DAG over partial programs. Here, repair (ğœ… ; Ë†ğ‘, ğ‘–, ğ‘Ÿ ) â€œrepairsâ€ ğœ… by accounting for how the indices in Ë†ğ‘ change after applying production ğ‘Ÿ to fill hole ğ‘– in Ë†ğ‘ . In particular, this operation changes the indices of nonter-minals in Ë†ğ‘ ; ğœ… â€² accounts for these changes without modifying the annotations themselves. Formally, if Ë†ğ‘ â€² = fill ( Ë†ğ‘, ğ‘ ğ‘– â†’ ğ‘€ 1...ğ‘€ â„, ğ‘– ), with Ë†ğ‘ = ğ‘ 1...ğ‘ ğ‘– ...ğ‘ ğ‘˜ and Ë†ğ‘ â€² = ğ‘ 1...ğ‘€ 1...ğ‘€ â„ ...ğ‘ ğ‘˜ , then we have 

ğœ… â€² ( ğ‘— ) =

ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³

ğœ… ( ğ‘— ) if ğ‘— < ğ‘– 

âˆ… if ğ‘– â‰¤ ğ‘— â‰¤ â„ğœ… ( ğ‘— âˆ’ â„ + 1) if ğ‘— > â„. 

In particular, ğœ… â€² includes the same annotations as ğœ… .Second, we consider children obtained by filling an unannotated hole with the interval [âˆ’âˆ , âˆ] :children âˆ ( Ëœğ‘ ) = {( Ë†ğ‘ â€², ğœ… â€²) âˆˆ ËœP | âˆƒ ğ‘– âˆˆ holes âˆ… ( Ëœğ‘ ) . Ë†ğ‘ â€² = Ë†ğ‘ âˆ§ ğœ… â€² (ğ‘– ) = [âˆ’âˆ , âˆ]} .

In other words, the partial program Ë†ğ‘ remains unchanged, but we introduce an annotation onto one of the previously unannotated holes of Ëœğ‘ .Third, we consider children obtained by replacing an annotation with a tighter annotation: children C ( Ëœğ‘ ) = {( Ë†ğ‘ â€², ğœ… â€²) âˆˆ ËœP | âˆƒ ğ‘– âˆˆ holes C ( Ëœğ‘ ) . Ë†ğ‘ â€² = Ë†ğ‘ âˆ§ subset (ğœ… â€² (ğ‘– ), ğœ… (ğ‘– ))} .

Here, subset ( Ë†ğ‘, Ë†ğ‘ â€²) checks whether Ë†ğ‘ = (ğ‘ 0, ğ‘ 1) is a strict subset of Ë†ğ‘ â€² = (ğ‘ â€²

> 0

, ğ‘ â€²

> 1

), that is, ğ‘ â€² 

> 0

â‰¤ ğ‘ 0 and 

ğ‘ 1 â‰¤ ğ‘ â€²

> 1

, and one of these inequalities is strict; equivalently, [ğ‘ 0, ğ‘ 1] âŠŠ [ğ‘ â€²

> 0

, ğ‘ â€²

> 1

].Finally, our overall search DAG is defined by the union of these children: children ( Ëœğ‘ ) = children âˆ… ( Ëœğ‘ ) âˆª children âˆ ( Ëœğ‘ ) âˆª children C ( Ëœğ‘ ). (8) Note that by defining children in this way, there may be infinitely many children; in addition, multiple children may cover the same concrete program, leading to redundancy in the search DAG. Practical implementations can restrict to a finite subset of these children as long as they satisfy (4)â€”i.e., the union of the concrete programs in the children of Ëœğ‘ cover all concrete programs in Ëœğ‘ .In addition, these children are ideally chosen so the overlap is minimal. 

Concretization Function. Recall that the concretization function ğ›¾ ( Ëœğ‘ ) contains concrete programs ğ‘ 

represented by Ëœğ‘ . We take the approach where we define ğ›¾ based on the children functionâ€”i.e., 

ğ‘ âˆˆ ğ›¾ ( Ëœğ‘ ) if there exists a sequence Ëœğ‘ 1, ..., Ëœğ‘ ğ‘› = ğ‘ such that Ëœğ‘ 1 = Ëœğ‘ , Ëœğ‘ ğ‘› = ğ‘ , and Ëœğ‘ ğ‘— +1 âˆˆ children ( Ëœğ‘ ğ‘— )

for all 1 < ğ‘— < ğ‘› . 

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:15

4.5 Interval Transformers for Partial Programs with Interval Constraints 

Next, we describe how to implement abstract interpretation for partial programs with interval constraints. While abstract interpretation is typically performed with respect to program inputs, in our case it is with respect to program constants. We assume all components ğ‘“ âˆˆ F have an abstract transformer Ë†ğ‘“ , and the objective ğœ™ 0 has an abstract transformer Ë†ğœ™ 0 (if they are monotone, their abstract transformers can be defined as in Sections 4.2 and 4.3). First, we modify the grammar of programs so that the constants C are replaced by abstract values (ğ‘ 0, ğ‘ 1) âˆˆ Ë†C. While the concrete semantics cannot be applied to these programs, we will define abstract semantics for them. Now, given a generalized partial program Ëœğ‘ = ( Ë†ğ‘, ğœ… ), for each unannotated hole ğ‘– âˆˆ holes âˆ… ( Ëœğ‘ ), we replace the corresponding nonterminal ğ‘ ğ‘– in Ë†ğ‘ with the abstract value (âˆ’âˆ , âˆ) , and for each annotated hole ğ‘– âˆˆ holes C ( Ëœğ‘ ), we replace the corresponding nonterminal ğ‘ ğ‘– with the annotation ğœ… (ğ‘– ). Finally, we replace any constant ğ‘ in Ë†ğ‘ with the abstract value (ğ‘, ğ‘ ). Once we have performed this transformation, we can define the following abstract semantics for Ë†ğ‘ :

âŸ¦ğ‘‹ âŸ§# (ğ‘¥ ) = ğ›¼ (ğ‘¥ ), âŸ¦Ë†ğ‘ âŸ§# (ğ‘¥ ) = Ë†ğ‘, âŸ¦ğ‘“ (ğ‘ 1, ..., ğ‘ ğ‘˜ )âŸ§ # (ğ‘¥ ) = Ë†ğ‘“ (âŸ¦ ğ‘ 1âŸ§# (ğ‘¥ ), ..., âŸ¦ğ‘ ğ‘˜ âŸ§# (ğ‘¥ )) ,

Now, we can combine âŸ¦Â·âŸ§ # with Ë†ğœ™ 0 to obtain the abstract objective âŸ¦Â·âŸ§ # 

> ğœ™

:

âŸ¦( Ë†ğ‘, ğœ… )âŸ§ # 

> ğœ™

(ğ‘ ) = Ë†ğœ™ 0

 {(âŸ¦ Ë†ğ‘ âŸ§# (ğ‘¥ ), ğ‘¦ )} (ğ‘¥,ğ‘¦ ) âˆˆ ğ‘ 

 .

In other words, we apply the abstract semantics âŸ¦ Ë†ğ‘ âŸ§# to each input ğ‘¥ , obtain the corresponding abstract output Ë†ğ‘¦ â€², and apply Ë†ğœ™ 0 to the resulting set {( Ë†ğ‘¦ â€², ğ‘¦ )} .

5 IMPLEMENTATION 

We instantiate our framework for two different domain specific languages (DSLs): 

â€¢ NEAR DSL: The NEAR language for the CRIM dataset (Section 5.1). The motivating example described in Section 2 is derived from this setting. 

â€¢ Quivr DSL: A query language over video trajectories, which uses constructs similar to regular expressions to match trajectories (Section 5.2). Although both of these DSLs process sequence data, their computation models are quite different: NEAR focuses on folding combinators over the inputs, whereas Quivrâ€™s primary operations reduce to matrix multiplication. For both DSLs, we refine the definition of children compared to Equation 8. In particular, in Section 4.4, when defining the search space over programs with interval constraints on holes, children C is defined such that the children of [ğ‘, ğ‘ ] are all of its strict subintervals. As discussed there, this means that each node may have infinitely many children, which is impractical for an implementation. Instead, our implementation splits intervals into just two childrenâ€”i.e., the children of [ğ‘, ğ‘ ] are [ğ‘, (ğ‘ + ğ‘ )/ 2] and [( ğ‘ + ğ‘ )/ 2, ğ‘ ]. These intervals partition [ğ‘, ğ‘ ], so all concrete programs are still contained in the search space, retaining soundness. This splits an interval into child intervals of equal length, but with additional domain-knowledge, other choices could be made (e.g. with a prior distribution over parameters, splitting into intervals of equal probability mass). Finally, we also describe how we construct the abstract transformer for the ğ¹ 1 score, which is commonly used as the objective function in binary classification problems (Section 5.3). 

5.1 NEAR DSL for Trajectory Labeling 

In the NEAR DSL [Shah et al . 2020], the program input is a featurized trajectory, which is a sequence of feature vectors ğ‘¥ âˆˆ ( Rğ‘› )âˆ—, where ğ‘› is the dimension of the feature vector for each frame. The output is a sequence of labels ğ‘¦ âˆˆ { t, f }âˆ— of the same length as the input, where ğ‘¦ [ğ‘¡ ] = t if a frame  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:16 Stephen Mell, Steve Zdancewic, and Osbert Bastani

exhibits the given behavior and ğ‘¦ [ğ‘¡ ] = f otherwise (i.e. the task is binary classification at the frame level). 

Syntax. This DSL has three kinds of expressions, encoded by their corresponding nonterminal in 

G: (i) ğ‘£ğ‘£ represents functions mapping feature vectors to real values (e.g. the body of map), (ii) 

â„“ğ‘£ represents functions mapping lists to real values (e.g. fold), and (iii) â„“â„“ represents functions mapping lists to lists (e.g. map). In particular, this DSL has the following productions: 

ğ‘£ğ‘£ F ğ‘§ ğ‘– | ğ‘§ ğ‘“ | ğ‘ âˆˆ R | ğ‘£ğ‘£ + ğ‘£ğ‘£ | ğ‘£ğ‘£ Â· ğ‘£ğ‘£ | ite (ğ‘£ğ‘£, ğ‘£ğ‘£, ğ‘£ğ‘£ ) (ğ‘– âˆˆ [ ğ‘› ]) 

â„“ğ‘£ F fold (ğ‘£ğ‘£ ) | ite (â„“ğ‘£, â„“ğ‘£, â„“ğ‘£ )

â„“â„“ F map (ğ‘£ğ‘£ ) | mapprefix (â„“ğ‘£ ) | ite (â„“ğ‘£, â„“â„“, â„“â„“ ),

The DSL syntax is in the combinatory style, so ğœ† s are omitted. In particular, the expressions encoded by ğ‘£ğ‘£ are combinators designed to be used within a higher-order function such as map or fold . These combinators are applied to individual elements of the input list, where ğ‘§ ğ‘– is a variable representing the ğ‘– th element of the current feature vector ğ‘§ = ğ‘¥ [ğ‘¡ ], and ğ‘§ ğ‘“ is a special symbol used inside fold to represent its accumulated running state. The start symbol is â„“â„“ . The DSL is structured such that a list of feature vectors ğ‘¥ is mapped to a list of real values ğ‘Ÿ of the same length. Each real-valued output ğ‘Ÿ [ğ‘¡ ] implicitly encodes the label 

ğ‘¦ [ğ‘¡ ] =

(

t if ğ‘Ÿ [ğ‘¡ ] â‰¥ 0

f otherwise .

The running example involved programs in a toy DSL of the form map (ğ‘‘ â‰¤ ğ‘ ). In the NEAR DSL, this would be represented as map (âˆ’ 1 Â· ğ‘§ 4 + ğ‘ ), as â€œdistance between miceâ€ is the 4th feature, and the label will be obtained by comparing the program output to 0.

Semantics. We let ğ‘‰ = Rğ‘› (where ğ‘› is the dimension of the feature space) be the space of feature vectors, ğ¿ = ğ‘‰ âˆ— is the space of trajectories, and ğ¾ = Râˆ— is the space of output sequences. Then, the nonterminal ğ‘£ğ‘£ in this DSL has semantics âŸ¦ğ‘£ğ‘£ âŸ§ : ğ‘‰ Ã— R â†’ R, where 

âŸ¦ğ‘§ ğ‘– âŸ§( ğ‘£, ğ‘  ) B ğ‘£ ğ‘– 

âŸ¦ğ‘§ ğ‘“ âŸ§( ğ‘£, ğ‘  ) B ğ‘  

âŸ¦ğ‘ âŸ§( ğ‘£, ğ‘  ) B ğ‘ 

âŸ¦ğ‘£ğ‘£ 1 + ğ‘£ğ‘£ 2âŸ§( ğ‘£, ğ‘  ) B âŸ¦ğ‘£ğ‘£ 1âŸ§( ğ‘£, ğ‘  ) + âŸ¦ ğ‘£ğ‘£ 2âŸ§( ğ‘£, ğ‘  )âŸ¦ğ‘£ğ‘£ 1 Â· ğ‘£ğ‘£ 2âŸ§( ğ‘£, ğ‘  ) B âŸ¦ğ‘£ğ‘£ 1âŸ§( ğ‘£, ğ‘  ) Â· âŸ¦ ğ‘£ğ‘£ 2âŸ§( ğ‘£, ğ‘  )âŸ¦ite (ğ‘£ğ‘£ 1, ğ‘£ğ‘£ 2, ğ‘£ğ‘£ 3)âŸ§( ğ‘£, ğ‘  ) B if (âŸ¦ ğ‘£ğ‘£ 1âŸ§( ğ‘£, ğ‘  ) â‰¥ 0) then âŸ¦ğ‘£ğ‘£ 2âŸ§( ğ‘£, ğ‘  ) else âŸ¦ğ‘£ğ‘£ 3âŸ§( ğ‘£, ğ‘  ).

Next, the nonterminal â„“ğ‘£ in this DSL has semantics âŸ¦â„“ğ‘£ âŸ§ : ğ‘‰ âˆ— â†’ R, where 

âŸ¦fold (ğ‘£ğ‘£ )âŸ§( â„“) B fold (ğœ†ğ‘£ğœ†ğ‘ . âŸ¦ğ‘£ğ‘£ âŸ§( ğ‘£, ğ‘  ), â„“, 0)âŸ¦ite (â„“ğ‘£ 1, â„“ğ‘£ 2, â„“ğ‘£ 3)âŸ§( â„“) B if (âŸ¦ â„“ğ‘£ 1âŸ§( â„“) â‰¥ 0) then âŸ¦â„“ğ‘£ 2âŸ§( â„“) else âŸ¦â„“ğ‘£ 3âŸ§( â„“).

Here, fold : (ğ‘‰ â†’ R â†’ R) â†’ ğ‘‰ âˆ— â†’ R â†’ R is standard, and passes the intermediate value as the ğ‘  

argument to ğ‘£ğ‘£ . The nonterminal â„“â„“ in this DSL has semantics âŸ¦â„“â„“ âŸ§ : ğ¿ â†’ ğ¾ , where 

âŸ¦map (ğ‘£ğ‘£ )âŸ§( â„“) B map (ğœ†ğ‘£. âŸ¦ğ‘£ğ‘£ âŸ§( ğ‘£, 0), â„“ )âŸ¦mapprefixes (â„“ğ‘£ )âŸ§( â„“) B map (ğœ†â„“ â€². âŸ¦â„“ğ‘£ âŸ§( â„“â€²), prefixes (â„“)) âŸ¦ite (â„“ğ‘£, â„“â„“ 1, â„“â„“ 2)âŸ§( â„“) B if (âŸ¦ â„“ğ‘£ âŸ§( â„“) â‰¥ 0) then âŸ¦â„“â„“ 1âŸ§( â„“) else âŸ¦â„“â„“ 2âŸ§( â„“). 

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:17

Here, map is standard 3 and 

prefixes : (ğ‘¥ 1, . . . , ğ‘¥ ğ‘› ) â†¦ â†’ (( ğ‘¥ 1), (ğ‘¥ 1, ğ‘¥ 2), (ğ‘¥ 1, ğ‘¥ 2, ğ‘¥ 3), . . . (ğ‘¥ 1, . . . , ğ‘¥ ğ‘› )) .

Finally, as described above, the labels ğ‘¦ are obtained by thresholding âŸ¦â„“â„“ âŸ§( â„“). That is, we let 

âŸ¦â„“â„“ âŸ§ğ‘ : ğ¿ â†’ { t, f }âˆ— denote the label 

(âŸ¦ â„“â„“ âŸ§ğ‘ (â„“)) [ ğ‘¡ ] =

(

t if (âŸ¦ â„“â„“ âŸ§( â„“)) [ ğ‘¡ ] â‰¥ 0

f otherwise .

Abstract Semantics. We abstract R with the usual real intervals, Ë†R. We abstract ğ¾ with ( Ë†R)âˆ—, products of real intervals. Addition is monotone, and multiplication [ğ‘ 1, ğ‘ 1] Â· [ ğ‘ 2, ğ‘ 2] can be shown to be abstracted by [min (ğ‘ 1ğ‘ 1, ğ‘ 1ğ‘ 2, ğ‘ 2ğ‘ 1, ğ‘ 2ğ‘ 2), max (ğ‘ 1ğ‘ 1, ğ‘ 1ğ‘ 2, ğ‘ 2ğ‘ 1, ğ‘ 2ğ‘ 2)] . We can represent ite (ğ‘, ğ‘, ğ‘ )

as 1[ğ‘ â‰¥ 0] Â· ğ‘ + 1[ğ‘ < 0] Â· ğ‘ , and thus can abstract it using addition, multiplication, and the abstraction sending 1[[ f, f]] = [0, 0], 1[[ f, t]] = [0, 1], and 1[[ t, t]] = [1, 1].

Search Space. The search space over ğ‘£ğ‘£ has a redundancy due to commutativity, associativity, and distributativity, which unduly hinders search. Instead, we use a normalized version, where 

ğ‘£ğ‘£ expressions are constrainted to be sums-of-products (fully distributed), and where we ignore commutativity and associative in the sums of products. Essentially, we only consider polynomials, where the variables are the features ğ‘§ ğ‘– , the fold variable ğ‘§ ğ‘“ , and indicator variables 1[ğ‘£ğ‘£ â‰¥ 0] for each ğ‘£ğ‘£ (to maintain the expressiveness of ite ). Further, we consider only constants in [âˆ’ 1, 1] and we also normalize the dataset so each feature ğ‘§ ğ‘– is in [âˆ’ 1, 1].We define a notion of size for programs, so that we can bound the search space, where ite , map 

and mapprefix have size 1, fold has size 0 (since mapprefix must contain a fold and already has size 1). Each monomial in the polynomial has size 1, and each polynomial variable in a monomial has size 1. In a given polynomial, the size of indicators is amortized: each nested ğ‘£ğ‘£ has size 1 and produces a new polynomial variable. 

5.2 Quivr DSL For Trajectory Queries 

In the Quivr DSL [Mell et al . 2023], the program input is a featurized trajectory, which is a sequence 

ğ‘¥ âˆˆ ( Rğ‘› )âˆ— as before. However, the output is now a single label ğ‘¦ âˆˆ { t, f } for the entire trajectory. This DSL is designed to allow the user to select trajectories that satisfy certain propertiesâ€”e.g. the user may want to identify all cars that make a right turn at a certain intersection in a traffic video. 

Syntax. This DSL is based on the Kleene algebra with tests [Kozen 1997], which, intuitively, are regular expressions where the â€œcharactersâ€ are actually predicates. Its syntax is 

ğ‘„ F ğ‘“ | ğ‘” (ğ¶ ) | ğ‘„ ; ğ‘„ | ğ‘„ âˆ§ ğ‘„ (ğ‘“ âˆˆ F âˆ…, ğ‘” âˆˆ F C )

ğ¶ F ğ‘ (ğ‘ âˆˆ R)

where Fâˆ… and FC are given sets of domain-specific predicates, the latter of which have constants 

ğ‘ âˆˆ R that need to be chosen by the synthesizer. 

Semantics. Expressions in this DSL denote functions mapping sequences of feature vectors ğ‘¥ âˆˆ ( Rğ‘› )âˆ—

to whole-sequence labels ( {t, f }), defined as 

âŸ¦ğ‘“ âŸ§( ğ‘¥ ) B ğ‘“ (ğ‘¥ )âŸ¦ğ‘” (ğ‘ )âŸ§( ğ‘¥ ) B ğ‘” (ğ‘¥ ) â‰¥ ğ‘ 

âŸ¦ğ‘„ 1 âˆ§ ğ‘„ 2âŸ§( ğ‘¥ ) B âŸ¦ğ‘„ 1âŸ§( ğ‘¥ ) âˆ§ âŸ¦ ğ‘„ 2âŸ§( ğ‘¥ )    

> 3But note that ğ‘§ ğ‘“ is treated as 0 if it appears inside an expression not within fold
> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:18 Stephen Mell, Steve Zdancewic, and Osbert Bastani

âŸ¦ğ‘„ 1 ; ğ‘„ 2âŸ§( ğ‘¥ ) B

> ğ‘›

Ãœ

> ğ‘˜ =0

âŸ¦ğ‘„ 1âŸ§( ğ‘¥ 0: ğ‘˜ ) âˆ§ âŸ¦ ğ‘„ 2âŸ§( ğ‘¥ ğ‘˜ :ğ‘› )

Each predicate ğ‘“ âˆˆ F âˆ… has type ğ‘“ : (Rğ‘› )âˆ— â†’ { t, f }, and indicates whether ğ‘¥ matches ğ‘“ , and each predicate ğ‘” âˆˆ F C has type ğ‘” : (Rğ‘› )âˆ— â†’ R, and produces a real-valued score thresholded at a given constant ğ‘ to indicate whether ğ‘¥ matches ğ‘” .

Abstract Semantics. Note that under the standard orders for reals R and Booleans B = {t, f } (i.e., 

f < t), the semantics âŸ¦ğ‘” (ğ‘ )âŸ§( ğ‘¥ ) is monotone decreasing with respect to ğ‘ . Furthermore, both conjunction and disjunction are monotone increasing in their inputs. Thus, we can use our interval transformers for monotone functions from Section 4.5. In particular, recall that Ë†R is the abstract domain of real intervals, and let Ë†B be the abstract domain of Boolean intervals 

Ë†B = {( f, f), (f, t), (t, t)} .

Then, we use the abstract transformers 

ğ‘” (ğ‘¥ ) â‰¥ ( ğ‘ 0, ğ‘ 1) =

ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³

(f, f) if ğ‘” (ğ‘¥ ) < ğ‘ 0

(f, t) if ğ‘ 0 â‰¤ ğ‘” (ğ‘¥ ) < ğ‘ 1

(t, t) if ğ‘ 1 â‰¤ ğ‘” (ğ‘¥ )

Ë†ğ‘¥ 1 âˆ§ Ë†ğ‘¥ 2 =

ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³

(t, t) if Ë†ğ‘¥ 1 = (t, t) âˆ§ Ë†ğ‘¥ 2 = (t, t)(f, f) if Ë†ğ‘¥ 1 = (f, f) âˆ¨ Ë†ğ‘¥ 2 = (f, f)(f, t) otherwise 

Ë†ğ‘¥ 1 âˆ¨ Ë†ğ‘¥ 2 =

ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³

(t, t) if Ë†ğ‘¥ 1 = (t, t) âˆ¨ Ë†ğ‘¥ 2 = (t, t)(f, f) if Ë†ğ‘¥ 1 = (f, f) âˆ§ Ë†ğ‘¥ 2 = (f, f)(f, t) otherwise .

Since the concrete semantics are defined in terms of these operators, composing their abstract versions gives an abstract semantics. 

5.3 Abstract ğ¹ 1 Score 

In Section 4, we described how to construct abstract transformers for monotone functions, which covers most components in these DSLs. However, many objectives commonly used in practice, such as the ğ¹ 1 score, are non-monotone. For non-monotone objectives, we need to provide a custom abstract transformer that overapproximates their concrete semantics for the interval domain. We now describe how to do so for the ğ¹ 1 score. Let ğ‘Š be the multiset of outcomes of the form (ğ‘¦ â€², ğ‘¦ ) where ğ‘¦ â€² is the prediction and ğ‘¦ is the ground truth. Then the ğ¹ 1 score is given by: 

ğ‘‡ ğ‘ƒ (ğ‘Š ) Bâˆ‘ï¸ 

> (ğ‘¦ â€²,ğ‘¦ ) âˆˆ ğ‘Š

1[ğ‘¦ = t âˆ§ ğ‘¦ â€² = t]

ğ¹ ğ‘ƒ (ğ‘Š ) Bâˆ‘ï¸ 

> (ğ‘¦ â€²,ğ‘¦ ) âˆˆ ğ‘Š

1[ğ‘¦ = f âˆ§ ğ‘¦ â€² = t]

ğ¹ 1 (ğ‘Š ) B 2 Â· ğ‘‡ ğ‘ƒ (ğ‘Š )

ğ‘‡ ğ‘ƒ (ğ‘Š ) + ğ¹ ğ‘ƒ (ğ‘Š ) + | ğ‘Š + | ,

where |ğ‘Š + | = |{( ğ‘¦ â€², ğ‘¦ ) âˆˆ ğ‘Š | ğ‘¦ = t}| , ğ‘‡ ğ‘ƒ is the number of true positives, and ğ¹ ğ‘ƒ is the number of false positives. Note that ğ‘‡ ğ‘ƒ and ğ¹ ğ‘ƒ are monotone in ğ‘¦ â€², so we can use the corresponding interval  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:19

transformers: 

ğ‘‡ ğ‘ƒ # ( Ë†ğ‘Š ) Bâˆ‘ï¸   

> (Ë†ğ‘¦ â€²,ğ‘¦ ) âˆˆ Ë†ğ‘Š

ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³

[1, 1] if ğ‘¦ = t âˆ§ Ë†ğ‘¦ â€² = (t, t)[0, 1] if ğ‘¦ = t âˆ§ Ë†ğ‘¦ â€² = (f, t)[0, 0] if ğ‘¦ = t âˆ§ Ë†ğ‘¦ â€² = (f, f)

ğ¹ ğ‘ƒ # ( Ë†ğ‘Š ) Bâˆ‘ï¸   

> (Ë†ğ‘¦ â€²,ğ‘¦ ) âˆˆ Ë†ğ‘Š

ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³

[1, 1] if ğ‘¦ = f âˆ§ Ë†ğ‘¦ â€² = (t, t)[0, 1] if ğ‘¦ = f âˆ§ Ë†ğ‘¦ â€² = (f, t)[0, 0] if ğ‘¦ = f âˆ§ Ë†ğ‘¦ â€² = (f, f).

Letting ğ‘‡ ğ‘ƒ # (ğ‘Š ) = [ğ‘ 1, ğ‘ 1] and ğ¹ ğ‘ƒ # (ğ‘Š ) = [ğ‘ 2, ğ‘ 2], and noting that for positive numbers we can abstract division as [ğ‘ 1, ğ‘ 1]/[ ğ‘ 2, ğ‘ 2] =

h ğ‘ 1 

> ğ‘ 2

, ğ‘ 1

> ğ‘ 2

i

, a naÃ¯ve strategy of applying abstractions for +, Â·,and / leads to the abstract ğ¹ 1 score 

ğ¹ #1 (ğ‘Š ) B 2 Â· [ğ‘ 1, ğ‘ 1][ğ‘ 1, ğ‘ 1] + [ ğ‘ 2, ğ‘ 2] + | ğ‘Š + | = 2 Â·

 ğ‘ 1

ğ‘ 1 + ğ‘ 2 + | ğ‘Š + | , ğ‘ 1

ğ‘ 1 + ğ‘ 2 + | ğ‘Š + |



,

where |ğ‘Š + | is independent of ğ‘¦ â€², so we can treat it as a constant. However, this abstraction is very loose, and we found it not to be useful in practiceâ€”e.g. it can be as loose as [0, 2] even though ğ¹ 1

scores never exceed 1. Instead, we can rewrite 

ğ¹ 1 (ğ‘Š ) = 2 Â·             

> ğ‘‡ ğ‘ƒ (ğ‘Š )
> ğ¹ ğ‘ƒ (ğ‘Š )+| ğ‘Š +|
> ğ‘‡ ğ‘ƒ (ğ‘Š )
> ğ¹ ğ‘ƒ (ğ‘Š )+| ğ‘Š +|

+ 1 = 2 Â· ğ¿ 

 ğ‘‡ ğ‘ƒ (ğ‘Š )

ğ¹ ğ‘ƒ (ğ‘Š ) + | ğ‘Š + |



where ğ¿ (ğ‘¥ ) = ğ‘¥ ğ‘¥ +1 is monotone, which leads to the abstract ğ¹ 1 score 

ğ¹ #1 (ğ‘Š ) B 2 Â· ğ¿ 

 [ğ‘ 1, ğ‘ 1][ğ‘ 2, ğ‘ 2] + | ğ‘Š + |



= 2 Â· ğ¿ 

  ğ‘ 1

ğ‘ 2 + | ğ‘Š + | , ğ‘ 1

ğ‘ 2 + | ğ‘Š + |

  

= 2 Â·

 ğ‘ 1

ğ‘ 1 + ğ‘ 2 + | ğ‘Š + | , ğ‘ 1

ğ‘ 1 + ğ‘ 2 + | ğ‘Š + |



.

6 EXPERIMENTS 

We experimentally evaluate our approach in the context of the NEAR and Quivr DSLs described in Section 5. We demonstrate that our synthesizer outperforms two synthesis baselines in scalability of running time: Metasketches [Bornholt et al . 2016], an optimal synthesizer based on SMT solvers (Section 6.2), as well as an ablation that uses breadth-first search instead of our abstract interpretation based heuristic and ğ´ âˆ— search (Section 6.3). 

6.1 Experimental Setup 

Benchmarks. We consider two different neurosymbolic program synthesis benchmarks, based on the DSLs described in Section 5: 

â€¢ CRIM13: The NEAR [Shah et al . 2020] DSL applied to two tasks in the CRIM13 dataset [Burgos-Artizzu et al . 2012] â€œsniffâ€ (A) and â€œotherâ€ (B). This dataset consists of featurized videos of two mice interacting in an enclosure, with 12,404 training examples with 100 frames each. 

â€¢ Quivr: The Quivr [Mell et al . 2023] DSL, applied to the 17 tasks that they evaluate on. Of these, 6 tasks are on the MABe22 [Sun et al . 2022] dataset, a dataset of interactions between 3 mice, and 10 tasks are on the YTStreams [Bastani et al . 2020] dataset, a dataset of video from fixed-position traffic cameras. 

Compute. We ran all experiments on a Intel Xeon Gold 6342 CPU (2.80GHz, 36 cores/72 threads). Our implementation uses PyTorch, a library which provides fast matrix operations.       

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:20 Stephen Mell, Steve Zdancewic, and Osbert Bastani 520 40 60 80 100 0.1 110 100 1000
> Number of Training Examples Time to Convergence (sec)

(a) Quivr, task G 3 5 10 15 20 0.1 110 100 1000 10k  

> Number of Training Examples Time to Convergence (sec)

(b) CRIM13, task A Fig. 2. The time (seconds, log scale) to identify the optimal program and prove its optimality, for our approach (blue, solid) and an SMT solver (red, dashed), as a function of the size of the training dataset, for two different tasks. 

6.2 Comparison to Metasketches 

Metasketches performs optimal synthesis using an SMT solver by, in addition to the correctness specification, adding an SMT constraint that the programâ€™s score be greater than the best score seen so far. Thus if the SMT solver returns â€œSATâ€, a better program will have been found, and the process is repeated. Note that in our setting, there is no correctness specification, and so achieving a better score is the only SMT constraint. A similar strategy, which we found to be more effective, is to do binary search on the objective score: supposing that the objective is in [0, 1], we ask the SMT solver whether there is a program achieving score at least 1/2; if it returns â€œSATâ€, we ask for 3/4, and if â€œUNSATâ€ we ask for 1/4,and so on. Our implementation uses the Z3 [De Moura and BjÃ¸rner 2008] SMT solver. For a fairer comparison, in this experiment we restricted PyTorch to a single CPU core. The two approaches were both run until they had converged to the exact optimal program. Figure 2 shows that the SMT solver scales very poorly as a function of the number of trajectories in the training dataset. While competitive for three or four trajectories, we would like to evaluate on datasets of hundreds or thousands of trajectories. 

6.3 Comparison to Breadth-First Search 

Next, to show the benefit of the search heuristic, we compare against an ablation which ignores the heuristic and does breadth-first search. At any point in the search process, there is a heap of search nodes, each of which has a lower and upper bound of ğ¹ 1 scores reachable from it. Rather than using the lower bound from the abstract objective value, we instead evaluate the concrete program whose parameters are the midpoint of the hyper-rectangle of abstract parameters, to get a concrete objective value; this is a better lower-bound, and it is cheap to compute. The greatest of these lower bounds provides a lower bound on the optimal ğ¹ 1 score, and the greatest of these upper bounds provides an upper bound on the optimal ğ¹ 1 score. Note that if the lower and upper bounds are equal, they equal the true optimal ğ¹ 1 score, and search terminates. To make search tractable, on the CRIM13 benchmarks we consider only expressions with struc-tural cost at most 4. Note that this rules out â€œ ite â€ expressions, but performs well in practice. On the  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:21

Table 1. Best ğ¹ 1 score found and range between upper and lower bounds, at a particular time during search, for different tasks and different algorithms. â€œCAâ€ and â€œCBâ€ are the NEAR CRIM13 queries, and â€œQAâ€ through â€œQQâ€ are the Quivr queries. â€œHâ€ is ğ´ âˆ— search and â€œBâ€ is breadth-first search. For each task and each time, the best ğ¹ 1 score is bolded and the smallest range is bolded. A range of 0 implies that search has converged to the optimal ğ¹ 1 score. 

Setting 10 sec 30 sec 1 min 2 min 5 min 10 min 

CA H 0.21 (0.79) 0.21 (0.79) 0.21 (0.79) 0.52 (0.14) 0.53 (0.03) 0.53 (0.00) 

B 0.21 (0.79) 0.21 (0.79) 0.21 (0.79) 0.22 (0.78) 0.46 (0.54) 0.50 (0.50) CB H 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.80 (0.05) 0.80 (0.04) 0.80 (0.01) 

B 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) QA H 0.12 (0.88) 0.77 (0.23) 0.77 (0.23) 0.77 (0.23) 0.77 (0.23) 0.77 (0.23) 

B 0.12 (0.88) 0.26 (0.74) 0.26 (0.74) 0.26 (0.74) 0.26 (0.74) 0.28 (0.72) QB H 0.27 (0.73) 0.31 (0.69) 0.52 (0.48) 0.52 (0.48) 0.52 (0.48) 0.52 (0.48) 

B 0.27 (0.73) 0.40 (0.60) 0.40 (0.60) 0.40 (0.60) 0.42 (0.58) 0.50 (0.50) QC H 0.06 (0.94) 0.14 (0.86) 0.30 (0.70) 0.33 (0.67) 0.38 (0.62) 0.38 (0.62) 

B 0.06 (0.94) 0.25 (0.75) 0.25 (0.75) 0.26 (0.74) 0.26 (0.74) 0.26 (0.74) QD H 0.04 (0.96) 0.25 (0.75) 0.25 (0.75) 0.25 (0.75) 0.40 (0.60) 0.44 (0.56) 

B 0.04 (0.96) 0.06 (0.94) 0.06 (0.94) 0.06 (0.94) 0.09 (0.91) 0.19 (0.81) QE H 0.04 (0.96) 0.20 (0.80) 0.44 (0.56) 0.44 (0.56) 0.44 (0.56) 0.44 (0.56) 

B 0.04 (0.96) 0.06 (0.94) 0.06 (0.94) 0.10 (0.90) 0.10 (0.90) 0.15 (0.85) QF H 0.38 (0.62) 0.78 (0.22) 0.78 (0.22) 0.78 (0.22) 0.78 (0.22) 0.78 (0.22) 

B 0.38 (0.62) 0.42 (0.58) 0.42 (0.58) 0.55 (0.45) 0.56 (0.44) 0.60 (0.40) QG H 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) B 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) QH H 0.74 (0.26) 0.74 (0.26) 0.74 (0.26) 0.74 (0.26) 0.74 (0.26) 0.74 (0.26) B 0.78 (0.22) 0.78 (0.22) 0.78 (0.22) 0.78 (0.22) 0.78 (0.22) 0.78 (0.22) 

QI H 0.75 (0.25) 0.80 (0.20) 0.80 (0.20) 0.80 (0.20) 0.80 (0.20) 0.80 (0.20) 

B 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) QJ H 0.64 (0.36) 0.64 (0.36) 0.64 (0.36) 0.64 (0.36) 0.64 (0.36) 0.64 (0.36) B 0.64 (0.36) 0.73 (0.27) 0.73 (0.27) 0.73 (0.27) 0.73 (0.27) 0.73 (0.27) 

QK H 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) B 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) 0.75 (0.25) QL H 0.71 (0.29) 0.91 (0.09) 0.91 (0.09) 0.91 (0.09) 0.91 (0.09) 0.91 (0.09) B 0.71 (0.29) 0.91 (0.09) 0.91 (0.09) 0.91 (0.09) 0.91 (0.09) 0.91 (0.09) QM H 0.73 (0.27) 0.92 (0.08) 0.92 (0.08) 0.92 (0.08) 0.92 (0.08) 0.92 (0.08) B 0.73 (0.27) 0.80 (0.20) 0.80 (0.20) 0.80 (0.20) 0.92 (0.08) 0.92 (0.08) QN H 0.73 (0.27) 0.73 (0.27) 0.89 (0.11) 0.89 (0.11) 0.89 (0.11) 0.89 (0.11) 

B 0.73 (0.27) 0.73 (0.27) 0.80 (0.20) 0.80 (0.20) 0.80 (0.20) 0.80 (0.20) QO H 0.67 (0.33) 0.67 (0.33) 0.86 (0.14) 0.86 (0.14) 0.86 (0.14) 0.86 (0.14) 

B 0.67 (0.33) 0.67 (0.33) 0.80 (0.20) 0.80 (0.20) 0.80 (0.20) 0.80 (0.20) QP H 0.50 (0.50) 0.50 (0.50) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) B 0.50 (0.50) 0.50 (0.50) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) QQ H 0.80 (0.20) 0.80 (0.20) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) B 0.80 (0.20) 0.80 (0.20) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 

Quivr benchmarks, we bound the search space in the same way that their paper does, limiting to programs with at most 3 predicates, at most 2 of which have parameters. We use 100 trajectories from each dataset. For CRIM13, these are randomly sampled from the training set. For Quivr, to ensure that we have some positive examples, because positives are very  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. 16:22 Stephen Mell, Steve Zdancewic, and Osbert Bastani

sparse on some tasks, we use 2 positive and 10 negative trajectories specially designated in the dataset, and the remaining 88 are sampled randomly from the training set. Table 1 shows, at different times during the search process, the best found ğ¹ 1 score (the lower-bound of the interval), as well as the width of the interval of optimal ğ¹ 1 scores. On most tasks, our approach (H) achieves higher ğ¹ 1 scores more quickly than the ablation (B), as well as tighter intervals. 

7 RELATED WORK 

Neurosymbolic Synthesis. There has been a great deal of recent interest in neurosymbolic synthe-sis [Chaudhuri et al . 2021], including synthesis of functional programs [Gaunt et al . 2016; Shah et al .2020; Valkov et al . 2018], reinforcement learning policies [Anderson et al . 2020; Inala et al . 2020], programs for extracting data from unstructured text [Chen et al . 2023, 2021; Ye et al . 2021], and programs that extract data from video trajectories [Bastani et al . 2021; Mell et al . 2023; Shah et al .2020]. Some of these approaches have proposed pruning strategies based on monotonicity [Chen et al . 2021; Mell et al . 2023], but for specific DSLs. NEAR is a general framework for neurosymbolic synthesis based on neural heuristics [Shah et al . 2020]; however, their approach is not guaranteed to synthesize optimal programs. To the best of our knowledge, our work proposes the first general framework for optimal synthesis of neurosymbolic programs. 

Optimal Synthesis. More broadly, there has been recent interest in optimal synthesis [Bornholt et al .2016; Smith and Albarghouthi 2016], typically focusing on optimizing performance properties of the program such as running time rather than accuracy; superoptimization is a particularly well studied application [Bansal and Aiken 2008; Massalin 1987; Mukherjee et al . 2020; Phothilimthana et al . 2016; Sasnauskas et al . 2017]. Our experiments demonstrate that our approach outperforms Bornholt et al .[2016], a general framework for optimal synthesis based on SMT solvers. There has also been work on synthesizing a program that maximizes an objective (expressed as a neural network scoring function) [Ye et al . 2021], but they do not consider real-valued constants, the quantitative objective is syntactic, not semantic, and abstract interpretation is only used for pruning according to the Boolean correctness specification, not the quantitative objective. Optimal synthesis has also been leveraged for synthesizing minimal guards for memory safety [Dillig et al . 2014], chemical reaction networks [Cardelli et al . 2017], and optimal layouts for quantum computing [Tan and Cong 2020]. 

Abstract Interpretation for Synthesis. There has been work on leveraging abstract interpretation for pruning portions of the search space in program synthesis [Guria et al . 2023; So and Oh 2017], as well as using abstraction refinement [Wang et al . 2017]; however, these approaches target traditional synthesis. Rather than evaluating an abstract semantics on partial programs, Wang et al . [2017] constructs a data structure compactly representing concrete programs whose abstract semantics are compatible with the input-output examples. However, it is not obvious how their data structure (which targets Boolean specifications) can be adapted to our quantitative synthesis setting. 

Abstract Interpretation for Planning. One line of work, initiated by the FF Planner [Hoffmann and Nebel 2001], uses abstract semantics to perform reachability analysis to prune invalid plans [Gregory et al . 2012; Hoffmann 2003; Zhi-Xuan et al . 2022]. However, other than pruning invalid plans, the reachability analysis is not used in the computation of the search heuristic. Instead, traditional heuristics such as â„max (which computes the shortest plan in a â€œrelaxed modelâ€ that drops delete lists from the postconditions of abstract actions, and outputs the length of this plan) are used. In contrast, we use an abstract transformer for the objective function to provide a lower bound that is directly used as the search heuristic.  

> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:23

A second line of work [Marthi et al . 2008; Vega-Brown and Roy 2018] considers computing optimal plans by underapproximating the cost function. They assume that the total cost of a plan equals the sum of the costs of the individual actions in that plan and then, given a lower bound on the cost of each action, simply sum these lower bounds to obtain a lower bound on the cost of the overall plan. This strategy makes strong assumptions about the structure of the overall cost function, whereas our abstract interpretation based approach requires no such assumptions. Another key difference is that we are abstracting over real-valued parameters of partial programs, whereas the above approaches are abstracting over continuous states. Thus, our framework requires a way to iteratively refine the program space (specified by the â€œchildrenâ€ function), which is absent from their frameworks. 

8 CONCLUSION 

We have proposed a general framework for synthesizing programs with real-valued inputs and outputs, using ğ´ âˆ— search in conjunction with a search heuristic based on abstract interpretation. Our framework searches over a space of generalized partial programs, which represent sets of concrete programs, and uses the search heuristic to establish upper bounds on the objective value of a given generalized partial program. In addition, we propose a natural strategy for constructing abstract transformers for components with monotone semantics. If our algorithm returns a program, then this program is guaranteed to be optimal. Our experimental evaluation demonstrates that our approach is more scalable than existing optimal synthesis techniques. Directions for future work include improving the scalability of our approach and applying it to additional synthesis tasks. 

ACKNOWLEDGEMENTS 

We thank the anonymous reviewers for their helpful feedback. This work was supported in part by NSF Award CCF-1910769, NSF Award CCF-1917852, ARO Award W911NF-20-1-0080, and Amazon/ASSET Gift for Research in Trustworthy AI. 

DATA AVAILABILITY STATEMENT 

An artifact is available [Mell 2023] with our implementation, which reproduces our experimental results (Figure 2 and Table 1) and may be useful for performing synthesis on other trajectory datasets or implementing our algorithm for other DSLs. 

REFERENCES             

> Greg Anderson, Abhinav Verma, Isil Dillig, and Swarat Chaudhuri. 2020. Neurosymbolic reinforcement learning with formally verified exploration. Advances in neural information processing systems 33 (2020), 6172â€“6183. Sorav Bansal and Alex Aiken. 2008. Binary Translation Using Peephole Superoptimizers.. In OSDI , Vol. 8. 177â€“192. Favyen Bastani, Songtao He, Arjun Balasingam, Karthik Gopalakrishnan, Mohammad Alizadeh, Hari Balakrishnan, Michael Cafarella, Tim Kraska, and Sam Madden. 2020. MIRIS: Fast Object Track Queries in Video. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data . 1907â€“1921. https://doi.org/10.1145/3318464.3389692 Favyen Bastani, Songtao He, Ziwen Jiang, Osbert Bastani, and Sam Madden. 2021. SkyQuery: an aerial drone video sensing platform. In Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software . 56â€“67. https://doi.org/10.1145/3486607.3486750 James Bornholt, Emina Torlak, Dan Grossman, and Luis Ceze. 2016. Optimizing synthesis with metasketches. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages . 775â€“788. https: //doi.org/10.1145/2837614.2837666 Xavier P. Burgos-Artizzu, Piotr DollÃ¡r, Dayu Lin, David J. Anderson, and Pietro Perona. 2012. Social behavior recognition in continuous video. In 2012 IEEE Conference on Computer Vision and Pattern Recognition . 1322â€“1329. https://doi.org/10. 1109/CVPR.2012.6247817 Luca Cardelli, Milan ÄŒeÅ¡ka, Martin FrÃ¤nzle, Marta Kwiatkowska, Luca Laurenti, Nicola Paoletti, and Max Whitby. 2017. Syntax-guided optimal synthesis for chemical reaction networks. In Computer Aided Verification: 29th International
> Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024.

16:24 Stephen Mell, Steve Zdancewic, and Osbert Bastani 

Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part II 30 . Springer, 375â€“395. https://doi.org/10. 1007/978-3-319-63390-9_20 Swarat Chaudhuri, Kevin Ellis, Oleksandr Polozov, Rishabh Singh, Armando Solar-Lezama, Yisong Yue, et al . 2021. Neurosymbolic programming. Foundations and Trends Â® in Programming Languages 7, 3 (2021), 158â€“243. https: //doi.org/10.1561/2500000049 Qiaochu Chen, Arko Banerjee, Ã‡aÄŸatay Demiralp, Greg Durrett, and Isil Dillig. 2023. Data Extraction via Semantic Regular Expression Synthesis. In OOPSLA . https://doi.org/10.1145/3622863 Qiaochu Chen, Aaron Lamoreaux, Xinyu Wang, Greg Durrett, Osbert Bastani, and Isil Dillig. 2021. Web question answering with neurosymbolic program synthesis. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation . 328â€“343. https://doi.org/10.1145/3453483.3454047 Patrick Cousot and Radhia Cousot. 1977. Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints. In Proceedings of the 4th ACM SIGACT-SIGPLAN symposium on Principles of programming languages . 238â€“252. https://doi.org/10.1145/512950.512973 Leonardo De Moura and Nikolaj BjÃ¸rner. 2008. Z3: An Efficient SMT Solver. In Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (Budapest, Hungary) (TACASâ€™08/ETAPSâ€™08) . Springer-Verlag, Berlin, Heidelberg, 337â€“340. https://doi.org/10.1007/978-3-540-78800-3_24 Thomas Dillig, Isil Dillig, and Swarat Chaudhuri. 2014. Optimal guard synthesis for memory safety. In Computer Aided Verification: 26th International Conference, CAV 2014, Held as Part of the Vienna Summer of Logic, VSL 2014, Vienna, Austria, July 18-22, 2014. Proceedings 26 . Springer, 491â€“507. https://doi.org/10.1007/978-3-319-08867-9_32 Alexander L Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow. 2016. Terpret: A probabilistic programming language for program induction. arXiv preprint arXiv:1608.04428 

(2016). Peter Gregory, Derek Long, Maria Fox, and J. Christopher Beck. 2012. Planning Modulo Theories: Extending the Planning Paradigm. Proceedings of the International Conference on Automated Planning and Scheduling 22, 1 (May 2012), 65â€“73. https://doi.org/10.1609/icaps.v22i1.13505 Sumit Gulwani, Susmit Jha, Ashish Tiwari, and Ramarathnam Venkatesan. 2011. Synthesis of Loop-Free Programs. In 

PLDIâ€™11, June 4-8, 2011, San Jose, California, USA (pldiâ€™11, june 4â€“8, 2011, san jose, california, usa ed.). https://doi.org/10. 1145/1993498.1993506 Sankha Narayan Guria, Jeffrey S Foster, and David Van Horn. 2023. Absynthe: Abstract Interpretation-Guided Synthesis. 

Proceedings of the ACM on Programming Languages 7, PLDI (2023), 1584â€“1607. https://doi.org/10.1145/3591285 JÃ¶rg Hoffmann. 2003. The Metric-FF Planning System: Translatingâ€œIgnoring Delete Listsâ€to Numeric State Variables. Journal of artificial intelligence research 20 (2003), 291â€“341. https://doi.org/10.1613/jair.1144 JÃ¶rg Hoffmann and Bernhard Nebel. 2001. The FF Planning System: Fast Plan Generation through Heuristic Search. J. Artif. Int. Res. 14, 1 (may 2001), 253â€“302. https://doi.org/10.1613/jair.855 Jeevana Priya Inala, Yichen Yang, James Paulos, Yewen Pu, Osbert Bastani, Vijay Kumar, Martin Rinard, and Armando Solar-Lezama. 2020. Neurosymbolic transformers for multi-agent communication. Advances in Neural Information Processing Systems 33 (2020), 13597â€“13608. Michael J Kearns and Umesh Vazirani. 1994. An introduction to computational learning theory . MIT press. https://doi.org/10. 7551/mitpress/3897.001.0001 Dexter Kozen. 1997. Kleene algebra with tests. ACM Transactions on Programming Languages and Systems (TOPLAS) 19, 3 (1997), 427â€“443. https://doi.org/10.1145/256167.256195 Bhaskara Marthi, Stuart Russell, and Jason Andrew Wolfe. 2008. Angelic Hierarchical Planning: Optimal and Online Algorithms.. In ICAPS . Citeseer, 222â€“231. Henry Massalin. 1987. Superoptimizer: a look at the smallest program. ACM SIGARCH Computer Architecture News 15, 5 (1987), 122â€“126. https://doi.org/10.1145/36177.36194 Stephen Mell. 2023. Artifact for Optimal Program Synthesis via Abstract Interpretation . https://doi.org/10.5281/zenodo. 10146270 Stephen Mell, Favyen Bastani, Steve Zdancewic, and Osbert Bastani. 2023. Synthesizing Trajectory Queries from Examples. In Computer Aided Verification , Constantin Enea and Akash Lal (Eds.). Springer Nature Switzerland, Cham, 459â€“484. https://doi.org/10.1007/978-3-031-37706-8_23 Manasij Mukherjee, Pranav Kant, Zhengyang Liu, and John Regehr. 2020. Dataflow-Based Pruning for Speeding up Superoptimization. Proc. ACM Program. Lang. 4, OOPSLA, Article 177 (nov 2020), 24 pages. https://doi.org/10.1145/3428245 Phitchaya Mangpo Phothilimthana, Aditya Thakur, Rastislav Bodik, and Dinakar Dhurjati. 2016. Scaling up superoptimiza-tion. In Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems . 297â€“310. https://doi.org/10.1145/2872362.2872387 Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024. Optimal Program Synthesis via Abstract Interpretation 16:25 

Raimondas Sasnauskas, Yang Chen, Peter Collingbourne, Jeroen Ketema, Gratian Lup, Jubi Taneja, and John Regehr. 2017. Souper: A synthesizing superoptimizer. arXiv preprint arXiv:1711.04422 (2017). https://doi.org/10.48550/arXiv.1711.04422 Ameesh Shah, Eric Zhan, Jennifer Sun, Abhinav Verma, Yisong Yue, and Swarat Chaudhuri. 2020. Learning differentiable programs with admissible neural heuristics. Advances in neural information processing systems 33 (2020), 4940â€“4952. Calvin Smith and Aws Albarghouthi. 2016. MapReduce program synthesis. Acm Sigplan Notices 51, 6 (2016), 326â€“340. https://doi.org/10.1145/2980983.2908102 Sunbeom So and Hakjoo Oh. 2017. Synthesizing Imperative Programs from Examples Guided by Static Analysis. In Static Analysis , Francesco Ranzato (Ed.). Springer International Publishing, Cham, 364â€“381. https://doi.org/10.1007/978-3-319-66706-5_18 Armando Solar-Lezama, Liviu Tancau, Rastislav Bodik, Sanjit Seshia, and Vijay Saraswat. 2006. Combinatorial Sketching for Finite Programs. In Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems (San Jose, California, USA) (ASPLOS XII) . Association for Computing Machinery, New York, NY, USA, 404â€“415. https://doi.org/10.1145/1168857.1168907 Jennifer J. Sun, Brian Geuther, Vivek Kumar, Alice Robie, Catherine Schretter, Keith Sheppard, Dipam Chakraborty, Kristin Branson, and Ann Kennedy. 2022. The MABe22 Benchmarks for Representation Learning of Multi-Agent Behavior. https://doi.org/10.22002/D1.20186 Jennifer J Sun, Tomomi Karigo, Dipam Chakraborty, Sharada P Mohanty, Benjamin Wild, Quan Sun, Chen Chen, David J Anderson, Pietro Perona, Yisong Yue, and Ann Kennedy. 2021. The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions. arXiv preprint arXiv:2104.02710 (2021). https://doi.org/10.48550/arXiv.2104.02710 Bochen Tan and Jason Cong. 2020. Optimal layout synthesis for quantum computing. In Proceedings of the 39th International Conference on Computer-Aided Design . 1â€“9. https://doi.org/10.1145/3400302.3415620 Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat Chaudhuri. 2018. Houdini: Lifelong learning as program synthesis. Advances in neural information processing systems 31 (2018). William Vega-Brown and Nicholas Roy. 2018. Admissible Abstractions for Near-Optimal Task and Motion Planning. In 

Proceedings of the 27th International Joint Conference on Artificial Intelligence (Stockholm, Sweden) (IJCAIâ€™18) . AAAI Press, 4852â€“4859. https://doi.org/10.24963/ijcai.2018/674 Xinyu Wang, Isil Dillig, and Rishabh Singh. 2017. Program Synthesis Using Abstraction Refinement. Proc. ACM Program. Lang. 2, POPL, Article 63 (dec 2017), 30 pages. https://doi.org/10.1145/3158151 Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2021. Optimal Neural Program Synthesis from Multimodal Specifications. In Findings of the Association for Computational Linguistics: EMNLP 2021 . 1691â€“1704. https://doi.org/10.18653/v1/2021. findings-emnlp.146 Tan Zhi-Xuan, Joshua B. Tenenbaum, and Vikash K. Mansinghka. 2022. Abstract Interpretation for Generalized Heuristic Search in Model-Based Planning. https://doi.org/10.48550/arXiv.2208.02938 arXiv:2208.02938 [cs.AI] 

Received 2023-07-11; accepted 2023-11-07 

Proc. ACM Program. Lang., Vol. 8, No. POPL, Article 16. Publication date: January 2024.