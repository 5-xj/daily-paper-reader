Title: BFS-PO: Best-First Search for Large Reasoning Models

URL Source: https://arxiv.org/pdf/2602.14917v1

Published Time: Tue, 17 Feb 2026 03:11:47 GMT

Number of Pages: 16

Markdown Content:
# BFS-PO: Best-First Search for Large Reasoning Models 

Fiorenzo Parascandolo 1 Wenhui Tan 2 Enver Sangineto 1 Ruihua Song 2 Rita Cucchiara 1

## Abstract 

Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent per-formance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the gener-ation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum en-tropy nodes. By generating progressively shorter responses during training, BFS-PO learns to pro-duce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accu-racy and shorten its answers. Our code and mod-els are available at: https://github.com/ aimagelab/BFS-PO .

## 1. Introduction 

Large Reasoning Models (LRMs) have recently achieved impressive accuracy results using inference-time scaling (OpenAI, 2024; DeepSeek-AI, 2025), in which the reason-ing process is decomposed into a (long) sequence of simpler steps, called Chain of Thought (CoT) (Wei et al., 2022). However, inference-time scaling usually leads to very long reasoning chains, which often consist of thousands or even tens of thousands of tokens (Ballon et al., 2025; Hou et al., 2025a), producing high computational and memory over-head. This phenomenon is called overthinking , and it is due to the presence of redundant steps (Song et al., 2025), errors in reasoning (Fatemi et al., 2025) and the use of many     

> 1Department of Engineering, University of Modena and Reggio Emilia, Italy. 2Gaoling School of Artificial Intelligence, Ren-min University of China, Beijing, China. Correspondence to: Fiorenzo Parascandolo <fiorenzo.parascandalo@unimore.it >, En-ver Sangineto <enver.sangineto@unimore.it >.
> Preprint. February 17, 2026.

words for the sole purpose of textual coherence (Hao et al., 2025). Moreover, several recent studies have empirically demonstrated that CoT length and accuracy are strongly negatively correlated (Fatemi et al., 2025; Su et al., 2025b; Yang et al., 2025; Hassid et al., 2025; Agarwal et al., 2025; Yi et al., 2025). For instance, in DAPO (Yu et al., 2025) and in (Fatemi et al., 2025; Liu et al., 2025), the authors observe that Reinforcement Learning (RL) algorithms like GRPO (Shao et al., 2024) encourage longer responses when the advantage is negative, due to the CoT length-based normal-ization contained in the objective function. A common method to reduce overthinking is based on fine-tuning a pre-trained LRM using a dataset containing a short ground truth CoT associated with each question (Sec. 2). One of the pioneering approaches of this paradigm is Token-Skip (Xia et al., 2025), which utilizes GPT-4 to associate each token in a given CoT with an importance score. Less important tokens are pruned, and the LRM is fine-tuned with the resulting dataset. Many other works (Xu et al., 2025; Jang et al., 2025; Xiao et al., 2025; Cheng et al., 2025; Chen et al., 2025; Munkhbat et al., 2025; Han et al., 2025; Kang et al., 2025; Zhao et al., 2025; Qiao et al., 2025) follow this direction and are based on the creation of a dataset of triplets (question, CoT, answer), where the correct answer to a question is associated with a short CoT. However, train-ing an LRM using Supervised Fine-Tuning (SFT) is often suboptimal (DeepSeek-AI, 2025), especially with a limited dataset, and can lead to overfitting. Empirical results show that most of these works succeed in reducing the length of the CoT, but at the cost of also reducing the accuracy. In this paper, we follow a different direction: instead of using a static dataset of pre-computed short CoT for SFT, we propose an RL algorithm in which exploration is biased toward correct, short reasoning paths . In this way, it is the LRM itself that generates its dataset of concise CoTs while it is trained. Specifically, we propose Best-First Search Policy Optimization (BFS-PO), an RL algorithm in which the training exploration process is guided by a Best-First Search (BFS) strategy on a search tree. Unlike GRPO-type algorithms, where multiple answers to a given question are generated independently of each other, in BFS-PO, sam-pling is conditioned on the best solution found so far, where “best” is defined as the shortest correct solution. Importantly, BFS-PO does not require external modules to evaluate par-1

> arXiv:2602.14917v1 [cs.CL] 16 Feb 2026 BFS-PO: Best-First Search for Large Reasoning Models

tial solutions and decide which ones to expand. Indeed, search tree-based strategies have been recently adopted in other works (Yao et al., 2023; Long, 2023; Besta et al., 2025). However, these methods need a Process Reward Model (PRM) (Uesato et al., 2022), i.e. a module which can assess the quality of the current reasoning step and guide the search tree exploration. Unfortunately, a PRM is usu-ally implemented as an additional Large Language Model (LLM), which needs to be trained with supervised datasets containing annotations for partial solutions. In contrast, we evaluate only full solutions, avoiding the need of a PRM. Specifically, once a full solution is generated, we use a stan-dard verifier (Lambert et al., 2025) to assess its correctness. Moreover, the solution length is used to select the shortest correct reasoning chain. Once the current best solution is selected, we use a backtracking mechanism which interprets this solution as a path in a search tree, and it individuates the most promising forking node in which the full solution should be split to take a different path. Since we do not have an external PRM, we select the backtracking node using the generation uncertainty. Indeed, as empirically shown in (Wang et al., 2025), the highest entropy CoT tokens behave as forking tokens in LRM reasoning, and they correspond to 

those points in which the LRM takes the most important de-cisions . Therefore, we use the highest entropy token as the backtracking point, and the partial path from the question to the backtracking point to bias the exploration of alternative partial paths (see Fig. 1). Very recently, Hou et al. (2025b) proposed TreeRL, in which, similarly to BFS-PO, a search tree is constructed using the token generation uncertainty for branching. However, unlike our entropy-based selection, Hou et al. (2025b) use the negative log-likelihood of the next token as the criterion to select (multiple) forking tokens in a given path. Moreover, in TreeRL, the goal of the search tree is to efficiently replace classic Monte Carlo Tree Search (MCTS) (Browne et al., 2012) methods, and it is not based on a specific search strategy like Best First. Finally, the advantage proposed in (Hou et al., 2025b) compares the node value with the value of the root and the value of its parent node. In contrast, we propose to extend the advantage formulation used in GRPO to our search tree, comparing the reward of each branch with the average reward of other branches generated from the same backtracking node. We evaluate BFS-PO using different reasoning benchmarks and base LRMs. Unlike most existing literature on CoT shortening, where brevity is achieved at the cost of an accu-racy drop, BFS-PO can simultaneously reduce the average CoT length and increase the inference-time accuracy, even when compared with GRPO/DAPO based fine-tuning. In summary, our contributions are: 1. We propose BFS-PO, an RL algorithm based on a BFS strategy which biases exploration towards short and correct answers. 2. We propose a formulation of the advantage which ex-tends the advantage of GRPO to a search tree. 3. We empirically show that BFS-PO can simultaneously decrease the average length and increase the accuracy of the generated reasoning chains. 

## 2. Related Work 

Given the significant application interest in reducing LRM overthinking, there has recently been a dramatic increase in work in this field. In this section, we briefly review the main methods, organized into different categories. 

Budget forcing approaches (Muennighoff et al., 2025; Li et al., 2025; Han et al., 2025; Fu et al., 2025) for CoT shortening are based on specifying a maximum number of thinking tokens (usually via a specific prompt) and forc-ing an early exit by adding an end-of-thought token once this number has been generated. The advantage of budget forcing is that it can be used as a training-free approach. However, its early exit usually leads to an accuracy drop. 

SFT with concise CoT. As mentioned in Sec. 1, one of the most popular CoT shortening paradigm is based on con-structing a dataset with ground truth short CoT associated with each question, and then using SFT to train the model. Approaches in this category differ mainly in how the dataset is built and how to assess the importance of each token, e.g., using an auxiliary LLM (Xia et al., 2025; Cheng et al., 2025; Kang et al., 2025), the influence of a token on the response likelihood (Xu et al., 2025; Jang et al., 2025; Xiao et al., 2025; Qiao et al., 2025) or other methods (Chen et al., 2025; Munkhbat et al., 2025; Han et al., 2025; Zhao et al., 2025). Unlike these approaches, we do not explicitly build a training dataset containing concise CoT, but we let the LRM itself generate short CoT, conditioning the exploration strategy toward the shortest correct solution. 

RL with length penalty. Another common CoT shortening paradigm is based on RL algorithms which include a length penalty in their reward function (Sel et al., 2025; Cheng et al., 2025; Hou et al., 2025a). For instance, in (Yi et al., 2025), the penalty is computed with respect to the current shortest positive answer, while in (Arora & Zanette, 2025; Luo et al., 2025), it is compared to the average length of the sampled CoT. We adopt the length penalty of DAPO (Yu et al., 2025) and we compare with DAPO in Sec. 5, showing that BFS-PO generates shorter CoT with a higher accuracy. 

Steering vector. Ma et al. (2025) build a dataset that asso-ciates CoT of different lengths with each question. They then train LoRA models (Hu et al., 2022) with both long 2BFS-PO: Best-First Search for Large Reasoning Models                

> GRPO/DAPO BFS-PO
> ...
> ××
> ✓
> ✓
> q
> (a)
> ...
> ××
> ✓
> wb
> oj
> ✓
> q
> Selection
> (b)
> ...
> ××
> ✓
> wb
> ✓
> ✓✓
> ✓
> ×
> q
> Expansion
> (c)
> ...
> ××
> ✓
> wb
> ✓
> ✓✓
> ✓
> ×
> q
> Selection
> (d)
> . . .
> Figure 1. A schematic comparison between GRPO/DAPO (a) and BFS-PO (b-d). In (a), the sampling mechanism of GRPO/DAPO is represented as a simple tree with only one forking node (the root node, conditioned on the question q). ✓and ✗represent correct and incorrect answers, respectively. In (b) and (d), the current best solution is selected, while in (c) new branches are added to the tree. For simplicity, in this figure we use G= 4 .

and short CoT. The difference in parameter space between the two models indicates the direction to shorten the CoT. A similar approach is used in (Lin et al., 2025b), where the steering vector is computed in the activation space of some LRM layers rather than in the parameter space, and using PCA instead of parameter arithmetic. 

Compression methods (Pang et al., 2024; Song et al., 2025; Zhang et al., 2025a) compress the tokens of the generated CoT, e.g., using specific attention masks which force the LRM to store information in intermediate tokens while dis-carding the others. Usually, this compression results in a decrease in the final accuracy. 

Latent CoT. The idea behind this category of methods (Deng et al., 2023; 2024; Hao et al., 2025; Saunshi et al., 2025; Shen et al., 2025; Su et al., 2025a; Geiping et al., 2025; Kong et al., 2025; Lin et al., 2025a; Tan et al., 2025) is that human reasoning is often abstract and not always verbal, thus, rather than generating CoT composed of dis-crete tokens, the LRM can generate continuous vectors in a latent space. Relaxing the textual coherence constraint, a sequence of latent tokens can potentially be shorter than a textual CoT. Although appealing, approaches belonging to this class have so far lagged behind the accuracy achieved by LRMs based on textual CoT. One reason is probably because sampling from a continuous distribution is more difficult than sampling from a distribution over a vocabulary of discrete tokens (LCM team et al., 2024). Soft Think-ing (Zhang et al., 2025b) partially alleviates this problem using the LRM discrete distribution to softly aggregate all the vocabulary tokens. However, very recent works (Wu et al., 2025; Butt et al., 2025) have shown that vanilla im-plementations of Soft Thinking often underperform their discrete counterparts, and its sampling mechanism mostly boils down to greedy decoding. 

LRM with tree search. This category groups together those works using a tree search with LRMs. In this case, the aim is not to shorten the generated CoT but to extend the lin-ear reasoning, typical of a CoT, into a structured search, which includes, for example, techniques such as Breadth-First Search or Depth-First Search (Besta et al., 2025). For instance, Tree of Thoughts (Yao et al., 2023; Long, 2023) break the reasoning process into atomic thoughts (where, e.g., a thought is a sentence), organized in a tree. Then, they use auxiliary LLMs and specific prompts to evaluate the current tree frontier and expand its nodes (i.e., thoughts). This mechanism has been extended in (Besta et al., 2024) to direct graphs. Hou et al. (2025b) observe that MCTS (Browne et al., 2012) cannot be easily applied to LRMs because of its high computational costs (see Sec. 1). Thus, they propose TreeRL, which, similarly to BFS-PO, struc-tures the exploration using a search tree. However, they 3BFS-PO: Best-First Search for Large Reasoning Models 

do not use BFS. Moreover, in addition to the differences mentioned in Sec. 1, in TreeRL, non-leaf tokens belonging to multiple paths are repeatedly included in the optimization process, increasing the risk of overfitting on these partial sequences (Hou et al., 2025b). Instead, we propose a dif-ferent formulation of the advantage that relies only on the current subtree and allows shared paths to be computed only once, resulting in a much faster algorithm and with a higher stability of the supervision signal (see Sec. 4.2 and 5). 

## 3. Preliminaries 

Problem Setup. Let πθ be a policy model (i.e. a base LRM) with parameters θ, and D = {(qj , a ∗ 

> j

)} the training dataset, composed of problems (qj , a ∗ 

> j

), where qj is the question and a∗ 

> j

the corresponding ground truth answer. Given qj ,the model autoregressively generates a token sequence oj =(w1, ..., w t, ..., w |oj |) according to oj ∼ πθ (·| qj ), where 

wt ∈ V is a textual token and V the vocabulary of all tokens. We indicate with oj,t = wt the t-th token of the reasoning chain oj (we will use oj,t and wt interchangeably). The output oj is usually formatted using special tokens (e.g., </think> ) to delimit the intermediate reasoning chain (commonly called CoT) from the final answer aj . A verifier v(·), usually based on simple parsing and string matching techniques, extracts aj from oj and compares it with a∗ 

> j

: v(oj , a ∗ 

> j

) = 1 if aj = a∗ 

> j

, and 0 otherwise. The goal is to fine-tune πθ to decrease the average length ( |o|)of its reasoning chains o while keeping (or increasing) its accuracy. 

GRPO and DAPO. The success of DeepSeek-R1 (DeepSeek-AI, 2025) has popularized GRPO (Shao et al., 2024), an RL algorithm which has been shown to greatly increase the accuracy of LRMs. The main idea in GRPO is to replace the costly value model in PPO (Schulman et al., 2017) with an advantage computed with respect to the mean reward of a group of G different outputs {oi}Gi=1 correspond-ing to the same question q. Specifically, given (q, a ∗) ∈ D ,

πθold is used to sample {oi}Gi=1 , and, for each oi, the advan-tage is computed using: 

ˆAi,t = Ri − mean( {Ri}Gi=1 )std( {Ri}Gi=1 ) , (1) where Ri is the sample-level reward and ˆAi,t is the token-level advantage, same for all tokens oi,t ∈ oi. DAPO (Yu et al., 2025) is an improved version of GRPO (see App. A), in which the same advantage is used in the following objective: 

JDAP O (θ) = E(q,a ∗)∼D ,{oi}Gi=1 ∼πθold (·| q)

 1

PGi=1 |oi|

> G

X

> i=1
> |oi|

X

> t=1



min (ri,t (θ) ˆAi,t , clip (ri,t (θ),

1 − ϵl, 1 + ϵh) ˆAi,t 

i 

s.t. 0 < |{ oi|v(oi, a ∗) = 1 }| < G, 

(2) where ri,t (θ) = πθ (oi,t |q,o i,<t )  

> πθold (oi,t |q,o i,<t )

. Moreover, in DAPO a penalty for very long sequences is added to the reward Ri:

Rl(o) = 



0 |o| ≤ Lmax − Lcache  

> (Lmax −Lcache )−| o|
> Lcache

Lmax − Lcache < |o| ≤ Lmax 

−1 Lmax < |o|,(3) and the final reward is computed using: 

Ri =

(

1 + Rl(oi) v(oi, a ∗) = 1 

−1 + Rl(oi) v(oi, a ∗) = 0 . (4) In this paper, we adopt the objective function of DAPO, including its length penalty, and, for a fair comparison, we 

use the same hyper-parameter values of DAPO (more details in App. A). In Fig. 1 (a) we schematically show the group of G outputs, which can be thought of as independent paths of a simple tree containing only one branching node (the root node). 

## 4. Method 

The G outputs used in GRPO and DAPO (see Sec. 3 and Fig. 1) are generated independently the one from the others and they are used to explore the solution space to provide possibly different solutions to the same question q. However, classical optimization methods for planning and problem solving organize the exploration of the solution space using a search tree and strategies such as BFS, with mechanisms such as backtracking to revisit and modify a partial solution (Russell & Norvig, 2010). For instance, in our case, the benefit of a BFS strategy over multiple independent random sampling is that we can steer the exploration towards a local optimum of the solution space by progressively refining the current best solution to make it increasingly shorter. To implement this process, we first select the best solution generated so far, where “best”, in our case, means correct and shortest. Specifically, if T = {oi}Gi=1 , then let oj ∈ T 

be such that: 4BFS-PO: Best-First Search for Large Reasoning Models 

v(oj , a ∗) = 1 , |oj | ≤ min oi∈T (|oi|). (5) Given q, oj is the best solution generated so far. We now want to condition the exploration process using oj . More specifically, we want to explore the solution space in the neighbourhood of oj , searching for a shorter, correct solu-tion. To do so, we need a backtracking mechanism, i.e., if we interpret oj as a path in a search tree ( T ), we need to individuate the most promising forking node in which the reasoning process should take a different path. As men-tioned in Sec. 1, and following the empirical findings in (Wang et al., 2025), we use the highest entropy tokens for backtracking. Specifically, we select the backtracking token 

wb ∈ oj (Fig. 1 (b)) using: 

wb = oj,t ∗ = arg max 

> oj,t ∈oj

H(oj,t ), (6) where: 

H(oj,t ) = − 

> |V|

X

> n=1

πθ (wn|q, o j, ≤t), log (πθ (wn|q, o j, ≤t)) .

(7) Using oj,t ∗ , we expand the current tree by adding new branches. Specifically, we use (oj, 1, ..., o j,t ∗ ) as a suffix which is concatenated to the question q to condition the generations of G new outputs, sampled using {oi}Gi=1 ∼

πθold (·| q, o j, 1, ..., o j,t ∗ ). The new partial solutions {oi}Gi=1 

are added to the search tree T (Fig. 1 (c)) and the process is iterated. The full algorithm is shown in Alg. 1, where B is the set of backtracking tokens. At iteration l, the new best solution oj is selected (line 5) among all complete paths from the root to a leaf using Eq. (5). Then, a backtracking token ( oj,t ∗ ) is chosen (line 6) in the current shortest correct solution oj , and we exclude previously selected backtrack-ing tokens ( oj,t /∈ B) to increase the exploration diversity. 

Algorithm 1 Tree Construction 

1: Input: sample problem (q, a ∗) ∼ D , policy model 

πθold , number of expansion iterations K.

2: T = {oi}Gi=1 ∼ πθold (·| q) ▷ Initialization 3: B = {} 

4: for l = 1 to K do 

5: Select a complete path oj using Eq. (5) ▷ Selection 6: oj,t ∗ = arg max oj,t ∈oj ,o j,t /∈B H(oj,t )

7: N = {oi}Gi=1 ∼ πθold (·| q, o j, 1, ..., o j,t ∗ ) ▷ Expansion 8: T ← T ∪ N

9: B ← B ∪ { oj,t ∗ }

10: end for 4.1. Optimization 

Once T is built, we associate a reward and an advantage to each token and then we compute the objective function. To make the notation more readable, we will use a single index ( n) to indicate the token of a generic node of T : wn.If wn is a backtracking token, we indicate it with wbn. Let 

B = {wb

> 1

, w b

> 2

, ..., } be the set of all backtracking tokens. We denote with wm = P (wn) the parent of wn. Similarly, we denote with C(wn) = {wm1 , ..., w mk } the set of children of 

wn. Specifically: if wbn ∈ B, then k = G+1 ; for leaf nodes, 

k = 0 ; and k = 1 for all the other nodes (Fig. 1). If wn is a leaf (i.e., an answer token), we can extract the complete reasoning path recursively applying P (·) until we reach the root ( q), obtaining the output sequence o = ( w1, ...w n) = (P (...P (wn)... ), ..., P (P (wn)) , P (wn), w n).We also define the sub-tree S(·) of a backtracking node, which will be used in our advantage formulation (see be-low). Specifically, given wbn ∈ B, with children C(wbn) = 

{wm1 , ..., w mk }, we associate wbn with the sub-tree rooted at wbn and composed of the k = G + 1 branches starting with one of the children of wbn: S(wbn) = {oi}ki=1 . The i-th branch oi ∈ S(wbn) is a sequence: oi = ( wmi , ...w t),which begins with the child node wmi and ends with wt,where wt is either a leaf or another backtracking point (see Fig. 2). S(·) can be easily computed, e.g., using the algo-rithm provided in App. B. 

S(wbn)

wbn

wm1

wmk

wbn+1 

...... .........

Figure 2. A schematic representation of the sub-tree S(wbn).

Using these definitions, we assign a reward to all the leaves 

(i.e., to all complete reasoning paths), adopting DAPO’s reward function (Eq. (4)) (see Sec. 3 and App. B for more details). The rewards for the internal nodes are recursively computed averaging the rewards of their children. Specifi-cally, if wn is the token of a non-leaf node, its reward R(wn)

is: 5BFS-PO: Best-First Search for Large Reasoning Models 

R(wn) = 1

|C(wn)|

X

> wm∈C(wn)

R(wm). (8) Note that, if wn /∈ B, then R(wn) = R(C(wn)) . After a reward has been assigned to each token, we compute the ad-vantage , extending the GRPO advantage formulation to our search tree. The intuitive idea is that, while in GRPO/DAPO the output sequences are generated in parallel starting from 

q, and then they are compared to the average reward in Eq. (1), in our case, for each wbn ∈ B, we should compare the subsequences generated from wbn with the reward as-sociated to wbn (i.e., R(wbn)), which gives us the benefit of following that specific path. These subsequences are the branches contained in S(wbn) (see above). Specifically, for each wbn ∈ B, we first compute the standard deviation of its children: σ(wbn) = std ({R(wmi )|wmi ∈ C(wbn)}). Then, for a generic token wm ∈ S(wbn), the advantage is: 

ˆAm = R(wm) − R(wbn)

σ(wbn) . (9) We call Eq. (9) branch advantage , and, using ˆAm, our ob-jective function is: 

JBF S −P O (θ) = E(q,a ∗)∼D ,w n∈T 

"

1

|T | 

X

> wn∈T



min (rn(θ) ˆAn, clip (rn(θ), 1 − ϵl, 1 + ϵh) ˆAn

#

,rn(θ) = πθ (wn|q, P (...P (wn)... ), ..., P (wn)) 

πθold (wn|q, P (...P (wn)... ), ..., P (wn)) ,

(10) where (P (...P (wn)... ), ..., P (wn)) is the path from the question to token wn (wn excluded). At inference time we use standard single-chain sampling (without tree): given a question q, we sample o ∼ πθ (·| q).

4.2. Discussion 

Comparing Eq. (10) with Eq. (2), the main difference is that, while GRPO/DAPO compute the objective func-tion using independently generated sequences ( {oi}Gi=1 ), in BFS-PO we use the chains extracted from the tree ((P (...P (wn)... ), ..., P (wn)) ). During training, we first ex-tract all the complete chains contained in T , i.e., all the full paths from the root to the leaves, and then we fill the training batch using the same number of total sequences used in DAPO (obtained using multiple questions and trees). However, in our case, some of the tokens are shared among different chains (because they have the same ancestors), which implies that we can reuse the previously computed KV cache values and largely increase the efficiency. In our experiments, this leads to an average speedup of 1.37 × with respect to DAPO. In contrast, the training time in TreeRL, is 2× slower than multi-chain sampling (Hou et al., 2025b). Since we use fewer tokens in a batch than DAPO, the ques-tion is: does this lead to a decrease in the diversity of solu-tions observed during training? In Sec. 5 we shows that BFS-PO consistently achieves higher accuracy than DAPO (as well as shorter responses), including in the out-of-domain experiments, which empirically shows that BFS-PO can generalize better than DAPO. We believe that there are two reasons for this. First, as shown in (Wang et al., 2025), the tokens with low entropy are mainly used by an LRM to ensure the linguistic coherence of the sentence, while the highest entropy tokens are the actual decision points which determine the logical trajectory of the generated solution. Thus, the textual difference of the independently generated chains in DAPO does not necessarily correspond to a deep semantic difference. In contrast, at every expansion step of BFS-PO (Alg. 1), we branch the highest entropy node of the current best solution, and in this way we allow the LRM to explore different trajectories by making decisions “logi-cally” different. Second, the reduced diversity of the tokens observed during training with respect to DAPO/GRPO is compensated by the denser supervision used in BFS-PO to compute the token-level rewards and advantages. Indeed, similarly to MCTS and TreeRL, each node of our tree can average the reward obtained by all its descendants, in this way making the reward/advantage estimate more stable than in DAPO. 

## 5. Experiments 

5.1. Experimental Setup 

We adopt the experimental setup of TokenSkip (Xia et al., 2025) (Sec. 1), which we extend with additional out-of-domain experiments and metrics. Specifically, following (Xia et al., 2025), we use three widely adopted reasoning 

benchmarks : GSM8K (Cobbe et al., 2021), MATH-500 (Lightman et al., 2024) and MMLU-STEM (Hendrycks et al., 2021). GSM8K and MATH-500 contain mathematical problems, while MMLU-STEM is based on Science, Tech-nology, Engineering, and Mathematics (STEM) subjects. We use these datasets for both in-domain and out-of-domain experiments following the protocol proposed in (Xia et al., 2025). Moreover, we extend this protocol with additional out-of-domain experiments based on two other common math benchmarks: AIME’25 (Balunovi ´c et al., 2025) and MINERVA-MATH (Lewkowycz et al., 2022). 

Base LRMs. Following (Xia et al., 2025), we use: LLaMA-3.1-8B-Instruct (Grattafiori et al., 2024), Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct (Yang et al., 2024). How-6BFS-PO: Best-First Search for Large Reasoning Models 

ever, unlike TokenSkip, we do not report experiments with Qwen-2.5-Instruct 14B because they are very computation-ally demanding. We believe that, by using only small-to-medium-sized models, our experiments can serve as a basis for comparison for a larger group of competitors. 

Metrics. We measure the correctness of the testing answers using standard Accuracy , and we use Average Length to report the average number of tokens in the reasoning chains (which includes the CoT and the final answer). Average Length measures how concise the reasoning is. Moreover, we use Accuracy Efficiency Score (AES) , proposed in (Luo et al., 2025), which combines accuracy and length in a single metric. AES evaluates the improvement in accuracy and the reduction in computational costs compared to the baseline model (see App. D for more details). 

5.2. Ablation 

We conduct our ablation experiments on GSM8K using Qwen2.5-3B-Instruct and we report the results in Tab. 1. The results of Zero-shot are taken from (Xia et al., 2025), while DAPO refers to our reproduction of DAPO (Yu et al., 2025) using the publicly available code and the hyper-parameter values suggested by the authors. Note that DAPO 

in Tab. 1 includes the length penalty ( Rl(·)) defined in Eq. (3). The only hyper-parameter specific to BFS-PO is the number of expansions K (Alg. 1). If K = 0 , then BFS-PO reduces to DAPO. Tab. 1 shows that, by varying the value of K from 0 ( DAPO ) to 3, all three metrics improve. In all the other experiments of this paper, we use K = 3 .                                                      

> Method K Branch Adv. Len. Penalty Acc. ↑Avg. Len. ↓AES ↑
> Zero-shot ---83.7 315 0.00 DAPO 0-✓85.6 290 0.15 BFS-PO 1✓✓85.9 286 0.17 BFS-PO 2✓✓86.2 271 0.23 BFS-PO 3✓✓86.2 269 0.24
> w/o BFS 3✓✓86.0 311 0.10 Global Adv. 3 ✗✓85.7 271 0.22 w/o Rl(·)3✓✗86.1 275 0.21

Table 1. Ablation experiments using GSM8K and Qwen2.5-3B-Instruct. 

In Tab. 1, w/o BFS refers to our tree search algorithm with-out BFS. Specifically, in this case, the backtracking point is 

not selected using the current best solution but only using the entropy. In more detail, in w/o BFS , we remove line 5 from Alg. 1 and we replace line 6 with: 

oj,t ∗ = arg max    

> oj,t ∈T ,o j,t /∈B

H(oj,t ). (11) We use w/o BFS to evaluate the contribution of a BFS strat-egy in which only the current best solution is expanded.                                                        

> Model Method Acc. ↑Avg. Len. ↓AES ↑
> Qwen2.5-3B-Instruct
> Zero-shot 83.7 315 0.00 SFT 83.4 319 -0.03 TokenSkip 83.2 263 0.14 DAPO 85.6 290 0.15 BFS-PO 86.2 269 0.24
> Llama-3.1-8B-Instruct
> Zero-shot 86.2 213 0.00 Tree of Thoughts 74.0 385 -1.51 SFT 86.7 214 0.01 TokenSkip 81.1 129 0.10 DAPO 87.6 209 0.07 BFS-PO 87.9 189 0.17
> Qwen2.5-7B-Instruct
> Zero-shot 91.4 298 0.00 SFT 91.7 296 0.02 TokenSkip 87.9 178 0.21 DAPO 91.9 277 0.09 BFS-PO 92.3 234 0.24

Table 2. Experimental results with training on GSM8K. 

w/o BFS is similar to TreeRL (Sec. 1 and 2) but with some important differences, the most important of which are: (1) In TreeRL the negative log-likelihood of the next token is used as the uncertainty criterion for tree expansion instead of entropy (Sec. 1); (2) In TreeRL N nodes are expanded simultaneously ( N is an hyper-parameter), while in w/o BFS 

we expand only the maximum entropy node; (3) In w/o BFS 

we use the branch advantage proposed in Sec. 4.1, while in TreeRL the advantage is computed comparing the value of the node with the root and the parent value. The results in Tab. 1 show that w/o BFS can boost the accuracy with re-spect to Zero-shot , which is also slightly better than DAPO .However, it is not able to significantly reduce the average number of generated tokens with respect to the base model (Zero-shot ), which in contrast shows the effectiveness of a BFS strategy in reducing overthinking. We believe this is be-cause our BFS strategy biases the LRM exploration towards progressively shorter solutions, thereby self-generating a training dataset of concise CoT (Sec. 1). The next entry in Tab. 1 is Global Adv. , in which we use Alg. 1 for exploration but we compute the objective func-tion in Eq. (10) using a DAPO/GRPO-like global advan-tage. Specifically, instated of using our branch advantage (Sec. 4.1), in Global Adv. , ˆAn is computed as follows. First, we extract all the complete reasoning chains from T (i.e., full paths from the root to a leaf) and we assign the reward as explained in Sec. 4.1. Then, we treat all these complete rea-soning chains as a single group of M sequences {oi}Mi=1 and we apply Eq. (1) to compute a token-level advantage. Com-pared to DAPO , Global Adv. generates significantly shorter sequences (Tab. 1). However, its accuracy is basically on par with DAPO (despite being better than Zero-shot ), which shows that the branch-based advantage proposed in Sec. 4.1 provides a better supervision signal when training with a search tree. Finally, in w/o Rl(·), we remove the DAPO length penalty (Eq. (3)) from Eq. (4) and we use a reward based only on 7BFS-PO: Best-First Search for Large Reasoning Models                                                                                                                                                 

> Method MATH GSM8K MMLU AIME MINERVA
> (in-domain) (out-of-domain) (out-of-domain) (out-of-domain) (out-of-domain)
> Acc. ↑Avg. Len. ↓AES ↑Acc. ↑Avg. Len. ↓AES ↑Acc. ↑Avg. Len. ↓AES ↑Acc. ↑Avg. Len. ↓AES ↑Acc. ↑Avg. Len. ↓AES ↑
> Llama-3.1-8B-Instruct
> Zero-shot 48.6 503 0.00 86.2 213 0.00 58.5 356 0.00 6.4 873 0.00 19.0 553 0.00 SFT 48.2 505 -0.05 86.0 214 -0.02 58.4 354 -0.01 ------TokenSkip 46.7 349 0.11 82.6 152 0.08 58.1 189 0.44 ------DAPO 50.8 497 0.15 85.8 215 -0.03 61.2 285 0.44 8.6 804 1.11 19.1 554 0.02
> Ours 50.6 398 0.39 86.5 199 0.08 61.7 236 0.67 13.6 585 3.71 22.4 468 0.69
> Qwen2.5-7B-Instruct
> Zero-shot 72.8 573 0.00 91.4 296 0.00 80.0 400 0.00 8.9 852 0.00 22.4 623 0.00 DAPO 73.8 517 0.15 91.6 281 0.06 80.9 357 0.14 6.4 804 -1.35 24.3 549 0.39
> Ours 74.2 463 0.30 92.1 245 0.20 80.7 297 0.28 10.9 646 0.92 24.3 493 0.49
> Table 3. Experimental results with training on MATH-500.

the sequence correctness ( Ri = 1 if v(oi, a ∗) = 1 and 

Ri = −1 if v(oi, a ∗) = 0 ), keeping all the rest of our method unchanged (Sec. 4). The results of w/o Rl(·) are slightly worse than our full method BFS-PO, however they are much better than DAPO , in which Rl(·) is used , which shows that the benefits of BFS-PO depend only marginally from the length penalty. 

5.3. Main Results 

We report our main results in Tab. 2 and 3. Specifically, fol-lowing (Xia et al., 2025), in Tab. 2 we train different models on GSM8K using a token budget of 512 tokens (see App. A), while in Tab. 3 we train on MATH-500 with a token budget of 1024. In Tab. 3 we show also out-of-domain experiments, in which the models, trained on MATH-500, are then tested on MMLU-STEM, AIME’25 and MINERVA-MATH. The last two datasets were added by us as they were not included in the TokenSkip experiments. Following common practice (Hou et al., 2025b), since AIME consists of only 30 ques-tions, it is evaluated 32 times with a sampling temperature of 1 (see App. C for more details). In these tables, DAPO 

refers to our reproduction of DAPO (see Sec. 5.2), while the results of Zero-shot and SFT , taken from (Xia et al., 2025), refer to the baseline without training and to plain SFT, respectively. Since in TokenSkip the compression ratio can be controlled by tuning an hyper-parameter, in Tab. 2 and 3 we report the results of (Xia et al., 2025) correspond-ing to the highest AES values , and we show the others in App. E. Finally, in Tab. 2 we also report the results of Tree of Thought (Yao et al., 2023) (Sec. 1 and 2) taken from (McDonald et al., 2025). The results in Tab. 2 and 3 show that BFS-PO consistently achieves the highest AES, across all datasets, LRM mod-els, and both with in-domain and out-of-domain scenarios .This demonstrates that BFS-PO achieves the best balance between accuracy and CoT length. Moreover, BFS-PO also achieves the highest accuracy in 10 out of 13 experiments (being on par or almost on par with DAPO in the other 3). We not only increase the accuracy with respect to the base model (Zero-shot), but we also outperform DAPO, which is one of the state-of-the-art RL methods for LRMs. For instance, in the AIME’25 out-of-domain experiments, the accuracy margin compared to DAPO is more than 4 points. These results show that conditioning exploration towards shorter responses does not entail the risk of overfit-ting and, indeed, strengthens the generalization capabilities of the model. We also largely outperform Tree of Thought (Tab. 2), despite its search tree is based on an auxiliary LLM that evaluates the nodes to be expanded (Sec. 1 and 2). On the other hand, TokenSkip consistently gets the shortest CoT (BFS-PO being the second best). However, all the results of TokenSkip show an accuracy drop compared with the Zero-shot baseline (Xia et al., 2025), which is in line with most of the works on CoT shortening, all of which are able to generate more concise reasoning chains but only at the cost of also reducing accuracy (Sec. 1 and 2). 

## 6. Conclusion 

We proposed BFS-PO, an RL algorithm for LRM that is based on a BFS strategy on a search tree. Specifically, we avoid the use of a costly PRM by selecting the best solution among the complete tree solutions, where a standard verifier can be applied and the path length measured. We then use the highest entropy tokens for backtracking and tree expan-sion, following recent literature showing the importance of these tokens in LRM decisions. Using different benchmarks and models, we showed that BFS-PO can simultaneously 

increase accuracy and shorten the average CoT length, in stark contrast to state-of-the-art CoT shortening methods that typically sacrifice accuracy for length reduction. 8BFS-PO: Best-First Search for Large Reasoning Models 

Limitations. In our experiments, we used LRMs with 3 to 8B parameters, which is one of the most common settings for CoT shortening methods. While extending the exper-iments using larger models would be interesting, training larger models using RL is very computationally expensive, and we believe the experimental protocol we adopted allows it to be used as a basis for comparison by a larger number of competitors. 

## Acknowledgements 

This work was supported by the PNRR project “Ital-ian Strengthening of Esfri RI Resilience (ITSERR)”, funded by the European Union – NextGenerationEU (CUP B53C22001770006) and by the EU Horizon project ELIAS (No. 101120237). We acknowledge the CINECA award under the ISCRA initiative, for the availability of high-performance computing resources and support. 

## Impact Statement 

This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. 

## References 

Agarwal, A., Sengupta, A., and Chakraborty, T. First fin-ish search: Efficient test-time scaling in large language models. arXiv:2505.18149 , 2025. Arora, D. and Zanette, A. Training language models to reason efficiently. In NeurIPS , 2025. Ballon, M., Algaba, A., and Ginis, V. The relation-ship between reasoning and performance in large lan-guage models – o3 (mini) thinks harder, not longer. 

arXiv:2502.15631 , 2025. Balunovi ´c, M., Dekoninck, J., Petrov, I., Jovanovi ´c, N., and Vechev, M. Matharena: Evaluating LLMs on uncontami-nated math competitions. SRI Lab, ETH Zurich , 2025. Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gi-aninazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., and Hoefler, T. Graph of Thoughts: Solving Elaborate Problems with Large Language Models. In AAAI , 2024. Besta, M., Memedi, F., Zhang, Z., Gerstenberger, R., Piao, G., Blach, N., Nyczyk, P., Copik, M., Kwa ´sniewski, G., M ¨uller, J., Gianinazzi, L., Kubicek, A., Niewiadomski, H., O’Mahony, A., Mutlu, O., and Hoefler, T. Demystify-ing chains, trees, and graphs of thoughts. IEEE Trans. on PAMI , 47(12):10967–10989, 2025. Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., and Colton, S. A survey of monte carlo tree search methods. IEEE Transactions on Computa-tional Intelligence and AI in Games , 4(1):1–43, 2012. Butt, N., Kwiatkowski, A., Labiad, I., Kempe, J., and Ol-livier, Y. Soft tokens, hard truths. arXiv:2509.19170 ,2025. Chen, Z., Ma, X., Fang, G., Yu, R., and Wang, X. Veri-Thinker: learning to verify makes reasoning model effi-cient. In NeurIPS , 2025. Cheng, X., Li, J., Zhang, Z., Tang, X., Zhao, W. X., Kong, X., and Zhang, Z. Incentivizing dual process thinking for efficient large language model reasoning. In NeurIPS ,2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv:2110.14168 , 2021. DeepSeek-AI. DeepSeek-R1 incentivizes reason-ing in LLMs through reinforcement learning. 

arXiv:2501.12948 , 2025. Deng, Y., Prasad, K., Fernandez, R., Smolensky, P., Chaud-hary, V., and Shieber, S. Implicit chain of thought rea-soning via knowledge distillation. arXiv:2311.01460 ,2023. Deng, Y., Choi, Y., and Shieber, S. From explicit cot to implicit cot: Learning to internalize cot step by step. 

arXiv:2405.14838 , 2024. Fatemi, M., Rafiee, B., Tang, M., and Talamadupula, K. Concise reasoning via reinforcement learning. 

arXiv:2504.05185 , 2025. Fu, Y., Chen, J., Zhu, S., Fu, Z., Dai, Z., Zhuang, Y., Ma, Y., Qiao, A., Rosing, T., Stoica, I., and Zhang, H. Efficiently scaling LLM reasoning with certaindex. 

arXiv:2412.20993 , 2025. Geiping, J., McLeish, S. M., Jain, N., Kirchenbauer, J., Singh, S., Bartoldson, B. R., Kailkhura, B., Bhatele, A., and Goldstein, T. Scaling up test-time compute with latent reasoning: A recurrent depth approach. In ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models ,2025. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., et al. The Llama 3 herd of models. arXiv:2407.21783 , 2024. Han, T., Wang, Z., Fang, C., Zhao, S., Ma, S., and Chen, Z. Token-budget-aware LLM reasoning. In ACL , 2025. 9BFS-PO: Best-First Search for Large Reasoning Models 

Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J., and Tian, Y. Training large language models to reason in a continuous latent space. In COLM , 2025. Hassid, M., Synnaeve, G., Adi, Y., and Schwartz, R. Don’t overthink it. Preferring shorter thinking chains for im-proved LLM reasoning. arXiv:2505.17813 , 2025. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In ICLR , 2021. Hou, B., Zhang, Y., Ji, J., Liu, Y., Qian, K., Andreas, J., and Chang, S. Thinkprune: Pruning long chain-of-thought of LLMs via reinforcement learning. arXiv:2504.01296 ,2025a. Hou, Z., Hu, Z., Li, Y., Lu, R., Tang, J., and Dong, Y. TreeRL: LLM Reinforcement learning with on-policy tree search. In ACL , 2025b. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-Rank adaptation of large language models. In ICLR , 2022. Jang, J., Kim, J., Kweon, W., Lee, S., and Yu, H. Verbosity-aware rationale reduction: Sentence-level rationale re-duction for efficient and effective reasoning. In ACL ,2025. Kang, Y., Sun, X., Chen, L., and Zou, W. C3oT: Gen-erating shorter chain-of-thought without compromising effectiveness. In AAAI . AAAI Press, 2025. Kong, D., Zhao, M., Xu, D., Pang, B., Wang, S., Honig, E., Si, Z., Li, C., Xie, J., Xie, S., and Wu, Y. N. La-tent thought models with variational bayes inference-time computation. In ICML , 2025. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, X., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training. In Second Conference on Language Modeling , 2025. LCM team, Barrault, L., Duquenne, P.-A., Elbayad, M., Kozhevnikov, A., Alastruey, B., Andrews, P., Coria, M., Couairon, G., Costa-juss `a, M. R., Dale, D., Elsa-har, H., Heffernan, K., Janeiro, J. M., Tran, T., Rop-ers, C., S ´anchez, E., Roman, R. S., Mourachko, A., Saleem, S., and Schwenk, H. Large concept models: Language modeling in a sentence representation space. 

arXiv:2412.08821 , 2024. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning prob-lems with language models. arXiv:2206.14858 , 2022. Li, G., Gao, Y., Li, Y., and Wu, Y. Thinkless: A training-free inference-efficient method for reducing reasoning redundancy. arXiv:2505.15684 , 2025. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let’s verify step by step. In ICLR , 2024. Lin, T., Xie, J., Yuan, S., and Yang, D. Implicit reasoning in transformers is reasoning through shortcuts. In ACL ,2025a. Lin, Z., Fu, Z., Chen, Z., Chen, C., Xie, L., Wang, W., Cai, D., Wang, Z., and Ye, J. Controlling thinking speed in reasoning models. In NeurIPS , 2025b. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective. arXiv:2503.20783 , 2025. Long, J. Large Language Model guided tree-of-thought. 

arXiv:2305.08291 , 2023. Luo, H., Shen, L., He, H., Wang, Y., Liu, S., Li, W., Tan, N., Cao, X., and Tao, D. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. 

arXiv:2501.12570 , 2025. Ma, X., Wan, G., Yu, R., Fang, G., and Wang, X. Cot-valve: Length-compressible chain-of-thought tuning. In ACL ,2025. McDonald, T., Colosimo, A., Li, Y., and Emami, A. Can we afford the perfect prompt? balancing cost and accuracy with the economical prompting index. In ACL , 2025. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand `es, E., and Hashimoto, T. s1: Simple test-time scaling. 

arXiv:2501.19393 , 2025. Munkhbat, T., Ho, N., Kim, S. H., Yang, Y., Kim, Y., and Yun, S.-Y. Self-training elicits concise reasoning in large language models. In ACL , 2025. OpenAI. Openai o1 system card. arXiv:22412.16720 , 2024. Pang, J., Ye, F., Wong, D. F., He, X., Chen, W., and Wang, L. Anchor-based large language models. In ACL , 2024. Qiao, Z., Deng, Y., Zeng, J., Wang, D., Wei, L., Meng, F., Zhou, J., Ren, J., and Zhang, Y. Concise: Confidence-guided compression in step-by-step efficient reasoning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing , 2025. 10 BFS-PO: Best-First Search for Large Reasoning Models 

Russell, S. and Norvig, P. Artificial Intelligence: A Modern Approach . Prentice Hall, 3 edition, 2010. Saunshi, N., Dikkala, N., Li, Z., Kumar, S., and Reddi, S. J. Reasoning with latent thoughts: On the power of looped transformers. In ICLR , 2025. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. 

arXiv:1707.06347 , 2017. Sel, B., Huang, L., Ramakrishnan, N., Jia, R., and Jin, M. LLMs can reason faster only if we let them. In ICML ,2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseek-math: Pushing the limits of mathematical reasoning in open language models. arXiv:2402.03300 , 2024. Shen, Z., Yan, H., Zhang, L., Hu, Z., Du, Y., and He, Y. CODI: Compressing chain-of-thought into continuous space via self-distillation. In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing , 2025. Song, J., Jo, D., Kim, Y., and Kim, J.-J. Reasoning path compression: Compressing generation trajectories for efficient LLM reasoning. In NeurIPS , 2025. Su, D., Zhu, H., Xu, Y., Jiao, J., Tian, Y., and Zheng, Q. To-ken assorted: Mixing latent and text tokens for improved language model reasoning. In ICML , 2025a. Su, J., Healey, J., Nakov, P., and Cardie, C. Between under-thinking and overthinking: An empirical study of reason-ing length and correctness in LLMs. arXiv:2505.00127 ,2025b. Tan, W., Li, J., Ju, J., Luo, Z., Song, R., and Luan, J. Think silently, think fast: Dynamic latent compression of LLM reasoning chains. In NeurIPS , 2025. Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solv-ing math word problems with process- and outcome-based feedback. arXiv:2211.14275 , 2022. Wang, S., Yu, L., Gao, C., Zheng, C., Liu, S., Lu, R., Dang, K., Chen, X.-H., Yang, J., Zhang, Z., Liu, Y., Yang, A., Zhao, A., Yue, Y., Song, S., Yu, B., Huang, G., and Lin, J. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for LLM reasoning. In NeurIPS , 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS , 2022. Wu, J., Lu, J., Ren, Z., Hu, G., Wu, Z., Dai, D., and Wu, H. LLMs are single-threaded reasoners: Demystifying the working mechanism of soft thinking. arXiv:2508.03440 ,2025. Xia, H., Leong, C. T., Wang, W., Li, Y., and Li, W. To-kenSkip: Controllable chain-of-thought compression in LLMs. In Proceedings of the 2025 Conference on Empir-ical Methods in Natural Language Processing (EMNLP) ,2025. Xiao, Y., Wang, J., Yuan, R., Xu, C., Xu, K., Li, W., and Liu, P. LIMOPro: Reasoning refinement for efficient and effective test-time scaling. In NeurIPS , 2025. Xu, X., Wang, S., Han, X., Liu, Z., Wu, H., Li, P., Liu, Z., Sun, M., and He, Z. A*-Thought: efficient reasoning via bidirectional compression for low-resource settings. In 

NeurIPS , 2025. Yang, A., Yang, B., Hui, B., Zheng, B., et al. Qwen2 technical report. arXiv:2407.10671 , 2024. Yang, W., Ma, S., Lin, Y., and Wei, F. Towards thinking-optimal scaling of test-time compute for LLM reasoning. 

arXiv:2502.18080 , 2025. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. R. Tree of thoughts: Deliberate prob-lem solving with Large Language Models. In NeurIPS ,2023. Yi, J., Wang, J., and Li, S. ShorterBetter: guiding reason-ing models to find optimal inference length for efficient reasoning. In NeurIPS , 2025. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., YuYue, Dai, W., Fan, T., Liu, G., Liu, J., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, R., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.-Y., Zhang, Y.-Q., Yan, L., Wu, Y., and Wang, M. DAPO: An open-source LLM reinforcement learning system at scale. In NeurIPS , 2025. Zhang, J., Zhu, Y., Sun, M., Luo, Y., Qiao, S., Du, L., Zheng, D., Chen, H., and Zhang, N. LightThinker: Thinking step-by-step compression. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing , 2025a. Zhang, Z., He, X., Yan, W., Shen, A., Zhao, C., and Wang, X. E. Soft thinking: Unlocking the reasoning potential of LLMs in continuous concept space. In NeurIPS , 2025b. Zhao, H., Yan, Y., Shen, Y., Xu, H., Zhang, W., Song, K., Shao, J., Lu, W., Xiao, J., and Zhuang, Y. Let LRMs break free from overthinking via self-braking tuning. In 

NeurIPS , 2025. 11 BFS-PO: Best-First Search for Large Reasoning Models 

## A. GRPO and DAPO 

The GRPO objective is: 

JGRP O = E(q,a ∗)∼D ,{oi}Gi=1 ∼πθold (·| q)

 1

G

> G

X

> i=1

1

|oi|

> |oi|

X

> t=1



min( ri,t (θ) ˆAi,t , clip (ri,t (θ), 1 − ϵ, 1 + ϵ) ˆAi,t − βD KL (πθ || πref )) 

 , (12) where: 

ˆAi,t = Ri − mean ({Ri}Gi=1 )

std ({Ri}Gi=1 ) , ri,t (θ) = πθ (oi,t |q, o i,<t )

πθold (oi,t |q, o i,<t ) . (13) In Eq. (12), the Kullback-Leibler divergence DKL (πθ || πref ) is used to regularize the difference between the online policy (πθ ) and the frozen reference policy ( πref ), where the latter is usually the base LRM model before the RL fine-tuning starts. 

β is an hyper-parameter which weights this KL penalty, while ϵ is the clipping-related hyper-parameter introduced in PPO for stabilizing training (Schulman et al., 2017). In DAPO (Yu et al., 2025), the KL penalty is removed, while the clipping operation is bounded below by ϵl and above by ϵu.Moreover, Yu et al. (2025) observe that, if all the G answers are correct or if they are all wrong, then the advantage is 0. Therefore, they re-sample those groups of outputs which do not contain at least one correct and one incorrect answer (see below). Finally, Yu et al. (2025) show that the normalization term 1 

> |oi|

in Eq. (12), which divides the objective function value of a given sample oi by its length, leads to penalizing long correct answers ( v(oi, a ∗) = 1 ) but also has the opposite effect, i.e. it decreases the penalty for long incorrect answers ( v(oi, a ∗) = 0 ) (Fatemi et al., 2025; Tan et al., 2025). Therefore, they replace this term by a group-based normalization formulated as 1PGi=1 |oi| :

JDAP O (θ) = E(q,a ∗)∼D ,{oi}Gi=1 ∼πθold (·| q)

 1

PGi=1 |oi|

> G

X

> i=1
> |oi|

X

> t=1



min (ri,t (θ) ˆAi,t , clip (ri,t (θ), 1 − ϵl, 1 + ϵh) ˆAi,t 



s.t. 0 < |{ oi|v(oi, a ∗) = 1 }| < G. 

(14) As mentioned above, the constraint 0 < |{ oi|v(oi, a ∗) = 1 }| < G is used to filter out trivial questions (where all the answers are correct) and questions which are too difficult for the current policy model πθ , both leading to a 0 advantage. Finally, very long sequences are penalized using Soft Overlong Punishment, defined as: 

Rl(o) = 



0 |o| ≤ Lmax − Lcache  

> (Lmax −Lcache )−| o|
> Lcache

Lmax − Lcache < |o| ≤ Lmax 

−1 Lmax < |o|,(15) and the final reward is computed using: 

Ri =

(

1 + Rl(oi) v(oi, a ∗) = 1 

−1 + Rl(oi) v(oi, a ∗) = 0 . (16) In all our experiments with DAPO we adopt the hyper-parameter values suggested in (Yu et al., 2025): ϵl = 0 .2, ϵh = 0 .28 ,

Lcache = 15 Lbudget and Lmax = Lbudget − Lcache , where Lbudget is the maximum number of tokens which can be generated by the LRM (and depends on the specific computational resources). Moreover, for a fair comparison, in all experiments with BFS-PO, we also use the same hyper-parameter values (ϵl = 0 .2, ϵh = 0 .28 , Lcache = 15 Lbudget ,

Lmax = Lbudget − Lcache ), which are kept fixed without any variation. 12 BFS-PO: Best-First Search for Large Reasoning Models 

## B. Auxiliary Algorithms 

Reward assignment for leaf nodes. As mentioned in Sec. 4.1, if wn is a leaf, we can extract the complete rea-soning path recursively applying P (·) until we reach the root ( q), obtaining the output sequence o = ( w1, ...w n) = (P (...P (wn)... ), ..., P (P (wn)) , P (wn), w n). Once o has been extracted, we can compute both v(o, a ∗) and |o|, which are the only values needed to compute the reward using Eq. (4). 

Sub-tree construction. Once T is built, given a backtracking token wbn ∈ B, Alg. 2 shows how S(wbn) can be constructed. 

Algorithm 2 Sub-Tree Construction 

Input: tree T , backtracking token wbn ∈ B.

S ← {} 

Children ← C(wbn)

for i = 1 to |Children | do 

wt ← Children [i]

oi ← () 

repeat 

Append (oi, w t)

N extChild ← C(wt)

wt ← N extChild [1] 

until |N extChild | = 1 ▷ Stop when you reach a leaf or a backtracking point 

S ← oi

end for return S

## C. Experimental Details 

We fine-tune all models using LoRA (Hu et al., 2022) with rank r = 8 and α = 16 . We use AdamW with learning rate 

1e −6 and weight decay 0.01 , without learning rate scheduling. In all our experiments we use a generation batch size of 

4096 tokens, a training batch size of 512 tokens, with a mini-batch size of 32 , corresponding to 16 training steps per training batch. As mentioned in Sec. A and 3, for both BFS-PO and DAPO, we adopt the hyper-parameter values used in DAPO (Yu et al., 2025): ϵh = 0 .28 and ϵl = 0 .2 for clipping, and Lcache = 15 Lbudget , Lmax = Lbudget − Lcache for Rl(·).Following TokenSkip (Xia et al., 2025), when training on GSM8K, we set the maximum token budget Lbudget = 512 , while for training on MATH-500, we use Lbudget = 1024 . When testing, again following (Xia et al., 2025), we use Lbudget = 512 

for GSM8K and MMLU-STEM, and Lbudget = 1024 for MATH-500. We use Lbudget = 1024 also for AIME’25 and MINERVA-MATH. All experiments are run on 8 NVIDIA A100 SXM4 GPUs with 64GB memory. Training BFS-PO on GSM8K requires 12 hours, while training BFS-PO on MATH-500 requires 24 hours. 

## D. Accuracy Efficiency Score 

The AES metric proposed in (Luo et al., 2025) evaluates the trade-off between improving accuracy and reducing computa-tional costs. It weights and sums the differences between the evaluated model and the baseline, taking into account both the average length of the reasoning chains and their accuracy. In our case, the baseline corresponds to the zero-shot performance of the model. The terms ∆Length and ∆Acc represent these relative changes, and they are computed as follows (Luo et al., 2025): 

∆Length = Length baseline − Length model 

Length baseline 

, ∆Acc = Acc model − Acc baseline 

Acc baseline 

(17) Using these terms, the AES score is defined as: 13 BFS-PO: Best-First Search for Large Reasoning Models 

AES =

(

α · ∆Length + β · | ∆Acc |, if ∆Acc ≥ 0

α · ∆Length − γ · | ∆Acc |, if ∆Acc < 0. (18) The coefficients α, β and γ are used to weight the different components of the score. In our experiments we use the default hyper-parameters values suggested in (Luo et al., 2025), which are α = 1 , β = 3 , and γ = 5 .

## E. Additional Results 

In this section, we extend the results of TokenSkip shown in Sec. 5.3 using additional perfomance values corresponding to all the compression factors reported in (Xia et al., 2025). Specifically, TokenSkip utilizes a ratio hyper-parameter, inserted in the prompt, to regulate the compression level of the reasoning chain. In-domain and out-of-domain experiments with Llama-3.1-8B-Instruct are reported in Table 4 and Table 5. Further results involving the Qwen2.5 model family are shown in Table 6. Unless otherwise specified, the results refer to in-domain experiments. The AES results reported in these tables show the efficacy of BFS-PO compared to TokenSkip independently of the chosen compression ratio . Although TokenSkip can generate more concise reasoning chains, it leads to a substantial reduction in accuracy with respect to Zero-shot. In contrast, BFS-PO improves the accuracy compared to the base LRM and achieves significantly higher AES values across all tested models and datasets. 

Llama-3.1-8B-Instruct 

Methods Ratio GSM8K MATH-500 

Acc. ↑ Avg. Len. ↓ AES ↑ Acc. ↑ Avg. Len. ↓ AES ↑

Zero-shot - 86.2 213 0.00 48.6 503 0.00 TokenSkip 1.0 86.7 214 0.02 48.2 505 -0.05 0.9 86.1 198 0.06 47.8 448 0.03 0.8 84.3 170 0.09 47.3 399 0.07 0.7 82.5 150 0.08 46.7 349 0.11 0.6 81.1 129 0.10 42.0 318 -0.31 0.5 78.2 113 0.01 40.2 292 -0.44 BFS-PO - 87.9 189 0.17 50.6 398 0.39  

> Table 4. Extended comparison with TokenSkip using Llama-3.1-8B-Instruct (in-domain experiments).

Llama-3.1-8B-Instruct 

Methods Ratio GSM8K (out-of-distribution) MMLU-STEM (out-of-distribution) 

Acc. ↑ Avg. Len. ↓ AES ↑ Acc. ↑ Avg. Len. ↓ AES ↑

Zero-shot - 86.2 213 0.00 58.5 356 0.00 TokenSkip 1.0 86.0 214 -0.02 58.4 354 0.00 0.9 84.9 202 -0.02 59.4 327 0.16 0.8 83.7 175 0.03 59.3 286 0.27 0.7 82.6 152 0.08 58.9 257 0.31 0.6 79.8 137 -0.01 59.2 225 0.43 0.5 76.6 123 -0.13 58.1 189 0.44 BFS-PO - 86.5 199 0.08 61.7 236 0.67  

> Table 5. Extended comparison with TokenSkip using Llama-3.1-8B-Instruct (out-of-domain experiments).

14 BFS-PO: Best-First Search for Large Reasoning Models 

Model Methods Ratio Acc. ↑ Avg. Len. ↓ AES ↑

Qwen2.5-3B-Instruct Zero-shot - 83.7 315 0.00 TokenSkip 1.0 83.4 319 -0.03 0.9 83.2 263 0.13 0.8 81.6 251 0.08 0.7 80.1 233 0.04 0.6 77.3 200 -0.02 0.5 74.4 171 -0.10 BFS-PO - 86.2 269 0.24 

Qwen2.5-7B-Instruct Zero-shot - 91.4 298 0.00 TokenSkip 1.0 91.7 296 0.02 0.9 91.1 255 0.13 0.8 90.1 237 0.13 0.7 89.9 217 0.19 0.6 87.9 178 0.21 0.5 86.0 151 0.20 BFS-PO - 92.3 234 0.26  

> Table 6. Extended comparison with TokenSkip on GSM8K with Qwen2.5-Instruct models (in-domain experiments).

## F. Qualitative Analysis 

In this section, we show how BFS-PO progressively generates shorter correct CoTs during training using the example in Fig. 3. Specifically, given a question, the LRM generates G answers as in GRPO/DAPO (line 2 of Alg. 1). Line 5 of Alg. 1 selects the shortest correct answer o1, which, in this example, has 277 tokens (see Fig. 3). Line 6 selects the first backtracking token wb

> 1

, which corresponds to pieces . Then, line 7 samples G partial output sequences starting from wb

> 1

(i.e., using the text Let ... pieces as a suffix to the question). In the second iteration, BFS-PO selects the new shortest correct solution, and we get o2, which differs from o1 starting from the token her . In o2, the new selected backtracking point wb 

> 2

is pies , which is used for the second expansion. In the third iteration, the new shortest correct output is o3, with only 198 tokens. Note that o3 is 28% shorter than o1.This example qualitatively shows how BFS-PO can reduce redundant verbosity in the generated CoT. For instance, comparing o1 with o2, the sentence are left after the family ate , contained in o1 after the backtracking point pieces , is replaced in o2 after the token her with the more concise sentence: her family ate . This leads to two cleaner math expressions: \frac {2}{3} \times 12 = 8 \text { pieces} and 12 - 8 = 4 \\text{ pieces} instead of the math block present in o1: \\[ 12 \text { pieces} - 8 \text { pieces} = 4\text { pieces} \\] , where the measurement units for all terms of the equation are redundant. Moreover, o3 is even shorter, and it differs from o1 and o2 mainly due to the absence of all units of measurement in the mathematical expressions. For instance, the math calculation present in o2: Each of the remaining 2 whole pumpkin pies was sliced into 6 pieces:\n \\[\n 2 \\text{ pies}\\times 6 \\text{ pieces per pie} =12 \\text{ pieces} is replaced in o3 with Each of the remaining 2 whole pumpkin pies is sliced into 6 pieces.\n 6. Thus, the total number of pie pieces is \\(2 \\times 6 = 12\\) pieces . Importantly, the absence of units of measurement does not lead to a loss of information since the context to which the mathematical terms refer is clear given the text preceding the expression. 15 BFS-PO: Best-First Search for Large Reasoning Models How BFS-PO searches for shorter answers 

Question ( q): Grace baked 4 whole pumpkin pies. She sold 1 whole pumpkin pie and gave 1 whole pumpkin pie to her friend. The remaining whole pumpkin pies were each sliced into 6 pieces. Her family ate 2/3 pieces. How many pieces of pumpkin pie were left? 

Answer: 4 

o1 (277 tokens): 

Let’s break down the problem step-by-step:\n \n 1. Grace baked 4 whole pumpkin pies.\n 2. She sold 1 whole pumpkin pie.\n 3. She gave 1 whole pumpkin pie to her friend.\n 4. Therefore, the number of pumpkin pies remaining is:\n \\[\n 4 - 1 - 1 = 2 \\text{ whole pumpkin pies}\n \\]\n \n 5. Each of the remaining 2 whole pumpkin pies was sliced into 6 pieces:\n \\[\n 2 \\text{ pies} \\times 6 \\text{ pieces per pie} = 12 \\text{ pieces}\n \\]\n \n 6. Her family ate \\(\\frac{2}{3}\\) of the total pieces of pie. To find out how many pieces were eaten:\n \\[\n \\frac{2}{3} \\text{ of } 12 \\text{ pieces} = \\frac{2}{3} \\times 12 = 8 \\text{ pieces}\n \\]\n \n 7. To find out how many pieces are left after the family ate:\n \\[\n 12 \\text{ pieces} - 8 \\text{ pieces} = 4 \\text{ pieces}\n \\]\n \nTherefore , the number of pieces of pumpkin pie left is:\n \\[\n \\boxed{4}\n \\] 

o2 (241 tokens): 

Let’s break down the problem step-by-step:\n \n 1. Grace baked 4 whole pumpkin pies.\n 2. She sold 1 whole pumpkin pie.\n 3. She gave 1 whole pumpkin pie to her friend.\n 4. Therefore, the number of pumpkin pies remaining is:\n \\[\n 4 - 1 - 1 = 2 \\text{ whole pumpkin pies}\n \\]\n \n 5. Each of the remaining 2 whole pumpkin pies was sliced into 6 pieces:\n \\[\n 2 \\text{ pies}\\times 6 \\text{ pieces per pie} = 12 \\text{ pieces}\n \\]\n \n 6. Her family ate \\(\\frac{2}{3}\\) of the total pieces of pie. To find out how many pieces her family ate:\n \\[\n \\frac{2}{3} \\times 12 = 8 \\text{ pieces}\n \\]\n \n 7. To find the number of pieces left:\n \\[\n 12 - 8 = 4 \\text{ pieces}\n \\]\n \nTherefore , the number of pieces of pumpkin pie that were left is \\(\\boxed{4}\\). 

o2 (241 tokens): 

Let’s break down the problem step-by-step:\n \n 1. Grace baked 4 whole pumpkin pies.\n 2. She sold 1 whole pumpkin pie.\n 3. She gave 1 whole pumpkin pie to her friend.\n 4. Therefore, the number of pumpkin pies remaining is:\n \\[\n 4 - 1 - 1 = 2 \\text{ whole pumpkin pies}\n \\]\n \n 5. Each of the remaining 2 whole pumpkin pies was sliced into 6 pieces:\n \\[\n 2 \\text{ pies}\\times 6 \\text{ pieces per pie} = 12 \\text{ pieces}\n \\]\n \n 6. Her family ate \\(\\frac{2}{3}\\) of the total pieces of pie. To find out how many pieces her family ate:\n \\[\n \\frac{2}{3} \\times 12 = 8 \\text{ pieces}\n \\]\n \n 7. To find the number of pieces left:\n \\[\n 12 - 8 = 4 \\text{ pieces}\n \\]\n \nTherefore , the number of pieces of pumpkin pie that were left is \\(\\boxed{4}\\). 

o3 (198 tokens): 

Let’s break down the problem step-by-step:\n \n 1. Grace baked 4 whole pumpkin pies.\n 2. She sold 1 whole pumpkin pie.\n 3. She gave 1 whole pumpkin pie to her friend.\n 4. Therefore, the number of whole pumpkin pies left is \\(4 - 1 - 1 = 2\\).\n 5. Each of the remaining 2 whole pumpkin pies is sliced into 6pieces.\n 6. Thus, the total number of pie pieces is \\(2 \\times 6 = 12\\) pieces.\n 7. Her family ate \\(\\frac{2}{3}\\) of the total pieces.\n 8. The number of pieces her family ate is \\(\\frac{2}{3} \\times 12 = 8\\) pieces.\n 9. Therefore, the number of pieces left is \\(12 - 8 = 4\\).\n \nPutting the final answer within the \\boxed{}:\n \\[\n \\boxed{4}\n \\] 

Figure 3. A qualitative example of the exploration process of BFS-PO using Qwen2.5-3B-Instruct during training on GSM8K. 

16