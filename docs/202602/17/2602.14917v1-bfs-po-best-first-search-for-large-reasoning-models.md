# BFS-PO: Best-First Search for Large Reasoning Models
# BFS-PO：大推理模型的最佳优先搜索

**Authors**: Fiorenzo Parascandolo, Wenhui Tan, Enver Sangineto, Ruihua Song, Rita Cucchiara \\
**Date**: 2026-02-16 \\
**PDF**: https://arxiv.org/pdf/2602.14917v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 6.0 \\
**Evidence**: Introduces a search-based RL algorithm for reasoning, aligning with evolution of heuristics and efficient algorithms \\

---

## Abstract
Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.

## 摘要
OpenAI o1 和 DeepSeek-R1 等大推理模型（LRMs）在利用长推理链处理推理任务时展现出了卓越的性能。然而，这也导致了计算成本的显著增加以及冗长输出的生成，这种现象被称为“过度思考”（overthinking）。强化学习（RL）算法（如 GRPO/DAPO）往往会加剧这种过度思考的倾向。在本文中，我们提出了 BFS-PO，这是一种利用最佳优先搜索探索策略来缓解该问题的强化学习算法。具体而言，BFS-PO 通过一种基于最大熵节点的回溯机制来寻找最短的正确答案。通过在训练过程中生成逐渐缩短的回答，BFS-PO 学习产生简洁的推理链。在不同基准测试和基础大推理模型上的实验表明，BFS-PO 能够同时提高模型的准确性并缩短其回答长度。

---

## 速览摘要（自动生成）

**问题**：大推理模型（LRM）存在输出冗长、计算成本高及