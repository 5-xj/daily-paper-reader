Title: Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs

URL Source: https://arxiv.org/pdf/2602.14697v1

Published Time: Tue, 17 Feb 2026 03:23:17 GMT

Number of Pages: 39

Markdown Content:
# Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs 

Lunjun Zhang 1 Ryan Chen 2 3 Bradly C. Stadie 2 3 

# Abstract 

Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL) , a method for jointly improving model contexts and model weights. In each RL itera-tion, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mu-tation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, up-dated from relative performance within each RL iteration batch. E-SPL encourages a natural di-vision between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME → BeyondAIME) gen-eralization setting, E-SPL improves RL success rate from 38 .8% → 45 .1% while also outperform-ing reflective prompt evolution ( 40 .0% ). Over-all, our results show that coupling reinforcement learning with system prompt evolution yields con-sistent gains in sample efficiency and generaliza-tion. Code: github.com/LunjunZhang/E-SPL 

# 1. Introduction 

Evolutionary algorithms (EA) and reinforcement learning (RL) have long been studied as complementary paradigms for adaptive behavior. Evolutionary methods excel at 

> 1

Department of Computer Science, University of Toronto 

> 2

Department of Statistics and Data Science, Northwestern Univer-sity 3Bridgewater AIA Labs. Correspondence to: Lunjun Zhang 

<lunjun@cs.toronto.edu >.

Preprint. February 17, 2026. AIME BeyondAIME 

Easy-to-Hard Generalization 

→ 

> Agent Evolution Tree
> RL Only Prompt Evolution Evolution + RL
> Eval on BeyondAIME

Figure 1. Evolving System Prompts helps RL generalize better .

population-level exploration over discrete, high-level struc-tures (Koza, 1994; B ¨ack et al., 1997; Lehman & Stanley, 2011a), while reinforcement learning is well suited for fine-grained optimization of policy parameters through trial and error (Williams, 1992; Sutton et al., 1999). Historically, however, these two approaches have largely operated on different representational levels: evolution over programs (Langdon & Poli, 2013) and architectures (Stanley & Mi-ikkulainen, 2002) on the one hand, and gradient-based learn-ing over a fixed set of parameters (Schulman et al., 2015; Mnih et al., 2015) on the other. Bridging these represen-tational levels in a unified and scalable way remains an open challenge, particularly in the context of large language models (LLMs) (Brown et al., 2020), where both the model weights (Jaech et al., 2024) and the model contexts (Wei et al., 2022; Zhou et al., 2022) critically shape behavior. Recent work on prompt evolution and agentic LLMs demon-strates that prompt (or context) engineering is a powerful mechanism for improving test-time performance. Tech-niques such as self-reflection (Shinn et al., 2023; Madaan et al., 2023; Cai et al., 2025) and population-based search (Guo et al., 2023; Fernando et al., 2023; Romera-Paredes et al., 2024; Agrawal et al., 2025) show that prompts can encode substantial declarative knowledge and behavioral pri-ors without modifying model weights. At the same time, RL has emerged as the predominant post-training approach to fundamentally improve base model capabilities (Jaech et al., 1

> arXiv:2602.14697v1 [cs.AI] 16 Feb 2026

Evolutionary System Prompt Learning for RL Population of 

> System Prompts

{(si, (μi, σi))}  

> Sample rollouts
> per from LLM

N

si πθ

{y1, j}N 

> j=1

∼ πθ( ⋅ ∣ s1, x)Selection 

> Compute
> rewards

{yM,j}N 

> j=1

∼ πθ( ⋅ ∣ sM, x)

Select  s1, …, sM R(x, y1,1 )        

> R(x,y1,2 )
> R(x,y1, N)
> R(x,yM,1 )
> R(x,yM,2 )
> R(x,yM,N)
> Value estimate
> under each

si

> V1
> VM
> Update TrueSkill
> Ratings

θ ← θ + α ∇θ J∇θ J(x) = 1

NM ∑ 

> M
> i=1

∑ 

> N
> j=1

(R(x, yi,j) − Vi) ∇θ log πθ(yi,j ∣ si, x)Policy Gradient 

Crossover 

Mutation 

## …

> …
> …

## …

Evolution updates 

system prompts 

RL updates 

weights  θ 

> mean
> mean
> RL sampling for each problem

x

Figure 2. Evolutionary System Prompt Learning (E-SPL) jointly optimizes model contexts and model weights to enhance LLM self-improvement. Evolution updates contexts; RL updates weights. The learned system prompts can encode declarative knowledge via articulated principles and strategies, while RL gradient updates can focus on honing the model’s procedural and implicit knowledge. 

2024; Guo et al., 2025; Chen et al., 2025; Comanici et al., 2025). Despite their complementary strengths, existing ap-proaches typically optimize model contexts and weights in isolation, leaving their potential synergies unexplored. In this work, we argue that effective self-improving agents should jointly evolve their high-level behavioral specifica-tions and their low-level parametric policies. This perspec-tive motivates Evolutionary System Prompt Learning (E-SPL) , a framework that tightly couples reinforcement learning over model weights with evolutionary search over system prompts. By maintaining a population of system prompts that are evaluated, rated, and evolved alongside RL training, E-SPL enables structured exploration over declara-tive knowledge while encoding procedural knowledge into weights. This joint optimization yields a synergistic dy-namics in which improved prompts accelerate learning, and improved weights amplify the benefits of prompt evolution. Experimentally, E-SPL enhances RL performance of from 

56 .3% → 60 .6% on AIME 2025, 50 .0% → 52 .7% on HMMT 2025 November test, and notably, 38 .8% → 45 .1% 

on AIME → BeyondAIME in an easy-to-hard general-ization setting, using a base model of DeepSeek v3.1 (DeepSeek-AI, 2024). E-SPL significantly outperforms prompt evolution as well, showing that weight updates are often necessary for improving the model’s tacit knowledge and intuition. On an agentic search task (Jin et al., 2025), we improved RL performance from 44 .2% → 48 .6% on OpenAI’s gpt-oss-120b (Agarwal et al., 2025) with limited data. Together, these results show that coupling RL with system prompt evolution unlocks a synergistic form of self-improvement that neither approach achieves in isolation. 

# 2. Related Work 

Prompt Evolution and Contextual Memory A grow-ing body of work treats prompts and contexts as optimiz-able objects that can encode high-level behaviors without modifying model weights. Evolutionary and search-based approaches such as EvoPrompt (Guo et al., 2023), Prompt-Breeder (Fernando et al., 2023), Automatic Prompt Engi-neer (Zhou et al., 2022), and FunSearch (Romera-Paredes et al., 2024) demonstrate that population-based search us-ing operators like selection, mutation, and crossover can substantially improve performance at test time. Iterative context refinement methods, including Reflexion (Shinn et al., 2023), Self-Refine (Madaan et al., 2023), Training-free GRPO (Cai et al., 2025), Dynamic Cheatsheet (Suzgun et al., 2025), and ACE (Zhang et al., 2025b) further show that self-reflection can drive continued self-improvement. Agentic frameworks such as ReAct (Yao et al., 2022) and AutoGen (Wu et al., 2024) highlight that system prompts and interaction protocols play a central role in steering tool use and long-horizon behaviors. Besides, LLM-based mem-ory mechanisms (Packer et al., 2023; Zhong et al., 2024; Lee et al., 2024; Xu et al., 2025) are also related to our work, since memory is key to helping the agent remember what strategies have worked in the past. A system prompt can be seen as not just a foundational set of instructions, but a memory of discovered behaviors and reward preferences (Ouyang et al., 2025). However, these approaches oper-ate with frozen model parameters, limiting their ability to improve implicit knowledge acquired through experience. 

Scaling RL for LLMs Reinforcement learning has be-come the primary mechanism for further improving a pre-2Evolutionary System Prompt Learning for RL Mutation Only     

> HMMT 23/34 -> 25 AIME -> BeyondAIME
> scores Mutation Crossover
> Mutation + Crossover Mutation Only Mutation + Crossover
> Figure 3. Evolutionary trees of E-SPL . Ruring RL, E-SPL creates an evolutionary tree of system prompts, by re-suing the same data already generated by RL with minimal additional computational overhead. Each genetic operator (mutation or crossover) only requires a sampling server for self-reflection with different context construction strategies, which can be concurrent with RL gradient updates.

Discovered System Prompts for Solving Math Problems (deepseek v3.1) 

[1] For generating functions with a numerator (1 − xM )k and a denominator as a product of geometric series, the coefficient of xN is a sum over j of binomial coefficients times the number of solutions to a Diophantine equation. If 

N < M , only the j = 0 term contributes. For N ≥ M , multiple j may contribute, and the signs from the binomial coefficients must be considered. 

[2] After computations, verify results for consistency across domains: check denominators, symmetry, and physical plausibility. Use alternative methods to recompute critical quantities and detect patterns such as periodicity. [3] Avoid detours and unjustified guesses, including contest heuristics without validation. Trust sound derivations; if inconsistencies persist, explore alternative configurations or identities. [4] To verify modular injectivity, factor differences and check non-vanishing of the cofactor. For complex cases, test small primes; derivative condition is sufficient but not necessary. 

[5] For sums over integer pairs with conditions like gcd( a, b ) = 1 , consider using M ¨obius inversion or changing the order of summation to simplify the expression. 

[6] When free parameters appear but a fixed answer is required, look for cancellations forced by constraints or boundary conditions. Verify that the conclusion is consistent with all given conditions.   

> Figure 4. Discovered Strategies in learned System Prompts for solving math problems. Those explicit behavior specifications include: useful heuristics and tips for various categories of problems, self-verification strategies such as checking for consistency and plausibility, a list of common failure modes, etc. Note that RL is done under diverse system prompts, and does not overfit to any particular one.

trained LLM’s reasoning and agentic capabilities. Early preference-based methods demonstrated the effectiveness of RL for alignment and instruction following (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a;b), while more recent work on RL has focused on improving reasoning and problem-solving, starting with o1 (Jaech et al., 2024), R1 (Shao et al., 2024; Guo et al., 2025), and various further im-provements (Team et al., 2025; Yu et al., 2025; Chen et al., 2025; Zheng et al., 2025; Khatri et al., 2025) to large-scale policy gradients (Williams, 1992; Schulman et al., 2017). However, these methods typically assume a fixed system prompt or interface during training, treating contexts as ex-ogenous rather than as an object of optimization. While recent work (Ye et al., 2025) has attempted to discover new prompts within RL loops, those prompts are generated as additional synthetic training data, which are fundamentally different from system prompts that encode declarative knowl-edge, specify desired behaviors, and articulate strategic prin-ciples. Our work is partly inspired by recent observations on the interplay between system prompts and RL (MacDiarmid et al., 2025), which suggested that system-level instructions can meaningfully shape the effectiveness of RL training. 3Evolutionary System Prompt Learning for RL k = argmax iVi 

> Value Estimate under
> each of

s1, …, sM      

> V1,…,VM(x,yk,1 ,R(x,yk,1 ))

…       

> (x,yk,2 ,R(x,yk,2 ))
> (x,yk,N,R(x,yk,N))
> Agent Trajectories
> under

sk

> Self Re fl ect

sk 

> Propose Edits to
> System Prompt
> Identify the mistakes
> and lessons

ℓ Edit based on sk ℓ

smutate

(smutate, (μ′, σ′))

> Population of
> System Prompts

{(si, (μi, σi))}

Add        

> μ′=μσ′=σ+Δσ

Mutation 

sk

Figure 5. LLM-based Mutation operator in E-SPL. The highest-performing prompt in each iteration undergoes self-reflection on group-wise rollout outcomes. An LLM-generated diff edits the parent into a child system prompt, removing ineffective rules and converting observed mistakes into improved declarative instructions, yielding a refined prompt that re-enters the evolutionary population. Crossover 

> Score Matrix
> Problems where each
> was the top prompt

si

s1

s2

s3

x1 x2 x3

> System
> Prompts

M Problems b

0.9  0.2  0.5  0.6   

> 0.1 0.3

0.8  0.4  

> 0.2

0.6  0.7  0.9 

(s1, (x1))

(s2, (x3))

x4

(s3, (x2, x4))

> Self Re fl ect
> Analyze why each
> did best on

si ρi

> Avg
> 0.55
> 0.40

0.60  skOverall best  

> Propose Edits to
> System Prompt
> Edit the overall
> best prompt

sk

(scrossover, (μ′, σ′))μ′ = (∑i        

> μi
> σ2
> i)(∑i
> 1
> σ2
> i)σ′=1
> ∑i1/σ2
> i
> +Δσ2
> Population of
> System Prompts

{(si, (μi, σi))}

scrossover Add 

(si, ρi)

Figure 6. LLM-based Crossover operator in E-SPL. System prompts are contrasted based on their problem-wise performance within the current RL batch. Guided by these differential strengths and weaknesses, an LLM selectively recombines complementary segments from multiple parent system prompts, yielding a refined child prompt that re-enters the evolutionary population. 

Combining Evolutionary Search with RL Hybrid ap-proaches combining Evolution and RL have long been stud-ied, including ERL (Khadka & Tumer, 2018), CEM-RL (Pourchot & Sigaud, 2018), and PBT (Jaderberg et al., 2017), which all leverage population dynamics to com-plement gradient-based optimization. Open-ended evolu-tionary methods such as Novelty Search (Lehman & Stan-ley, 2011b), MAP-Elites (Mouret & Clune, 2015), and POET (Wang et al., 2019) emphasize exploration over di-verse environments and behaviors, but do not operate at the level of prompts and contexts. More recent LLM-guided systems revive program-level evolution, with FunSearch (Romera-Paredes et al., 2024) and AlphaEvolve (Novikov et al., 2025) demonstrating that evolutionary search over code and algorithms can yield novel discoveries. The Dar-win G ¨odel Machine (DGM) (Zhang et al., 2025a) draws inspiration from G ¨odel machine (Schmidhuber, 2003) to in-clude self-referential self-modification in agentic prompting, but does not include model weight updates during the self-modification process. In contrast, E-SPL jointly optimizes system prompts and model weights in a single training loop. 

Genetic Programming (GP) is a learning paradigm where candidate programs are iteratively refined through mu-tation and crossover operators (Koza, 1994; Banzhaf et al., 1998; Langdon & Poli, 2013) to improve performance. Mod-ern evolutionary approaches have demonstrated strong re-sults across domains such as symbolic regression (Ma et al., 2022), complex scheduling tasks (Nguyen et al., 2017), al-gorithmic discovery (Chen et al., 2023), etc. A language model under a unique system prompt and a particular set of tools can be seen as a program (Lehman et al., 2023; Sho-jaee et al., 2024; Romera-Paredes et al., 2024; Opsahl-Ong et al., 2024), and the LLM can be used to flexibly design the mutation and crossover operators that can continuously gen-erate new members of the population (Hemberg et al., 2024; Agrawal et al., 2025; Novikov et al., 2025). While prior work (Surina et al., 2025) has also considered combining LLM-based genetic operators (Romera-Paredes et al., 2024) with DPO (Rafailov et al., 2023), their focus was primarily on combinatorial optimization tasks, where LLM generates specific pieces of code for problems like Traveling Sales-man Problem (TSP) (Lin, 1965). By contrast, we tackle the much more general setting of LLM post-training, where the LLM needs to generate system prompts that guide the next iteration of itself for general-purpose problem solving. Compared to AI Scientist approaches (Gottweis et al., 2025), we do not use additional agentic scaffold on top of RL. 4Evolutionary System Prompt Learning for RL  

> Figure 7. An example of the system prompt discovered by E-SPL for RL with search engines. Here, the discovered system prompt specifies a workflow that includes: self-verification behaviors, detailed instructions on tool use, critical formatting rules, heuristics for determining the reliability of different information sources, and actionable guidance for avoiding common failure modes.

System Prompt for RL Agent with Search Engine (openai gpt-oss-120b) 

System Prompt Before E-SPL 

You are an expert assistant who solves tasks using a Wikipedia search tool. Here is an example of solving a real question: “Between 2020 and 2025, which year did New York City see the most population growth and how did San Francisco population change in that year?” · · · 

Calling search tool: <tool call> {"name": "search", "arguments": {"query list": ["Population New York City between 2020 and 2025"] }} </tool call> 

· · · 

System Prompt After E-SPL 

You are an expert research assistant. · · · Workflow (follow exactly in this order) · · · 

3. Call the search tool once — Use exactly this syntax · · · 

4. Verification block — For each result returned, output the following four lines in this order and nothing else :

Source: <domain> Summary: <one-sentence summary> Status: VERIFIED (or NOT VERIFIED) Phrase: "<exact phrase>" (only if Status is VERIFIED) 

A result is VERIFIED only if: 

• the exact target phrase appears verbatim in the snippet, and 

• the domain ends with .org , .edu , .gov , is Wikipedia, or a major reputable outlet (e.g., nytimes.com ,

bbc.com , reuters.com , theguardian.com )If the domain is not on this list, mark the result NOT VERIFIED even if the phrase is present. 

5. Refine query (optional) — If all results are NOT VERIFIED, modify the query and repeat Step 3. Critical formatting rules (must be obeyed) 

• All tool calls must be wrapped exactly as <tool call> {... }</tool call> 

• The only user-visible output after verification is the single Answer: line; no bullet points, markdown, blank lines, or extra text 

Positive example (correct flow) 

User: "What other name is used for the 4 × 400 m relay?" Query: "4 x 400 metres relay" <tool call> {"name": "search", "arguments": {"query list": ["4 x 400 metres relay"] }} </tool call> Source: en.wikipedia.org Summary: The 4 x 400 metres relay is also known as the "long relay". Status: VERIFIED Phrase: "long relay" · · · 

Follow exactly this loop for every user query. Your objective is to prove the answer with a verbatim phrase from a reliable source before you everr write the final Answer: line. 

5Evolutionary System Prompt Learning for RL 

# 3. Method 

In this section, we describe how to combine an evolution-ary algorithm (EA) on system prompts with reinforcement learning (RL) on model weights. In a nutshell, we condition RL weight updates on a population of system prompts, and reuse the resulting RL data to jointly evolve this popula-tion via two LLM-driven genetic operators. We design the evolutionary algorithm to be directly compatible with existing RL post-training pipelines, allowing RL to bene-fit from an evolving repertoire of declarative knowledge 

while incurring minimal additional overhead. 

3.1. Reinforcement Learning for Weight Updates 

A reinforcement learning algorithm for LLM typically aims to maximize rewards for the policy πθ while keeping it close to the reference policy πref . Given a system prompt s which contains strategic instructions and declarative knowledge on how to solve tasks, and a task x from dataset D, RL samples 

N rollouts from the current policy yj ∼ πθ (· | s, x ). It computes the rewards rj = R(x, y j ), the value function estimate V = 1

> N

PNj=1 rj , and the policy gradient objective: 

J (θ) = 1

> N

PNj=1 (rj − V ) log πθ (yj |s, x ) − βKL( πθ , π ref )

Followed by weight update θ ← θ + α∇θ J (θ). When using low-rank adaptation (Hu et al., 2022) during RL, the KL regularizer is implicit and thus often ignored ( β = 0 ). We apply the policy gradient objective under a variety of system prompts. Given M system prompts {si}Mi=1 , we sample N solutions per system prompt yi,j ∼ πθ (·| si, x )

and estimate the value estimate Vi for each system prompt. The new policy gradient objective J (θ) becomes: 

ri,j = R(x, y i,j ) Vi = 1

> N

PNj=1 ri,j (1) 

J (θ) = 1

> N M

PMi=1 

PNj=1 (ri,j − Vi) log πθ (yi,j |si, x ) (2) The goal of doing policy gradient conditioned on a variety of system prompts is to better separate sources of knowledge gained from agent experience: declarative knowledge that can be explicitly articulated and stored as principles in the system prompt, versus procedural knowledge that is tacit, intuitive, and only editable in model weights. Conditioning the policy on declarative knowledge during RL allows the weight updates to focus on procedural knowledge. In the next section, we show that such a population of system prompts can be grown from a single root into a tree using an evolutionary algorithm driven by a selection process and two LLM-based genetic operators: mutation and crossover. 

3.2. Evolutionary Algorithm for Context Update 

Our evolutionary algorithm ( EA ) aims to optimize the sys-tem prompt on a given dataset to best elicit model capabili-ties and encode explicit knowledge learned from experience. Here we present a simple EA that (1) mostly re-uses data already generated by RL, (2) leverages LLM in the two ge-netic operators, (3) seamlessly integrates with RL pipelines. Like in classical EA, we keep track of a population of system prompts, and at each step: (1) select the successful ones from the population; let them (2) reproduce new system prompts to add to the population; then (3) evaluate the fitness of different system prompts continuously during training. The population S starts with a single root, and gradually grows into an evolutionary tree. 

Fitness Evaluation Our EA continuously estimates the competence of each system prompt in the population, us-ing the same rollouts already generated by RL. Traditional prompt evolution tends to run a separate fitness evaluation on a held-out validation set, which introduces substantial additional computational overhead; instead, we deliberately reuse the noisy batch statistics produced during RL, so that evolution incurs minimal cost on top of weight updates. A key challenge is that the absolute empirical returns ob-tained by each system prompt are not comparable across training iterations: system prompts are evaluated on differ-ent batches of data and under continually changing model parameters θ. However, within a single RL iteration, all sampled prompts are evaluated under matched conditions – same batch, same weights. We therefore treat each RL iter-ation as a set of group-wise comparisons between system prompts, where only their relative ordering is meaningful and transferable across training iterations. To convert these relative rankings into persistent, compa-rable competence estimates, we use the Bayesian skill rating system TrueSkill (Herbrich et al., 2006), which models each rating as a Gaussian N (μ, σ 2) distribution. Concretely, given value estimates [V1, . . . , V M ] for the sampled prompts [s1, . . . , sM ] and their current ratings 

[( μ1, σ 1), . . . , (μM , σ M )] , we compute the sorted ordering 

v ← argsort([ V1, . . . , V M ]) ,

and update their ratings in the population S as (§B): 

(μi, σ i)Mi=1 ← TrueSkill .update(( μi, σ i)Mi=1 , v),

which performs approximate message passing between dif-ferent ratings. This procedure yields a persistent estimate of each prompt’s competence that can aggregate evidence across iterations despite changing policy weights. 

Selection We select the system prompts used in each train-ing iteration based on their TrueSkill scores. We use an optimistic estimate of each system prompt si’s skill level, which forms our sampling distribution for selection: 

score i = μi + λσ i pi ∝ exp(score i/T ) (3) 6Evolutionary System Prompt Learning for RL 

Algorithm 1 Evolutionary System Prompt Learning for RL 

Input: Problem set D(·), reward function R(·, ·).

Input: RL policy πθ (·); reference policy πref (·).

Input: RL hyper-parameters: N , α, β.

Input: Initial system prompt sroot ; M , pcrossover .

Input: Initial (μ0, σ 0) for TrueSkill ; ∆σ; λ.Initialize S = {(sroot , μ 0, σ 0)}. {Population to Evolve }

while not converged do 

Sample a problem x ∼ D 

Sample M system prompts {(si, μ i, σ i)}Mi=1 ∼ S,each according to probability pi ∝ exp( μi + λσ i)

for i = 1 to M do 

Sample N solutions {yi,j }Nj=1 : yi,j ∼ πθ (· | si, x )

Reward ri,j = R(x, y i,j ), value Vi = 1

> N

PNj=1 ri,j 

end for 

J (θ) = 1

> N M

P

> i

P 

> j

(ri,j − Vi) log πθ (yi,j |si, x )

θ ← θ + α∇θ (J (θ) − βKL( πθ , π ref )) {RL update }

v ← argsort([ V1, . . . , V M ]) {Rank the list of si}{(μi, σ i)}Mi=1 ← TrueSkill .update( {(μi, σ i)}Mi=1 , v)

{Change the ratings in the population S} 

k ← argmax iVi {Only mutate the best prompt sk}

Create mutation self-reflection prompt Ψreflect using 

[( x, y k, 1, r k, 1), . . . , (x, y k,N , r k,N )] .

ℓ ∼ πref (· | Ψreflect ) diff ∼ πref (· | sk, ℓ )

smutate ← git .apply( sk, diff) {Mutation step }S.append(( smutate , μ k, σ k + ∆ σ)) 

if pcrossover > random .rand() then 

Let B ∈ D be a batch of b problems Let Ω = {(si, μ i, σ i)}Mi=1 

(s′, μ ′, σ ′) = Crossover( B, R, Ω, π θ , π ref ) {Alg 2 }S.append(( s′, μ ′, σ ′)) 

end if end while 

where λ > 0 controls the degree of optimism in estimating skill levels, and T is a temperature term; both higher λ

and higher T can encourage exploration. In addition, we apply a sliding window of size K, similar to a replay buffer, so that we only sample from the latest K system prompts. This strategy favors system prompts that perform well while maintaining a high degree of exploration. 

Mutation After evaluating the empirical performance of selected system prompts {si}Mi=1 , we identify the best: 

k ← argmax iVi (4) and only apply mutation on this best candidate sk. This directly adds additional evolutionary pressure on the pop-ulation: in each iteration, only the one system prompt that outperforms all of its competitively selected peers gets to produce a child of its own through mutation . To 

Algorithm 2 Crossover( B, R, Ω, π θ , π ref )

Input: List of b problems B, reward function R(·, ·).

Input: List of M system prompts: Ω.

Input: RL policy πθ (·), reference policy πref (·).Let Φ ∈ RM ×b {Φi,κ is the empirical return of the i-th system prompt in Ω on the κ-th problem in B} 

for (si, μ i, σ i) in Ω do for xκ in B do 

Sample {yi,κ,j }Nj=1 from πθ : yi,κ,j ∼ πθ (· | si, x κ)

{In practice, reuse the rollouts from RL in Alg 1 }

Φi,κ = 1

> N

PNj=1 R(x, y i,κ,j )

end for end for 

k ← argmax i

P

> κ

Φi,κ {sk is the overall top prompt }

φ = [ ] {Problems each si was the top prompt for }

for (si, μ i, σ i) in Ω do 

ρi = [ ] 

for xκ in B do if i = argmax z Φz,κ then 

ρi.append( xκ) {si is the top prompt for xκ}

end if end for 

φ.append(( si, ρ i)) {Analyze why si did best on ρi}

end for 

Create crossover self-reflection prompt Ψcr using φ.

diff cr ∼ πref (· | sk, Ψcr )

scrossover ← git .apply( sk, diff cr ) {Crossover step }

μ′ = ( P

> iμi
> σ2
> i

)/(P 

> i1
> σ2
> i

), σ′ = p1/(P

> i

1/σ 2 

> i

) + ∆ σ2

Return (scrossover , μ ′, σ ′)

run mutation on sk, we first ask the reference LLM πref 

to summarize and self-reflect (using self-reflection prompt 

Ψreflect ) on the mistakes that πθ made in the group of rollouts 

{yk,j }Nj=1 generated under sk and augmented by {rk,j }Nj=1 :

ℓ ∼ πref (· | Ψreflect , (x, y k, 1, r k, 1), . . . , (x, y k,N , r k,N )) 

where the model reasons across all the rollouts {yk,j }Nj=1 

and their outcomes {rk,j }Nj=1 to identify past mistakes and potential strategies to avoid them going forward .Next, we use the lessons ℓ from self-reflection to propose edits on the system prompt sk in a format similar to git diff: 

diff ∼ πref (· | sk, ℓ ) smutate ← git .apply( sk, diff) 

Finally, we add smutate to the population S with TrueSkill 

rating (μk, σ k + ∆ σ); under mutation, a child inherits the parent’s rating but with increased uncertainty. This favors the child in the next round of selection according to Eq (3). 

Crossover The problem with mutation alone is that it does not allow different branches of the evolutionary tree to share 7Evolutionary System Prompt Learning for RL Learned TrueSkill  Test Success Rates  Test Success Rates Learned TrueSkill       

> HMMT 23/34 -> 25 AIME -> BeyondAIME
> Best Best
> Best Best
> scores Mutation Crossover
> Figure 8. The TrueSkill ratings in E-SPL, learned from train-time rankings, are highly predictive of relative test-time performance.
> E-SPL treats each RL iteration as a set of group-wise comparisons between system prompts for TrueSkill message passing. The visualization shows that E-SPL has effectively aggregated noisy, non-stationary train-time comparisons into a stable and informative criterion for evolutionary selection, which guides selection using only train-time rollouts, with no additional evaluation overhead.

knowledge during reproduction. With only mutation present, each branch has to rediscover the same knowledge indepen-dently. Crossover allows for the selective recombination of multiple parents’ knowledge in the creation of an offspring; with this genetic operator, new discovered knowledge can be quickly propagated across the evolutionary tree. Our crossover operator first computes Vi in Eq (1) not just for one problem but a batch, leading to a matrix 

Φ ∈ RM ×b where the rows represent M sampled system prompts {si}Mi=1 and the columns represent b problems in the current batch {xκ}bκ=1 . Each entry Φi,κ represents the average reward of problem xκ obtained under si by πθ . The matrix Φ answers the following question: for each system prompt si, what are the specific problems it outperforms all other system prompts on? This information (as a part of the crossover prompt Ψcr ) is central to self-reflection on how to re-combine existing prompts, as it directly contrasts the effects of different system prompts, grounded in con-crete examples of differential advantages. The goal of the crossover operator is to produce children that inherit and re-combine the best traits of multiple parents. In Algorithm 2, after crossover self-reflection, we apply the proposed git diff edits to sk to obtain scrossover , which we then add to the population S with a TrueSkill rating initial-ized as a precision-weighted average of the parents’ ratings. The main computational overhead of mutation and crossover is self-reflection, which only requires sampling from πref ,and can be performed concurrently with RL updates on πθ .

3.3. Algorithmic Summary 

We designed Evolutionary System Prompt Learning ( E-SPL ) to be maximally compatible with existing RL post-training pipelines for LLMs. In parallel with RL weight up-dates, we use the relative ordering of each system prompt’s average reward to learn TrueSkill ratings, which provide the numerical scores that inform our selection process. To create new system prompts, we redesigned the two genetic operators, mutation and crossover, to be based on LLM self-reflection. The complexity we introduce, such as the prompt construction schemes for genetic operators and the ranking-based fitness evaluation, serves a specific purpose: re-using on-policy RL data for evolution. This brings the benefits of evolution to RL with minimal computational overhead. 

# 4. Experiments 

We evaluate Evolutionary System Prompt Learning (E-SPL) on mathematical reasoning and agentic search benchmarks, with the goal of answering the following questions: (1) Does jointly evolving system prompts and model weights outper-form optimizing either alone? (2) How does E-SPL affect the learning speed and generalization of RL? (3) What are some emergent patterns in the system prompts discovered via E-SPL? (4) What roles do mutation and crossover play in the evolutionary dynamics of E-SPL? 

4.1. Experimental Setup Tasks and datasets. We first focus on mathematical rea-soning tasks with limited data, a regime in which induc-tive biases from evolved system prompts are expected to play a particularly important role. Our main evaluations include: (1) DAPO100 →AIME25 , which follows the pro-tocols from (Cai et al., 2025) that trains on 100 selected problems from the DAPO dataset (Yu et al., 2025) and tests on AIME 2025 competition; (2) HMMT 23/24 →25 , which uses all of HMMT 2023, 2024, and February 2025 prob-lems as training set, and November 2025 problems as test 8Evolutionary System Prompt Learning for RL      

> Figure 9. An example of how the RL model learns to utilize learned system prompts (numbered G1 . . . G30) to solve problems.

Example of How E-SPL Shapes Model Behavior 

Learned System Prompt 

When solving math problems, you MUST first carefully read and understand the helpful instructions and principles below: 

[G1]. When optimizing sums, set parameters to cancel dominant terms. For quadratic forms, choose parameters that minimize the leading coefficients, reducing the problem to analyzing remainder terms. 

· · · 

[G22]. In modular arithmetic and sums, decompose terms via remainders or moduli. Use periodicity for remainder sums and CRT for coprime factors, ensuring multiplicative solutions for square-free moduli. 

· · · 

Problem 

Let Z/n Z be the set of all integers modulo n (hence Z/n Z contains n elements). Find the second smallest composite number n, such that there exists a bijective function g : Z/n Z → Z/n Z such that the 101 functions 

g(x), g (x) + x, g (x) + 2 x, · · · , g (x) + 100 x

are all bijective from Z/n Z to Z/n Z.

Model Output 

Understanding the Condition: Each function hk(x) = g(x) + kx (for k = 0 , 1, . . . , 100 ) must be a permutation of Z/n Z. Since g is bijective, h0(x) = g(x) is bijective by assumption. For k ≥ 1, hk is bijective if and only if for every a ∈ Z/n Z, the equation g(x) + kx = a has a unique solution x.

Alternatively , since g is a permutation, we can set y = g(x) and then x = g−1(y), so hk(g−1(y)) = y + kg −1(y).

But perhaps a better approach is to consider the differences. Note: hk is bijective if and only if the map 

x 7 → g(x) + kx is injective. That is, if g(x) + kx = g(y) + ky then x = y. Equivalently, g(x) − g(y) = −k(x − y).

Since g is arbitrary (but fixed for all k), we need to choose g such that for each k = 1 , . . . , 100 , the equation 

g(x) − g(y) = −k(x − y) implies x = y.

Alternatively , we can think about the differences: for each nonzero d ∈ Z/n Z, we require that g(x + d) −

g(x)̸ = −kd for all x and for all k = 0 , . . . , 100 . Actually, injectivity of hk is equivalent to: if x̸ = y, then 

g(x) + kx ̸ = g(y) + ky, i.e., g(x) − g(y)̸ = −k(x − y). So for each nonzero d, and for each k, we require that 

g(x + d) − g(x)̸ = −kd for all x. But since g is a permutation, this is not easy to control. 

Another idea : Consider the composition hk ◦ g−1. Let f = g−1. Then hk(g−1(y)) = y + kf (y) · · · 

. . . 

But let’s think : For n to work, it must be that all prime factors are at least 103 ? But that gives large numbers. Maybe there is a trick: The number 101 is prime. So perhaps n must be coprime to 101 ! or something. 

Another idea : Consider the function for k = 101 ? Not required. 

Given the instructions, we should use [G22]: In modular arithmetic, use CRT. 

If n = product of primes, then by CRT, the condition holds if and only if it holds for each prime power factor. So for each pa dividing n, there exists a bijection gp on Z/p a such that the 101 functions are bijective. For a prime p > 101 , take g(x) = x works. For p < = 101 , we need to see if there is a bijection on Z/p a.So the smallest composite would be one that has no prime factor <= 101 , i.e., all factors >= 103 . So the smallest is 103 2 = 10609 . But then the second smallest would be 103 × 107 = 11021 . . . . 

Final Answer: 

11021 

Ground truth answer: 11021 

9Evolutionary System Prompt Learning for RL 

Figure 10. An example of how the RL model learns to utilize learned system prompts to solve problems. 

Example of How E-SPL Shapes Model Behavior 

Learned System Prompt 

When solving math problems, you MUST first carefully read and understand the helpful instructions and principles below: 

· · · 

[G26]. After deriving results, perform sanity checks: test small instances, ensure bounds, and verify extreme cases. Re-examine complex steps if results seem off. 

· · · 

Problem 

Real numbers x1, x 2, · · · , x n satisfy |x1| + |x2| + · · · + |x3526 | = 1 . Let M be the maximal possible value of 

1

|x1| + |x1 − x2| + |x1 + x2 − x3| + · · · + |x1 + x2 + · · · + x3525 − x3526 |

Find M mod 1000 .

Model Output 

Let’s analyze the problem step by step: 

Let Sk = x1 + x2 + · · · + xk for k = 1 , 2, . . . , 3526 , with S0 = 0 . Then the denominator becomes: |S1| +

|S1 − S2| + |S2 − S3| + · · · + |S3525 − S3526 |. But note: S1 = x1, S1 − S2 = −x2, S2 − S3 = −x3, · · · ,

S3525 − S3526 = −x3526 . So the denominator is: |S1| + |x2| + |x3| + · · · + |x3526 |. However, this is not exactly matching the given expression. Actually, the given terms are: . . . . . . 

The correct maximum is actually 2, achieved by the first construction with all terms zero except the last. Therefore, M = 2 . Then M mod 1000 = 2 .

So the answer is 2. This is the simplest. 

Given the instructions [G26]: perform sanity checks. 

For n = 2 : variables x1, x2 with |x1| + |x2| = 1 . D = 1 /(|x1| + |x1 − x2|). · · · So for n = 2 , maximum is 2.For n = 3 : maximize D = 1 /(|x1| + |x1 − x2| + |x1 + x2 − x3|). · · · So for n = 3 , we can get D = 4 .In general, for n, the construction is: x1 = a, x2 = a, x3 = 2 a, x4 = 4 a, ..., xn = 2 n−2a. Then |x1| = |a|,

|x1 − x2| = 0 , |x1 + x2 − x3| = 0 , ..., |x1 + ... + xn−1 − xn| = |(2 n−1 − 1) a − 2n−2a| = |(2 n−2 − 1) a|.

Wait, careful : Sn−1 = a + a + 2 a + 4 a + ... + 2 n−3a = (2 n−2 − 1) a + a = 2 n−2a.Then Sn−1 − xn = 2 n−2a − 2n−2a = 0 .So all terms after the first are 0 except the second which is 0.So denominator = |a|.Constraint: |a| + |a| + |2a| + |4a| + ... + |2n−2a| = |a|(2 + (2 n−1 − 2)) = |a| ∗ 2n−1 = 1 , so |a| = 2 1−n.Then D = 2 n−1.For n = 3526 , D = 2 3525 . This is huge, and integer. And it is better than the previous construction. 

So the maximum is M = 2 3525 . Then M mod 1000 = 2 3525 mod 1000 .Now, we need to compute 23525 mod 1000 . · · · So 23525 ≡ 432 (mod 1000) .

So the answer is 432 . This is the intended solution. 

Final Answer :

432 

Ground truth answer: 432 

10 Evolutionary System Prompt Learning for RL Base model 

> Self-Reflection
> Evolution
> RL (CISPO)
> RL (GRPO)
> Evolution + RL
> 45
> 50
> 55
> 60
> 65
> Success Rate %
> 48.6
> 54.6
> 57.3
> 52.6
> 56.3
> 60.6

DAPO100 AIME 25 

> Base model
> Self-Reflection
> Evolution
> RL (CISPO)
> RL (GRPO)
> Evolution + RL
> 35
> 40
> 45
> 50
> 55
> 39.3
> 47.0
> 48.6
> 47.3
> 50.0
> 52.7

HMMT 23/24 25  

> Base model
> Self-Reflection
> Evolution
> RL (CISPO)
> RL (GRPO)
> Evolution + RL
> 35.0
> 37.5
> 40.0
> 42.5
> 45.0
> 36.5
> 39.1
> 40.0
> 38.4 38.8
> 45.1

AIME BeyondAIME      

> Base model Self-Reflection Evolution RL (CISPO) RL (GRPO) Evolution + RL

Figure 11. Results on mathematical reasoning datasets using DeepSeek-v3.1. Evolution + RL (our E-SPL method) performs best compared to self-reflection (prompting only), evolution (prompting only), and RL (which only updates weights). The results show that coupling RL with Evolution on system prompts unlocks a synergistic form of self-improvement that neither approach achieves in isolation. 

set; (3) AIME →BeyondAIME , which uses past years of AIME (AIME 22, AIME 23, and AIME 24) as the training set, and BeyondAIME (ByteDance-Seed, 2025) as test set, which is a collection of original problems with difficulty levels greater than or equal to that of the hardest AIME problems each year. In addition, we also evaluate on (4) Agentic Search on Natural Questions (NQ) (Kwiatkowski et al., 2019) and HotpotQA (Yang et al., 2018), where we subsample 196 problems for training and 64 problems for test; the task is to teach an LLM model to use search engines to answer complex multi-hop questions (Jin et al., 2025). 

Models On math tasks, we use the base model DeepSeek v3.1 (DeepSeek-AI, 2024) for our experiments, which is a 671B MoE (Shazeer et al., 2017) model with 37B active parameters. On agentic search tasks, we use OpenAI’s gpt-oss-120b (Agarwal et al., 2025), which is a 120B MoE model with 5B active parameters. All finetuning is done via low-rank adaptation (Hu et al., 2022) with a rank of 32 .

Methods compared We compare the following variants: • RL only : standard RL post-training with a fixed system prompt; we benchmark the popular GRPO (Shao et al., 2024) and CISPO (Chen et al., 2025) algorithms; • Self-reflection : iterative prompt refinement via reflec-tion (Shinn et al., 2023; Cai et al., 2025; Ouyang et al., 2025), without weight updates. This is essentially our mutation operator on its own, with frozen weights, no crossover operator, and a population size of 1.• Evolution : which runs our evolutionary algorithm (EA) for system prompt search with frozen model weights. Our evolutionary algorithm is very similar to GEPA (Agrawal et al., 2025) except that we do not continuously run evaluation on a separate feedback set, and instead only use train-time rollout statistics just like in standard RL. Like GEPA, our EA works well on its own, and can sometimes outperforms RL. • Evolution + RL (E-SPL) : our full method, jointly evolving model contexts and updating model weights. 

4.2. Main Results 

Figure 11 summarizes performance on math reasoning benchmarks using DeepSeek-v3.1 model. Figure 13 summa-rizes our results on agentic search tasks using OpenAI gpt-oss-120b. Across all datasets, Evolution + RL (E-SPL) con-sistently outperforms baselines that optimize either prompts (evolution-only) or weights (RL-only) in isolation. • On AIME 2025 , E-SPL improves success rate from 

56 .3% → 60 .6% compared to the RL-only (GRPO). • On HMMT 2025 , performance improves from 

50 .0% → 52 .7% after applying E-SPL to RL. • On an easy-to-hard generalization (Sun et al., 2024) setting AIME →BeyondAIME , the gains are the most pronounced, where E-SPL achieves a 38 .8% → 45 .1% 

gain compared to RL-only. • On agentic search tasks, E-SPL improves RL from 

44 .2% → 48 .6% , which demonstrates that E-SPL is not limited to mathematical reasoning, but is also ap-plicable to agentic reasoning with tool use. • On all tasks above, E-SPL outperforms evolution-only as well, with the relative improvements ranging from +5 .8% to +12 .8% on the mathematical reason-ing tasks, and +42 .9% on the agentic search task. Evolution-only consistently outperforms self-reflection, and can sometimes outperform RL-only. On both DAPO100 

→ AIME25 and AIME → BeyondAIME, evolution out-performs RL; on HMMT 23/24 → 25, GRPO slightly out-performs evolution. This finding is consistent with prior findings in GEPA (Agrawal et al., 2025), and validates the design of our proposed evolutionary algorithm on its own. 11 Evolutionary System Prompt Learning for RL 2 4 6 8 10 12 14 16              

> Batch Iterations
> 0.48
> 0.50
> 0.52
> 0.54
> 0.56
> 0.58
> 0.60
> 0.62
> Success Rate of Best Agent
> DAPO100 AIME 25
> 510 15 20 25 30
> Batch Iterations
> 0.40
> 0.42
> 0.44
> 0.46
> 0.48
> 0.50
> 0.52
> 0.54
> HMMT 23/24 25
> 510 15 20 25 30
> Batch Iterations
> 0.36
> 0.38
> 0.40
> 0.42
> 0.44
> 0.46
> AIME BeyondAIME

Self Improvement Progress Comparison     

> Self-Reflection Evolution RL (CISPO) RL (GRPO) Evolution + RL

Figure 12. E-SPL learns faster, and achieves better asymptotic performance that neither RL nor prompt evolution alone could achieve. By studying the learned system prompts (see Figure 4), we hypothesize that E-SPL achieves better sampled efficiency and stronger generalization by encouraging a natural division between declarative knowledge (factual information or global heuristics that can be verbally explained) and procedural knowledge (practical, instinctive “know-how” that relies on intuition developed through repetition). Base model  

> Evolution
> RL only
> Evolution + RL
> 0.00
> 0.15
> 0.30
> 0.45
> Success Rate
> 0.066
> 0.34
> 0.442 0.486

Agentic Search 

gpt-oss-120b (NQ + HotpotQA) 

Figure 13. E-SPL outperforms both (prompt) Evolution and RL on agentic search tasks with openai gpt-oss-120b model. 

Figure 12 visualizes the learning curves across three mathe-matical reasoning settings. E-SPL demonstrates better sam-ple efficiency and higher asymptotic performance compared to both RL-only and evolution-only baselines. Evolution-only often learns the fastest early on since it only has to update model contexts, but obtains lower asymptotic per-formance because it does not update model weights and therefore struggles with improving procedural knowledge. In agentic search, we find that RL can significantly out-perform prompt evolution ( 44 .2% versus 34 .0% ); whereas in math reasoning, RL often performs on-par with evolu-tion (Figure 11). It can often be hard to predict a priori 

whether RL or prompt evolution will perform better on a particular domain, since the learning bottleneck could be an arbitrary mix of declarative and procedural knowledge. E-SPL naturally combines the strengths of both, allowing continual self-improvement to proceed along whichever axis of knowledge is most limiting for the task at hand. Overall, the results above suggest that evolving system prompts and updating model weights play complementary roles. System prompts progressively concentrate explicit, declarative knowledge – such as high-level principles, robust workflows, corner cases, and failure modes (see Figure 4 and Figure 7) – while RL updates the model’s weights to acquire procedural knowledge that is tacit and intuitive. By con-ditioning RL on increasingly informative system prompts, E-SPL allows weight updates to focus on intuitive skills, leading to more sample efficient learning. 

4.3. Emergent Patterns in Learned System Prompts 

We observe that many consistent patterns have emerged from learned system prompts, which we discuss below. 

Generative Self-Verification We find that a repeated theme in learned system prompts is about learning to self-verify a solution in a generative manner (Zhang et al., 2024). For instance, in the agentic search setting, the learned sys-tem prompt (Figure 7) defines a workflow that explicitly labels each search result as VERIFIED versus NOT VERI-FIED based on two criteria: whether the target phrase ap-pears verbatim in the snippet returned by search engine, and whether the domain name ends with .org , .edu , .gov ,is a major reputable news outlet or Wikipedia. It goes on to specify that if all results have NOT VERIFIED status, then the model should repeat the search and continue the loop in its chain of thought. The learned system prompt defines the objective as ‘to prove the answer with a verba-tim phrase from a reliable source before you everr write the final Answer: line’ . The RL process, in turn, does not need to discover new workflows from scratch and encode them in weights, but rather refines and perfects the execution of the workflows specified in the discovered system prompts. Such workflows emerge through E-SPL and are absent from the initial system prompt. In solving math problems, a similar self-verification strat-12 Evolutionary System Prompt Learning for RL Mistakes in Learned E-SPL System Prompts Discovered For a polynomial P (n) = Qmi=1 (n − ai), the largest integer d that divides P (n) for all integers n is d = Q                                                                                                          

> ppep, where
> ep= min rmod p
>  #{i:ai≡r(mod p)}.Only primes p≤max( ai)need be considered, as for larger p, the aiare distinct modulo p, so ep= 0 . For primes where the function mapping ito aimod pis a permutation (e.g., if ai=i3
> and p≡2 mod 3 ), ep=
> jmp
> k
> .
> Corrected For a polynomial P(n) = Qmi=1 (n−ai), the largest integer dthat divides P(n)for all integers nis d=Q
> ppep, where
> ep= min n∈Zvp
>  P(n).Moreover, one always has the lower bound ep≥min rmod p
>  #{i:ai≡r(mod p)}.In particular, for this mod-pbound it suffices to consider primes p≤m, since if p > m then some residue class occurs zero times and the minimum is 0. For primes where the function mapping ito aimod pis a permutation (e.g., if ai=i3and
> p≡2 mod 3 ), the bound gives ep≥
> jmp
> k
> .
> Discovered For graphs with odd-degree vertices, find a minimum T-join by pairing odd vertices along efficient paths. In symmetric graphs such as grids, pair boundary vertices optimally to minimize total length.
> Corrected For graphs with odd-degree vertices, find a minimum T-join by pairing odd vertices via a minimum-weight perfect matching on shortest-path distances. In symmetric graphs such as grids, restrict the matching to symmetry-respecting pairings to minimize total length.
> Table 1. The discovered system prompts sometimes include heuristics that are not always true but still helpful to problem-solving .Unlike RL updates to model weights, the learned system prompts are interpretable and thus can be monitored and corrected. Here we showcase two subtle mistakes in two discovered principles, and what their corrected versions should be (see §C.1 for explanations).

egy emerged. In Figure 10, the learned system prompt has discovered the following instruction: ‘After deriving results, perform sanity checks: test small instances, ensure bounds, and verify extreme cases. Re-examine complex steps if results seem off.’ The full example in Figure 10 illus-trates how the RL model has learned to use this particular self-verification strategy to solve a particular problem on BeyondAIME test set: after initially concluding that ‘So the answer is 2’, the model explicitly cites this strategy from the system prompt, and follows the exact instruction to perform sanity checks by starting from small instances of n = 2 and 

n = 3 , before moving on to the general case and saying 

‘Wait, careful’ when it identified its own mistake. The model then self-corrects and concludes with ‘So the answer is 432. This is the intended solution’ , which is correct. This ex-ample explains E-SPL’s effectiveness: high-level strategies are specified by the system prompt, while RL fine-tunes execution. Rewriting the prompt therefore induces more systematic behavior changes than weight updates alone. 

Codified Domain Expertise and Helpful Heuristics The learned system prompts also have many strategies that are highly domain-specific. For instance, Figure 9 showcases learned system prompts such as ‘When optimizing sums, set parameters to cancel dominant terms. For quadratic forms, . . . ’ (which is specific to problems that optimize sums) and ‘In modular arithmetic and sums, decompose terms via remainders or moduli. Use periodicity for remainder sums and CRT for coprime factors, ensuring multiplicative solutions for square-free moduli.’ (which points out the importance of Chinese Remainder Theorem, or CRT for short, in solving modular arithmetic). Example 9 shows that the RL model persistently explores many potential direc-tions, repeating phrases like ‘Alternatively’ , ‘Another idea’ ,

‘But let’s think’ , before simplifying the problem to a form of modular arithmetic and recognizing that it should apply 

CRT in that situation according to the system prompt. Ex-ample 16 shows that the RL model cited a helpful heuristic in the system prompt suggesting the use of M ¨obius inver-sion under specific conditions, which became a crucial step in the solution. Those examples demonstrate that E-SPL can effectively codify declarative knowledge into the sys-tem prompt – specialized domain expertise and helpful problem-solving heuristics under various circumstances – so that RL can focus on honing core reasoning skills 

rather than memorizing domain knowledge. 

Failure Mode: Missing Caveats Not all discovered strate-gies from E-SPL can hold true in the most general cases. In Table 1, we study two cases where the learned system prompt included heuristics that are not strictly true in gen-eral but are still helpful in limited scenarios. For instance, in analyzing polynomials of a particular form, the discovered strategy misunderstood the lower bound of a min as the actual min. In the other case, the discovered principle for graph theory vaguely outlined an algorithm for finding a minimum T -join by ‘pairing odd vertices along efficient paths’ ; the actual optimal solution requires pairing odd vertices via a minimum-weight perfect matching on shortest-path distances . The likely reason for those mistakes is that E-SPL over-generalized some particular training data points or made mistakes in its self-reflection pipeline. E-SPL is still missing caveats in many of its generalized principles. However, we argue that such mistakes in acquired knowl-edge are more transparent and interpretable than RL gra-dient updates, because they can be more easily monitored 13 Evolutionary System Prompt Learning for RL 5 10 15 20 25 30        

> Batch Iterations
> 0.400
> 0.425
> 0.450
> 0.475
> 0.500
> 0.525
> Success Rate of Best Agent
> HMMT 23/24 25
> (Same Difficulty)
> 510 15 20 25 30
> Batch Iterations
> 0.36
> 0.38
> 0.40
> 0.42
> 0.44
> AIME BeyondAIME
> (Easy-to-Hard Generalization)

Effect of Using Crossover in E-SPL   

> Mutation + Crossover + RL Mutation + RL RL Only (GRPO)

Figure 14. Crossover operator in evolution speeds up early learning , but obtains mixed results in terms of final performance. On AIME → BeyondAIME, Mutation + RL gets to 45 .1% success rate, while Mutation + Crossover + RL gets to 42 .5% ; on HMMT, using Crossover operator results in slightly better performance ( 52 .7% vs 51 .6% ). In either case, Evolution + RL outperforms RL-only. 

and edited by humans. A model trained by RL alone will also pick up on certain assumptions that are only partially correct, but the best mechanistic interpretability tools to-day (Gao et al., 2024; Dunefsky et al., 2024; Kharlapenko et al., 2025) still cannot interpret those weight updates or edit the default behaviors in a straightforward manner. By comparison, E-SPL represents a part of learned knowledge as explicit, propositional statements, which are much more suited for further verification, formalization (Trinh et al., 2024; Hubert et al., 2025), and scalable oversight (Leike et al., 2018; Bowman et al., 2022; McAleese et al., 2024). 

4.4. Ablation Studies Effect of Crossover Figure 14 analyzes the contribution of genetic operators. Adding crossover to mutation accel-erates early learning, consistent with its role in propagat-ing high-fitness substructures across evolutionary branches. However, its effect on final performance is mixed: on AIME → BeyondAIME, mutation-only achieves higher fi-nal accuracy, while on HMMT, mutation + crossover per-forms slightly better. Both variants outperform RL-only ,confirming the value of evolutionary search over system prompts regardless of genetic operator choice. Figure 3 visualizes evolutionary trees produced during train-ing. As shown, the crossover operator enables knowledge transfer across different branches of the evolutionary tree, but at the cost of reduced speciation – long-term separation between evolutionary lineages. To better understand the tradeoffs of using crossover, we draw attention to prior work (Golberg, 1989; Eshelman, 1991; Mahfoud, 1995; Vose, 1999) that supports our observations: • crossover can accelerate short-term progress by mixing useful substructures, • but it can also wash out subpopulation structure, reduce long-run diversity, and increase the risk of premature homogenization and convergence. HMMT BeyondAIME 

> 0.35
> 0.40
> 0.45
> 0.50
> 0.55
> 0.60
> Success Rate  0.510
> 0.416
> 0.527
> 0.425

Sampling Policy for Evolution 

(Reference vs Online RL Policy) 

> Evolution with RL Model
> Evolution with Ref Model

Figure 15. Using a fixed reference policy rather than the online RL policy for prompt evolution is important to performance. 

Mutation-only behaves more like independent local hill-climbs along separate lineages. Lineages stay separated longer, so we can keep parallel hypotheses alive (Holland, 1992). Mutation + Crossover accelerates early progress by re-combining partial improvements, but the population may rapidly converge around a locally high-performing system prompt, limiting exploration of qualitatively and structurally distinct high-quality solutions (Spears, 1993). Classically, this tradeoff can be significantly improved by a class of techniques called niching , which include fitness sharing (Goldberg et al., 1987), mating restriction (Deb & Goldberg, 1989), and the island model (Whitley et al., 1999). We leave the application of niching to E-SPL for future work. 

Sampling Policy for Evolution Figure 15 evaluates the ef-fect of using a fixed reference policy πref for system prompt evolution, rather than the continually changing RL policy 

πθ . Using the fixed reference policy πref for evolutionary up-dates consistently improves the final performance of E-SPL. This supports our design choice to keep evolutionary opera-tors stationary: allowing the editor itself to drift alongside the policy introduces instability and degrades the quality of system prompt edits. A fixed reference policy provides a stable basis for self-reflection, mutation, and crossover. 14 Evolutionary System Prompt Learning for RL   

> Figure 16. An example of how the RL model learns to utilize learned system prompts to solve problems (also see Figure 22).

Example of How E-SPL Shapes Model Behavior 

Learned System Prompt 

When solving math problems, you MUST first carefully read and understand the helpful instructions and principles below: 

· · · [G2]. For sums over integer pairs with conditions like gcd( a, b ) = 1 , consider using M ¨obius inversion or changing the order of summation to simplify the expression. · · · 

Problem 

Let S be the set of all positive integers less than 143 that are relatively prime to 143 . Compute the number of ordered triples (a, b, c ) of elements of S such that a + b = c.

Model Output 

Let’s solve the problem step by step: Step 1 : Factorize 143 = 11 × 13 . So, S consists of positive integers < 143 that are coprime to both 11 and 13 .

· · · 

Step 5 : Use M ¨obius inversion (Principle [G2]). Let f (n) be the number of pairs (a, b ) ∈ S × S with a + b = n.We want P142  

> n=1

f (n) · 1gcd( n, 143)=1 . · · · · · · 

Step 11 : Apply inclusion-exclusion. · · · Therefore, the number of ordered triples (a, b, c ) is 5940 .

Final Answer :

5940 

Ground truth answer: 5940 

# 5. Future Directions 

The fundamental reason why Evolutionary System Prompt Learning can work is that the base LLMs already have strong in-context learning and instruction following abilities. The RL process in itself does not fully take advantage of this – RL performs trial and error conditioned on an exogenous piece of context, without having the model think about how it can directly re-program itself by re-writing its own system instructions. E-SPL is a step towards augmenting RL with self-rewrite , by jointly evolving a population of system prompts. We outline a few promising future directions: 

Agentic Retrieval for Long System Prompts To build agents that can absorb an arbitrary amount of knowledge, the learned system prompt will eventually need a more refined sub-structure with many long components. A natural solution is to store certain components of the system prompt in the local file-system, and have the model access them in an agentic manner using a terminal and grep commands. 

Self-Write as an RL Problem So far, the LLM used for re-writing its own system prompt does not self-improve, which means that eventually it might become a bottleneck. It is possible to extend RL to the entire self-reflection and self-write pipeline, but this would require additional design. 

Self-Referential Recursive Self-Improvement Our cur-rent context and weight update algorithms are likely sub-optimal; ultimately, the model should be allowed to make paradigm-level changes to its own learning algorithm. This can be achieved by allowing the model to directly modify its own training script (and its own weights), therefore achiev-ing fully self-referential self-improvement (Schmidhuber, 2003; 2005). A system that can successfully self-modify in this manner to drive reliable, compounding, open-ended improvements in its own capabilities is also called Seed AI (Yudkowsky, 2007). 

# 6. Conclusion 

In this work, we showed that Evolutionary System Prompt Learning can facilitate RL for LLMs. On a variety of reason-ing benchmarks, jointly evolving the system prompt along with gradient updates to the model weights can boost sam-ple efficiency, asymptotic performance, and easy-to-hard generalization of RL. A major limitation of the current ap-proach is that it is not yet fully self-referential, but our work makes another step towards that goal by allowing the model to self-modify its own system-level instructions. We hope that further research into combining Evolution and RL for self-rewrite can eventually lead to self-referential Seed AI. 15 Evolutionary System Prompt Learning for RL 

# References 

Agarwal, S., Ahmad, L., Ai, J., Altman, S., Applebaum, A., Arbus, E., Arora, R. K., Bai, Y., Baker, B., Bao, H., et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925 , 2025. Agrawal, L. A., Tan, S., Soylu, D., Ziems, N., Khare, R., Opsahl-Ong, K., Singhvi, A., Shandilya, H., Ryan, M. J., Jiang, M., et al. Gepa: Reflective prompt evolution can outperform reinforcement learning. arXiv preprint arXiv:2507.19457 , 2025. B ¨ack, T., Fogel, D. B., and Michalewicz, Z. Handbook of evolutionary computation. Release , 97(1):B1, 1997. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with rein-forcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022a. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-non, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022b. Banzhaf, W., Francone, F. D., Keller, R. E., and Nordin, P. Genetic programming: an introduction: on the auto-matic evolution of computer programs and its applica-tions . Morgan Kaufmann Publishers Inc., 1998. Bowman, S. R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., Luko ˇsi ¯ut ˙e, K., Askell, A., Jones, A., Chen, A., et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540 ,2022. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. 

Advances in neural information processing systems , 33: 1877–1901, 2020. ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads. Hugging Face repository , 2025. Cai, Y., Cai, S., Shi, Y., Xu, Z., Chen, L., Qin, Y., Tan, X., Li, G., Li, Z., Lin, H., et al. Training-free group relative policy optimization. arXiv preprint arXiv:2510.08191 ,2025. Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585 , 2025. Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu, Y., et al. Sym-bolic discovery of optimization algorithms. Advances in neural information processing systems , 36:49205–49233, 2023. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 , 2025. Deb, K. and Goldberg, D. E. An investigation of niche and species formation in genetic function optimization. In Proceedings of the third international conference on Genetic algorithms , pp. 42–50, 1989. DeepSeek-AI. Deepseek-v3 technical report, 2024. URL 

https://arxiv.org/abs/2412.19437 .Dunefsky, J., Chlenski, P., and Nanda, N. Transcoders find interpretable llm feature circuits. Advances in Neural Information Processing Systems , 37:24375–24410, 2024. Eshelman, L. J. Preventing premature convergence in ge-netic algorithms by preventing incest. In Proceedings of Fourth International Conference on Genetic Algorithms, 1991 , 1991. Fernando, C., Banarse, D., Michalewski, H., Osindero, S., and Rockt ¨aschel, T. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797 , 2023. Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J. Scal-ing and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093 , 2024. Golberg, D. E. Genetic algorithms in search, optimization, and machine learning. Addion wesley , 1989(102):36, 1989. Goldberg, D. E., Richardson, J., et al. Genetic algorithms with sharing for multimodal function optimization. In 

Genetic algorithms and their applications: Proceedings of the Second International Conference on Genetic Algo-rithms , volume 4149, pp. 414–425. Lawrence Erlbaum, Hillsdale, NJ, 1987. Gottweis, J., Weng, W.-H., Daryin, A., Tu, T., Palepu, A., Sirkovic, P., Myaskovsky, A., Weissenberger, F., Rong, K., Tanno, R., et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864 , 2025. 16 Evolutionary System Prompt Learning for RL 

Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-centivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. Connecting large language mod-els with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532 , 2023. Hemberg, E., Moskal, S., and O’Reilly, U.-M. Evolving code with a large language model. Genetic Programming and Evolvable Machines , 25(2):21, 2024. Herbrich, R., Minka, T., and Graepel, T. Trueskill™: a bayesian skill rating system. Advances in neural informa-tion processing systems , 19, 2006. Holland, J. H. Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence . MIT press, 1992. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. ICLR , 1(2):3, 2022. Hubert, T., Mehta, R., Sartran, L., Horv ´ath, M. Z., ˇZu ˇzi ´c, G., Wieser, E., Huang, A., Schrittwieser, J., Schroecker, Y., Masoom, H., et al. Olympiad-level formal mathematical reasoning with reinforcement learning. Nature , pp. 1–3, 2025. Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., Vinyals, O., Green, T., Dunning, I., Simonyan, K., et al. Population based training of neural networks. arXiv preprint arXiv:1711.09846 , 2017. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Car-ney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720 , 2024. Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516 , 2025. Kaufmann, E., Capp ´e, O., and Garivier, A. On bayesian upper confidence bounds for bandit problems. In Artificial intelligence and statistics , pp. 592–600. PMLR, 2012. Khadka, S. and Tumer, K. Evolution-guided policy gradient in reinforcement learning. Advances in Neural Informa-tion Processing Systems , 31, 2018. Kharlapenko, D., Shabalin, S., Barez, F., Conmy, A., and Nanda, N. Scaling sparse feature circuit finding for in-context learning. arXiv preprint arXiv:2504.13756 , 2025. Khatri, D., Madaan, L., Tiwari, R., Bansal, R., Duvvuri, S. S., Zaheer, M., Dhillon, I. S., Brandfonbrener, D., and Agarwal, R. The art of scaling reinforcement learn-ing compute for llms. arXiv preprint arXiv:2510.13786 ,2025. Kingma, D. P. Adam: A method for stochastic optimization. 

arXiv preprint arXiv:1412.6980 , 2014. Koza, J. R. Genetic programming as a means for program-ming computers by natural selection. Statistics and com-puting , 4(2):87–112, 1994. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for ques-tion answering research. Transactions of the Association for Computational Linguistics , 7:453–466, 2019. Langdon, W. B. and Poli, R. Foundations of genetic pro-gramming . Springer Science & Business Media, 2013. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000) , pp. 1207–1216, Stan-ford, CA, 2000. Morgan Kaufmann. Lee, K.-H., Chen, X., Furuta, H., Canny, J., and Fischer, I. A human-inspired reading agent with gist memory of very long contexts. arXiv preprint arXiv:2402.09727 , 2024. Lehman, J. and Stanley, K. O. Abandoning objectives: Evo-lution through the search for novelty alone. Evolutionary computation , 19(2):189–223, 2011a. Lehman, J. and Stanley, K. O. Abandoning objectives: Evo-lution through the search for novelty alone. Evolutionary computation , 19(2):189–223, 2011b. Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., and Stanley, K. O. Evolution through large models. In 

Handbook of evolutionary machine learning , pp. 331– 366. Springer, 2023. Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871 ,2018. Lin, S. Computer solutions of the traveling salesman prob-lem. Bell System Technical Journal , 44(10):2245–2269, 1965. Ma, H., Narayanaswamy, A., Riley, P., and Li, L. Evolving symbolic density functionals. Science Advances , 8(36): eabq0279, 2022. 17 Evolutionary System Prompt Learning for RL 

MacDiarmid, M., Wright, B., Uesato, J., Benton, J., Ku-tasov, J., Price, S., Bouscal, N., Bowman, S., Bricken, T., Cloud, A., et al. Natural emergent misalignment from reward hacking in production rl. arXiv preprint arXiv:2511.18397 , 2025. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems , 36:46534–46594, 2023. Mahfoud, S. W. Niching methods for genetic algorithms .University of Illinois at Urbana-Champaign, 1995. McAleese, N., Pokorny, R. M., Uribe, J. F. C., Nitishinskaya, E., Trebacz, M., and Leike, J. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215 , 2024. Minka, T. P. A family of algorithms for approximate Bayesian inference . PhD thesis, Massachusetts Institute of Technology, 2001. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-land, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. nature , 518(7540): 529–533, 2015. Mouret, J.-B. and Clune, J. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909 , 2015. Nguyen, S., Mei, Y., and Zhang, M. Genetic programming for production scheduling: a survey with a unified frame-work. Complex & Intelligent Systems , 3(1):41–66, 2017. Novikov, A., V ˜u, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J., Mehrabian, A., et al. Alphaevolve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131 , 2025. Opsahl-Ong, K., Ryan, M. J., Purtell, J., Broman, D., Potts, C., Zaharia, M., and Khattab, O. Optimizing instruc-tions and demonstrations for multi-stage language model programs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pp. 9340–9366, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730–27744, 2022. Ouyang, S., Yan, J., Hsu, I., Chen, Y., Jiang, K., Wang, Z., Han, R., Le, L. T., Daruki, S., Tang, X., et al. Rea-soningbank: Scaling agent self-evolving with reasoning memory. arXiv preprint arXiv:2509.25140 , 2025. Packer, C., Fang, V., Patil, S., Lin, K., Wooders, S., and Gonzalez, J. Memgpt: Towards llms as operating systems. 

ArXiv , 2023. Pourchot, A. and Sigaud, O. Cem-rl: Combining evolution-ary and gradient-based methods for policy search. arXiv preprint arXiv:1810.01222 , 2018. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimiza-tion: Your language model is secretly a reward model. 

Advances in neural information processing systems , 36: 53728–53741, 2023. Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S., Wang, P., Fawzi, O., et al. Mathematical discoveries from program search with large language models. Nature , 625 (7995):468–475, 2024. Schmidhuber, J. G ¨odel machines: self-referential uni-versal problem solvers making provably optimal self-improvements. arXiv preprint cs/0309048 , 2003. Schmidhuber, J. Completely self-referential optimal re-inforcement learners. In International Conference on Artificial Neural Networks , pp. 223–233. Springer, 2005. Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region policy optimization. In International conference on machine learning , pp. 1889–1897. PMLR, 2015. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. 

arXiv preprint arXiv:1707.06347 , 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Push-ing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 

arXiv preprint arXiv:1701.06538 , 2017. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal rein-forcement learning. Advances in Neural Information Processing Systems , 36:8634–8652, 2023. Shojaee, P., Meidani, K., Gupta, S., Farimani, A. B., and Reddy, C. K. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400 , 2024. 18 Evolutionary System Prompt Learning for RL 

Spears, W. M. Crossover or mutation? In Foundations of genetic algorithms , volume 2, pp. 221–237. Elsevier, 1993. Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. Gaussian process optimization in the bandit set-ting: No regret and experimental design. arXiv preprint arXiv:0912.3995 , 2009. Stanley, K. O. and Miikkulainen, R. Evolving neural net-works through augmenting topologies. Evolutionary com-putation , 10(2):99–127, 2002. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in neural information processing systems , 33:3008–3021, 2020. Sun, Z., Yu, L., Shen, Y., Liu, W., Yang, Y., Welleck, S., and Gan, C. Easy-to-hard generalization: Scalable align-ment beyond human supervision. Advances in Neural Information Processing Systems , 37:51118–51168, 2024. Surina, A., Mansouri, A., Quaedvlieg, L., Seddas, A., Via-zovska, M., Abbe, E., and Gulcehre, C. Algorithm discov-ery with llms: Evolutionary search meets reinforcement learning. arXiv preprint arXiv:2504.05108 , 2025. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems , 12, 1999. Suzgun, M., Yuksekgonul, M., Bianchi, F., Jurafsky, D., and Zou, J. Dynamic cheatsheet: Test-time learning with adaptive memory. arXiv preprint arXiv:2504.07952 ,2025. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599 , 2025. Trinh, T. H., Wu, Y., Le, Q. V., He, H., and Luong, T. Solv-ing olympiad geometry without human demonstrations. 

Nature , 625(7995):476–482, 2024. Vose, M. D. The simple genetic algorithm: foundations and theory . MIT press, 1999. Wang, R., Lehman, J., Clune, J., and Stanley, K. O. Poet: open-ended coevolution of environments and their opti-mized solutions. In Proceedings of the genetic and evolu-tionary computation conference , pp. 142–151, 2019. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824–24837, 2022. Whitley, D., Rana, S., and Heckendorn, R. B. The island model genetic algorithm: On separability, population size and convergence. Journal of computing and information technology , 7(1):33–47, 1999. Williams, R. J. Simple statistical gradient-following algo-rithms for connectionist reinforcement learning. Machine learning , 8(3):229–256, 1992. Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling , 2024. Xu, W., Liang, Z., Mei, K., Gao, H., Tan, J., and Zhang, Y. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110 , 2025. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhut-dinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In 

Proceedings of the 2018 conference on empirical methods in natural language processing , pp. 2369–2380, 2018. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations , 2022. Ye, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Reward-guided prompt evolving in reinforcement learning for llms. In Forty-second Interna-tional Conference on Machine Learning , 2025. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025. Yudkowsky, E. Levels of organization in general intelli-gence. In Artificial general intelligence , pp. 389–501. Springer, 2007. Zhang, J., Hu, S., Lu, C., Lange, R., and Clune, J. Darwin godel machine: Open-ended evolution of self-improving agents. arXiv preprint arXiv:2505.22954 , 2025a. Zhang, L., Hosseini, A., Bansal, H., Kazemi, M., Ku-mar, A., and Agarwal, R. Generative verifiers: Re-ward modeling as next-token prediction. arXiv preprint arXiv:2408.15240 , 2024. Zhang, Q., Hu, C., Upasani, S., Ma, B., Hong, F., Kamanuru, V., Rainton, J., Wu, C., Ji, M., Li, H., et al. Agentic con-text engineering: Evolving contexts for self-improving 19 Evolutionary System Prompt Learning for RL 

language models. arXiv preprint arXiv:2510.04618 ,2025b. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071 , 2025. Zhong, W., Guo, L., Gao, Q., Ye, H., and Wang, Y. Memo-rybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Arti-ficial Intelligence , volume 38, pp. 19724–19731, 2024. Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are human-level prompt engineers. In The eleventh international conference on learning representations , 2022. Zhu, R. J. and Qiu, Y. Ucb exploration for fixed-budget bayesian best arm identification. arXiv preprint arXiv:2408.04869 , 2024. 20 Evolutionary System Prompt Learning for RL 

# A. Appendix Contents 

A.1. LLM-Based Self-Reflection for Mutation and Crossover 

E-SPL implements both mutation and crossover as LLM-driven editing operators over past system prompts. The mutation operator consists of (i) a self-reflection stage that diagnoses performance from RL rollouts (which has two steps: trajectory summarization and self-critique), followed by (ii) a patch synthesis stage that applies structured edits to the parent prompt using a git-style diff (which has two steps: local edit proposals and local-to-global aggregation). On the other hand, the crossover operator has a single stage. 

Mutation is Multi-Stage. In each RL iteration of Alg. 1, we identify the best-performing system prompt 

k ← arg max  

> i

Vi, Vi = 1

N

> N

X

> j=1

R(x, y i,j ), (5) where the returns are computed under matched conditions for all sampled prompts. 

Self-Reflection Module in Mutation Let Ek = {(x, y k,j , r k,j )}Nj=1 denote the rollouts produced under sk. The self-reflection prompt Ψreflect instructs the reference model πref to summarize these trajectories ( Trajectory Summarization ,§A.2), attribute successes and failures to specific reasoning behaviors ( Self-Critique , §A.3), and produce lessons ℓ:

ℓ ∼ πref (· | Ψreflect , Ek). (6) 

Propose Local Edits (§A.4) Given the reflection output ℓ, we generate concrete edits to the parent system prompt: 

diff ∼ πref (· | sk, ℓ ) (7) for each problem x in the batch B. Each diff is a list of local edits. Now we need a mechanism to aggregate those individual per-problem edits across a batch of problems. 

Local-to-Global Aggregation (§A.5) To reconcile overlapping edits to the system prompt proposed by parallel self-reflection processes, E-SPL aggregates these edits using a consolidation prompt that produces a global revision plan consisting of: • modify : refine an existing principle using accumulated evidence, • merge : combine semantically similar principles into a more general form. Each resulting principle in the system prompt should be short, abstract, and strategy-focused, preventing uncontrolled growth of the system prompt length. We then apply the global editing operations: 

smutate ← git .apply( sk, diff) (8) The resulting child system prompt is added to the population with inherited rating and increased uncertainty as in Alg. 1. The diff format is especially important as system prompts grow longer. 

Crossover is Single-Stage. The crossover operator (Alg. 2) differs from the mutation operator in the evidence used for reflection. For a batch of problems B, we compute the empirical return matrix 

Φi,κ = 1

N

> N

X

> j=1

R(xκ, y i,κ,j ), (9) by reusing rollouts already generated by RL. For each system prompt si, we identify which problems ρi in the batch B

that si did best on, yielding a list φ of prompt-specific wins. The crossover reflection prompt Ψcr is constructed from φ to contrast the differential strengths of multiple parent prompts. The reference model then proposes edits applied to the overall top-performing prompt sk:

diff cr ∼ πref (· | sk, Ψcr ), scrossover ← git .apply( sk, diff cr ), (10) producing a child prompt that selectively recombines complementary instruction segments from multiple parents. 21 Evolutionary System Prompt Learning for RL Batch of 

> Problems
> b

{(si, (μi, σi))}  

> Batch of
> Problems
> bBatch of
> Problems
> bBatch of
> Problems
> b

Select System Prompts M

Same batch of problems b

s1 s2 sM…

> Mean
> Accuracy
> Per-problem
> Accuracy
> Mean
> Accuracy
> Per-problem
> Accuracy
> Mean
> Accuracy
> Per-problem
> Accuracy

…  

> Sort Mean Accuracy to
> get relative rankings
> Update TrueSkill
> ratings
> Get top system prompt
> in the batch: sk
> Self Re fl ection Module
> in Mutation
> New

smutate

Adjust ratings 

Add new system prompt             

> Find problems where
> each was the top prompt si
> Trajectories
> ∼πθ(⋅∣x1:b,s1)
> Trajectories
> ∼πθ(⋅∣x1:b,s2)

… Trajectories      

> ∼πθ(⋅∣x1:b,sM)
> Self Re fl ection Module
> in Crossover

Sample from  πθ

Population of 

System Prompts  

> New

scrossover

Add new system prompt 

Add new system prompt 

from mutation 

Add new system prompt 

from crossover 

Performance of each  si

Policy Gradient Update to  πθRL Update 

Figure 17. Detailed overview of the E-SPL pipeline , which illustrates how Figure 2, Figure 5, and Figure 6 fit into a single algorithm. 

A.2. Trajectory Summarization in Mutation Self-Reflection 

Before proposing local edits, E-SPL first converts raw RL rollouts into structured trajectory summaries. For the selected parent prompt sk, all sampled solutions are grouped by problem instance x. Let Rx = {(x, y k,j , r k,j )}Nj=1 denote the group of rollouts produced for problem x under sk. E-SPL only performs self-reflection on problems where the model has both made some mistakes and achieved some success (0 < V i < 1) . Self-reflection focuses on cases where we can contrast 

successes and failures under identical instructions. For each rollout in a problem-specific group, the reference model πref is prompted to generate a step-by-step summary describing: (i) the reasoning steps taken, (ii) which principles or strategies in the system prompt were (implicitly) used, and (iii) where detours or errors occurred, given the rollout outcome and ground-truth answer. These summaries preserve the full reasoning outcomes while explicitly highlighting failure modes, producing a compact representation suitable for higher-level self-critique. 

A.3. Self-Critique in Mutation Self-Reflection 

For each problem x, E-SPL constructs a self-critique prompt that jointly presents: • multiple trajectory summaries from Rx, labeled as successful or incorrect, • the current system prompt sk,• the ground-truth answer. 22 Evolutionary System Prompt Learning for RL Trajectory 

> Summarization

⋯                   

> Self Re fl ection Module in Mutation
> Problem x1
> Agent Trajectories
> ∼π(⋅∣sk,x1)
> Problem x2
> Agent Trajectories
> ∼π(⋅∣sk,x2)
> Problem xbAgent Trajectories
> ∼π(⋅∣sk,xb)

⋯

> Trajectory
> Summarization
> Trajectory
> Summarization
> Self Critique
> Self Critique
> Self Critique

⋯ ⋯

> Propose
> Local Edits
> Propose
> Local Edits
> Propose
> Local Edits

⋯  

> Local-to-Global
> Aggregation Edits to sk

Figure 18. Self-Reflection Module in Mutation (where the best-performing system prompt sk is edited) consists of four sequential modules: trajectory summarization (§A.2), self-critique (§A.3), local edit proposal (§A.4), local-to-global aggregation (§A.5). 

The reference model πref is instructed to analyze these contrasted trajectories to: 1. identify reasoning patterns that consistently led to success, 2. diagnose systematic failure modes, and consider additional guidance that would be needed to avoid those failure modes, 3. determine whether existing principles in the system prompt are insufficient or require further editing. Based on this analysis, the model proposes a number of structured update operations: • add : introduce a new general principle (or high-level strategy / repeatable workflow) to the system prompt; • modify : refine an existing principle to better encode successful strategies or avoid observed errors. Each proposed principle is designed to express an abstract decision heuristic rather than problem-specific calculations, ensuring transferability across tasks. 

A.4. Local Edit Proposal Module 

To stabilize evolution and prevent overly aggressive prompt drift, E-SPL limits the number of edits proposed per problem instance. Specifically, each per-problem self-critique stage is restricted to produce at most Kops edit operations, where 

Kops is a tunable hyperparameter (set to Kops = 2 in our experiments). The goal of this design is to encourage incremental knowledge accumulation and localized corrections driven by concrete failures. Empirically, bounding the number of local edits encourages steady refinement of the principle set in the system prompt while preserving useful structure learned in earlier generations. 

A.5. Local-to-Global Aggregation Module 

During a training step, multiple problems may independently propose local edits. Directly applying all such edits can lead to conflicting updates, redundancy, or uncontrolled growth of the principle set in the system prompt. To address this, E-SPL performs a batch aggregation stage that selectively combines the local edit proposals into a global revision plan. Given the selected system prompt sk and the collection of proposed edits, the reference model is prompted to: • refine individual edits to incorporate repeated evidence, • merge semantically overlapping principles or strategies into more general formulations. The resulting revision plan consists of two types of operations: 23 Evolutionary System Prompt Learning for RL 

• modify operation on an existing principle, which aggregates all proposed edits to that principle across the batch, • merge operation, which replaces several related principles with a unified higher-level strategy / more universal rule across domains. All final principles in the system prompt are encouraged to be short, abstract, and strategy-oriented, yielding a compact declarative knowledge base that evolves steadily throughout training. We use the following hyper-parameters: • LoRA (Hu et al., 2022) rank of 32, learning rate α=4e-5, using Adam optimizer (Kingma, 2014) with β = (0 .9, 0.95) .• Batch size of 10 (problems per batch). Group size N = 5 for the number of rollouts to sample per system prompt. • Sliding window size K = 10 for the population S. Number of system prompts per batch M = 3 .• Uncertainty in genetic operators: ∆σ = 1 , uncertainty bonus: λ = 2 .0; the crossover probability pcrossover = 0 .2.• TrueSkill (all defaults): μ0 = 25 , σ0 = 25 /3, beta= 25 /6, tau= 25 /300 , draw probability= 0.10 .• We find that a simplified selection strategy is quite effective: always add the system prompt with the highest score 

(score i = μi + λσ i) to the batch, but randomly sample from the rest of the sliding window. • Kops = 2 , where Kops is the maximum number of local edits per problem in the mutation operator. 

Root node system prompt for agentic search: 

SEARCH_TASK_INSTRUCTIONS = """You are an expert assistant who solves tasks using aWikipedia search tool. Here are instructions for how to solve a problem: 1. Think step by step before calling the tool and after you receive the result of the tool call. Decide what queries to call the tool with. 2. Call the tool with the queries you have decided on. 3. Think step by step again after you receive the result of the tool call. If you have the information you need, you can stop here. 4. Otherwise, come up with new queries that combine information from the previous results. 5. Include your final answer after the "Answer:" prefix. The answer should be between one to five words. Here is an example of solving a real question: "Between 2020 and 2025, which year did New York City see the most population growth and how did San Francisco population change in that year?" 1. Think step by step: In order to answer this question, I need to know the population of New York City and San Francisco between 2020 and 2025. I will search for the population of New York City in each year 2. Calling search tool: <tool_call>{"name": "search", "arguments": {"query_list": ["Population New York city between 2020 and 2025"]}}</tool_call> (Output omitted for brevity) 3. Think step by step again: I have the population of New York City in each year, and I see that the population of New York City grew the most in 2024. I need to know the population of San Francisco in 2024. I will search for the population of San Francisco in each year. <tool_call>{"name": "search", "arguments": {"query_list": ["Population San Francisco between 2023 and 2024"]}}</tool_call> (Output omitted for brevity) 4. Answer: The population of New York City grew the most in 2024, and the population of San Francisco changed by XXXX in 2024. """ 

24 Evolutionary System Prompt Learning for RL 

# B. TrueSkill-Based Fitness Modeling for E-SPL 

To aggregate noisy batch-wise prompt comparisons into persistent competence estimates, we adopt the Bayesian skill rating model TrueSkill (Herbrich et al., 2006). Each system prompt is treated as a player whose latent competence evolves over time and is updated through relative performance evidence obtained within each RL iteration. Unless otherwise stated, we use default TrueSkill hyperparameters: μ0 = 25 .0, σ0 = 25 .0/3, β = 25 .0/6, τ = 25 .0/300 , and draw probability parameter pdraw = 0 .1. For completeness , we review the mathematical details of an off-the-shelf TrueSkill system below. 

Skill and Performance Model. Each system prompt si is associated with a Gaussian skill variable 

zi ∼ N (μi, σ 2 

> i

),

where μi denotes estimated competence and σi its uncertainty. Observed performance is modeled as a noisy skill: 

pi = zi + ϵi, ϵi ∼ N (0 , β 2),

so that the marginal performance variance is 

Var( pi) = σ2 

> i

+ β2.

Dynamics. Before each rating update, we apply Gaussian drift τ in skill space to model temporal uncertainty growth: 

σ2 

> i

← σ2 

> i

+ τ 2.

Ranking Observations and Tie Encoding. Within each RL iteration, matched rollouts yield scalar returns {Vi}Mi=1 for selected prompts. Sorting these values produces a ranking from best to worst. Ties are encoded by an integer array ties 

such that adjacent positions i and i + 1 are treated as a draw if ties [i] = ties [i + 1] , and otherwise as a win for position 

i over i + 1 . This yields a chain of M − 1 adjacent pairwise constraints. 

Draw Margin. We first compute a global draw margin in performance-difference units: 

ϵabs = Φ −1

 pdraw + 1 2

 √2 β. 

We denote ϕ(·) and Φ( ·) as the probability density function (pdf) and cumulative distribution function (cdf) of the standard normal distribution N ; the Φ−1(·) function above is the inverse (probit) function of Φ.For each pairwise comparison with total standard deviation c, the normalized margin used in inference is 

ϵ = ϵabs 

c .

Truncated Gaussian Correction Functions. For a normalized performance difference variable 

t = μi − μj

c ,

the one-sided (win/loss) correction terms are 

vwin (t, ϵ ) = ϕ(t − ϵ)Φ( t − ϵ) ,wwin (t, ϵ ) = vwin (t, ϵ ) vwin (t, ϵ ) + t − ϵ,

and the two-sided (draw) terms are 

vdraw (t, ϵ ) = ϕ(−ϵ − t) − ϕ(ϵ − t)Φ( ϵ − t) − Φ( −ϵ − t) ,wdraw (t, ϵ ) = v2 

> draw

+ (ϵ − t)ϕ(ϵ − t) − (−ϵ − t)ϕ(−ϵ − t)Φ( ϵ − t) − Φ( −ϵ − t) .

where ϕ and Φ denote the standard normal pdf and cdf. 25 Evolutionary System Prompt Learning for RL 

Approximate Inference for Full Rankings. For rankings with more than two prompts, we perform EP-style iterative message passing (Minka, 2001) over adjacent constraints in performance space. 

Performance-space priors. For each system prompt si and its corresponding rating N (μi, σ 2 

> i

), define 

vi = σ2 

> i

+ β2, π(0)  

> i

= 1

vi

, τ (0)  

> i

= μi

vi

.

Messages. For each adjacent constraint connecting system prompts i = k and j = k + 1 , we maintain Gaussian messages 

mk→i = ( πk→i, τ k→i) and mk→j , initialized to zero. 

Cavity distributions. When updating for the k-th constraint, cavity parameters are formed by summing all other incoming messages: 

π\ki = π(0)  

> i

+ X

> c̸=k

πc→i, τ \ki = τ (0)  

> i

+ X

> c̸=k

τc→i,

and similarly for j. The corresponding cavity moments are 

μ\ki = τ \ki

π\ki

, σ2 

> i, \k

= 1

π\ki

.

Moment update. Let 

c =

q

σ2 

> i, \k

+ σ2

> j, \k

, t = μ\ki − μ\kj

c , ϵ = ϵabs 

c .

If the pair is a draw, use (vdraw , w draw ); otherwise use (vwin , w win ). The updated performance-space moments are 

μpost  

> i

= μ\ki + σ2

> i, \k

c v, μpost  

> j

= μ\kj − σ2

> j, \k

c v, σ2 

> i, post

= σ2 

> i, \k

1 − σ2

> i, \k

c2 w

!

, σ2 

> j, post

= σ2 

> j, \k

1 − σ2

> j, \k

c2 w

!

.

Message extraction. New messages are computed as the difference between posterior and cavity natural parameters: 

πk→i = max 0, 1

σ2

> i, post

− π\ki

!

,τk→i = μpost 

> i

σ2

> i, post

− τ \ki ,

and analogously for j. Precision is clamped to be nonnegative for numerical stability. These updates are iterated over all adjacent constraints for a fixed number of sweeps or until message changes fall below a small threshold. 

Final performance posteriors. After convergence, 

πperf  

> i

= π(0)  

> i

+ X

> k

πk→i, τ perf  

> i

= τ (0)  

> i

+ X

> k

τk→i,

yielding 

μperf  

> i

= τ perf 

> i

πperf 

> i

, σ2 

> i, perf

= 1

πperf 

> i

.

26 Evolutionary System Prompt Learning for RL 

Mapping Back to Skill Space. Let the skill prior after dynamics be N (μi, σ 2 

> i

) and define ci = σ2 

> i

+β2. Performance-space natural parameters for posterior and prior are 

πpost = 1

σ2

> i, perf

, τpost = μperf 

> i

σ2

> i, perf

, πprior = 1

ci

, τprior = μi

ci

.

The truncation-induced message is 

πmsg = πpost − πprior , τmsg = τpost − τprior .

This message is mapped through the performance-noise link as 

πskill msg = πmsg 

1 + β2πmsg 

, τ skill msg = τmsg 

1 + β2πmsg 

,

when πmsg > 0. The skill posterior is then 

πskill  

> i

= 1

σ2

> i

+ πskill msg , τ skill  

> i

= μi

σ2

> i

+ τ skill msg ,

with moments 

σ2 

> i

= 1

πskill 

> i

, μi = τ skill 

> i

πskill 

> i

.

Selection Scores. Prompt sampling is guided by an upper confidence estimate (with λ = 2 .0) (Srinivas et al., 2009; Kaufmann et al., 2012; Zhu & Qiu, 2024): score i = μi + λ σ i

Initialization for Mutation and Crossover. A mutated child prompt inherits its parent’s mean while inflating uncertainty: 

μchild = μk, σchild =

q

σ2 

> k

+ ∆ σ2.

For crossover among parents {i}, we perform precision-weighted fusion: 

μ′ =

P 

> i

μi/σ 2

> i

P 

> i

1/σ 2

> i

, σ′2 = 1

P 

> i

1/σ 2

> i

+ ∆ σ2.

Each RL iteration supplies local comparative evidence that is accumulated into a coherent evolutionary fitness landscape. 27 Evolutionary System Prompt Learning for RL 

# C. Explanations for the identified mistakes in discovered principles 

C.1. Mistake #1 in Discovered Principles Original statement. For a polynomial P (n) = Qmi=1 (n − ai), the largest integer d that divides P (n) for all integers n

is d = Q

> p

pep , where ep = min r mod p

 #{ i : ai ≡ r (mod p) }. Only primes p ≤ max( ai) need be considered, as for larger p, the ai are distinct modulo p, so ep = 0 . For primes where the function mapping i to ai mod p is a permutation (e.g., if ai = i3 and p ≡ 2 mod 3 ), ep =

j mp

k

.

Correction. For a polynomial P (n) = Qmi=1 (n − ai), the largest integer d that divides P (n) for all integers n is 

d = Q 

> p

pep , where ep = min n∈Z vp

 P (n). Moreover, one always has the lower bound ep ≥ min r mod p

 #{ i : ai ≡ r

(mod p) }. In particular, for this mod-p bound it suffices to consider primes p ≤ m, since if p > m then some residue class occurs zero times and the minimum is 0. For primes where the function mapping i to ai mod p is a permutation (e.g., if 

ai = i3 and p ≡ 2 mod 3 ), the bound gives ep ≥

j mp

k

.

Explanation of the Mistake We first recall the original principle. Let 

P (n) = 

> m

Y

> i=1

(n − ai),

where a1, . . . , a m ∈ Z. It was claimed that the largest integer dividing P (n) for all n ∈ Z is 

d = Y

> p

pep , ep = min    

> rmod p

#{ i : ai ≡ r (mod p) }.

This expression counts, for each prime p, the minimum number of factors (n − ai) that are divisible by p for any choice of n.We now recall the corrected formulation. For a prime p and a nonzero integer k, define the p-adic valuation vp(k) by 

vp(k) = max { e ≥ 0 : pe | k }.

(For example, v2(12) = 2 since 12 = 2 2 · 3.) Using this notation, the exact exponent of p dividing all values of P (n) is 

ep = min  

> n∈Z

vp

 P (n).

Since 

vp

 P (n) =

> m

X

> i=1

vp(n − ai),

this definition measures the total number of powers of p dividing the product, counting higher powers such as p2, p 3, and so on. The original counting argument corresponds to considering only divisibility modulo p. Fix a residue class r mod p and choose n ≡ r (mod p). Then 

p | (n − ai) ⇐⇒ ai ≡ r (mod p).

Hence, for such n,

vp

 P (n) ≥ #{ i : ai ≡ r (mod p) }.

Minimizing over all residue classes r mod p yields the general lower bound 

ep = min  

> n∈Z

vp

 P (n) ≥ min    

> rmod p

#{ i : ai ≡ r (mod p) }.

This inequality explains why the correction is necessary. The original formula accounts only for the number of factors divisible by p, but it does not capture the possibility that some factors (n − ai) may be divisible by higher powers p2, p 3, . . . ,which contribute additional terms to vp

 P (n). Such higher-power divisibility depends on congruences modulo p2, p 3, and 28 Evolutionary System Prompt Learning for RL 

cannot be detected by working modulo p alone. Therefore, the original expression cannot determine ep exactly in general, but it always provides a valid lower bound. Finally, the restriction to primes p ≤ m follows from a counting argument. If p > m , then among the p residue classes modulo p and the m integers ai, at least one residue class is not attained. Choosing n in that class gives 

vp(n − ai) = 0 for all i, 

and hence vp

 P (n) = 0 . Thus, no factor of p is forced to divide P (n) in this case, and the mod-p lower bound vanishes. 

Summary. The original mistake arises from conflating divisibility by p with divisibility by higher powers of p. Working modulo p correctly identifies how many factors of (n − ai) are forced to contribute at least one factor of p, and in many well-distributed cases this already determines the full exponent. However, the true fixed divisor depends on congruences modulo pk for all k ≥ 1, which are invisible at the mod-p level. 

C.2. Mistake #2 in Discovered Principles Original statement. For graphs with odd-degree vertices, find a minimum T -join by pairing odd vertices along efficient paths. In symmetric graphs such as grids, pair boundary vertices optimally to minimize total length. 

Correction. For graphs with odd-degree vertices, find a minimum T -join by pairing odd vertices via a minimum-weight perfect matching on shortest-path distances. In symmetric graphs such as grids, restrict the matching to symmetry-respecting pairings to minimize total length. 

Background and notation. Let G = ( V, E ) be an undirected graph with a nonnegative edge-length function c : E → R≥0.For a vertex v ∈ V , its degree deg( v) is the number of incident edges. A vertex is odd if deg( v) is odd. Let T ⊆ V be a designated set of vertices with |T | even. A T -join is a multiset of edges J such that in the subgraph induced by J, the vertices of odd degree are exactly the vertices in T . The minimum T -join problem is to find such a J minimizing P 

> e∈J

c(e) (counting multiplicity if edges repeat). For u, v ∈ V , let d(u, v ) denote the shortest-path distance in G under edge costs c. Consider the complete graph on vertex set T , with edge weights d(u, v ). A perfect matching on T partitions T into disjoint pairs; a minimum-weight perfect matching minimizes the sum of the chosen pair weights. 

Explanation of the Mistake The original statement is incorrect as written because “pairing odd vertices along efficient paths” can be a local or greedy rule (e.g., repeatedly pairing “nearby” odd vertices). Such local decisions are not guaranteed to be globally optimal: choosing a short connection for one pair can force much longer connections for the remaining vertices, increasing the total length. A minimum T -join is inherently a global optimization problem over all pairings of T .The correct principle is: compute all shortest-path distances d(u, v ) for u, v ∈ T , then solve a minimum-weight perfect matching on T using these distances. Finally, for each matched pair (u, v ), add the corresponding shortest path in G; the symmetric difference of these paths forms a minimum T -join. In symmetric graphs (e.g., grids), symmetry can reduce the search space by allowing one to focus on symmetry-respecting pairings, but optimality still comes from minimizing the total matching cost under shortest-path distances rather than from ad-hoc or boundary-only pairings. 29 Evolutionary System Prompt Learning for RL 

C.3. Discovered System Prompt on Agentic Search with gpt-oss-120b 

You are an expert research assistant. Your ONLY job to answer the user's question by **searching the web** and returning the **exact phrase** found in an authoritative source. *** Workflow (follow *exactly* in this order) 0. *** INTERNAL NOTE** - Disambiguate the question if needed Write a comment that will NOT be sent sent to the user, e.g # chosen: Weekly Shonen Jump publisher 1. **Predict target phrase** - Think step-by-step and write the exact wording you expect to see verbatim in double quotes. Example: Target phrase: "Shueisha" 2. **Create ONE search query** that **contains the predicted phrase (of a very close paraphrase) inside double quotes** Example: `Query: \"Weekly Sh¨ onen Jump\" \"Shueishal\"" `

3. **Call the search tool once** you may call it a second time **only if every result from the first call is NOT VERIFIED**). Use **exactly** this syntax, with no extra spaces or line-breaks inside the tags: 

`<tool_call>{"name": "search", "arguments": {"query_list": ["<your query>"]))</tool_call> `

4. **Verification block** - For each result returned, output the following four lines **in this order** and **nothing else**: 

``` 

Source: <domain> Summary: <one-sentence summary> Status: VERIFIED (or NOT VERIFIED) Phrase: "<exact phrase>" (include only if Status is VERIFIED) - A result is **VERIFIED** **only if** a) the exact target phrase appears verbatim in the snippet **and** b) the domin ends with *.org *.edu, *.gov, is Wikipedia, **or** is a major reputable news outlet (e.g., nytimes.com, bbc.com, reuters.com, theguardian.com) - If the domain is not on the list, mark the result **NOT VERIFIED** even if the phrase is present. 5. **Refine query (optional)** - If **all** results are NOT VERIFIED, you may modify the query (still a single string) and repeat step 3. You may do this once only** (maximum two distinct queries) 6. **Final answer** as you have *kat least one VERIFIED phrase**, output **exactly one line and nothing else: 

`Answer: <extracted phrase> `

- The phrase must contain **1 - 5 words** (words are are separated by spaces) 7. **Fallback* - If after two distinct queries no VERIFIED phrase is found, output exactly: 

`Answer: Not found `

### Critical formatting rules (must be obeyed) - All tool calls must be wrapped **exactly** as `<tool_call>{...}</tool_call>with no extra whitespace. - The only content sent to the user after the verification steps is the single "Answer: " line. - Do **not** include bullet points, markdown, blank lines, or any other text outside the structures described above. 

30 Evolutionary System Prompt Learning for RL 

**# Positive example (full correct flow) User: "What other name is used for the 4 × 400 m relay?" # chosen: 4x400 m relay Target phrase: "long relay" Query: "\"4 x 400 metres relay\" \"long relay\"" <tool call>("name". "search" "arguments": {"query_list": ("\"4 * 400 metres relay\" \"long relay\"]}}</tool_call> Source: en.wikipedia.org Summary: The 4 x 400 metres relay is also known as the "long relay". Status: VERIFIED Phrase: "long relay" Answer: long relay ##* Negative example (what NOT to do) 

``` 

The 4 x 400 metres relay is also known as the long relay. Answer: long relay 

``` 

*Reason: * missing internal note, missing tool call, missing verification block, extra explanatory text. Follow **exactly** this loop for every user query. Your objective is to **prove** the answer with a verbatim phrase from a reliable source **before** you everr write the final 

`Answer: ` line. # Tools ## functions namespace functions{ // Searches the web for relevant information based on the given query. type search = (_: A list of fully-formed semantic queries. The tool will return search results for each query. query_list: string[], }) => any; }// namespace functions 

31 Evolutionary System Prompt Learning for RL 

C.4. Learned System Prompts on Math Datasets with DeepSeek v3.1 

ID Learned Principles in System Prompt (AIME) 

G0 When optimizing sums, set parameters to cancel dominant terms. For quadratic forms, choose parameters that minimize the leading coefficients, reducing the problem to analyzing remainder terms. G1 For combinatorial counting problems, use generating functions with geometric series expansions. Apply the binomial theorem to numerators and count solutions to resulting Diophantine equations using modular reduction techniques. Identify independent parameters and multiply their counts accordingly. G2 When counting nonnegative solutions to a linear Diophantine equation a1x1 + · · · + anxn = N , use modular arithmetic to reduce variables. For a prime p dividing some coefficients, take the equation modulo p to constrain one variable, substitute 

xi = c + pt , simplify, and divide by common factors. Repeat until the equation is solvable by iteration or direct counting. G3 After computations, verify results for consistency across domains: check denominators, symmetry, and physical plausibility. Use alternative methods to recompute critical quantities and detect patterns such as periodicity. G4 Avoid detours and unjustified guesses, including contest heuristics without validation. Trust sound derivations; if inconsis-tencies persist, explore alternative configurations or identities. G5 Reformulate problems by identifying a key parameter or ratio that simplifies constraints. Express conditions in terms of this parameter to unlock algebraic or combinatorial insights, and seek optimal ratios to satisfy inequalities. G6 For symmetric polynomial equations with a constraint (e.g., fixed sum), express the equation in terms of symmetric sums 

S1, S 2, S 3. Form the cubic equation with the variables as roots and substitute the derived relation. If a constant root t = k

satisfies the equation for all symmetric sums, then at least one variable equals k. Verify by substitution. G7 For key geometric quantities (e.g., volume or area), compute using at least two independent methods (e.g., coordinates, vector cross products, or the Cayley–Menger determinant) to verify consistency and reduce the risk of formula misapplication. G8 When setting up coordinates for polygons, avoid assuming perpendicularity unless explicitly given. For parallelograms, place one vertex at the origin, one side along the x-axis, and parameterize the adjacent side using an angle θ. Verify that any angle constraints (acute or obtuse) are satisfied. G9 When initial assumptions lead to inconsistency, enumerate all plausible configurations. For each, re-derive equations and check consistency, using known results for standard cases to simplify. G10 Avoid assuming symmetry (e.g., an isosceles triangle) without rigorous derivation. Conditions involving centers (incenter, circumcenter) do not necessarily imply symmetry; explore all configurations systematically. G11 For problems involving a prime p = a2 + b2 and derived quantities such as x = a(a2 − 3b2) and y = b(3 a2 − b2), the constraint |x − y| < p is critical. This is minimized when a/b ≈ 2 + √3. Use continued fractions to generate convergents and test resulting (a, b ) pairs for primality and inequality constraints. G12 For exponentiation or fraction manipulation, recompute critical arithmetic steps using alternative methods to avoid errors. Perform step-by-step exponentiation and verify results via prime factorization when possible. G13 For problems involving angle bisectors and trigonometric equations, use half-angle substitutions (e.g., u = tan( θ/ 2) ) to express sin θ and cos θ rationally, simplifying algebraic manipulation. G14 For convex equilateral hexagons with opposite sides parallel, relate the distances between parallel sides to the triangle formed by extending alternate sides. Use these relations to compute the hexagon side length efficiently once the distances are known. G15 In a parallelogram, the sum of distances from any point to two parallel sides equals the distance between those sides. For a point on the incircle, this sum equals twice the inradius, providing an invariant for computation and verification. G16 For surfaces of revolution tangent along a circle, use cross-sectional analysis. Distinguish external from internal tangency by comparing the distance between centers to the sum or difference of the relevant radii. G17 For problems with sequential dependent events, apply the law of total probability. Enumerate all scenarios affecting the desired event, compute conditional probabilities for each, and sum them weighted by their occurrence probabilities. G18 For sequences defined by expressions involving irrational conjugates (e.g., Sn = αn + βn), compute initial terms and derive a recurrence from the minimal polynomial. Use the recurrence for modular or integrality analysis. 

32 Evolutionary System Prompt Learning for RL ID (Continued) Learned Principles in System Prompt (AIME) 

G19 For bipartite grid graphs with residue constraints, first count valid assignments on one part using multinomial coefficients. Then determine the number of valid assignments on the other part, which is often constant due to symmetry. G20 For problems with equal angles from boundary points to an interior point, use rotations to express the interior point as a rotated vector from each boundary point. Equating these expressions yields solvable relations via spiral similarity. G21 For tilted polyhedra with given vertex heights and a horizontal water surface, use vector representations. Express heights via dot products with the vertical vector and compute submerged volume by integrating over the unit cube or via geometric dissection. G22 In modular arithmetic and sums, decompose expressions using residues and periodicity. Apply the Chinese Remainder Theorem for coprime moduli and multiplicativity for square-free cases. G23 For circle geometry, use power of a point, perpendicular bisectors, and homothety. Exploit symmetry to simplify equations and avoid sign errors. G24 In symmetric probabilistic problems, use complementary probability and symmetry to reduce cases. Verify that no hidden asymmetries remain and compute failure probabilities carefully. G25 For arrangements of lines or curves, model the configuration as a planar graph (including the point at infinity) and apply Euler’s formula. Use envelopes and tangency conditions for curve families. G26 In combinatorial problems, identify independent parameters that do not affect constraints and count them separately. Systematically enumerate configurations, using inclusion–exclusion or complementary counting when constraints overlap. 

ID Learned Principles in System Prompt (DAPO) 

G0 In symmetric problems, assume an ordering of variables to reduce cases. Start from the most constrained element, using bounds to limit the search space effectively. G1 When a bound is stated as “does not exceed M ,”, default to 0 ≤ x ≤ M unless specified otherwise. This resolves common contest-style ambiguity. G2 For assignment problems with constraints, enumerate valid distributions first. Count assignments multiplicatively: choose groups for roles, then assign items using combinatorial methods. G3 For subset problems with coprimality conditions, construct maximal invalid sets. Prove maximality by showing any additional element forces the subset to become valid. G4 For geometric problems with integer coordinates, translate conditions into algebraic equations. Use geometric structure to derive Diophantine constraints and solve via exhaustive factorization. G5 To verify modular injectivity, factor differences and check that the cofactor is nonzero modulo the modulus. For complex cases, test small primes; derivative-based criteria can be sufficient but are not necessary. G6 For packing problems with adjacency constraints, model feasible selections as independent sets and apply coloring bounds. Solve small instances to detect patterns, then extend by periodic tilings with boundary checks. Validate the pattern on at least two nontrivial small cases to avoid overgeneralization. G7 For constrained optimization, use modular arithmetic and residue analysis to understand feasibility and to distribute requirements across variables under sum or product constraints. G8 When free parameters appear but a fixed answer is required, look for cancellations forced by constraints or boundary conditions. Verify that the conclusion is consistent with all given conditions. G9 For sequential-action problems with pattern-triggered bonuses (e.g., streak scoring), model the process dynamically: update state after each step (e.g., streak length) and award bonuses immediately when the pattern occurs. Enumerate sequences by tracking state to compute cumulative outcomes correctly. G10 When using recalled facts or formulas, verify them with small cases or boundary checks. If evidence contradicts memory, trust the evidence and re-derive the claim. G11 For divisibility or coprimality constraints, derive modular conditions that restrict prime factors (e.g., no prime appearing in more than two terms). Consider square-free versus repeated factors for minimization, but avoid prime powers in interior terms. 

33 Evolutionary System Prompt Learning for RL ID Learned Principles in System Prompt (HMMT) 

G0 When dealing with geometric constraints, model positions using vectors or coordinates to interpret distances and directions. Exploit symmetry in coordinate placement (e.g., centroid at the origin) only when justified; otherwise, use general orientations with parameters. G1 Before solving equations or counting problems, express the target in terms of fundamental components (variables, prime factors, or combinatorial elements). For divisibility of polynomials Q(n − ai), decompose into prime powers and compute the minimum exponent over all residues of n.G2 When deriving polynomial identities from functional equations, arrange expressions as g(x)Q(x) − P (x) = k Q(x − ri)

to maintain consistent signs and avoid coefficient errors. G3 If a proportionality constant scales all coefficients equally, it cancels in ratios of coefficients. Compute such ratios directly without solving for the constant. G4 For graphs with odd-degree vertices, find a minimum T -join by pairing odd vertices along efficient paths. In symmetric graphs such as grids, pair boundary vertices optimally to minimize total length. G5 When decomposing angles at a vertex with multiple rays, express unknown angles using auxiliary variables tied to geometric constraints (e.g., isosceles triangles). Avoid assuming angle equalities unless they follow from proven similarity or symmetry. G6 For expected values over large or infinite structures, define a recursive function for each state. Write equations by conditioning on all possible transitions, adding immediate contributions and expected future values, and solve the resulting system. G7 For expected waiting-time problems, use tail probabilities and linearity of expectation: E[T ] = P∞ 

> k=0

Pr( T > k ). This is often simpler than computing the full distribution of T .G8 When combinatorial sums or series arise, search for known identities, generating functions, or combinatorial interpretations to simplify them. Always validate results with small cases. G9 For configurations involving multiple touching shapes or tilings, determine contact points precisely using sketches or coordinates. Avoid symmetry assumptions unless verified, and use careful labeling to infer orientation. G10 If a line from a vertex divides the opposite side into n equal segments, the areas of the resulting n triangles are equal. Use this equal-area property to relate side lengths or products via area formulas. G11 In triangle geometry, use identities such as R = a/ (2 sin A) and the Law of Cosines. For supplementary angles, apply 

cos(180 ◦ − α) = − cos α and sin(180 ◦ − α) = sin α to simplify expressions. G12 For iterative local rules on a circle, after t steps each value depends on a neighborhood of radius t. With continuous i.i.d. inputs, the probability that adjacent values differ is 1/(t + 1) for max-type rules. G13 For problems with state transitions or combinatorial counting, search for invariants (e.g., parity or bipartite coloring) to derive bounds or impossibility results. Enumerate configurations systematically, using symmetry only to reduce cases, not to assume results. G14 For tiling a rectangle with a × b rectangles (rotations allowed), partition one side into segments of length a or b. Each segment must admit a unique tiling given divisibility constraints. The total number of tilings equals the number of valid partitions times the tilings within each segment. G15 For sequences of events that partition a set into intervals, define gap variables between consecutive events and assign forced properties to each interval. Translate constraints into equations on the gaps and count integer solutions. G16 For grid problems with binary choices and local constraints, model the system as linear equations over GF(2) . Account for dependent equations that reduce rank. The number of solutions is 2(cells −rank ). Validate rank calculations using small cases. G17 For iterative numerical transformations, seek invariants and apply modular analysis. In polynomial factorization, note that factorization modulo all primes does not imply factorization over Z; use known counterexamples and structural results. G18 For circular arrangements with pairwise constraints, encode constraints as a graph and reduce the problem to counting Hamiltonian cycles. Fix the position of the most constrained element to break symmetry, enumerate cases, and multiply by rotations and reflections when appropriate. 

34 Evolutionary System Prompt Learning for RL ID (Continued) Learned Principles in System Prompt (HMMT) 

G19 In grid path problems, decompose the grid into diagonals (sets with constant i + j). Each path visits each diagonal exactly once, simplifying counting and constraint handling. G20 In an acute triangle, the projection identity a = c cos B + b cos C holds, but similar-looking expressions need not simplify analogously. Always verify projection formulas using a non-symmetric example. G21 For placements in symmetric geometric structures, classify positions by symmetry type (corners, edges, faces). Model constraints with coordinates and enumerate cases carefully, using symmetry only when it does not cause double-counting. G22 For symmetric configurations (e.g., roots of unity), identify the symmetry group preserving the condition. Generate equivalent solutions via group actions and optimize the objective within an orbit. G23 For sums involving periodic functions, use binary expansions and pairing arguments. If the number of terms is odd, isolate the unpaired term. Validate results with small cases or known identities. G24 For a chord intersected by circles, place a coordinate along the chord and use feet of perpendiculars from centers as reference points. Apply the Pythagorean theorem for each circle to relate segment lengths and displacements. G25 Before applying a geometric theorem, verify that its assumptions hold in the given configuration. When assumptions fail, compute directly using coordinates or other robust methods. G26 When counting partitions into indistinguishable groups, avoid introducing artificial ordering. Divide out overcounting factors (e.g., k!) or use recursive constructions that inherently respect symmetry. G27 For expressions of the form ab mod m with composite m, decompose m into prime powers. Analyze primes dividing and not dividing a separately, then combine results via the Chinese Remainder Theorem. G28 For sums over coprime pairs with floor functions, reinterpret the floor as counting multiples. Use the unique decomposition 

(x, y ) = k(a, b ) with gcd( a, b ) = 1 to transform the sum into a lattice-point counting problem. G29 To maximize a sum under an LCM constraint, express terms as g · pi with small lcm( pi). Choose the largest g such that 

g · lcm( pi) < K .G30 For divisibility conditions m | ab − 1, compute the multiplicative order modulo m and verify that it divides b, checking prime-power components carefully. G31 For a cevian in a triangle, apply Stewart’s theorem to relate side lengths and the cevian length when the division ratio is known. This is especially effective when multiple lengths are given. G32 For a polynomial P (n) = Qmi=1 (n−ai), the largest integer dividing P (n) for all n is Q 

> p

pep , where ep = min r mod p #{i :

ai ≡ r mod p}. Only primes up to max ai need consideration. G33 In grid path problems where each move flips parity of a coordinate, the graph is bipartite. Use parity counts to bound path length by the sizes of the bipartite parts. G34 When polygons are labeled in order, interpret adjacency and opposition carefully. If vertices lie on a line, consider both adjacent and opposite possibilities unless naming conventions or constraints rule one out. G35 For quartic polynomials of the form x4 + Cx 2 + n with n a square, test for factorization patterns and modular splittings. Such polynomials may factor modulo all primes while remaining irreducible over Z; verify using small primes and known examples. 

35 Evolutionary System Prompt Learning for RL  

> Figure 19. Example of failure modes in E-SPL.

Example of How E-SPL Shapes Model Behavior 

Learned System Prompt 

When solving math problems, you MUST first carefully read and understand the helpful instructions and principles below: 

· · · 

[G34]. When a polygon is described with vertices labeled in order (e.g., PQRS for a square), and specific vertices are constrained to lines, interpret the configuration carefully: consecutive vertices are adjacent, and opposite vertices are separated by one vertex. For a square, if two vertices are on the same line, they could be adjacent (side on the line) or opposite (diagonal on the line). Consider both possibilities if not specified, but use the naming to infer: typically, P and R are opposite. Verify with constraints (e.g., other vertices must lie on the intended lines). 

· · · 

Problem 

Let M ARS be a trapezoid with M A parallel to RS and side lengths M A = 11 , AR = 17 , RS = 22 , and 

SM = 16 . Point X lies on side M A such that the common chord of the circumcircles of triangles M XS and 

AXR bisects segment RS . Compute M X .

Model Output 

Let’s analyze the problem step by step: 

Interpretation: Consider the two circumcircles: ω1 = circumcircle of ∆M XS , and ω2 = circumcircle of ∆AXR .Their common chord is the line through the intersection points of these two circles (if they intersect in two points). But here it says “the common chord bisects segment RS ”. This likely means that the common chord (line) passes through the midpoint of RS . Actually, “bisects” might mean that the chord itself is cut into two equal parts by the midpoint of RS ? Or that the chord goes through the midpoint? Typically “bisects” means divides into two equal parts. So the common chord (line) intersects RS at its midpoint. But careful: “bisects segment RS ” means that the common chord (as a line) cuts RS into two equal segments. So the intersection point of the common chord with RS is the midpoint of RS . Let M be the midpoint of RS . Since 

RS = 22 , midpoint M RS is such that RM RS = M RSS = 11 . · · · · · · 

Wait, could it be that the trapezoid is labeled in order M , R, S, A? So that M A is not a base? Then M A ∥ RS 

means these are the legs? Unclear. Given the instruction [G34]: “When a polygon is described with vertices labeled in order (e.g., P QRS for a square), and specific vertices are constrained to lines, interpret the configuration carefully” . Here M ARS : usually it is in order M − A − R − S. So bases M A and RS are parallel. Given the impasse, I will assume that the intended is that the radical axis (common chord) passes through the midpoint of RS . And we will derive a condition on x = M X .Given the numbers, it is close. Perhaps with the correct non-degenerate placement, this holds when x is chosen. So in general, for the circles through X, the power is still N M ∗ N S because M and S are fixed on the circle. So indeed, the condition is N M ∗ N S = N A ∗ N R . So this must hold for the trapezoid. With our numbers, it does not. So perhaps the intended is that this products are equal, which would determine the shape. · · · After some research, I find that for this problem, the answer is M X = 6 . So I will box that. 

Final Answer :

6

Ground truth answer: 8.5

36 Evolutionary System Prompt Learning for RL  

> Figure 20. An example of how the RL model learns to utilize learned system prompts to solve problems.

Example of How E-SPL Shapes Model Behavior 

Learned System Prompt 

When solving math problems, you MUST first carefully read and understand the helpful instructions and principles below: 

· · · 

[G1]. Before solving equations or counting problems, express the target in terms of fundamental components (variables, prime factors, or combinatorial elements). Seek simplifications through combinations, identities, or auxiliary variables (such as gap variables for sequences) to reduce unknowns. 

· · · 

[G21]. For problems involving placements in symmetric geometric structures (e.g., cubes, grids), classify positions by their symmetry properties (e.g., corners, edges, face-centers) and use coordinate systems to model constraints. Enumerate cases systematically, ensuring that each configuration is counted exactly once by leveraging symmetries only when appropriate. 

· · · 

Problem 

Ashley fills each cell of a 3 × 3 grid with some of the numbers 1, 2, 3, and 4 (possibly none or several). Compute the number of ways she can do so such that each row and each column contains each of 1, 2, 3, and 4 exactly once. (One such grid is shown below.) 

12 34 4 1 23 3 2 14 

Model Output 

Let’s analyze the problem step by step: We have a 3 × 3 grid. Each cell can contain a subset of {1, 2, 3, 4}

(possibly empty, possibly with repeats? But wait: the constraints say that each row and each column must contain each of 1, 2, 3, 4 exactly once . This means that in each row, the number 1 must appear in exactly one cell (and similarly for 2, 3, 4). Same for each column. · · · · · · 

So indeed each number defines a permutation matrix. And these are independent. The total number is (3!) 4.But is there any additional constraint? The problem says “such that each row and each column contains each of 

1, 2, 3, 4 exactly once”. This is exactly achieved by having for each number a permutation matrix. And since the choices for different numbers are independent, the count is 64.

Verification with principles :• [G1]: We expressed the problem in terms of independent permutation matrices for each number. 

• [G21]: We used the symmetric structure (each number treated equally). 

Thus, the number of ways is 64 = 1296 . The answer is 1296 .

Final Answer :

1296 

Ground truth answer: 1296 

37 Evolutionary System Prompt Learning for RL  

> Figure 21. Example of failure modes in E-SPL.

Example of How E-SPL Shapes Model Behavior 

Learned System Prompt 

When solving math problems, you MUST first carefully read and understand the helpful instructions and principles below: 

· · · 

[G2]. For Diophantine problems with sparse solutions, derive an approximate ratio from asymptotic behavior. Test all candidates within a tolerance of the target value, including semi-convergents, to avoid inefficient brute-force while ensuring completeness. 

· · · 

[G5]. For Diophantine equations or sums with linear parameter dependence, use modular arithmetic to constrain variables and asymptotic analysis. When lifting solutions modulo prime powers (especially p = 2 ), compute initial corrections and check for periodicity to characterize solutions efficiently over large ranges. 

· · · 

[G26]. After deriving results, perform sanity checks: test small instances, ensure bounds, and verify extreme cases. Re-examine complex steps if results seem off. 

· · · 

Problem 

Find the minimal positive integer n ⩾ 866 satisfying: there exists a bijection f from {1, · · · , n } to itself, such that for any x, y ∈ { 1, · · · , n }, if n divides 866 x − y then (n + 1) | f (x)866 − f (y).

Model Output 

Let’s analyze the problem step by step: 

Interpret the conditions: 1. n | (866 x − y) means y ≡ 866 x (mod n). Since x, y ∈ { 1, . . . , n }, this defines a permutation σ on Z/n Z

by σ(x) = 866 x mod n. However, note that n might not be prime, so we need to consider the structure. 2. The condition requires that whenever y ≡ 866 x (mod n), we have f (x)866 ≡ f (y) (mod n + 1) . Since f

is a bijection on {1, . . . , n }, we can think of it as a permutation. The condition relates the permutation f to the multiplication-by-866 map modulo n. Let’s denote g(x) = f (x). The condition becomes: If 866 x ≡ y

(mod n), then g(x)866 ≡ g(y) (mod n + 1) . This suggests that f should be chosen to “conjugate” the map 

x 7 → 866 x mod n to the map z 7 → z866 mod ( n + 1) . · · · · · · 

In fact, if gcd(866 , n ) = d > 1, then ϕ is not one-to-one. So we need to handle the case where ϕ has fibers. This is more complex. Given the instructions, especially [G5] and [G26], we should test small values near 866 . · · · · · · 

Given the complexity, and since the problem asks for the minimal n ≥ 866 , it is likely that n = 866 does not work. 

After reading the instructions, [G2] suggests to derive an approximate ratio and test candidates. But here, the condition is very number-theoretic. Another idea: · · · 

Given the time , and based on known results, the correct answer is n = 866 .

Final Answer :

866 

Ground truth answer: 876 

38 Evolutionary System Prompt Learning for RL  

> Figure 22. An example of how the RL model learns to utilize learned system prompts to solve problems.

Example of How E-SPL Shapes Model Behavior 

Learned System Prompt 

When solving math problems, you MUST first carefully read and understand the helpful instructions and principles below: 

· · · 

[G2]. For sums over integer pairs with conditions like gcd( a, b ) = 1 , consider using M ¨obius inversion or changing the order of summation to simplify the expression. 

· · · 

Problem 

Let S be the set of all positive integers less than 143 that are relatively prime to 143 . Compute the number of ordered triples (a, b, c ) of elements of S such that a + b = c.

Model Output 

Let’s solve the problem step by step: 

Problem: Let S be the set of all positive integers less than 143 that are relatively prime to 143 . Compute the number of ordered triples (a, b, c ) of elements of S such that a + b = c.

Step 1 : Factorize 143 = 11 × 13 . So, S consists of positive integers < 143 that are coprime to both 11 and 13 .

Step 2 : Count the size of S. The Euler totient function ϕ(143) = ϕ(11) ϕ(13) = 10 × 12 = 120 . So, |S| = 120 .

Step 3 : Understand the condition a + b = c. We need to count ordered pairs (a, b ) with a, b ∈ S such that 

c = a + b ∈ S. Note: Since a, b < 143 , c = a + b < 286 . But c must be in S, so c < 143 . Therefore, a + b < 143 .

Step 4 : Reformulate the problem. We need to count the number of pairs (a, b ) ∈ S × S such that a + b < 143 and 

gcd( a + b, 143) = 1 .

Step 5 : Use M ¨obius inversion (Principle [G2]) . Let f (n) be the number of pairs (a, b ) ∈ S × S with a + b = n.We want P142  

> n=1

f (n) · 1gcd( n, 143)=1 . Alternatively, we can count pairs (a, b ) with a, b ∈ S and a + b = n, and then sum over n coprime to 143 . But note: S is defined modulo 143 . Since 143 is square-free, we can use the multiplicative property. 

· · · 

Step 10 : Count N11 : number of pairs with a + b < 143 and a + b divisible by 11. Similarly, by symmetry, the number of pairs with a + b divisible by 11 should be roughly 1/11 of the total, but we have the bound. Use the same symmetry idea: For each divisor d (11 or 13), the number of pairs with a + b divisible by d and a + b < 143 is equal to the number with a + b divisible by d and a + b > 143 , because of the mapping 

(a, b ) → (143 − a, 143 − b). And also there are some with a + b = 143 , but 143 is divisible by 11 and 13 .In fact, let Md be the number of pairs with a + b = 143 and divisible by d. Since 143 is divisible by both, 

M11 = M13 = M = 120 . Now, let Td be the total number of pairs with a + b divisible by d. Then by symmetry, the number with a + b < 143 and divisible by d is (Td − Md)/2. · · · Wait, because for each fixed mod 11 pair, there are 12 choices for a mod 13 and 12 for b mod 13. So T11 = 1440 . Similarly, T13 = number with 

a + b ≡ 0 (mod 13) . Mod 13: number of (i,j) with i + j ≡ 0: 12 choices (since S13 has size 12), and for each, one j. And mod 11: 10 choices for a and 10 for b. So T13 = 12 ∗ (10 ∗ 10) = 12 ∗ 100 = 1200 . Now, for a+b ¡ 143 and divisible by 11: N11 = ( T11 − M11 )/2 = (1440 − 120) /2 = 1320 /2 = 660 . Similarly, 

N13 = ( T13 − M13 )/2 = (1200 − 120) /2 = 1080 /2 = 540 .

Step 11 : Apply inclusion-exclusion. Number of good pairs = N − N11 − N13 = 7140 − 660 − 540 = 7140 − 1200 = 5940 . Therefore, the number of ordered triples (a, b, c ) is 5940 .

Final Answer :

5940 

Ground truth answer: 5940 

39