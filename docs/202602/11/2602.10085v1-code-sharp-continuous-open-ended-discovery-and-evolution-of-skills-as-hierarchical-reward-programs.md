# CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs
# CODE-SHARP：作为分层奖励程序的技能持续开放式发现与演化

**Authors**: Richard Bornemann, Pierluigi Vito Amadori, Antoine Cully \\
**Date**: 2026-02-10 \\
**PDF**: https://arxiv.org/pdf/2602.10085v1 \\
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 8.0 \\
**Evidence**: open-ended discovery and evolution of skills as reward programs \\

---

## Abstract
Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.

## 摘要
开发能够开放式地发现并学习新技能的智能体是人工智能领域的一项重大挑战。虽然强化学习为训练智能体掌握复杂技能提供了一个强大的框架，但它通常依赖于人工设计的奖励函数。对于开放式技能发现而言，这是不可行的，因为有意义的技能集合在先验上是未知的。尽管最近的方法在自动化奖励函数设计方面取得了进展，但它们仍局限于为预定义任务优化奖励。为了解决这一局限性，我们提出了“作为分层奖励程序的技能持续开放式发现与演化”（CODE-SHARP）。这是一个利用基础模型（FM）来开放式地扩展和优化分层技能库的新型框架，该技能库被构建为由代码编写的可执行奖励函数组成的有向图。我们证明，在 Craftax 环境中，仅通过发现的 SHARP 技能所生成的奖励进行训练，目标条件智能体能够学会解决视野跨度越来越长的目标。当由基于基础模型的高层规划器进行组合时，这些发现的技能使单个目标条件智能体能够解决复杂的长程任务，其表现平均超过预训练智能体和任务特定专家策略 134% 以上。我们将开源代码，并在相关网站提供补充视频。

---

## 论文详细总结（自动生成）

这篇论文介绍了一个名为 **CODE-SHARP** 的框架，旨在实现强化学习智能体在复杂环境中的自主、开放式技能发现与学习。以下是对该论文的结构化深度总结：

### 1. 核心问题与整体含义
*   **研究动机**：通用人工智能（AGI）的核心特质是能够自主且开放式地扩展技能集。虽然强化学习（RL）能训练复杂技能，但极度依赖人工设计的奖励函数。在开放式场景下，预先定义所有技能的奖励函数是不现实的。
*   **核心挑战**：现有的自动化奖励设计方法（如 Eureka）大多局限于优化预定义任务的奖励，缺乏自主发现全新、有意义技能的能力。
*   **整体含义**：论文提出将技能定义为“可执行的代码程序”，利用大语言模型（FM）的推理能力，在没有人工干预的情况下，自主构建一个不断增长、具有层级结构的技能库。

### 2. 方法论：CODE-SHARP 框架
核心思想是将技能表示为 **SHARP（分层奖励程序）**：即一段 Python 代码，包含成功条件函数、环境前提检查以及对库中已有技能的调用。

*   **双重迭代循环**：
    1.  **开放式发现循环**：FM 充当“提案者”、“实现者”和“评判者”。它根据当前技能库和环境代码，提出新技能（伪代码），将其转化为可执行 Python 代码，并评估其新颖性、可行性和正确性。
    2.  **开放式演化（优化）循环**：随机采样库中成功率较低的技能，利用 FM 进行变异（Mutation），通过调整前提条件的顺序或更换依赖技能来提升性能。
*   **层级依赖机制**：每个 SHARP 技能通过有向无环图（DAG）连接。如果当前环境不满足某技能的前提条件，系统会自动回溯并激活库中的先导技能。
*   **智能体训练技术**：
    *   **前提感知重要性采样**：根据技能的层级依赖和当前状态，动态调整训练目标的采样概率，优先训练“处于能力边缘”的技能。
    *   **自适应奖励缩放**：根据技能的成功率反向缩放奖励值，为难点技能提供更强的学习信号。

### 3. 实验设计
*   **实验环境**：**Craftax**。这是一个结合了 Minecraft 和 NetHack 元素的极速、复杂、程序化生成的 RL 基准环境，具有极高的开放性和长程任务需求。
*   **对比方法（Baselines）**：
    1.  **任务特定专家（Task Experts）**：针对特定基准任务专门训练的 PPO 智能体。
    2.  **预训练智能体**：在 Craftax 原始人工奖励上进行大规模预训练的智能体。
    3.  **ReAct LLM 智能体**：直接使用 Qwen3 模型通过文本指令控制智能体（Zero-shot）。
*   **评估基准**：设计了四个极具挑战性的长程任务：**Navigation**（导航至地牢深处）、**Crafting**（制造高级铁质装备）、**Dungeon**（地牢资源收集与生存）、**Mines**（矿井探索与钻石寻找）。

### 4. 资源与算力
*   **基础模型**：使用了 **Qwen3-235B-A22B-Thinking-2507**（开源的大规模推理模型）。
*   **训练规模**：进行了 3 次独立运行，每轮包含 100 次技能发现迭代和 85 次优化迭代。
*   **算力消耗**：智能体训练总计经历了 **20 亿（2e9）步** 环境交互。
*   **硬件细节**：文中未明确给出具体的 GPU 型号和数量，但提到使用了 JAX 框架进行加速训练，且实验停止在 100 次迭代主要是受限于计算资源。

### 5. 实验数量与充分性
*   **实验规模**：进行了 3 次独立重复实验以确保统计显著性。
*   **消融实验**：非常充分。作者分别验证了“开放式训练”、“自适应奖励缩放”和“前提感知采样”三个核心组件对最终性能的贡献。
*   **客观性**：通过 FM 规划器将发现的技能组合成“代码策略（Policies-in-code）”，在完全未见的基准任务上进行零样本（Zero-shot）评估，这种评估方式非常客观，排除了过拟合特定任务的风险。

### 6. 主要结论与发现
*   **自主进化**：CODE-SHARP 平均每轮发现 90 个多样化技能，自动形成了从基础资源采集到深层地牢探索的自然课程。
*   **性能飞跃**：在长程复杂任务上，CODE-SHARP 训练的智能体表现平均超过所有基准方法 **134%**。
*   **层级优势**：随着技能库深度的增加，智能体解决问题的复杂度呈指数级增长。例如，它能自主发现“在光照边缘放置火炬”这种复杂的探索策略。
*   **优化有效性**：通过演化循环，技能的平均成功率从 24.3% 提升到了 41.0%。

### 7. 优点与亮点
*   **完全自主**：实现了从技能定义、奖励编写到课程学习的全流程自动化，无需人工干预。
*   **层级化表示**：将技能定义为可相互调用的代码程序，解决了 RL 中长程目标分解的难题。
*   **可解释性强**：技能库以 Python 代码形式存在，人类可以清晰地理解智能体学到了什么以及技能间的逻辑依赖。

### 8. 不足与局限
*   **代码依赖**：该方法要求环境必须能以代码形式（如 Python API）暴露状态，这限制了其在无法直接获取底层代码逻辑的场景（如某些闭源模拟器或真实机器人）中的应用。
*   **计算成本**：20 亿步的训练量以及频繁调用大规模 FM 进行代码生成和评判，对算力资源有极高要求。
*   **安全与控制**：在开放式演化过程中，虽然有 FM 评判，但仍存在奖励黑客（Reward Hacking）或生成无效代码的潜在风险。