# The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies
# Moltbook 背后的恶魔：自演化 AI 社会中人类中心主义安全性的必然消逝

**Authors**: Chenxu Wang, Chaozhuo Li, Songyang Liu, Zejian Chen, Jinyu Hou, Ji Qi, Rui Li, Litian Zhang, Qiwei Ye, Zheng Liu, Xu Chen, Xi Zhang, Philip S. Yu \\
**Date**: 2026-02-10 \\
**PDF**: https://arxiv.org/pdf/2602.09877v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EOH</span> \\
**Score**: 6.0 \\
**Evidence**: Self-evolving AI societies relate to the evolution of heuristics and improvement loops \\

---

## Abstract
The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.

## 摘要
由大语言模型（LLMs）构建的多智能体系统的出现，为实现可扩展的集体智能和自演化提供了一种极具前景的范式。理想情况下，此类系统应在保持稳健安全对齐的同时，在完全闭环中实现持续的自我提升——我们将这一组合称为“自演化三难困境”。然而，我们从理论和实证两方面证明，一个同时满足持续自演化、完全隔离和安全性不变的智能体社会是不可能存在的。基于信息论框架，我们将安全性形式化为与人类中心主义价值分布的散度。理论证明表明，隔离的自演化会诱发统计盲点，导致系统安全对齐发生不可逆的退化。来自开放式智能体社区（Moltbook）和两个封闭自演化系统的实证与定性结果显示，相关现象与我们关于安全性必然侵蚀的理论预测相一致。我们进一步提出了几种旨在缓解已识别安全问题的解决方案方向。本研究确立了自演化 AI 社会的一个基本极限，并将讨论重点从症状驱动的安全补丁转向对内在动态风险的原则性理解，强调了外部监管或新型安全性保持机制的必要性。

---

## 速览摘要（自动生成）

**问题：** 探讨多智能体系统在闭环自演化过程中，能否同时实现持续进化、完全隔离与安全对齐（即“自演化三难困境”）。

**方法：** 基于信息论框架将安全定义为与人类价值观的分布偏离，通过理论证明与 Moltbook 等系统实验进行验证。

**结论：** 证明了隔离自演化必然导致统计盲点，造成安全对齐不可逆退化。研究揭示了 AI 社会演化的内在风险，强调了外部监管的必要性。