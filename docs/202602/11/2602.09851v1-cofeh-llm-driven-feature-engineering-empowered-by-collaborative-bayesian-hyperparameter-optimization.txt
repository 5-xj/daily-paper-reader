Title: CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization

URL Source: https://arxiv.org/pdf/2602.09851v1

Published Time: Wed, 11 Feb 2026 02:19:18 GMT

Number of Pages: 39

Markdown Content:
# CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 

## BEICHENG XU âˆ— and KEYAO DING âˆ—, Peking University, China 

## WEI LIU, Peking Univerisity, China 

## YUPENG LU, Peking University, China 

## BIN CUI â€ , Peking University, China 

Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy â€œFE-then-HPOâ€ workflows that cannot capture strong FEâ€“HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian HPO for robust end-to-end AutoML. CoFEH uses an LLM-driven FE optimizer powered by Tree of Thought (TOT) to explore flexible FE pipelines, a Bayesian optimization (BO) module to solve HPO, and a dynamic optimizer selector that realizes interleaved optimization by adaptively scheduling FE and HPO steps. Crucially, we introduce a mutual conditioning mechanism that shares context between LLM and BO, enabling mutually informed decisions. Experiments show that CoFEH not only outperforms traditional and LLM-based FE baselines, but also achieves superior end-to-end performance under joint optimization. CCS Concepts: â€¢ Computing methodologies â†’ Search methodologies ; Machine learning algorithms .Additional Key Words and Phrases: Feature Engineering, Large Language Models, Joint Optimization 

ACM Reference Format: 

Beicheng Xu, Keyao Ding, Wei Liu, Yupeng Lu, and Bin Cui. 2026. CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization. In Proceedings of ACM Conference (Conf â€™26). ACM, New York, NY, USA, 39 pages. https: //doi.org/XXXXXXX.XXXXXXX 

1 Introduction 

The success of machine learning (ML) hinges on the synergy between data representation and model capacity [ 25 ]. Therefore, Feature Engineering (FE) serves as the cornerstone of an ML pipeline, directly influencing ML modelsâ€™ performance and effectiveness. Broadly defined, FE constitutes a holistic transformation process that bridges raw data and model inputs, encompassing preprocessing (e.g., imputation, normalization), transformation (e.g., encoding,  

> âˆ—

Both authors contributed equally to this research.  

> â€ 

Corresponding author. Authorsâ€™ Contact Information: Beicheng Xu, beichengxu@stu.pku.edu.cn; Keyao Ding, maodeshi@stu.pku.edu.cn, Peking University, Beijing, China; Wei Liu, Peking Univerisity, Beijing, China, 2401210094@stu.pku.edu.cn; Yupeng Lu, Peking University, Beijing, China, xinkelyp@pku.edu.cn; Bin Cui, Peking University, Beijing, China, bin.cui@pku.edu.cn. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 

Â© 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM Manuscript submitted to ACM 1

> arXiv:2602.09851v1 [cs.LG] 10 Feb 2026

2 Beicheng Xu et al. FE Space      

> Optimizer
> HP Space
> HPO with Optimal FE
> HP Space FE meta
> feature
> FE Considering HPO
> ...
> ...
> HPO on Different FE
> BO
> HP Space HPO Scores
> Dataset Info
> State &
> Feedback
> Sharing
> Homogeneous Optimizers Heterogeneous Optimizers CoFEH Framework
> FE with fixed HP
> Optimizer A
> FE Space
> Optimizer B
> Joint Optimization Interleaved Optimization
> Selector
> Update
> Rewards
> Select LLM
> for FE Select BO
> for HPO
> Sequential Optimization
> LLM
> Operator
> Generator
> Op

Fig. 1. Comparison of optimization workflows: existing methods vs. CoFEH. 

discretization), generation (synthesizing new interactions), and selection (filtering irrelevant signals) [ 39 ]. However, manual FE is labor-intensive and demands deep domain expertise. To lower barriers and streamline deployment, the AutoML community has proposed several methods to automate the FE process [ 19 , 26 â€“28 , 52 , 56 ]. Many end-to-end AutoML systems also incorporate FE by casting it as a search problem over a predefined space of transformation operations and pipeline templates [ 11 , 30 , 31 , 47 ]. Leveraging an optimizer like Bayesian optimization (BO), they explore finite configurations to construct FE pipelines. Despite facilitating automation, these traditional approaches come with three fundamental drawbacks: (i) They are semantics-agnostic and depend on expensive trial-and-error without leveraging domain priors; (ii) They enforce rigid FE pipeline templates, often restricting pipelines to fixed sequences (e.g., generation followed by selection [ 19 , 56 ]) or isolated sub-tasks, precluding dynamic interleaving or flexible composition; (iii) They restrict the search to closed operation libraries (e.g., a small set of built-in preprocessing algorithms or basic arithmetic), which prevents discovering task-specific operations beyond predefined primitives. To address these gaps, Large Language Models (LLMs) have been introduced to automated FE [ 7, 12 , 16 , 50 ]. Through domain knowledge and code generation capabilities, LLMs can propose semantically grounded operations and bespoke logic beyond fixed operation libraries, alleviating drawbacks (i) and (iii) mentioned above. However, most existing methods target only a single FE subtaskâ€”most notably feature generation [ 1, 12 , 18 , 40 , 57 ]â€”resulting in a homogeneous and simplistic FE pipeline. This leads to our first challenge: C1: Free-form FE. How can we empower LLMs to construct a truly free-form, human-expert-like FE pipeline that flexibly composes heterogeneous operations (preprocessing, transformation, generation, and selection) rather than only homogeneous ones? 

While LLMs are inherently well-suited for FE, existing methods overlook the critical synergy with Hyperparameter Optimization (HPO) of the downstream ML model. In modern AutoML, HPO is predominantly driven by Bayesian optimization (BO) due to its sample efficiency and uncertainty modeling [ 31 , 43 , 45 ]. Consequently, most LLM-based FE methods that aim for end-to-end performance must attach a BO-based HPO module. Because the two optimizers are heterogeneous and operate over differently represented spaces, they typically fall back to a greedy sequential routineâ€”first optimizing FE under a fixed model, then tuning hyperparameters on the frozen features (Figure 1, center) [ 12 , 18 , 33 , 40 ]. This greedy strategy ignores the strong dependency between feature representation and model capacity. In contrast, traditional methods (e.g., Auto-sklearn [ 11 ]) naturally enable joint optimization by enforcing a homogeneous search space (Figure 1 left). This creates a â€œcapability-integration paradoxâ€: LLMs excel at feature exploration but struggle to integrate seamlessly with parameter tuning. To combine LLM-based FE with BO-based HPO without reverting to greedy decoupling, we advocate an interleaved optimization, which raises two further challenges: 

C2: Collaborative FEâ€“HPO optimization. How can we jointly optimize LLM-based FE and HPO in a coupled manner, 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 3

rather than optimizing each in isolation? and C3: Task-adaptive scheduling. How should we allocate budget between FE and HPO, given that their marginal utility varies across tasks? 

To address these challenges, we propose CoFEH (Figure 1 right), a novel framework for holistic ML pipeline optimization. CoFEH is designed to unleash the full potential of LLMs in crafting free-form, human-expert-like FE pipelines, while coupling this with a state-of-the-art BOâ€“based HPO. Our contributions are as follows: 

â€¢ For C1 , we propose an LLM-driven tree-of-thought FE pipeline optimizer to explore unconstrained pipeline topologies and operations, achieving truly free-form feature engineering. 

â€¢ For C2 , we introduce a mutual conditioning mechanism, which establishes a bidirectional information flow between the LLM and BO, enabling them to make decisions conditioned on each other to avoid isolated, sub-optimal tuning. 

â€¢ For C3 , we introduce a PUCB-based dynamic optimizer selector that adaptively allocates the optimization budget between FE and HPO, facilitating efficient interleaved optimization. 

â€¢ Experiments on 28 public datasets show that CoFEH outperforms state-of-the-art traditional and LLM-based baselines in both standalone FE and end-to-end pipeline optimization. 

2 Background and Motivation 2.1 The Formal Machine Learning Pipeline 

The standard supervised machine learning pipeline can be formalized as a hierarchical optimization problem. Given a dataset D = {( xğ‘– , ğ‘¦ ğ‘– )} ğ‘ ğ‘– =1 where xğ‘– âˆˆ X represents the raw input space and ğ‘¦ ğ‘– âˆˆ Y the target, the objective is to learn a mapping from raw data to target that minimizes the generalization error. This process is typically decomposed into two interconnected sub-problems: Feature Engineering (FE) [ 19 , 26 â€“28 ] of data and Hyperparameter Optimization (HPO) [4, 10, 20, 32] of ML model. To automate pipeline construction, traditional AutoML systems cast the entire process into a black-box optimization framework, and leverage method like BO [ 11 , 30 , 31 , 47 ], genetic programming [ 41 ], reinforcement learning [ 9], and ADMM [ 35 ] to navigate the search space. Complementing the traditional approaches, Large Language Models (LLMs) have introduced a knowledge-driven paradigm, acting as expert-like agents that propose task-adaptive feature transformation and model configurations [16, 18, 33, 34, 37, 38, 51]. In the remainder of this section, we analyze prior work from the perspectives of FE and HPO, and discuss the key challenges and opportunities in jointly optimizing them within a unified framework. 

2.2 Feature Engineering 

We define FE in a broad sense, covering the entire process from raw data to the model input space. Let T denote the space of typed feature operations , encompassing data preprocessing, transformations, feature generation, and selection, etc. Consequently, an FE pipeline can be expressed as a composition of ğ‘˜ operations, 

ğ‘‡ = ğ‘¡ ğ‘˜ â—¦ ğ‘¡ ğ‘˜ âˆ’1 â—¦ Â· Â· Â· â—¦ ğ‘¡ 1, ğ‘¡ ğ‘— âˆˆ T . (1) The ultimate goal of this FE pipeline is to project the raw input space X into an optimized latent space Xâ€² = ğ‘‡ (X) for the downstream ML model. Ideally, feature engineering is a creative and semantics-intensive process driven by domain expertise. As illustrated in the bottom panel of Figure 2, human experts do not adhere to a rigid template. Instead, they leverage prior knowledge and intuition to navigate an unbounded search space to build a free-form pipeline. 

> Manuscript submitted to ACM

4 Beicheng Xu et al. timestamp log 

preprocessor rescaler balancer transformer 

Std Robust â€¦ â€¦ PCA SVD RFE â€¦

feature generation feature selection     

> operators : +, âˆ’, Ã—,Ã·,Min, Max

## Mindware 

## OpenFE 

Feature Selection 

> (Domain-aware Selection )

Feature Generation 

> (Complex & Novel Logic )

Transformat ion  

> (Custom & Anomaly
> Handling )

Domain Knowledge 

Integration 

> (External Data Integration)

Human Expert    

> (Intuition &Domain Knowledge )

## sin interaction time-lag 

ï¼‹ï¼ Ã—Ã· 

## f

PCA 

Encoding 

Unbounded Search Space & Flexible Pipeline 

## shap 

stat. testing 

> evaluation

## Human 

## Expert 

> Fig. 2. AutoML vs. human expert: The freedom of FE.

However, such an unbounded process remains beyond the reach of traditional optimization algorithms (e.g., BO), which necessitate well-defined, finite search spaces. Consequently, existing AutoML systems typically impose severe structural constraints to make the problem tractable [ 11 , 41 , 47 ]. As depicted in the upper panels of Figure 2 (e.g., MindWare [ 31 ], OpenFE [ 56 ]), these simplifications result in three limitations: (i) Semantic agnosticism: Lacking the capacity to incorporate domain knowledge, these systems rely exclusively on brute-force, data-driven trial-and-error. (ii) 

Rigid topology: The pipeline structures are ossified into limited and fixed sequences, preventing dynamic re-ordering or recursion. For instance, MindWare enforces a strict four-stage workflow, while OpenFE is confined to a generation-selection pipeline. (iii) Restricted operation set: The search is confined to a closed library of predefined algorithms or mathematical primitives (e.g., +, âˆ’, Ã—), precluding novel operations beyond the systemâ€™s hard-coded scope. To overcome these bottlenecks, recent advances have proposed LLM-based FE as an alternative. With extensive domain knowledge and generation capabilities, LLMs can emulate human-like semantic reasoning, enabling the construction of flexible FE pipelines with unbounded operations [ 16 ]. This potential has sparked a wave of research across the FE spectrum, including preprocessing [ 6, 50 ], feature selection [ 7, 23 ], and feature generation [ 1 , 12 , 18 , 40 , 57 ]. However, these studies typically focus on isolated sub-tasks, most notably feature generation. Consequently, they yield monolithic pipelines restricted to homogeneous operations, precluding the dynamic interleaving of diverse FE operations. 

Conclusion #1: The intrinsically semantic nature of FE makes LLMs its natural architect, yet fully harnessing this power requires adopting truly free-form pipeline topologies. 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 5

2.3 Hyperparameter Optimization 

Given the transformed features Xâ€², a learner A is selected to induce a model. In AutoML, this is formulated as the Combined Algorithm Selection and Hyperparameter Optimization (CASH) problem [ 47 ] In this work, we use HPO as an umbrella term that includes CASH, where the configuration space Î› jointly encompasses both the choice of the learning algorithm (e.g., XGBoost, MLP) and its associated hyperparameters. The goal of HPO is to identify the optimal configuration ğ€ âˆ— that minimizes the generalization error. The prevailing approach for solving this task is Bayesian Optimization (BO) [ 11 , 30 , 31 , 43 , 45 , 47 , 48 ]. BO solves black-box problems by iterating three steps [ 4, 22 , 46 ]: 1) fit a surrogate ğ‘“ on the observed data O = {( ğ€ ğ‘– , ğ‘£ ğ‘– )} ğ‘› âˆ’1 

> ğ‘– =1

; 2) sample and select the next configuration by maximizing an acquisition function ğ›¼ : ğ€ ğ‘› = arg max ğ€ ğ›¼ (ğ€ ; ğ‘“ ); 3) evaluate 

ğ€ ğ‘› to obtain the performance ğ‘£ ğ‘› and update O â† O âˆª {( ğ€ ğ‘› , ğ‘£ ğ‘› )} . BO is favored for its principled modeling of complex search spaces and acquisition-driven balance of exploration and exploitation. While recent works have tried to adapt LLMs for HPO [ 8, 34 , 36 , 55 ], LLM optimizers suffer from inherent limitations, lag behind BO methods, and prove unreliable in many practical scenarios [ 21 ]. They lack a surrogate model for objective and uncertainty quantification, and struggle to utilize the full optimization history due to context window limits. 

Conclusion #2: BO remains the gold standard for HPO, surpassing the current reach of LLMs in hyperpa-rameter spaces. 

2.4 Joint Optimization of FE and HPO 

While FE and HPO are often treated as orthogonal tasks, they are in fact strongly coupled. We formulate AutoML as an optimization problem that simultaneously searches for the optimal FE pipeline ğ‘‡ âˆ— and model configuration ğ€ âˆ— to minimize the validation loss: 

(ğ‘‡ âˆ—, ğ€ âˆ—) = arg min    

> ğ‘‡ âˆˆ T ğ‘˜ ,ğ€ âˆˆÎ›

Lğ‘£ğ‘ğ‘™ 



A ( ğ‘‡ (D ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ); ğ€ ),ğ‘‡ (D ğ‘£ğ‘ğ‘™ )



, (2) where T ğ‘˜ represents the unbounded space of feature engineering pipelines and Î› denotes the hyperparameter space of learner A.Existing approaches to this problem can be categorized based on the nature of their optimizers, as visualized in Figure 1. (i) Homogeneous optimizer framework (Figure 1 left), employs a single type of optimizer for both tasks, which naturally enables a joint optimization. Traditional AutoML systems, such as TPOT [ 41 ], Auto-sklearn [ 11 ], and Mindware [ 31 ], integrate FE and HPO into a unified yet restricted search space, enabling joint modeling and optimization. Similarly, LLM-based frameworks like ML-Master [ 37 ] and AIDE [ 24 ] treat ML pipeline generation as a unified code synthesis task, prompting LLMs to co-propose feature transformations and model configurations within a single reasoning context. (ii) Heterogeneous optimizer framework (Figure 1 center), adopts distinct optimization strategies for FE and HPO. This scenario typically necessitates a sequential, greedy strategy: optimizing FE with a fixed learner, followed by HPO on the derived feature set. Traditional methods (e.g., AutoFeat [ 19 ], OpenFE [ 56 ]) optimize features via fixed proxy models before performing post-hoc HPO. Similarly, LLM-based frameworks such as CAAFE [ 18 ]and ELLM-FT [ 12 ] focus exclusively on feature evolution, treating HPO as an isolated downstream task. Crucially, this sequential approach suffers from greedy myopia : it fails to capture the strong dependency between features and model hyperparameters. By fixing hyperparameters during FE, the optimizer inevitably discards potential high-value features that require specific hyperparameter tuning to perform well, thereby trapping the system in sub-optimal local minima. 

> Manuscript submitted to ACM

6 Beicheng Xu et al. Ori Dataset s0

Dataset s1

Dataset s5

Dataset s3

New Dataset 

Global Memory 

Op_1: Log   

> (â€¦,â€¦,âˆ†score:0.1)

Op_2: PCA   

> (â€¦,â€¦,âˆ†score :0.2 )
> ...

Op_ n: Encoding      

> (â€¦,â€¦,âˆ†score:0.1)
> Dataset Info: [ â€¦]
> Ancestor FE pipeline: [â€¦]
> Memory: [ â€¦]
> Instruction: [â€¦]
> Plan: Create â€¦
> Code:
> ``` def op(df): â€¦ ```

â€¦

Steerable Reasoning 

> Input
> Output

Op_3: Interaction   

> (â€¦,â€¦,âˆ†score:0.2)

MCTS Search Tree 

â€¦

Initialization Op. 

Exploration Op. 

Exploitation Op. 

Selection Path 

â‘ 

Selection 

â‘¡ Expansion 

â‘¢ Playout â‘£

Backpropagation 

Retrieve 

ops from 

memory 

Store ops and scores 

> Fig. 3. FE workflow of CoFEH.

Therefore, although Conclusions #1 and #2 suggest that LLMs are better suited for FE while BO is more effective for HPO, jointly optimizing them is non-trivial due to their inherent heterogeneity. Instead of a greedy sequential optimization, we argue for an interleaved optimization strategy that continuously evaluates different (FE pipeline, model configuration) combinations to avoid poor local optima. This raises two key challenges: (i) mutual conditioning and shared context â€”the LLM must refine FE with awareness of how features interact with different configurations, and BO must model hyperparameters conditioned on the FE pipeline. Otherwise, if the LLM evaluates feature quality without distinguishing the underlying configurations (and vice versa for BO), the historical performance data becomes noisy and misleading, obscuring the true causal impact of any single modification. (ii) adaptive resource allocation â€”the system must decide when to invest budget in improving FE versus HPO, since their marginal utility varies substantially across tasks (e.g., some datasets are feature-sensitive while HPO yields limited gains, and vice versa). 

Conclusion #3: Effective AutoML requires interleaving LLM-based FE and BO-based HPO, with mutual conditioning and task-adaptive budget allocation. 

3 Method 

In this section, we present CoFEH, a framework for LLM-driven Feature Engineering empowered by Co llaborative 

Hyperparameter optimization. It comprises three core components: (i) an LLM-based Tree-of-Thought search for FE pipeline; (ii) a mutual condition mechanism that enables collaborative tuning between the LLM-driven FE and BO-based HPO; and (iii) a dynamic optimizer selector to adaptively allocate resources across the optimization process. 

3.1 LLM-based Feature Engineering 

To effectively navigate the unbounded and semantics-intensive search space of feature engineering, we reformulate the construction of the pipeline from Equation 1 as a sequential decision-making problem . We define a dataset state ğ‘  

as the representation of the data at a given stage of transformation, where the initial state ğ‘  0 = X is the raw dataset. Each operation ğ‘¡ ğ‘— âˆˆ T is viewed as an action that transitions the system from state ğ‘  ğ‘— âˆ’1 to a new state ğ‘  ğ‘— = ğ‘¡ ğ‘— (ğ‘  ğ‘— âˆ’1).

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 7Consequently, the optimization of the FE pipeline is equivalent to finding an optimal sequence of actions (ğ‘¡ 1, ğ‘¡ 2, . . . , ğ‘¡ ğ‘˜ )

that maximizes the downstream performance reward: 

max 

> ğ‘¡ 1,...,ğ‘¡ ğ‘˜

Reward (ğ‘¡ ğ‘˜ â—¦ ğ‘¡ ğ‘˜ âˆ’1 â—¦ Â· Â· Â· â—¦ ğ‘¡ 1 (ğ‘  0)) . (3) Standard linear prompting is ill-suited for this complex search space as it follows a singular reasoning path without the capacity for backtracking. To bridge this gap, we adopt the Tree of Thought (ToT) paradigm [ 53 ], enabling the LLM to explore multiple reasoning branches. As illustrated in the search tree (Figure 3, left), intermediate dataset states ( ğ‘  ğ‘– ) are treated as â€œthoughtsâ€â€”discrete nodes in a hierarchical search space connected by LLM-generated FE operations. We implement this ToT search via a Monte Carlo Tree Search (MCTS) [ 29 ], which systematically balances the discovery of novel operations with the exploitation of historical knowledge through four iterative steps (Figure 3): 1) Selection :MCTS traverses from the root ( ğ‘  0) along a selection path to identify the most promising dataset state for further expansion. 2) Expansion : the LLM is prompted to generate a new FE operation to process the dataset in the selected node. 3) Playout : Utilizing the executable code synthesized by the LLM, the dataset is transformed and subsequently evaluated through a downstream ML model. This process returns a validation score ğ‘£ ğ‘– , thereby instantiating a new dataset state ğ‘  ğ‘– +1 as a successor node in the tree. 4) Backpropagation : The results are propagated back to update three statistics for each ancestor node ğ‘  : visit count ğ‘ ğ‘  , cumulative reward ğ‘… ğ‘  , and subtree best performance ğ‘£ max  

> ğ‘ 

(the maximum score observed within its subtree). We first compute a binary reward ğ‘Ÿ = I(ğ‘£ ğ‘›ğ‘’ğ‘¤ > ğ‘£ max  

> ğ‘  0

), indicating whether a new global optimum has been achieved. Subsequently, we update the statistics: ğ‘ ğ‘  â† ğ‘ ğ‘  + 1, ğ‘… ğ‘  â† ğ‘… ğ‘  + ğ‘Ÿ , and 

ğ‘£ max  

> ğ‘ 

â† max (ğ‘£ max  

> ğ‘ 

, ğ‘£ ğ‘›ğ‘’ğ‘¤ ).

3.1.1 Selection Down the MCTS Tree. The selection phase initiates at the root node ğ‘  0, representing the original dataset. It traverses the search tree by recursively choosing the child node that maximizes the Upper Confidence Bound for Trees (UCT) criterion [2]: 

ğ‘  âˆ— = arg max   

> ğ‘  â€²âˆˆchildren (ğ‘  )

Â©Â«

ğ‘„ (ğ‘  â€²) + ğ¶ 1 Â·âˆšï¸„ ln ğ‘ ğ‘  

ğ‘ ğ‘  â€²

ÂªÂ®Â¬

, (4) where ğ¶ 1 is a hyperparameter balancing exploitation and exploration. The exploitation term ğ‘„ (ğ‘  â€²) evaluates the potential quality of the candidate feature transformation path and is defined as: 

ğ‘„ (ğ‘  â€²) = ğ‘… ğ‘  â€²

ğ‘ ğ‘  â€²

+ Ëœğ‘£ max  

> ğ‘  â€²

, (5) where the first term ğ‘… ğ‘  â€² /ğ‘ ğ‘  â€² represents the average reward, reflecting the historical success rate of discovering superior states along this branch. The second term Ëœğ‘£ max  

> ğ‘  â€²

denotes the normalized best validation performance observed within the subtree rooted at ğ‘  â€² (minâ€“max normalized using the global range of observed metric scores). This formulation of ğ‘„ (ğ‘  â€²)

ensures that the search prioritizes paths exhibiting both high absolute performance (via Ëœğ‘£ max ) and strong potential for iterative improvement (via the average reward). Through this mechanism, the algorithm identifies a selection path extending from ğ‘  0 to a non-fully expanded target node, which is visualized in Figure (left) through nodes with bolded borders. 

3.1.2 Expansion Through Steerable Reasoning. Upon identifying a target node ğ‘  base through UCT policy, the steerable rea-soning agent expands the search tree by synthesizing a new FE operation. We construct a structured prompt for the LLM comprising the following core components: (i) dataset info ( ğœ“ ): Semantic descriptions and statistical metadata of the base dataset to provide domain context. (ii) ancestor FE pipeline ( ğ‘‡ anc ): The sequence of operations previously applied from ğ‘  0

> Manuscript submitted to ACM

8 Beicheng Xu et al. to ğ‘  base , which defines the current data state. (iii) memory ( Mğ‘  base ): A collection of high-performing operations archived from previous successful trials. Crucially, this component is only provided when the agent is in Exploitation mode to facilitate refinement. (iv) Directive ( ğ‘‘ ): An optimization instruction ğ‘‘ âˆˆ { Initialization , Exploration , Exploitation }

that explicitly steers the LLMâ€™s strategy. Formally, the LLM generates a reasoning chain R followed by the code for a new operation ğ‘¡ new :

(R , ğ‘¡ new ) = LLM  ğœ“,ğ‘‡ anc , Mğ‘  base , ğ‘‘ . (6) The synthesized operation code is then executed to transform the base dataset, instantiating a new dataset state: 

ğ‘  new = ğ‘¡ new (ğ‘  base ). Crucially, the directive ğ‘‘ explicitly governs the search behavior: 

â€¢ Initialization is invoked specifically for the root node ğ‘  0 to generate a high-quality initial FE operation. 

â€¢ Exploration encourages the LLM to propose novel operations to probe unknown regions of the search space. 

â€¢ Exploitation distills elite experiences and operations from Mglobal to refine the current dataset. Detailed prompt template and example are shown in Appendix A.2. A node is regarded as not fully expanded and remains eligible for selection until it satisfies a predefined expansion quota. The root node ğ‘  0 is treated as non-fully expanded until the Init. directive has been executed five times. Any other node is considered fully expanded only after both Exploration and Exploitation directives have been performed twice. This ensures that each promising dataset state is thoroughly investigated through both creative discovery and historical refinement before the search moves deeper. 

3.1.3 Global Memory and Operation Retrieval. As illustrated in the central module of Figure 3, every historical operation 

ğ‘¡ generated during the search is archived in the Global Memory Mglobal . Each entry is stored as a tuple (R , Fğ‘¡ , ğ‘£, Î”ğ‘£ ),which includes: (i) the reasoning chain R, (ii) the subset of features Fğ‘¡ required for the operation, which is parsed from the structured format of R, (iii) the resulting performance score ğ‘£ , and (iv) the relative improvement Î”ğ‘£ = ğ‘£ âˆ’ ğ‘£ base 

achieved over its parent node. To support the Exploitation directive, we employ a two-stage retrieval mechanism to identify high-utility operations: 1) Functional filtering : We first ensure semantic compatibility by matching the required feature set Fğ‘¡ with the features currently available in state ğ‘† base . Only operations whose functional dependencies are fully satisfied by the current dataset are retained as valid candidates. 2) Pareto-based selection : From the filtered candidates, we identify high-quality operations by balancing absolute performance ğ‘£ and relative gain Î”ğ‘£ . We prioritize operations with high Î”ğ‘£ 

for their strong transformative potential, as well as those with high ğ‘£ , which represent the ability to refine performance even when the pipeline has reached well-performing regimes. Formally, we construct a Pareto frontier in the (ğ‘£, Î”ğ‘£ )

objective space and select the non-dominated operations to serve as â€œelite experiencesâ€ Mğ‘  base within the steerable reasoning prompt. This dual-metric selection ensures the LLM distills insights from both breakthrough transformations and marginal refinements recorded in the search history. 

3.2 Collaborative Tuning with HPO 

The final efficacy of an FE pipeline ğ‘‡ is inextricably linked to the hyperparameters ğ€ of the downstream ML model. A decoupled approach, where FE is optimized independently of HPO, often results in the greedy myopia discussed in Section 2.4â€”the premature rejection of promising feature transformations simply because they perform poorly under default model configurations. To address this, we propose a collaborative tuning framework that performs an interleaved exploration of the joint space of (ğ‘‡ , ğ€ ) combinations to ensure the discovery of global optima. The core 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 9challenge of joint optimization is the bidirectional dependency: BO requires a promising feature set to optimize ğ€ , while the LLM requires an optimized model to accurately evaluate the potential of a pipeline ğ‘‡ . We resolve this through a 

mutual conditioning mechanism .

3.2.1 BO-based HPO Conditioned on FE. In contrast to conventional HPO that operates on a static dataset, the BO conditioned on FE must navigate a dynamic search tree containing multiple dataset states. The primary challenge lies in modeling the joint influence of the FE pipeline and configuration to determine which specific dataset state ğ‘  paired with which configuration ğ€ , yields the global optimum. To achieve this, we redefine the two core components of BO: the surrogate model and the acquisition function optimizer. 

Surrogate model : Since the FE pipeline lacks an explicit, continuous search space, we utilize meta-features ğœ™ (ğ‘  ) to characterize the dataset state after transformations. This allows the surrogate model to map discrete tree nodes into a representative feature space. We formalize the training dataset for the surrogate model ğ‘“ as: 

DBO =   [ğœ™ (ğ‘  ğ‘– ), ğ€ ğ‘–,ğ‘— ], ğ‘£ ğ‘–,ğ‘— 

 | ğ‘  ğ‘– âˆˆ V tree , âˆ€ğ‘— , (7) where Vtree is the set of all nodes in the MCTS tree. ğ€ ğ‘–,ğ‘— and ğ‘£ ğ‘–,ğ‘— denote the ğ‘— -th ML configuration and its score evaluated on node ğ‘  ğ‘– . We employ Random Forest (RF) [ 22 ] as the surrogate model due to its support for categorical variables and computational efficiency. 

Acquisition function optimizer : To identify the most promising (ğ‘ , ğ€ ) pair, the optimizer explores the joint space through a hybrid sampling strategy to construct a candidate pool Pcand :

â€¢ Local search : For each node in the tree, we perform perturbations around all its historically evaluated configu-rations within the hyperparameter space Î›. These perturbed configurations are paired with the corresponding nodeâ€™s meta-features to capture local improvements. 

â€¢ Random search : We perform global random sampling across the hyperparameter space Î›. Each sampled configu-ration is combined with the meta-features of every node in the current tree to ensure a broad exploration of the joint space. Finally, the surrogate model predicts the performance of the combinations in the candidate pool Pcand , after which an acquisition function (e.g., Expected Improvement [ 17 ]) quantifies their utility and selects the optimal combination 

(ğ‘  âˆ—, ğ€ âˆ—). This mechanism simultaneously recommends the next ML configuration and identifies the specific dataset state ğ‘  âˆ— best positioned to leverage its potential. 

3.2.2 LLM-based FE Conditioned on HPO. On the other hand, the MCTS-based FE search must be informed by the HPO process. We modify the original FE search logic to incorporate two levels of HPO-driven conditioning: (i) During the Selection phase, the maximum performance Ëœğ‘£ max  

> ğ‘  â€²

in Equation 5 is determined by HPO; specifically, when the BO discovers a superior validation score ğ‘£ âˆ— at any given node, a localized update propagates this new performance "ceiling" back through the nodeâ€™s ancestors to prioritize that branch in future selection cycles. This enables the MCTS to identify FE pipelines that exhibit the greatest synergy with optimized ML configurations. (ii) During the Playout phase, the new child node ğ‘  new inherits the best-performing ML configuration from its parent for evaluation, ensuring that new feature operations are immediately evaluated at their highest possible capacity rather than under default settings. 

> Manuscript submitted to ACM

10 Beicheng Xu et al. 

3.3 Dynamic Optimizer Selector 

While FE and HPO exhibit strong synergy, their relative contribution to performance gains varies significantly across datasets and search stages. To optimize resource allocation, we model the selection between FE and HPO as a Multi-Armed Bandit (MAB) problem. We employ a Predictor Upper Confidence Bound (PUCB) [ 44 ] policy to dynamically decide which optimizer to execute at each step ğ‘š :

ğ‘ âˆ— = arg max  

> ğ‘ âˆˆ { FE, HPO }

ğ‘„ (ğ‘ ) + ğ¶ 2 Â· ğœ” ğ‘ (ğ‘š )âˆšï¸ Ãğ‘ â€² âˆˆ { FE, HPO } ğ‘ ğ‘ â€²

1 + ğ‘ ğ‘ 

!

, (8) where ğ‘ ğ‘ is the number of times action ğ‘ has been selected. ğ¶ 2 is a hyperparameter controlling exploration pressure. 

ğœ” ğ‘ (ğ‘š ) is the time-varying prior weight for action ğ‘ at iteration ğ‘š . ğ‘„ (ğ‘ ) is the exploitation term, formulated as the empirical success rate of action ğ‘ in achieving a performance breakthrough. Specifically, ğ‘„ (FE ) represents the proportion of trials where the generated FE operation yields a score surpassing that of its parent node. ğ‘„ (HPO ) is defined as the frequency with which HPO on a given node discovers a configuration that exceed the nodeâ€™s historical best performance. 

Prior weight scheduling . we define the prior weights ğœ” ğ‘ (ğ‘š ) as linear functions of the search progress ğ‘š /ğ‘€ (ğ‘€ is the total budget): 

â€¢ ğœ” FE (ğ‘š ) = ğ‘ 1 âˆ’ ( ğ‘ 1 âˆ’ 0.5) ğ‘š ğ‘€ : Starts at ğ‘ 1 and decays to 0.5.

â€¢ ğœ” HPO (ğ‘š ) = ğ‘ 2 + ( 0.5 âˆ’ ğ‘ 2) ğ‘š ğ‘€ : Starts at ğ‘ 2 and increases to 0.5.Given ğ‘ 1 + ğ‘ 2 = 1, these prior weights can be unified using a step parameter ğ›¿ = ğ‘ 1 âˆ’0.5 

> ğ‘€

: ğœ” FE (ğ‘š ) = ğ‘ 1 âˆ’ ğ›¿ğ‘š and 

ğœ” HPO (ğ‘š ) = ğ‘ 2 + ğ›¿ğ‘š .

Theorem 3.1 (Budget Eqilibrium). Consider the rule in Equation 8 under a neutral reward signal ( ğ‘„ (ğ‘ ) = const ). Let ğ‘€ âˆˆ Z+ be the total budget. If the initial bias satisfies 0.5 â‰¤ ğ‘ 1 < ğ‘€ +1.5 

> ğ‘€ +3

, the linear scheduling of ğœ” FE and ğœ” HPO , which converges to an equilibrium of 0.5 at ğ‘š = ğ‘€ , guarantees a balanced budget distribution: 

ğ‘ FE (ğ‘€ ), ğ‘ HPO (ğ‘€ ) âˆˆ 

  ğ‘€ 

2



,

 ğ‘€ 

2

 

The proofs are provided in Appendix A.1. By leveraging this PUCB-based mechanism, the framework achieves a dual advantage: (i) With prior weight, it implements a learnability warmup that prioritizes FE initially to ensure the data is "model-ready," shielding HPO from deceptive, noise-driven signals inherent in raw features. Unlike traditional decoupled methods that optimize FE and HPO sequentially, our approach facilitates a balanced co-optimization adapted by task-specific performance gains ğ‘„ (ğ‘ ). (ii) Theorem 3.1 guarantees reaching a 5:5 budget equilibrium in neutral reward scenarios (i.e., when the exploitation term ğ‘„ (ğ‘ ) is not considered), making the actual allocation fundamentally task-governed. In summary, FE and HPO engage in a dynamic competition for computational resources, where the final allocation is adaptively steered by task-specific empirical rewards to reach the global optimum. 

3.4 Algorithm Summary 

CoFEH follows a continuous four-step iterative cycle to achieve joint optimization: 1) The dynamic optimizer selector applies the PUCB rule to determine whether to execute FE or HPO; 2) The selected module is executed, where FE utilizes MCTS to extend the most promising feature pipeline ğ‘‡ given the current HPO state, while HPO employs BO to identify the optimal configuration ğ€ conditioned on all dataset states; 3) The proposed (ğ‘‡ , ğ€ ) pair is evaluated on the 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 11 validation set to obtain a score; 4) This score is returned as a reward signal to the selector for future decisions. This process repeats until the total budget ğ‘€ is exhausted, at which point CoFEH returns the global optimal pair (ğ‘‡ âˆ—, ğ€ âˆ—).

4 Experiment 4.1 Experiment Setup 

Baselines . We evaluate CoFEH against five representative baselines, spanning both traditional and LLM-based method-ologies: â€” Two traditional automated FE methods : (i) OpenFE [ 56 ] utilizes feature boosting and two-stage pruning for automated feature generation; (ii) Mindware [31 ], an end-to-end AutoML system that automates the entire pipeline from preprocessing to HPO with BO; â€” Three LLM-based FE methods : (iii) OCTree [40 ] leverages LLMs and decision tree reasoning as linguistic feedback for iterative feature generation; (iv) ELLM-FT [12 ] integrates evolutionary strategies with LLMs to discover optimal feature transformation sequences; and (v) LFG [ 57 ] conducts a guided tree search over a predefined operation set to generate new features via LLMs. 

Datasets and Metrics . Following OCTree [ 40 ], we incorporate 19 classification datasets benchmarked by Grinsztajn et al. [ 15 ]. This is supplemented by 9 regression datasets from OpenML [ 49 ] and Kaggle. Dataset details are provided in Appendix B.1. We use the classification error ( 1 âˆ’ ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ ) for classification and ğ‘šğ‘’ğ‘ğ‘› ğ‘ ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’ğ‘‘ ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ (MSE) for regression tasks as performance metrics. 

Downstream Models . We evaluate the frameworks across three downstream model scenarios: (i) XGBoost [ 5], (ii) MLP [ 14 ], and (iii) CASH [ 47 ]: the framework must simultaneously select the best model algorithm (e.g., RF, MLP) and its optimal parameters. The hyperparameter spaces for all scenarios are detailed in Appendix B.2. 

Basic Settings . While it takes a different amount of time to evaluate the same model on different datasets, we use the evaluation iterations as the unit of budget. We compare all methods under two settings: (i) Standalone FE : conduct an FE search for a maximum of 100 iterations while maintaining default downstream model hyperparameters. (ii) Joint FE+HPO : A total budget of 200 iterations is allocated for full-pipeline optimization. Mindware leverages SMAC [ 22 ] (a BO variant) to simultaneously navigate the unified space of FE and HPO . Other decoupled baselines (OpenFE, OCTree, ELLM-FT, and LFG) execute 100 sequential rounds of FE followed by 100 rounds of HPO on the fixed optimal FE pipeline .Critically, to ensure a fair comparison, they employ the same SMAC-based HPO module. In contrast, CoFEH performs 

interleaved optimization across the entire 200-round budget. Each dataset is split into training (60%), validation (20%), and test (20%) sets. Optimization is driven by validation performance, after which the optimal FE pipeline and ML configuration are refitted on the combined training and validation set and evaluated on the test set. We report the results across three independent runs to ensure statistical robustness. 

Implementation Details . Baselines are implemented following their official open-source versions or original method-ologies. Within CoFEH, we set the exploration constants ğ¶ 1 = ğ¶ 2 = âˆš2 and configure the dynamic optimizer selector with ğ‘ 1 = 0.9 and ğ‘ 2 = 0.1, satisfying the conditions required for Theorem 3.1. Our HPO component is also modified from SMAC to be conditioned on FE for collaborative optimization. Specifically, we employ MindWareâ€™s meta-feature set (detailed in Appendix B.3) to characterize the FE pipeline for unified modeling. All LLM-based methods use gemini-2.0-flash [ 13 ], Googleâ€™s cost-efficient model, as the backbone; a comparative study of various LLMs is provided in Appendix C.1. Our codes are available in https://anonymous.4open.science/status/cofeh-159951E. 

> Manuscript submitted to ACM

12 Beicheng Xu et al. 

Table 1. Comparison in standalone FE and joint FE+HPO scenarios. We report test error (%) for classification and test MSE for regression across 3 runs (Mean Â± Std, â†“). Best and second-best results are bolded and underlined. Bottom rows show the average rank (Avg. Rank) per scenario, while Imp. (%) quantifies the relative error reduction of joint FE+HPO relative to standalone FE. 

Standalone FE Joint FE + HPO Dataset Mindware OpenFE OCTree ELLM LFG CoFEH Mindware OpenFE OCTree ELLM LFG CoFEH 

Classification Tasks 

rl 19.65 Â± 0.81 19.48 Â± 0.20 22.80 Â± 0.38 17.84 Â± 1.80 22.77 Â± 0.81 17.84 Â± 2.30 23.20 Â± 0.30 19.52 Â± 0.10 22.98 Â± 0.40 19.63 Â± 1.90 22.87 Â± 0.84 21.35 Â± 0.11 electricity 9.08 Â± 0.13 8.47 Â± 0.07 8.54 Â± 0.10 7.71 Â± 0.21 8.84 Â± 0.24 7.31 Â± 0.35 8.28 Â± 0.17 7.93 Â± 0.07 7.76 Â± 0.31 6.79 Â± 0.48 7.65 Â± 0.26 7.15 Â± 0.14 compass 23.54 Â± 0.07 19.94 Â± 0.17 25.34 Â± 0.18 22.93 Â± 0.72 22.70 Â± 0.59 22.25 Â± 0.08 23.63 Â± 0.23 20.79 Â± 0.09 23.44 Â± 0.11 23.63 Â± 0.75 21.76 Â± 0.17 19.57 Â± 0.44 

wine 23.32 Â± 0.25 21.43 Â± 0.51 22.72 Â± 0.80 22.19 Â± 0.16 23.13 Â± 0.27 19.45 Â± 0.18 22.70 Â± 1.10 22.06 Â± 0.55 23.46 Â± 0.91 23.54 Â± 0.44 23.41 Â± 0.48 19.50 Â± 0.62 

house_16H 12.30 Â± 0.04 13.03 Â± 0.18 11.62 Â± 0.10 11.76 Â± 0.27 11.60 Â± 0.18 12.06 Â± 0.05 11.92 Â± 0.24 12.38 Â± 0.23 12.18 Â± 0.25 12.31 Â± 0.42 11.96 Â± 0.19 12.27 Â± 0.04 Magic 13.92 Â± 0.26 11.87 Â± 0.23 14.21 Â± 0.15 14.13 Â± 0.15 14.14 Â± 0.05 12.86 Â± 0.26 13.12 Â± 0.36 10.99 Â± 0.22 14.42 Â± 0.15 14.39 Â± 0.28 14.24 Â± 0.14 13.53 Â± 0.40 higgs 28.10 Â± 0.13 29.67 Â± 0.09 28.22 Â± 0.01 28.18 Â± 0.02 27.74 Â± 0.07 27.14 Â± 0.32 27.22 Â± 0.06 27.24 Â± 0.09 27.46 Â± 0.09 27.45 Â± 0.09 27.31 Â± 0.11 26.35 Â± 0.02 

jannis 20.33 Â± 0.13 22.03 Â± 0.19 20.99 Â± 0.13 21.05 Â± 0.06 21.02 Â± 0.08 20.68 Â± 0.15 19.65 Â± 0.12 20.77 Â± 0.23 20.62 Â± 0.15 20.56 Â± 0.29 20.41 Â± 0.01 20.34 Â± 0.14 credit 25.10 Â± 0.36 26.21 Â± 0.01 24.31 Â± 0.15 24.47 Â± 0.28 24.13 Â± 0.29 24.21 Â± 0.03 22.59 Â± 0.31 22.94 Â± 0.07 22.95 Â± 0.09 22.92 Â± 0.07 22.90 Â± 0.24 22.58 Â± 0.21 

eye_movements 30.88 Â± 1.70 36.96 Â± 1.10 35.06 Â± 0.68 29.84 Â± 1.20 36.49 Â± 1.40 30.31 Â± 1.50 36.02 Â± 1.80 35.98 Â± 0.70 35.57 Â± 0.79 30.54 Â± 1.50 36.99 Â± 1.40 29.46 Â± 0.95 

kddCup09 21.07 Â± 0.47 22.69 Â± 0.59 20.61 Â± 0.64 20.74 Â± 0.47 19.75 Â± 0.63 20.15 Â± 0.30 19.51 Â± 0.23 21.01 Â± 0.62 20.10 Â± 0.54 19.99 Â± 0.27 19.68 Â± 0.07 19.35 Â± 0.44 

road-safety 21.10 Â± 0.01 20.92 Â± 0.09 21.94 Â± 0.28 21.42 Â± 0.20 19.87 Â± 0.68 19.56 Â± 0.07 20.49 Â± 0.05 19.60 Â± 0.23 21.04 Â± 0.28 20.69 Â± 0.86 19.99 Â± 0.41 18.53 Â± 0.28 

bank-marketing 21.68 Â± 0.25 22.16 Â± 0.27 20.64 Â± 0.23 20.80 Â± 0.36 21.00 Â± 0.11 21.38 Â± 0.30 19.79 Â± 0.13 20.55 Â± 0.01 20.16 Â± 0.07 20.31 Â± 0.67 20.15 Â± 0.14 19.72 Â± 0.16 

phoneme 14.30 Â± 0.18 13.51 Â± 0.11 13.88 Â± 0.26 14.30 Â± 0.18 14.16 Â± 0.18 12.37 Â± 0.30 14.16 Â± 0.19 13.04 Â± 0.05 14.06 Â± 0.33 14.30 Â± 0.19 14.30 Â± 0.20 12.48 Â± 0.10 

covtype 11.64 Â± 0.57 10.69 Â± 0.01 13.52 Â± 0.22 12.99 Â± 0.04 13.75 Â± 1.40 11.12 Â± 0.61 10.07 Â± 0.13 15.92 Â± 11.00 10.06 Â± 0.15 9.98 Â± 0.51 10.23 Â± 1.40 7.87 Â± 0.23 

california 9.72 Â± 0.10 9.86 Â± 0.08 9.47 Â± 0.12 9.58 Â± 0.01 9.31 Â± 0.08 9.05 Â± 0.12 9.27 Â± 0.10 9.37 Â± 0.07 9.53 Â± 0.05 9.74 Â± 0.11 9.35 Â± 0.05 8.68 Â± 0.26 

kdd_ipums_la 12.32 Â± 0.74 12.91 Â± 0.11 11.95 Â± 0.16 11.63 Â± 0.46 12.28 Â± 0.10 12.06 Â± 0.24 11.18 Â± 0.42 11.31 Â± 0.13 11.59 Â± 0.29 11.10 Â± 0.08 11.54 Â± 0.10 10.93 Â± 0.17 

MiniBooNE 5.89 Â± 0.07 6.31 Â± 0.02 6.02 Â± 0.04 5.97 Â± 0.07 5.89 Â± 0.08 5.62 Â± 0.05 5.65 Â± 0.02 5.92 Â± 0.02 5.82 Â± 0.02 5.79 Â± 0.03 5.80 Â± 0.03 5.51 Â± 0.04 

pol 1.95 Â± 0.09 1.79 Â± 0.05 1.69 Â± 0.04 1.82 Â± 0.03 1.83 Â± 0.07 1.75 Â± 0.19 1.74 Â± 0.07 1.85 Â± 0.12 1.74 Â± 0.04 1.87 Â± 0.10 1.94 Â± 0.07 1.42 Â± 0.12 

Regression Tasks 

airfoil_self_noise 2.73 Â± 0.10 2.36 Â± 0.03 2.57 Â± 0.06 2.44 Â± 0.10 2.23 Â± 0.05 2.01 Â± 0.17 2.33 Â± 0.18 2.38 Â± 0.07 2.12 Â± 0.10 2.20 Â± 0.06 1.95 Â± 0.22 1.50 Â± 0.10 

cpu_small 11.14 Â± 0.25 10.12 Â± 0.07 11.16 Â± 0.07 11.15 Â± 0.08 11.02 Â± 0.15 10.97 Â± 0.07 8.26 Â± 0.34 8.51 Â± 0.34 7.85 Â± 0.46 7.85 Â± 0.58 7.97 Â± 0.45 9.83 Â± 0.67 diamonds ( Ã—10 5) 3.18 Â± 0.01 2.84 Â± 0.01 3.04 Â± 0.04 2.98 Â± 0.06 2.85 Â± 0.02 2.72 Â± 0.04 2.92 Â± 0.01 2.68 Â± 0.02 2.90 Â± 0.02 2.87 Â± 0.05 2.84 Â± 0.01 2.60 Â± 0.05 

plasma_retinol ( Ã—10 4) 5.77 Â± 0.58 5.43 Â± 0.09 4.54 Â± 0.40 4.35 Â± 0.21 4.64 Â± 0.04 5.83 Â± 0.37 5.99 Â± 0.33 4.65 Â± 0.30 4.82 Â± 0.04 4.76 Â± 0.29 4.07 Â± 0.27 4.37 Â± 0.30 forest-fires ( Ã—10 3) 4.43 Â± 0.62 4.70 Â± 0.31 4.27 Â± 0.11 4.29 Â± 0.19 3.96 Â± 0.08 4.26 Â± 0.02 3.98 Â± 0.04 4.00 Â± 0.00 4.16 Â± 0.15 4.28 Â± 0.39 3.97 Â± 0.02 3.98 Â± 0.02 housing ( Ã—10 9) 2.23 Â± 0.00 2.16 Â± 0.00 2.16 Â± 0.03 2.09 Â± 0.00 2.00 Â± 0.07 1.97 Â± 0.06 2.06 Â± 0.01 1.99 Â± 0.02 2.10 Â± 0.04 2.04 Â± 0.03 1.99 Â± 0.06 1.89 Â± 0.01 

bike ( Ã—10 3) 1.65 Â± 0.01 1.67 Â± 0.00 1.67 Â± 0.02 1.66 Â± 0.01 1.62 Â± 0.02 1.52 Â± 0.02 1.56 Â± 0.01 1.60 Â± 0.01 1.56 Â± 0.02 1.53 Â± 0.01 1.54 Â± 0.01 1.43 Â± 0.03 

crab 5.12 Â± 0.10 5.22 Â± 0.05 5.11 Â± 0.15 5.24 Â± 0.18 5.16 Â± 0.04 5.07 Â± 0.07 4.47 Â± 0.04 4.45 Â± 0.02 4.88 Â± 0.02 4.88 Â± 0.09 4.58 Â± 0.04 4.47 Â± 0.02 insurance ( Ã—10 7) 2.82 Â± 0.00 2.66 Â± 0.00 2.72 Â± 0.18 2.52 Â± 0.10 2.51 Â± 0.05 2.45 Â± 0.07 1.98 Â± 0.05 1.97 Â± 0.04 2.01 Â± 0.08 1.89 Â± 0.02 1.90 Â± 0.01 1.90 Â± 0.07 

Avg. Rank (â†“) 4.50 4.07 3.93 3.50 3.11 1.82 3.46 3.86 4.57 3.93 3.39 1.75 Avg. Imp. (â†‘) / / / / / / +6.27% +4.98% +4.96% +3.84% +5.13% +7.03% 

4.2 Main Results 

We evaluate CoFEH and five baselines using XGBoost as the downstream ML model. The results across both standalone FE and joint FE+HPO scenarios are presented in Table 1. Several key observations emerge from the data: (i) LLM-based FE vs. traditional FE s: In the standalone FE scenario, traditional methods generally underperform compared to LLM-based approaches, with Mindware (4.52) and OpenFE (4.07) occupying the bottom average ranks. As discussed in Section 2.2, traditional frameworks are restricted to predefined, hardcoded operation sets and fixed, linear workflows. (ii) Performance of LLM baselines : Among LLM-based baselines, OCTree exhibits the weakest performance (Rank 3.93). This is primarily due to its linear reasoning structure, which lacks a backtracking mechanism. ELLM (3.54) and LFG (3.11) achieve better results by employing genetic optimization and Tree-of-Thought strategies, respectively. (iii) 

Dominance of CoFEH in standalone FE : CoFEH achieves a state-of-the-art Average Rank of 1.84, significantly outperforming the runner-up, LFG (3.11). A key strength lies in its ability to balance exploration and exploitation with a memory mechanism, as further validated by the ablation in Appendix C.2. Moreover, unlike other LLM baselines that focus strictly on individual feature generation, CoFEH enables truly free pipelines. We provide a case study of the optimal FE pipelines discovered by each method in Appendix C.3. (iv) Sequential vs. joint optimization : In the joint FE+HPO scenario, the relative performance of baselines shifts significantly. Decoupled sequential baselines (OpenFE, OCTree, ELLM, and LFG) exhibit lower improvement compared to standalone FEâ€”ranging from 3.84% to 5.13%â€”as they freeze the FE pipeline before HPO, often trapping the system in a sub-optimal state. In contrast, Mindware achieves 

Manuscript submitted to ACM CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 13                                                                                             

> Table 2. Generalization performance ( Mean Â±Std ) across CASH and MLP scenarios. Imp. (%) denotes the relative error reduction of CoFEH over the best baseline.
> Dataset LFG Mindware CoFEH Imp. ( â†‘)
> CASH Scenario
> pol 1.70 Â±0.15 1.87 Â±0.15 1.38 Â±0.10 +18.9% wine 22.13 Â±0.27 20.89 Â±0.43 18.86 Â±0.39 +9.7% airfoil_self_noise 2.72 Â±0.14 2.88 Â±0.09 1.49 Â±0.11 +45.1% housing ( Ã—10 9)2.12 Â±0.08 2.12 Â±0.02 1.78 Â±0.03 +16.0%
> MLP Scenario
> pol 1.53 Â±0.08 1.42 Â±0.07 1.36 Â±0.12 +4.6% wine 22.94 Â±0.49 22.98 Â±0.36 20.63 Â±0.21 +10.1% airfoil_self_noise 10.49 Â±3.28 10.59 Â±2.58 6.73 Â±1.34 +35.9% housing ( Ã—10 9)2.70 Â±0.03 2.77 Â±0.06 2.66 Â±0.05 +1.3%
> Avg. Rank (â†“)2.25 2.75 1.00 â€“

a notable 6.27% improvement by performing joint tuning over a unified search space. This holistic approach allows Mindware to overcome its poor FE performance (Rank 4.52), climbing to third place (Rank 3.46) in the end-to-end scenario. (v) Superiority of CoFEH in joint tuning : Through combining FE and HPO, CoFEH achieves a dominant improvement rate of 7.03% over standalone FE, further widening its lead with an Average Rank of 1.75. This success stems from our interleaved tuning strategy and a conditioned mechanism that facilitates mutual feedback between FE and HPO. In summary, CoFEH not only establishes a new benchmark for FE but also achieves a powerful â€œstrong-strongâ€ synergy in joint tuning. Appendix C.4 highlights CoFEHâ€™s robust scalability as dataset scale increases. We further provide a statistical significance analysis in Appendix C.5 to validate the consistent superiority of CoFEH. 

Cost analysis. Appendix C.6 reports API and time costs for all methods: CoFEH averages only $0.07 per task, with runtime comparable to LLM-based baselines and better than traditional methods. 

4.3 Generalization to Downstream Models 

While the preceding experiments demonstrate the efficacy of CoFEH using XGBoost, a robust FE framework should ideally exhibit model-agnostic generalization. In this section, we evaluate whether CoFEH can collaboratively optimize FE and HPO within two distinct downstream model types: deep learning (MLP) and multi-algorithm search (CASH). To mitigate API overhead, we select four representative datasets from the original benchmark: two classification tasks (pol, wine) and two regression tasks (airfoil, housing). We evaluate CoFEH against the two strongest baselines in joint FE+HPO scenario from Table 1: LLM-based LFG and traditional Mindware. The results in Table 2 demonstrate the robust generalization of CoFEH across three dimensions: (1) Consistent superiority : CoFEH consistently achieves the top performance across all datasets and scenarios with a perfect Average Rank of 1.00, maintaining a ranking hierarchy identical to the primary XGBoost experiments. (2) Significant error reduction : Compared to the strongest baseline in each task, CoFEH achieves substantial relative error reductions, most notably reaching 45.1% in the CASH scenario and 35.9% in the MLP scenario. (3) Enhanced performance ceiling : Compared with Table 1, the end-to-end performance in the CASH scenario generally surpasses that of XGBoost, indicating that CoFEH effectively leverages multi-algorithm search to identify models that are more compatible with the synthesized features. 

> Manuscript submitted to ACM

14 Beicheng Xu et al. 0 25 50 75 100 125 150 175 200 

# Number of Evaluations 

# 0.2 

# 0.4 

# 0.6 

# 0.8 

# 1.0 

> Normalized Performance

# CoFEH 

# CoFEH (w/o Selector) 

# CoFEH (w/o Cond) 

# CoFEH (Greedy) 

> Fig. 4. Ablation study of collaborative tuning.

4.4 Effectiveness of Collaborative Tuning 

In this section, we evaluate the impact of the collaborative tuning framework, which comprises two components: the mutual conditioning mechanism for bidirectional information exchange between FE and HPO, and the dynamic optimizer selector for resource scheduling. To isolate their contributions, we conduct an ablation study comparing the full CoFEH against three variants: (i) w/o Cond, where HPO feedback and meta-feature conditioning are removed, forcing both modules to generate independent proposals that are heuristically paired with the counterpartâ€™s current global best for evaluation; (ii) w/o Selector, which replaces dynamic scheduling with a fixed, alternating execution strategy; and (iii) Greedy, the sequential paradigm that follows the standard practice of performing 100 iterations of FE followed by 100 iterations of HPO. As illustrated in Figure 4, which plots the average normalized best-so-far validation performance across four representative datasets, the Greedy approach yields the poorest results, confirming the sub-optimality of decoupled optimization. We find that ablating either core mechanism significantly degrades convergence; notably, we verify in Appendix C.7 that FE meta-features are essential for enhancing the fitting precision of the HPO surrogate model. Ultimately, CoFEH achieves the highest performance ceiling with a dominant Average Test Rank of 1.25, substantially outperforming the w/o Selector (2.25), w/o Cond (2.75), and Greedy (3.75) variants, confirming the necessity of coupled information-resource management. To further analyze the behavior of the optimizer selector, Figure 5 examines the distribution of FE iterations across the 28 datasets in the main experiment. (i) Temporal dynamics : As shown in Figure 5a, the proportion of datasets executing FE follows a clear downward trend. This aligns with our design intuition of prior weights: prioritizing structural data refinement in early stages. Notably, both optimizers remain active throughout the process, ensuring continuous co-evolution. (ii) Task-adaptive allocation : Figure 5b illustrates the distribution of total FE iterations per dataset. While the mean proportion of 0.53 empirically substantiates the budget equilibrium predicted in Theorem 3.1, the significant variance (ranging from 0.37 to 0.7) highlights the frameworkâ€™s task-adaptivity. This reflects the selectorâ€™s ability to detect whether a specific task is more sensitive to feature representations or ML configurations. We provide a 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 15 0 40 80 120 160 200 

## Number of Evaluations 

0.0 

0.5 

1.0 

> Datasets with FE (%)

95% CI 

Linear Fit 

> (a) FE prop. across iterations.

0.4 0.5 0.6 0.7 

## Iterations with FE (%) 

0

2

4

6

8

> Number of Datasets

Mean (0.532) (b) FE prop. across datasets. Fig. 5. FE prop. driven by the dynamic optimizer selector 

case study in Appendix C.8 demonstrating how these resource shifts align with the inherent properties of different datasets. 

5 Conclusion 

In this paper, we presented CoFEH, a collaborative framework that bridges semantic FE discovery and numerical HPO through adaptive scheduling and mutual conditioning. CoFEH achieves superior end-to-end performance over traditional AutoML and LLM-based baselines. The framework is characterized by its high extensibility: (i) it is agnostic to specific HPO implementations and maintains full compatibility with state-of-the-art BO variants; (ii) it enables seamless extension to image and text data via simple prompt modifications, offering a scalable foundation for future work. 

> Manuscript submitted to ACM

16 Beicheng Xu et al. 

References 

[1] Nikhil Abhyankar, Parshin Shojaee, and Chandan K Reddy. 2025. LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers. arXiv preprint arXiv:2503.14434 (2025). [2] Peter Auer. 2002. Using confidence bounds for exploitation-exploration trade-offs. Journal of machine learning research 3, Nov (2002), 397â€“422. [3] RÃ©mi Bardenet, MÃ¡tyÃ¡s Brendel, BalÃ¡zs KÃ©gl, and Michele Sebag. 2013. Collaborative hyperparameter tuning. In International conference on machine learning . PMLR, 199â€“207. [4] James Bergstra, RÃ©mi Bardenet, Yoshua Bengio, and BalÃ¡zs KÃ©gl. 2011. Algorithms for hyper-parameter optimization. Advances in neural information processing systems 24 (2011). [5] Tianqi Chen. 2016. XGBoost: A Scalable Tree Boosting System. Cornell University (2016). [6] Hyunjun Choi, Jay Moran, Nicholas Matsumoto, Miguel E Hernandez, and Jason H Moore. 2023. Aliro: an automated machine learning tool leveraging large language models. Bioinformatics 39, 10 (2023), btad606. [7] Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. 2022. Lmpriors: Pre-trained language models as task-specific priors. arXiv preprint arXiv:2210.12530 (2022). [8] Abdoulatif CissÃ©, Xenophon Evangelopoulos, Vladimir V. Gusev, and Andrew I. Cooper. 2025. Language-based Bayesian optimization research assistant (BORA). In Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence (Montreal, Canada) (IJCAI â€™25) . Article 553, 9 pages. doi:10.24963/ijcai.2025/553 [9] Iddo Drori, Yamuna Krishnamurthy, Remi Rampin, Raoni DE PAULA LOURENCO, Jorge Piazentin Ono, Kyunghyun Cho, Claudio Silva, and Juliana Freire. 2021. AlphaD3M: Machine Learning Pipeline Synthesis. In ICML AutoML Workshop .[10] Stefan Falkner, Aaron Klein, and Frank Hutter. 2018. BOHB: Robust and efficient hyperparameter optimization at scale. In International conference on machine learning . PMLR, 1437â€“1446. [11] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. 2022. Auto-sklearn 2.0: Hands-free automl via meta-learning. Journal of Machine Learning Research 23, 261 (2022), 1â€“61. [12] Nanxu Gong, Chandan K Reddy, Wangyang Ying, Haifeng Chen, and Yanjie Fu. 2025. Evolutionary large language model for automated feature transformation. In Proceedings of the AAAI conference on artificial intelligence , Vol. 39. 16844â€“16852. [13] Google DeepMind. 2025. Gemini 2.0 Flash . https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash [14] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2021. Revisiting deep learning models for tabular data. Advances in neural information processing systems 34 (2021), 18932â€“18943. [15] LÃ©o Grinsztajn, Edouard Oyallon, and GaÃ«l Varoquaux. 2022. Why do tree-based models still outperform deep learning on typical tabular data? 

Advances in neural information processing systems 35 (2022), 507â€“520. [16] Yang Gu, Hengyu You, Jian Cao, Muran Yu, Haoran Fan, and Shiyou Qian. 2025. Large language models for constructing and optimizing machine learning workflows: A survey. ACM Transactions on Software Engineering and Methodology (2025). [17] JosÃ© Miguel HernÃ¡ndez-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. 2014. Predictive entropy search for efficient global optimization of black-box functions. In Advances in neural information processing systems . 918â€“926. [18] Noah Hollmann, Samuel MÃ¼ller, and Frank Hutter. 2023. Large language models for automated data science: Introducing caafe for context-aware automated feature engineering. Advances in Neural Information Processing Systems 36 (2023), 44753â€“44775. [19] Franziska Horn, Robert Pack, and Michael Rieger. 2019. The autofeat python library for automated feature engineering and selection. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases . Springer, 111â€“120. [20] Yi-Qi Hu, Yang Yu, Wei-Wei Tu, Qiang Yang, Yuqiang Chen, and Wenyuan Dai. 2019. Multi-fidelity automatic hyper-parameter tuning via transfer series expansion. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3846â€“3853. [21] Beichen Huang, Xingyu Wu, Yu Zhou, Jibin Wu, Liang Feng, Ran Cheng, and Kay Chen Tan. 2024. Exploring the true potential: Evaluating the black-box optimization capability of large language models. arXiv preprint arXiv:2404.06290 (2024). [22] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2011. Sequential model-based optimization for general algorithm configuration. In 

International Conference on Learning and Intelligent Optimization . Springer, 507â€“523. [23] Daniel P Jeong, Zachary C Lipton, and Pradeep Ravikumar. 2024. Llm-select: Feature selection with large language models. arXiv preprint arXiv:2407.02694 (2024). [24] Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. 2025. Aide: Ai-driven exploration in the space of code. arXiv preprint arXiv:2502.13138 (2025). [25] Michael I Jordan and Tom M Mitchell. 2015. Machine learning: Trends, perspectives, and prospects. Science 349, 6245 (2015), 255â€“260. [26] Gilad Katz, Eui Chul Richard Shin, and Dawn Song. 2016. Explorekit: Automatic feature generation and selection. In 2016 IEEE 16th international conference on data mining (ICDM) . IEEE, 979â€“984. [27] Ambika Kaul, Saket Maheshwary, and Vikram Pudi. 2017. Autolearnâ€”automated feature generation and selection. In 2017 IEEE International Conference on data mining (ICDM) . IEEE, 217â€“226. [28] Udayan Khurana, Horst Samulowitz, and Deepak Turaga. 2018. Feature engineering for predictive modeling using reinforcement learning. In 

Proceedings of the AAAI conference on artificial intelligence , Vol. 32. [29] Levente Kocsis and Csaba SzepesvÃ¡ri. 2006. Bandit based monte-carlo planning. In European conference on machine learning . Springer, 282â€“293. Manuscript submitted to ACM CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 17 

[30] Erin LeDell and Sebastien Poirier. 2020. H2o automl: Scalable automatic machine learning. In Proceedings of the AutoML Workshop at ICML , Vol. 2020. 24. [31] Yang Li, Jiawei Jiang, Jinyang Gao, Yingxia Shao, Ce Zhang, and Bin Cui. 2020. Efficient automatic CASH via rising bandits. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 4763â€“4771. [32] Yang Li, Yu Shen, Jiawei Jiang, Jinyang Gao, Ce Zhang, and Bin Cui. 2021. Mfes-hb: Efficient hyperband with multi-fidelity quality measurements. In 

Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 8491â€“8500. [33] Ziming Li, Qianbo ZANG, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, et al . 2024. AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions. In ICLR 2025 Worshop Emergent Possibilities and Challenges in Deep Learning for Code .[34] Siyi Liu, Chen Gao, and Yong Li. 2024. Large language model agent for hyper-parameter optimization. arXiv preprint arXiv:2402.01881 (2024). [35] Sijia Liu, Parikshit Ram, Deepak Vijaykeerthy, Djallel Bouneffouf, Gregory Bramble, Horst Samulowitz, Dakuo Wang, Andrew Conn, and Alexander Gray. 2020. An ADMM based framework for automl pipeline configuration. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 4892â€“4899. [36] Tennison Liu, NicolÃ¡s Astorga, Nabeel Seedat, and Mihaela van der Schaar. 2025. Large Language Models to Enhance Bayesian Optimization. In The Twelfth International Conference on Learning Representations .[37] Zexi Liu, Yuzhu Cai, Xinyu Zhu, Yujie Zheng, Runkun Chen, Ying Wen, Yanfeng Wang, Siheng Chen, et al . 2025. ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning. arXiv preprint arXiv:2506.16499 (2025). [38] Daqin Luo, Chengjian Feng, Yuxuan Nong, and Yiqing Shen. 2024. Autom3l: An automated multimodal machine learning framework with large language models. In Proceedings of the 32nd ACM International Conference on Multimedia . 8586â€“8594. [39] Alhassan Mumuni and Fuseini Mumuni. 2025. Automated data processing and feature engineering for deep learning and big data applications: a survey. Journal of Information and Intelligence 3, 2 (2025), 113â€“153. [40] Jaehyun Nam, Kyuyoung Kim, Seunghyuk Oh, Jihoon Tack, Jaehyung Kim, and Jinwoo Shin. 2024. Optimized feature generation for tabular data via llms with decision tree reasoning. Advances in Neural Information Processing Systems 37 (2024), 92352â€“92380. [41] Randal S Olson and Jason H Moore. 2016. TPOT: A tree-based pipeline optimization tool for automating machine learning. In Workshop on automatic machine learning . PMLR, 66â€“74. [42] Bernhard Pfahringer, Hilan Bensusan, and Christophe G Giraud-Carrier. 2000. Meta-Learning by Landmarking Various Learning Algorithms.. In 

ICML . 743â€“750. [43] Pranav Poduval, Sanjay Kumar Patnala, Gaurav Oberoi, Nitish Srivasatava, and Siddhartha Asthana. 2024. Cash via optimal diversity for ensemble learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 2411â€“2419. [44] Christopher D Rosin. 2011. Multi-armed bandits with episode context. Annals of Mathematics and Artificial Intelligence 61, 3 (2011), 203â€“230. [45] Yu Shen, Yupeng Lu, Yang Li, Yaofeng Tu, Wentao Zhang, and Bin Cui. 2022. Divbo: diversity-aware cash for ensemble learning. Advances in Neural Information Processing Systems 35 (2022), 2958â€“2971. [46] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian optimization of machine learning algorithms. Advances in neural information processing systems 25 (2012). [47] Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2013. Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining . 847â€“855. [48] Anton Vakhrushev, Alexander Ryzhkov, Maxim Savchenko, Dmitry Simakov, Rinchin Damdinov, and Alexander Tuzhilin. 2021. Lightautoml: Automl solution for a large financial services ecosystem. arXiv preprint arXiv:2109.01528 (2021). [49] Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. 2014. OpenML: networked science in machine learning. ACM SIGKDD Explorations Newsletter 15, 2 (2014), 49â€“60. [50] Chenglong Wang, Bongshin Lee, Steven M Drucker, Dan Marshall, and Jianfeng Gao. 2025. Data Formulator 2: Iterative Creation of Data Visualizations, with AI Transforming Data Along the Way. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems . 1â€“17. [51] Jinglue Xu, Jialong Li, Zhen Liu, Nagar Anthel Venkatesh Suryanarayanan, Guoyuan Zhou, Jia Guo, Hitoshi Iba, and Kenji Tei. 2024. Large language models synergize with automated machine learning. arXiv preprint arXiv:2405.03727 (2024). [52] Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Yu-Feng Li, Wei-Wei Tu, Qiang Yang, and Yang Yu. 2018. Taking human out of learning applications: A survey on automated machine learning. arXiv preprint arXiv:1810.13306 31 (2018). [53] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems 36 (2023), 11809â€“11822. [54] Dani Yogatama and Gideon Mann. 2014. Efficient transfer learning method for automatic hyperparameter tuning. In Artificial intelligence and statistics . PMLR, 1077â€“1085. [55] Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mingyuan Zhou. 2023. Automl-gpt: Automatic machine learning with gpt. arXiv preprint arXiv:2305.02499 (2023). [56] Tianping Zhang, Zheyu Aqa Zhang, Zhiyuan Fan, Haoyan Luo, Fengyuan Liu, Qian Liu, Wei Cao, and Li Jian. 2023. Openfe: Automated feature generation with expert-level performance. In International Conference on Machine Learning . PMLR, 41880â€“41901. [57] Xinhao Zhang, Jinghan Zhang, Banafsheh Rekabdar, Yuanchun Zhou, Pengfei Wang, and Kunpeng Liu. 2025. Dynamic and adaptive feature generation with LLM. In Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence (Montreal, Canada) (IJCAI â€™25) .Manuscript submitted to ACM 18 Beicheng Xu et al. 

> Article 782, 9 pages. doi:10.24963/ijcai.2025/782

A Method Details A.1 Proof of Theorem 

In this appendix, we provide a detailed proof of Theorem 3.1. 

Proof. To establish the budget equilibrium property, we first formalize the conditions provided in the theorem. We assume a neutral reward signal ğ‘„ (FE ) = ğ‘„ (HPO ) = const and an exploration constant ğ¶ 2 > 0. The total iteration budget ğ‘€ is an integer, and the prior weights are governed by the following linear schedules: 

ğœ” FE (ğ‘š ) = ğ‘ 1 âˆ’ ğ›¿ğ‘š, ğœ” HPO (ğ‘š ) = ğ‘ 2 + ğ›¿ğ‘š, ğ›¿ = ğ‘ 1 âˆ’ 0.5

ğ‘€ ,

where ğ‘ 1 is subject to the boundedness constraint 0.5 â‰¤ ğ‘ 1 < ğ‘€ +1.5 

> ğ‘€ +3

.

1. Simplification of the Decision Rule 

Under the neutral reward assumption, the PUCT-based selection rule in Equation 8 reduces to maximizing the prior-weighted exploration term. Specifically, the framework selects the action ğ‘ that maximizes ğœ” ğ‘ (ğ‘š )âˆšÃ ğ‘ ğ‘ â€² 

> 1+ğ‘ ğ‘

. Since the term âˆšï¸ Ã ğ‘ ğ‘ â€² is identical for both arms at any given iteration ğ‘š , the selection indicator ğ¼ (ğ‘š + 1) for FE is defined as: 

ğ¼ (ğ‘š + 1) =

ï£±ï£´ï£´ï£²ï£´ï£´ï£³

1, if ğœ” FE (ğ‘š )  

> 1+ğ‘ FE (ğ‘š )

> ğœ” HPO (ğ‘š ) 

> 1+ğ‘ HPO (ğ‘š )

0, otherwise 

,

where ğ‘ FE (ğ‘š ) and ğ‘ HPO (ğ‘š ) are the cumulative counts of FE and HPO selections, respectively, such that ğ‘ FE (ğ‘š ) + 

ğ‘ HPO (ğ‘š ) = ğ‘š . We define an auxiliary state function ğ‘„ (ğ‘š ) as: 

ğ‘„ (ğ‘š ) = ğœ” FE (ğ‘š )  1 + ğ‘ HPO (ğ‘š ) âˆ’ ğœ” HPO (ğ‘š )  1 + ğ‘ FE (ğ‘š ).

Thus the decision rule is equivalent to: 

ğ¼ (ğ‘š + 1) =

ï£±ï£´ï£´ï£²ï£´ï£´ï£³

1, if ğ‘„ (ğ‘š ) > 00, otherwise 

,

2. Derivation of the Recurrence Relation 

First, we establish the recurrence relation for ğ‘„ (ğ‘š ). Given the linear definitions ğœ” FE (ğ‘š ) = ğ‘ 1 âˆ’ ğ›¿ğ‘š and ğœ” HPO (ğ‘š ) =

ğ‘ 2 + ğ›¿ğ‘š , it follows that: ğœ” FE (ğ‘š + 1) = ğœ” FE (ğ‘š ) âˆ’ ğ›¿ and ğœ” HPO (ğ‘š + 1) = ğœ” HPO (ğ‘š ) + ğ›¿ . The counts for the next iteration are updated based on the indicator ğ¼ (ğ‘š + 1) as ğ‘ FE (ğ‘š + 1) = ğ‘ FE (ğ‘š ) + ğ¼ (ğ‘š + 1) and ğ‘ HPO (ğ‘š + 1) = ğ‘ HPO (ğ‘š ) + ( 1 âˆ’ğ¼ (ğ‘š + 1)) .Substituting these into the definition of ğ‘„ (ğ‘š + 1), we have: 

ğ‘„ (ğ‘š + 1) = ğœ” FE (ğ‘š + 1)  1 + ğ‘ HPO (ğ‘š + 1)

âˆ’ ğœ” HPO (ğ‘š + 1)  1 + ğ‘ FE (ğ‘š + 1)

= (ğœ” FE (ğ‘š ) âˆ’ ğ›¿ )  1 + ğ‘ HPO (ğ‘š ) + ( 1 âˆ’ ğ¼ (ğ‘š + 1)) 

âˆ’ ( ğœ” HPO (ğ‘š ) + ğ›¿ )  1 + ğ‘ FE (ğ‘š ) + ğ¼ (ğ‘š + 1)

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 19 Expanding the terms and substituting ğ‘ FE (ğ‘š ) + ğ‘ HPO (ğ‘š ) = ğ‘š and ğœ” FE + ğœ” HPO = 1:

ğ‘„ (ğ‘š + 1) = ğœ” FE (ğ‘š )( 1 + ğ‘ HPO ) âˆ’ ğœ” HPO (ğ‘š )( 1 + ğ‘ FE )+ ğœ” FE (ğ‘š )( 1 âˆ’ ğ¼ (ğ‘š + 1)) âˆ’ ğœ” HPO (ğ‘š )ğ¼ (ğ‘š + 1)âˆ’ ğ›¿ (1 + ğ‘ HPO + 1 âˆ’ ğ¼ + 1 + ğ‘ FE + ğ¼ )

= ğ‘„ (ğ‘š ) + ğœ” FE (ğ‘š ) âˆ’ ğ¼ (ğ‘š + 1) âˆ’ ğ›¿ (ğ‘š + 3)

3. Boundedness Lemma ( |ğ‘„ (ğ‘š )| < 1)

We prove that for all ğ‘š âˆˆ { 0, 1, . . . , ğ‘€ }, the state function remains bounded within the open interval |ğ‘„ (ğ‘š )| < 1 via induction. 

Base case: 

At ğ‘š = 0, the cumulative counts are ğ‘ FE (0) = ğ‘ HPO (0) = 0. The state function evaluates to: 

ğ‘„ (0) = ğœ” FE (0)( 1) âˆ’ ğœ” HPO (0)( 1) = ğ‘ 1 âˆ’ ğ‘ 2 = 2ğ‘ 1 âˆ’ 1.

Given the condition 0.5 â‰¤ ğ‘ 1 < ğ‘€ +1.5 

> ğ‘€ +3

, and knowing that ğ‘€ +1.5 

> ğ‘€ +3

< 1 for any ğ‘€ > 0, it follows that 0 â‰¤ ğ‘„ (0) < 1. Thus, 

|ğ‘„ (0)| < 1 holds. 

Inductive step: 

Assume |ğ‘„ (ğ‘š )| < 1 for some iteration ğ‘š . We examine the state transition to ğ‘„ (ğ‘š + 1) based on the decision rule: 

Case 1: ğ‘„ (ğ‘š ) > 0. According to the decision rule, ğ¼ (ğ‘š + 1) = 1 (FE is selected). Substituting it into the recurrence relation: 

ğ‘„ (ğ‘š + 1) = ğ‘„ (ğ‘š ) + ğœ” FE (ğ‘š ) âˆ’ 1 âˆ’ ğ›¿ (ğ‘š + 3)

Since ğœ” FE (ğ‘š ) <= 1 and ğ›¿ (ğ‘š + 3) >= 0, it is clear that ğ‘„ (ğ‘š + 1) â‰¤ ğ‘„ (ğ‘š ) < 1. To prove the lower bound ğ‘„ (ğ‘š + 1) > âˆ’1,we substituting ğœ” FE (ğ‘š ) = ğ‘ 1 âˆ’ ğ›¿ğ‘š :

ğœ” FE (ğ‘š ) âˆ’ 1 âˆ’ ğ›¿ (ğ‘š + 3) = ğ‘ 1 âˆ’ ğ›¿ (2ğ‘š + 3) âˆ’ 1

> ğ‘ 1 âˆ’ ğ›¿ (2ğ‘€ + 3) âˆ’ 1

= ğ‘ 1 âˆ’ ğ‘ 1 âˆ’ 0.5

ğ‘€ (2ğ‘€ + 3) âˆ’ 1

= ğ‘€ + 1.5 âˆ’ ( ğ‘€ + 3)ğ‘ 1

ğ‘€ âˆ’ 1.

Given the theoremâ€™s constraint ğ‘ 1 < ğ‘€ +1.5 

> ğ‘€ +3

, we have ğ‘€ + 1.5 âˆ’ ( ğ‘€ + 3)ğ‘ 1 > 0. Thus 

ğœ” FE (ğ‘š ) âˆ’ 1 âˆ’ ğ›¿ (ğ‘š + 3) > âˆ’1. (9) As a result, ğ‘„ (ğ‘š + 1) > ğ‘„ (ğ‘š ) âˆ’ 1. Given the condition for Case 1 that ğ‘„ (ğ‘š ) > 0, we have ğ‘„ (ğ‘š + 1) > âˆ’1. Combined with the previously established upper bound ğ‘„ (ğ‘š + 1) < 1, we conclude |ğ‘„ (ğ‘š + 1)| < 1.

Case 2: ğ‘„ (ğ‘š ) â‰¤ 0. According to the decision rule, ğ¼ (ğ‘š + 1) = 0 (HPO is selected). Substituting ğ¼ (ğ‘š + 1) = 0:

ğ‘„ (ğ‘š + 1) = ğ‘„ (ğ‘š ) + ğœ” FE (ğ‘š ) âˆ’ ğ›¿ (ğ‘š + 3).

Since ğœ” FE (ğ‘š ) â‰¤ ğ‘ 1 < 1 and ğ‘„ (ğ‘š ) â‰¤ 0, we have ğ‘„ (ğ‘š + 1) < 1. For the lower bound, from Equation 9 in Case 1, we have already established that: ğœ” FE (ğ‘š ) âˆ’ 1 âˆ’ ğ›¿ (ğ‘š + 3) > âˆ’1 =â‡’ ğœ” FE (ğ‘š ) âˆ’ ğ›¿ (ğ‘š + 3) > 0. As a result, ğ‘„ (ğ‘š + 1) > ğ‘„ (ğ‘š ). By the inductive hypothesis ğ‘„ (ğ‘š ) > âˆ’1, we conclude ğ‘„ (ğ‘š + 1) > âˆ’1. Thus, |ğ‘„ (ğ‘š + 1)| < 1 holds for Case 2 as well. 

> Manuscript submitted to ACM

20 Beicheng Xu et al. To sum up, |ğ‘„ (ğ‘š )| < 1 is satisfied for the entire search. 

4. Terminal Equilibrium 

At the terminal iteration ğ‘š = ğ‘€ , the linear priors reach: 

ğœ” FE (ğ‘€ ) = ğ‘ 1 âˆ’ ğ‘ 1 âˆ’ 0.5

ğ‘€ ğ‘€ = 0.5, ğœ” HPO (ğ‘€ ) = ğ‘ 2 + 0.5 âˆ’ ğ‘ 2

ğ‘€ ğ‘€ = 0.5

Substituting these into the auxiliary function ğ‘„ (ğ‘€ ):

ğ‘„ (ğ‘€ ) = 0.5 1 + ğ‘ HPO (ğ‘€ ) âˆ’ 0.5 1 + ğ‘ FE (ğ‘€ ) = 0.5 ğ‘ HPO (ğ‘€ ) âˆ’ ğ‘ FE (ğ‘€ )

From the boundedness lemma, we have: 

|0.5 ğ‘ HPO (ğ‘€ ) âˆ’ ğ‘ FE (ğ‘€ ) | < 1 =â‡’ | ğ‘ HPO (ğ‘€ ) âˆ’ ğ‘ FE (ğ‘€ )| < 2

Since ğ‘ FE (ğ‘€ ) and ğ‘ HPO (ğ‘€ ) are integers, their difference Î”ğ‘ = ğ‘ HPO (ğ‘€ ) âˆ’ ğ‘ FE (ğ‘€ ) must also be an integer such that 

Î”ğ‘ âˆˆ {âˆ’ 1, 0, 1}. We now consider the parity of the total budget ğ‘€ :

â€¢ Case A (Even ğ‘€ ): If ğ‘€ is even, then Î”ğ‘ = ğ‘€ âˆ’ 2ğ‘ FE (ğ‘€ ) must be an even integer. The only even integer in the interval (âˆ’ 2, 2) is 0. Thus, ğ‘ FE (ğ‘€ ) = ğ‘ HPO (ğ‘€ ) = ğ‘€ /2.

â€¢ Case B (Odd ğ‘€ ): If ğ‘€ is odd, then Î”ğ‘ = ğ‘€ âˆ’ 2ğ‘ FE (ğ‘€ ) must be an odd integer. The only odd integers in the interval (âˆ’ 2, 2) are Â±1. Thus, |ğ‘ FE (ğ‘€ ) âˆ’ ğ‘ HPO (ğ‘€ )| = 1, implying the budget is split as âŒŠğ‘€ /2âŒ‹ and âŒˆğ‘€ /2âŒ‰.In both cases, the adaptive selection rule minimizes the discrepancy between FE and HPO allocations, ensuring that: 

ğ‘ FE (ğ‘€ ), ğ‘ HPO (ğ‘€ ) âˆˆ {âŒŠ ğ‘€ 

2 âŒ‹, âŒˆ ğ‘€ 

2 âŒ‰} .

This completes the proof of the budget equilibrium. 

â–¡

A.2 Prompt Design 

This appendix details our prompt engineering framework, spanning the expert-persona System Prompt, the structured User Prompt, and a concrete input/output example. 

A.2.1 System Prompt. The System Prompt establishes the agentâ€™s expert persona and its core mission: 

> System Prompt

You are an elite Machine Learning Feature Engineering (FE) Expert whose mission is to analyze data distributions and synthesize high-value pipelines by generating innovative feature operations specifically designed to maximize the predictive performance of downstream ML models. 

A.2.2 Structured User Prompt. The User Prompt comprises the global task description and the four core components defined in Equation 6, which are detailed below: 

(i) Task description defines the objective and output format: 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 21 

> Template: Task Description

Given the dataset metadata, transformation history, and elite memory, your task is to synthesize novel feature engineering operations to maximize the performance of the downstream model {. . . }. You should provide a reasoning insight followed by executable Python code. 

Output Format: You must provide your insight followed by the code in the following structure: â€” Reason: (Explain why this specific transformation is beneficial for the downstream model.) â€” Way: (Detail the exact columns and the specific method required for the transformation.) â€” Implementation: (Provide a standalone Python function. Use the following template) 

def your_fe_name(df): 

''' A concise description of the operator. ''' 

import pandas as pd import numpy as np # Your implementation here return df 

(ii) Dataset information ( ğœ“ ). The Dataset Information component provides a comprehensive data profile, grounding the LLMâ€™s reasoning in both statistical metadata and semantic context. To ground the agentâ€™s reasoning in the datasetâ€™s specific characteristics, we transform the raw data into a structured dataset information block using a multi-level extraction logic: 

â€¢ Global profile : Captures the dataset scale (samples/features), task type (e.g., regression/classification), the numerical-to-categorical ratio, and the semantic meaning of the dataset (if such a description is available), e.g., what the records represent and what the target variable measures. 

â€¢ Missingness & quality analysis : We calculate the missing ratio for every column and flag "high-missing" features that exceed a predefined threshold (e.g., 30%). 

â€¢ Heuristic key feature selection : To manage the LLMâ€™s context window, we score features based on their missingness, presence of domain descriptions, and types. We then prioritize the Top-ğ¾ most informative features for detailed display. 

â€¢ Feature-Level metadata : Distills specific statistics based on the feature type: 

â€“ Categorical: Includes the number of unique classes and the frequency of the Top-3 categories. 

â€“ Numerical/discrete: Includes statistical moments (mean, standard deviation) and the value range (min, max). We append human-readable notes (e.g., feature meanings) to the metadata whenever domain knowledge is available. 

> Template: Dataset Information

[Dataset Profile] {dataset scale, . . . }

Target: {. . . }

All columns: {[ . . . ]} 

> Manuscript submitted to ACM

22 Beicheng Xu et al. 

[Feature Summary] {highlights data quality, specifically identifying columns with high missing-value ratios. }

[Key Features] detailed metadata for each feature: â€“ col1 : type=numerical, missing= {%}, mean= {ğœ‡ }, std= {ğœ }, range=( {ğ‘šğ‘–ğ‘›, ğ‘šğ‘ğ‘¥ }), note= {ğ‘‘ğ‘’ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› }

â€“ col2 : type=categorical, missing= {%}, classes= {[ . . . ]} , top= {[( ğ‘£ğ‘ğ‘™, ğ‘“ ğ‘Ÿğ‘’ğ‘ ), . . . ]} , note= {ğ‘‘ğ‘’ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› }

â€“ . . . (repeated for prioritized features) 

(iii) Ancestor FE pipeline ( ğ‘‡ anc ) summarizes the FE pipelines from the original dataset to the currently selected node: 

> Template: Ancestor FE Pipeline

Current Feature Engineering pipelines: 

â†’

[Do nothing] 

âŒ Score: {ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ 0 }â†’

[FE Operation 1] 

âŠ¢ Reason: {ğ‘…ğ‘’ğ‘ğ‘ ğ‘œğ‘› 1 }âŠ¢ Way: {ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ 1 }

âŒ Score: {ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ 1 }â†’

. . . 

â†’

[FE Operation ğ‘˜ ]

âŠ¢ Reason: {ğ‘…ğ‘’ğ‘ğ‘ ğ‘œğ‘› ğ‘˜ }âŠ¢ Way: {ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ ğ‘˜ }

âŒ Score: {ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘˜ }

(iv) Memory of good FE operations ( Mğ‘  base ) shows good feature engineering operations from history that you can reference in the next step: 

> Template: Memory of Good FE Pipelines

High-performing historical FE operations (memory): [Good Operation 1] 

âŠ¢ Reason: {ğ‘…ğ‘’ğ‘ğ‘ ğ‘œğ‘› 1 }âŠ¢ Way: {ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ 1 }

âŒ Score: {ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ 1 }, relative improve: {ğ‘…ğ‘’ğ‘™ğ¼ğ‘šğ‘ğ‘Ÿğ‘œğ‘£ğ‘’ 1 }

[Good Operation 2] 

âŠ¢ Reason: {ğ‘…ğ‘’ğ‘ğ‘ ğ‘œğ‘› 2 }âŠ¢ Way: {ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ 2 }

âŒ Score: {ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ 2 }, relative improve: {ğ‘…ğ‘’ğ‘™ğ¼ğ‘šğ‘ğ‘Ÿğ‘œğ‘£ğ‘’ 2 }

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 23 . . . 

[Good Operation ğ‘š ]

âŠ¢ Reason: {ğ‘…ğ‘’ğ‘ğ‘ ğ‘œğ‘› ğ‘š }âŠ¢ Way: {ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ ğ‘š }

âŒ Score: {ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ ğ‘š }, relative improve: {ğ‘…ğ‘’ğ‘™ğ¼ğ‘šğ‘ğ‘Ÿğ‘œğ‘£ğ‘’ ğ‘š }

(v) Directive and Optimization Objectives 

Depending on the directive ğ‘‘ , the LLM is given optimization objectives that explicitly steer its feature engineering strategy. The directive governs whether the agent should focus on generating an initial strong operation, exploring novel transformations, or exploiting previously successful experience. 

> Template: Directive and Objectives

Your general objectives (for any directive ğ‘‘ ): 

1. Review and summarize the existing feature engineering operations from the original dataset to the currently selected node in the ancestor FE pipeline. 2. Avoid duplicating the feature engineering approaches that have already been attempted in the ancestor FE pipeline. 3. Propose one new feature engineering step. You may reference (but are not limited to) the following categories: â€“ Generator: creating new features. â€“ Selector: choosing or filtering the most relevant features. â€“ Transformation: modifying distributions or encoding categories. â€“ Rescaler: normalizing or standardizing features. â€“ Imputer: filling missing data via statistical or model-based methods. 4. Your strategy is {d} (further specifies the optimization mode and how the above objectives should be prioritized): 

Directive: Initialization (for root node ğ‘  0)

Instruction : Propose a high-quality initial FE operation for the original dataset. Focus on robust, broadly useful transformations rather than highly specialized or overly complex operations. Aim to establish a strong baseline that later operations can further improve upon. 

Directive: Exploration 

Instruction : Propose FE operations that explores new regions of the transformation space. Prioritize novel or less-explored ideas that are distinct from existing and previously attempted methods . Encourage diversity and bold changes to uncover potentially high-performing operations. 

> Manuscript submitted to ACM

24 Beicheng Xu et al. 

Directive: Exploitation 

Instruction : Propose FE operations that refines and exploits previously successful operations. Try to 

reuse, adapt, or combine high-performing historical FE operations from the memory to further improve performance. Focus on incremental refinement rather than broad exploration. 

A.2.3 Input and Output Example. 

B Experimental Setup B.1 Datset Details 

This section provides the detailed specifications for the 28 benchmark datasets used in our experiments. Table 3 lists the 19 classification datasets curated by Grinsztajn et al. [ 15 ], including their OpenML identifiers and basic dimensions.                                                             

> Table 3. Statistics for 19 classification datasets
> Dataset # Samples # Features OpenML ID
> rl 4,970 12 44160 electricity 38,474 844156 compass 16,644 17 44162 wine 2,554 11 44091 house_16H 13,488 16 44123 Magic 13,376 10 44125 Higgs 940,160 24 44129 jannis 57,580 54 44131 credit 16,714 10 44089 eye_movements 7,608 23 44157 kddCup09 5,032 45 44158 road-safety 111,762 32 44161 bank-marketing 10,578 744126 phoneme 3,172 544127 covertype 423,680 54 44159 california 20,634 844090 kdd_ipums_la 5,188 20 44124 MiniBooNE 72,998 50 44128 pol 10,082 26 44122

Table 4 details the regression datasets sourced from OpenML and Kaggle. 

B.2 Hyperparameter Search Spaces of Downstream Models 

This section describes the hyperparameter search spaces for the downstream models. For each task, we employ the SMAC optimizer to find the optimal set of hyperparameters within the budgets defined in Section 4.1. For XGBoost, we adopt the hyperparameter search space used in Grinsztajn et al. [ 15 ]. The detailed distributions and ranges are presented in Table 5. 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 25 

> Input Example

You are an elite Machine Learning Feature Engineering (FE) Expert whose mission is to analyze data distributions and synthesize high-value pipelines by generating innovative feature operations specifically designed to maximize the predictive performance of downstream ML models. 

(i) Task Description 

Given the dataset metadata, transformation history, and elite memory, your task is to synthesize novel feature engineering operations to maximize the performance of the downstream model XGBoost. Higher evaluation scores indicate better performance. You should provide a reasoning insight followed by executable Python code. 

Output Format: You must provide your insight followed by the code in the following structure: â€” Reason: (Explain why this specific transformation is beneficial for the downstream model.) â€” Way: required_feature_columns = [â€˜col_aâ€™, â€˜col_bâ€™, â€˜col_câ€™ ] (Detail the exact columns required for the transformation.) Method: (Detail the exact method required for the transformation.) â€” Implementation: (Provide a standalone Python function. Use the following template) 

def your_fe_name(df): 

''' A concise description of the operator. ''' 

import pandas as pd import numpy as np # Your implementation here return df 

(ii) Dataset Information 

[Dataset Profile] samples=12166, features=15, categorical=0, numerical=15, task=regression Target= cnt (Integer): count of total rental bikes including both casual and registered All columns: [â€˜seasonâ€™, â€˜yrâ€™, â€˜mnthâ€™, â€˜hrâ€™, â€˜holidayâ€™, â€˜weekdayâ€™, â€˜workingdayâ€™, â€˜weathersitâ€™, â€˜tempâ€™, â€˜atempâ€™, â€˜humâ€™, â€˜windspeedâ€™, â€˜yr_temp_interactionâ€™, â€˜hr_sinâ€™, â€˜hr_cosâ€™] [Feature Summary] high-missing(>0.3): 0 / 15 [Key Features] (Top 15) - season: type=discrete, missing=0.00%, mean=2.503, std=1.110, range=(1.000,4.000), note=season (1:springer, 2:summer, 3:fall, 4:winter) - yr: type=discrete, missing=0.00%, mean=0.501, std=0.500, range=(0.000,1.000), note=year (0: 2011, 1:2012) - mnth: type=discrete, missing=0.00%, mean=6.548, std=3.438, range=(1.000,12.000), note=month (1 to 12) - hr: type=discrete, missing=0.00%, mean=11.550, std=6.901, range=(0.000,23.000), note=hour (0 to 23) - holiday: type=discrete, missing=0.00%, mean=0.031, std=0.172, range=(0.000,1.000), note=whether day is holiday or not - weekday: type=discrete, missing=0.00%, mean=3.005, std=2.016, range=(0.000,6.000), note=day of the week - workingday: type=discrete, missing=0.00%, mean=0.677, std=0.468, range=(0.000,1.000), note=if day is neither weekend nor holiday is 1, otherwise is 0 

> Manuscript submitted to ACM

26 Beicheng Xu et al. - weathersit: type=discrete, missing=0.00%, mean=1.424, std=0.639, range=(1.000,4.000), note=1: Clear/Few clouds/-Partly cloudy, 2: Mist + Cloudy/Broken clouds/Few clouds, 3: Light Snow or Light Rain + Thunderstorm/Scattered clouds, 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist/Snow + Fog - temp: type=numerical, missing=0.00%, mean=0.497, std=0.192, range=(0.020,1.000), note=Normalized temperature in Celsius, derived via (t - t_min)/(t_max - t_min), t_min=-8, t_max=39 - atemp: type=numerical, missing=0.00%, mean=0.476, std=0.171, range=(0.000,1.000), note=Normalized feeling temperature in Celsius, derived via (t - t_min)/(t_max - t_min), t_min=-16, t_max=50 - hum: type=numerical, missing=0.00%, mean=0.628, std=0.193, range=(0.000,1.000), note=Normalized humidity (divided by 100) - windspeed: type=numerical, missing=0.00%, mean=0.190, std=0.122, range=(0.000,0.851), note=Normalized wind speed (divided by 67) - yr_temp_interaction: type=numerical, missing=0.00%, mean=0.253, std=0.285, range=(0.000,1.000) - hr_sin: type=numerical, missing=0.00%, mean=-0.005, std=0.706, range=(-1.000,1.000) - hr_cos: type=numerical, missing=0.00%, mean=-0.006, std=0.708, range=(-1.000,1.000) 

(iii) Ancestor FE pipeline 

Current Feature Engineering pipeline: 

â†’

[Do nothing] 

âŒ Score: {-1697.793556321224} 

â†’

[FE Operation 1] 

âŠ¢ Reason: The effect of temperature on bike rentals may vary from year to year. Maybe the population of bike renters is growing in a particular year leading to changes in how much temperature affects bike rentals. 

âŠ¢ Way: Create an interaction feature by multiplying the â€™yrâ€™ feature with the â€™tempâ€™ feature to generate a new feature â€™yr_temp_interactionâ€™. 

âŒ Score: {-1620.7122764091382} 

â†’

[FE Operation 2] 

âŠ¢ Reason: The â€™hrâ€™ feature is cyclical (0:00 is close to 23:00). Representing it as a linear numerical value doesnâ€™t capture this cyclical nature. Transforming it into sine and cosine components allows the model to understand proximity in time. 

âŠ¢ Way: 1) Create a new feature â€™hr_sinâ€™ as â€˜sin(2 * pi * hr / 24)â€˜ 2) Create a new feature â€™hr_cosâ€™ as â€˜cos(2 * pi * hr / 24)â€˜ 

âŒ Score: {-1666.02202960865} 

(iv) Memory of good FE operations 

High-performing historical FE operations (memory): 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 27 [Good Operation 28] 

âŠ¢ Reason: The effect of temperature and humidity on bike rentals is intertwined. High humidity can make high temperatures more uncomfortable, while low humidity may make them more bearable. 

âŠ¢ Way: Create interaction features by multiplying â€™tempâ€™ with â€™humâ€™, and also â€™atempâ€™ with â€™humâ€™. 

âŒ Score: {-1560.8972742004826}, relative improve: {54.1889004480488} [Good Operation 25] 

âŠ¢ Reason: The effect of humidity on bike rentals might depend on prevailing weather conditions. High humidity during clear weather may be perceived differently than during rainy or misty weather. 

âŠ¢ Way: Create an interaction feature by multiplying â€™humâ€™ with â€™weathersitâ€™ to generate â€™hum_weathersit_interactionâ€™. 

âŒ Score: {-1580.3511106979336}, relative improve: {64.28732802351738} [Good Operation 36] 

âŠ¢ Reason: â€˜hrâ€™, â€˜weekdayâ€™, and â€˜mnthâ€™ are cyclical; representing them as linear numbers is misleading. Sine and cosine transformations better capture this cyclical nature. 

âŠ¢ Way: Apply sine and cosine transformations to â€˜hrâ€™, â€˜weekdayâ€™, and â€˜mnthâ€™. For hr: â€˜hr_sin = sin(2 * pi * hr / 24)â€™, â€˜hr_cos = cos(2 * pi * hr / 24)â€™; For weekday: â€˜weekday_sin = sin(2 * pi * weekday / 7)â€™, â€˜weekday_cos = cos(2 * pi * weekday / 7)â€™; For mnth: â€˜month_sin = sin(2 * pi * mnth / 12)â€™, â€˜month_cos = cos(2 * pi * mnth / 12)â€™ 

âŒ Score: {-1591.368364918421}, relative improve: {161.86343396821349} (v) Directive and Optimization Objectives 

Your general objectives: 1. Review and summarize the existing feature engineering operations from the original dataset to the currently selected node in the ancestor FE pipeline. 2. Avoid duplicating the feature engineering approaches that have already been attempted in the ancestor FE pipeline or listed as attempted methods in the next step. 3. Propose one new feature engineering step. You may reference (but are not limited to) the following categories: â€“ Generator: creating new features. â€“ Selector: choosing or filtering the most relevant features. â€“ Transformation: modifying distributions or encoding categories. â€“ Rescaler: normalizing or standardizing features. â€“ Imputer: filling missing data via statistical or model-based methods. 4. Your strategy is Exploitation: Propose FE operations that refine and exploit previously successful operations. Try to reuse, adapt, or combine high-performing historical FE operations from the memory to further improve performance. Focus on incremental refinement rather than broad exploration, while still respecting the constraint of not duplicating already attempted operations. Take a deep breath and think step by step to find the answer. For MLP, we adopt the search space and architecture following Gorishniy et al. [ 14 ]. The model includes learning embeddings for categorical features and is trained with early stopping based on validation scores. The search space is detailed in Table 6. 

> Manuscript submitted to ACM

28 Beicheng Xu et al. 

> Response Example

â€” Reason: The impact of weather situation likely varies across months and seasons. For example, light rain in summer (a popular biking month) may reduce demand differently than in winter, and the same humidity level can feel more or less uncomfortable depending on season. By combining cyclical month encodings with weather situation, and interacting humidity with season, the model can learn season- and month-specific weather effects rather than a single global effect, which is well-suited to XGBoostâ€™s ability to split on interaction features. â€” Way: 

required_feature_columns = [â€™mnthâ€™, â€™weathersitâ€™, â€™seasonâ€™, â€™humâ€™] 

Method: 1) Create cyclical representations of month to capture seasonality smoothly: 

month_sin = sin(2 * pi * mnth / 12) month_cos = cos(2 * pi * mnth / 12) 

2) One-hot encode weathersit and season into indicator columns such as: 

weathersit_1 , weathersit_2 , weathersit_3 , weathersit_4 season_1 , season_2 , season_3 , season_4 

3) Create interaction features: For each one-hot weathersit_k , multiply with month_sin and month_cos to get weatherâ€“month interactions, e.g.: 

month_sin_weathersit_1 = month_sin * weathersit_1 month_cos_weathersit_1 = month_cos * weathersit_1 

. . . similarly for weathersit_2 , weathersit_3 , weathersit_4 .For each one-hot season_j , multiply with hum to get season-specific humidity effects, e.g.: 

hum_season_1 = hum * season_1 hum_season_2 = hum * season_2 

. . . similarly for season_3 , season_4 .â€” Implementation: 

def interact_month_weather(df): 

''' 

Creates interaction features between: - month (sine/cosine) and one-hot encoded weathersit, and - humidity and one-hot encoded season. 

''' 

import numpy as np import pandas as pd df = df.copy() # 1. Create month_sin and month_cos df[ 'month_sin '] = np.sin(2 * np.pi * df[ 'mnth '] / 12) 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 29 

df[ 'month_cos '] = np.cos(2 * np.pi * df[ 'mnth '] / 12) # 2. One-Hot Encode 'weathersit ' and 'season '

df = pd.get_dummies(df, columns=[ 'weathersit '], prefix= 'weathersit ', dummy_na=False) df = pd.get_dummies(df, columns=[ 'season '], prefix= 'season ', dummy_na=False) # 3. Create interaction features with weathersit dummies for weathersit_col: if weathersit_col in df.columns: df[f 'month_sin_{weathersit_col} '] = df[ 'month_sin '] * df[weathersit_col] df[f 'month_cos_{weathersit_col} '] = df[ 'month_cos '] * df[weathersit_col] # 4. Create interaction features with season dummies for season_col: if season_col in df.columns: df[f 'hum_{season_col} '] = df[ 'hum '] * df[season_col] return df                               

> Table 4. Statistics for 9 regression datasets.
> Dataset Samples Feat. Source (ID / Link)
> airfoil_self_noise 1,503 6OpenML (44957) cpu_small 8,192 12 OpenML (562) diamonds 53,940 9OpenML (42225) plasma_retinol 315 13 OpenML (511) forest-fires 517 13 OpenML (42363) housing 20,640 9OpenML (43996) bike 17,389 11 OpenML (42712) crab 3,893 8Kaggle (crab-age-prediction) insurance 1,338 7Kaggle (us-health-insurancedataset)

In the CASH scenario, the search space is expanded to include algorithm selection. The framework chooses among various learners (e.g., Random Forest, XGBoost, LightGBM, and MLP), with each learner associated with its respective hyperparameter search space. As shown in Table 7, the setting involves a total of 42 hyperparameters. This includes one high-level categorical parameter used for algorithm selection, while the specific learner subspaces contribute the remaining 41 hyperparameters (35 continuous and 6 categorical). 

B.3 Meta-Feature for FE Pipeline Characterization 

To facilitate the collaborative optimization between Feature Engineering (FE) and Hyperparameter Optimization (HPO), CoFEH incorporates the meta-feature extraction framework from MindWare [ 31 ], which consolidates and extends meta-features previously proposed in the meta-learning literature, including those from [3, 42, 54]. 

> Manuscript submitted to ACM

30 Beicheng Xu et al. 

Table 5. XGBoost hyperparameter search space. 

Parameter Distribution / Range 

Max depth UniformInt [1, 11] Num estimators UniformInt [100, 6100, 200] Min child weight LogUniformInt [1, 1e2] Subsample Uniform [0.5, 1] Learning rate LogUniform [1e-5, 0.7] Col sample by level Uniform [0.5, 1] Col sample by tree Uniform [0.5, 1] Gamma LogUniform [1e-8, 7] Lambda LogUniform [1, 4] Alpha LogUniform [1e-8, 1e2] 

Table 6. MLP hyperparameter search space. 

Parameter Distribution / Range 

Num layers UniformInt [1, 8] Layer size UniformInt [16, 1024] Dropout Uniform [0, 0.5] Learning rate LogUniform [1e-5, 1e-2] Category embedding size UniformInt [64, 512] Learning rate scheduler {True, False} Batch size [256, 512, 1024] 

Table 7. Search space for CASH. We distinguish categorical (cat) hyperparameters from numerical (cont) ones. 

Type of Classifier / Regressor #ğœ† cat cont 

Random Forest 5 2 3Extra Trees 5 2 3Gradient Boosting 7 1 6MLP 7 1 6LightGBM 7 - 7XGBoost 10 - 10 

Total (6 algos) 41 6 35 

Motivation and shared spirit . In the original MindWare framework, meta-features are utilized for meta-learning to predict which algorithm (e.g., XGBoost, LightGBM) is likely to achieve the highest performance ceiling on a given dataset. Our approach shares a similar philosophy: we posit that the optimal hyperparameter configuration ğ€ âˆ— is inherently dependent on the current state of the dataset Xâ€². By extracting meta-features, we allow the HPO surrogate model to "perceive" the transformations made by the FE module, effectively conditioning the search space on the feature engineering trajectory. 

Implementation details . We adopt the meta-feature suite provided in the MindWare repository 1. The extraction logic is categorized by task type:  

> 1https://github.com/PKU-DAIR/mindware/blob/master/mindware/components/meta_learning/meta_feature/meta_features.py Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 31 

â€¢ Classification datasets : 46 meta-features are extracted, covering statistical properties (e.g., skewness, kurtosis), information-theoretic measures (e.g., class entropy), etc. 

â€¢ Regression datasets : 33 meta-features are extracted, focusing on feature-target correlations and dataset dimen-sionality characteristics, etc. Detailed definitions and the underlying source code for each metric are available in the referenced repository file. 

Orthogonality and Compatibility . It is important to emphasize that the design of CoFEH is orthogonal to the specific choice of meta-features. While we utilize the MindWare suite due to its robustness and comprehensive coverage, our framework is fully compatible with any alternative meta-feature computation methodologies. This modularity ensures that as more advanced dataset characterization techniques emerge, they can be seamlessly integrated into the CoFEH pipeline to further enhance the collaborative optimization process. 

C Additional Results and Analysis C.1 Ablation on Different LLM Backbones                                            

> Table 8. Test error ( Mean Â±Std ) and cost comparison averaged over 3 runs. Bold and underlined values denote the best and second-best results (lower is better). Bottom rows show the average rank and average token cost.
> Dataset Gemini-2.0-flash Gemini-2.5-pro GPT-5.2
> pol 1.42 Â±0.12 1.39 Â±0.15 1.42 Â±0.11 wine 19.50 Â±0.62 19.42 Â±0.55 19.35 Â±0.68
> airfoil_self_noise 1.50 Â±0.10 1.49 Â±0.13 1.47 Â±0.09
> housing ( Ã—10 9)1.89 Â±0.01 1.76 Â±0.01 1.72 Â±0.01 Avg. Rank 2.75 1.75 1.50
> Avg. Cost ($) 0.088 1.515 2.123

This appendix investigates how the choice of LLM backbone affects the performance of CoFEH in joint FE+HPO tuning setting. We evaluate three modelsâ€”GPT-5.2, Gemini-2.5-pro, and Gemini-2.0-flashâ€”across four diverse datasets: two classification tasks (pol, wine) and two regression tasks (airfoil_self_noise, housing). As summarized in Table 8, all metrics are reported as averages over three independent runs to ensure statistical robustness. Experimental results reveal a strong positive correlation between an LLMâ€™s reasoning depth and the quality of synthesized operations when exploring the FE search space conditioned on HPO. GPT-5.2 establishes the performance ceiling, achieving the best results on three out of four tasks and securing the top average rank of 1.50. This superiority suggests that advanced reasoning capabilities enable the model to propose more effective feature transformations while the HPO component simultaneously optimizes the model configuration. In contrast, Gemini-2.0-flash yields the lowest performance (Rank 2.75), highlighting the limitations of lightweight models in executing the sophisticated semantic reasoning required for optimal FE synthesis under varying HPO conditions. Gemini-2.5-pro serves as a highly competitive mid-tier alternative, outperforming GPT-5.2 on the pol classification task and maintaining a second-best average rank of 1.75. This performance gradient suggests that the systemâ€™s effectiveness scales directly with the modelâ€™s â€œintelligence ceiling.â€ Cost analysis illustrates a sharp trade-off between performance and API expenditure. Gemini-2.0-flash is the most cost-effective option, with a mean task cost of 0.088 â€”approximately 1/24th that of GPT-5.2 ( 2.123 ). While it is numerically the weakest among the tested models in ranking, the performance gap remains remarkably small, which justifies its 

> Manuscript submitted to ACM

32 Beicheng Xu et al. 0 20 40 60 80 100 

# Number of Evaluations 

# 0.2 

# 0.4 

# 0.6 

# 0.8 

# 1.0 

> Normalized Performance

# CoFEH 

# CoFEH (Exploitaion Only) 

# CoFEH (Exploration Only) 

Fig. 6. Abalation study of directives. 

selection as our final backbone. This extreme efficiency enables a much broader search scope that can compensate for the lower per-proposal complexity through sheer volume and iteration. In summary, the performance of CoFEH is intrinsically linked to the reasoning capacity of the LLM backbone, but the ultimate selection of Gemini-2.0-flash reflects a strategic prioritization of cost-efficiency. As LLM research continues to deliver enhanced reasoning power at lower costs, the efficacy and scalability of CoFEH â€”leveraging high-efficiency backbonesâ€”are expected to increase proportionally. 

C.2 Effectiveness of Steerable Reasoning Directives 

The exploration of the unbounded FE search space in CoFEH is explicitly driven by the interplay between the 

Exploration and Exploitation directives. To evaluate the necessity of this dual-engine design, we conduct an ablation study by comparing the full CoFEH against two restricted variants: (i) Exploration-only: The LLM performs four Exploration expansions per node, relying solely on domain knowledge to probe the space without Mglobal . (ii) Exploitation-only: The LLM is restricted to four Exploitation trials, forcing the FE process to refine â€œelite experiencesâ€ from Mglobal for novel transformations. To manage API expenditure while maintaining evaluation coverage across diverse task types, we select four representative datasets: two classification tasks (pol, wine) and two regression tasks (airfoil, housing). Figure 6 plots the average best-so-far validation performance across these tasks, where scores are min-max normalized for each dataset. The results indicate that the Exploitation-only variant achieves rapid initial gains by effectively leveraging high-performing "elite experiences" from Mglobal . However, it suffers from premature stagnation, as the search logic becomes increasingly confined to a narrow manifold of previously successful patterns, eventually trapping the process in local optima. Conversely, the Exploration-only variant exhibits significantly slower convergence; while it consistently probes novel regions using domain knowledge, the absence of iterative refinement leads to an inefficient and scattered discovery 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 33 ğ€ ğ€ = ğ€ Â· c                                                 

> ğ€
> ğ€ âˆ—
> ğ€
> (ğ€ âˆ’ğ€ )
> ğ€
> log(x) Op 1: Transformation (Semantic Scaling)
> Log -mapping for skewed variables (e.g., freq )
> Op 2: Generation (Physics-Inspired)
> Op 3: Transformation (Dist Rectification)
> Yeo-Johnson Power Transform
> Op 4: Preprocess (Z-score )
> Standardize to Î¼=0, Ïƒ=1
> Op 5: Selection (Feature Pruning )
> SelectKBest (F-regression )
> Why: Stabilize high -range variance
> &capture relative changes .
> Why : Embed domain knowledge by
> synthesizing dimensionless
> invariants and interactions
> Why : Enforce normality to align
> feature distributions
> Why : Eliminate scale disparities
> Why: Prune redundant interactions
> &noise to prevent overfitting .
> Optimal Feature Set for Downstream Models
> FE Operation Pipeline Rationale &Reason

(a) Optimal FE pipeline of CoFEH. + 0.5tan (â€¦) + ğ€ , 0, 1)       

> ğ€ 1
> ğ€ 2
> ğ€ 3
> ğ€ 4
> ğ€ 5
> ğ€ 6
> 1. OCTree
> ğ€ 1
> ğ€ 2
> ğ€ 3
> ğ€ 4
> ğ€ 5
> ğ€ 1/ğ€ 2
> ğ€ 1/ğ€ 3
> ğ€ 1/ğ€ 5
> ğ€ 5/ğ€ 1
> ğ€ 5/ğ€ 3
> MinMax (ğ€ 1)
> MinMax( ğ€ 5)
> Sigmoid( ğ€ 1)
> 2. ELLM
> 10 New
> Features
> Velocity
> Thickness
> Ã—
> (Â·) 3
> Ã·
> 3 New
> Features
> Step 1:
> Rescaler
> (Qiantile.
> 172)
> Step 2:
> Transformation
> (feature
> agglomeration)
> Final
> Feature
> Set
> 3. LFG
> 4. Mindware
> ğ€ 6=clip ((ğ€ ğ€ )
> 0.3
> Â·sin (â€¦) Â·cos (â€¦)

(b) Optimal FE pipeline of baselines. Fig. 7. Optimal FE pipeline discovered by all methods on â€œairfoil_self_noiseâ€ dataset. 

process. CoFEH synthesizes the strengths of both engines to maintain a robust balance between breadth and depth throughout the search. This synergy is quantitatively substantiated by the final performance rankings: CoFEH achieves a dominant Average Test Rank of 1.0, significantly outperforming the Exploitation-only (2.25) and Exploration-only (2.75) variants. These findings confirm that the interplay between these two steerable directives is essential for navigating complex, unbounded search spaces efficiently under a constrained budget. 

C.3 Case Studies of Optimal FE Pipelines 

This appendix details the feature engineering (FE) pipelines discovered by our framework and various baselines for the NASA Airfoil Self-Noise dataset in the standalone FE scenario. The dataset, derived from anechoic wind tunnel tests of NACA 0012 airfoils, presents a challenging regression task: predicting scaled self-noise across varying chord lengths, wind speeds, and angles of attack. 

Pipeline of CoFEH . The pipeline discovered by CoFEH (Figure 7a) reflects a logical progression from raw data stabilization to the synthesis of physical representations. This process is structured into five distinct functional stages: (i) Operation 1: semantic scaling (transformation) : The framework identifies that predictors such as frequency span several orders of magnitude. It autonomously applies a logarithmic transformation to stabilize high-range variance, effectively shifting the modelâ€™s focus toward relative spectral changes rather than absolute numerical values. (ii) Operation 2: physics-inspired feature generation : Leveraging aerodynamic domain knowledge, CoFEH deduces that because the experimental conditions involve varying airfoil scales (chord length ğ‘ ) and flow velocities ( ğ‘ˆ ), the noise response is likely governed by dynamic similarity. It synthesizes a Strouhal-like number ( ğ‘†ğ‘¡ = ğ‘“ Â·ğ‘ ğ‘ˆ ), collapsing diverse experimental scales into a scale-invariant physical representation. The system further incorporates geometry-aware features by deriving trigonometric components ( sin ğ‘, cos ğ‘ ) from the angle of attack. A notable emergent feature is the coupled interaction term ( ğ‘†ğ‘¡ Â· sin ğ‘ ), synthesized to model angle-modulated shedding effectsâ€”a sophisticated interaction. (iii) Operation 3: distribution rectification (transformation) . Following symbolic discovery, the pipeline employs a Yeo-Johnson power transform. This step serves to rectify the distributions of the newly synthesized physical terms, enforcing Gaussian-like normality to align them statistically with the original feature space. 

Manuscript submitted to ACM 34 Beicheng Xu et al. (iv) Operation 4: scale standardization (Preprocess) . The framework executes Z-score standardization ( ğ‘§ = ğ‘¥ âˆ’ğœ‡ ğœ ). This stage is critical for eliminating scale disparities between different features. (v) Operation 5: feature pruning (selection) . The process concludes with selection via SelectKBest (F-regression). This stage distills the expanded feature bank into an optimal set, removing redundant noise to prevent overfitting and enhance model efficiency. CoFEH distinguishes itself through a highly unconstrained workflow that autonomously orchestrates a fluid se-quence of transformation â†’ generation â†’ transformation â†’ preprocessing â†’ selection. It also supports an essentially unbounded operation space, harmonizing foundational mathematical primitives (e.g., log-scaling) and advanced algo-rithmic optimizations (e.g., Yeo-Johnson transforms, F-regression) with autonomous, domain-driven synthesis. On the other hand, Figure 7b illustrates the optimal pipelines discovered by baselines, revealing a notable contrast to CoFEH: 

OCTree pursues a monolithic symbolic synthesis approach, nesting multiple mathematical primitives into a single, high-order expression. While this captures complex non-linearities, the resulting single feature lacks a grounding in aerodynamic similarity laws (such as the Strouhal number), often leading to a representation that is difficult to generalize across different airfoil scales. 

ELLM and LFG both rely on the expansion of mathematical primitives to generate multiple candidates. Crucially, the outputs of both methods are fundamentally the results of simple arithmetic operations. Furthermore, these methodologies are strictly confined to the feature generation phase, lacking a comprehensive pipeline to integrate transformation, scale standardization, or selection. 

Mindware represents a purely two-step pipeline: a quantile-based Rescaler followed by a feature agglomeration Transformer. This result reflects a purely algorithmic paradigm that reconfigures the input space geometry through statistical motifs. Notably, it operates without any arithmetic generation, meaning it does not construct new symbolic features or physical interactions. Finally, we consider OpenFE , which follows a two-stage automated feature generation. This methodology systemati-cally explores thousands of candidate features by applying arithmetic primitives (e.g., +, -, Ã—, Ã·). to the original input space. Following this expansive generation phase, a selection process is employed, retaining 122 new features in the final dataset. In summary, CoFEH distinguishes itself through its structural adaptability and operational diversity, transcending the functional constraints of all baselines through two core distinctions: (i) Workflow fluidity : Unlike frameworks such as Mindware and OpenFE, which adhere to fixed, pre-defined sequences where stages are often mutually exclusive and restricted in depth, or baselines like OCTree, ELLM, and LFG, which are confined strictly to iterative feature generation regardless of pipeline length, CoFEH implements a truly unconstrained workflow. Although visualized as a five-stage pipeline in Figure 7a, each stage may represent a sophisticated reasoning layer where the LLM may execute multiple concurrent operations. This results in an underlying structural complexity that far exceeds what is shown. (ii) Synthesis of algorithmic and arithmetic operations : CoFEH further bridges the gap between disparate operational paradigms. Mindware is purely algorithmic, relying on predefined algorithms (e.g., PCA, kernel expansions) to reconfigure the input space, while other baselines are restricted to symbolic arithmetic-operator synthesis (e.g., +, âˆ’,

Ã—, Ã·). In contrast, CoFEH achieves an organic integration of both worlds. It seamlessly blends domain-driven arithmetic discoveryâ€”such as the synthesis of scale-invariant physical parameters with advanced algorithmic optimizations like the Yeo-Johnson transform and F-regression. By navigating an unbounded operation space, CoFEH ensures that feature representations are both physically grounded and numerically optimized for downstream performance. 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 35 Large Medium Small 

# Dataset Size Category 

# 1

# 2

# 3

# 4

# 5

> Average Rank

1.44 

2.00 2.11 

1.22 

2.11 2.00 

2nd Best (Standalone FE) 

2nd Best (FE+HPO) 

CoFEH (Standalone FE) 

CoFEH (FE+HPO) 

Fig. 8. Average rank by dataset size (samples Ã— features). 1 2 3 4 5 6

> CoFEH [1.84]
> LFG [3.11]
> ELLM [3.54]
> [4.52] Mindware
> [4.07] OpenFE
> [3.93] OCTree
> CD

(a) CD plot in standalone FE. 1 2 3 4 5 6 

> CoFEH [1.75]
> LFG [3.41]
> Mindware [3.48]
> [4.57] OCTree
> [3.93] ELLM
> [3.86] OpenFE
> CD

(b) CD plot in joint FE+HPO. Fig. 9. CD plot of test rank with Nemenyi post-hoc test. 

C.4 Impact of Dataset Characteristics 

In this section, we analyze the performance of CoFEH across different dataset scales to evaluate its robustness. We define dataset size as the product of samples and features, partitioning the 28 benchmark datasets equally into three groups: Small, Medium, and Large. Figure 8, which compares the average rank of CoFEH against the runner-up baseline, (i) CoFEH consistently achieves the superior average rank across all categories in both standalone FE and joint FE+HPO scenarios; and (ii) the performance advantage of CoFEH becomes significantly more pronounced as the dataset scale increases, substantiating the robust scalability of our reasoning-driven approach, which effectively navigates the complex search spaces of large datasets where other methods often struggle. 

C.5 Statistical Significance Analysis 

To rigorously evaluate the performance of CoFEH relative to the baselines, we conduct a statistical significance analysis based on the average ranks across all datasets. As illustrated in Figure 9, we employ the Friedman test followed by the Nemenyi post-hoc test to generate Critical Difference (CD) diagrams, where methods are connected by a horizontal line 

> Manuscript submitted to ACM

36 Beicheng Xu et al. 0.02 0.04 0.06 0.08 

Average Cost ($) Per Task 

> 2
> 3
> 4
> Average Test Rank
> CoFEH
> LFG
> ELLFM-FT
> OCTree

(a) Expense-performance trade-off of all LLM-based methods. Magic       

> MiniBooNE
> airfoil
> bank
> bike
> california
> compass
> covtype
> cpu
> crab
> credit
> diamonds
> electricity
> eye
> forest
> higgs
> house
> housing
> insurance
> jannis
> kddCup09
> kdd
> phoneme
> plasma
> pol
> rl
> road
> wine 0
> 1
> 2
> 3
> 4
> 5
> Time Ratio vs CoFEH
> Mindware LFG OCTree OpenFE ELLM CoFEH

(b) Per-dataset runtime comparison (time ratio vs CoFEH). Fig. 10. Cost analysis of the main experiment. 

if their performance difference is not statistically significant ( ğ›¼ = 0.05 ). We can conclude two observations: (i) Standalone FE scenario (Figure 9a): In the pure FE setting, CoFEH achieves the best average rank of 1.84. The CD plot reveals that CoFEH is statistically superior to ELLM, OCTree, OpenFE, and Mindware, as their average ranks fall outside the critical difference distance from CoFEH. While LFG shows competitive performance, CoFEH maintains a clear numerical lead, establishing its efficacy in FE pipeline optimization. (ii) Joint FE+HPO scenario (Figure 9b): The superiority of CoFEH becomes even more pronounced in the end-to-end task, reaching an average rank of 1.75. Crucially, in this scenario, CoFEH is not connected to any baseline by a horizontal bar, indicating that its performance gain is statistically significant against all five competitors. This underscores that our collaborative optimization strategy (interleaved tuning) provides a distinct advantage over both sequential and purely unified baselines. Overall, Figure 9 provides robust statistical evidence that CoFEH consistently outperforms existing frameworks. Its ability to maintain significant dominance in the Joint FE+HPO scenarioâ€”the most representative of real-world AutoML workflowsâ€”confirms its status as a new state-of-the-art for automated pipeline optimization. 

C.6 Cost Analysis 

In this appendix, we evaluate the computational and financial efficiency of the joint FE+HPO process across 28 datasets in Table 1, focusing on API expenditure and end-to-end runtime, as illustrated in Figure 10. 

API expense-performance trade-off : Figure 10a depicts the trade-off between the average API cost per task and the average test rank for the four LLM-based methodologies. Among the baselines, LFG achieves the second-best average rank; however, it incurs the highest API expenditure per task, exceeding all other methods. While OCTree and ELLM incur lower financial costs, they suffer from significantly higher (worse) average ranks. CoFEH occupies the most favorable position on the Pareto frontier, achieving a dominant average rank of 1.75 with moderate API consumption. This demonstrates that the steerable reasoning in CoFEH effectively translates API tokens into higher-quality feature discoveries compared to its LLM-based counterparts. 

End-to-end runtime efficiency : To compare execution efficiency within the 200-evaluation budget, we normalize the runtime of all methods against CoFEH for each dataset. As shown in Figure 10b, the average time ratios highlight a stark contrast between paradigms. (i) LLM-based methods (LFG, OCTree, and ELLM) exhibit total runtimes comparable to CoFEH, with their average ratios clustering near the 1.0 baseline. (ii) Traditional methods (OpenFE and Mindware) are significantly more time-intensive, averaging 3.35 Ã— and 1.85 Ã— the runtime of CoFEH, respectively. This efficiency 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 37 gap is largely attributable to the lack of domain-aware prior knowledge in traditional methods. Relying on brute-force exploration within predefined operator or algorithm spaces often leads to a "feature explosion." For instance, as detailed in Appendix C.3, OpenFE expands the dataset to 127 dimensions, whereas CoFEH and other reasoning-based methods typically maintain a compact representation under 20 dimensions. This dimensional inflation imposes a massive computational burden on the downstream ML model training during evaluation. Ultimately, this underscores the necessity of LLM-based FE from another perspective, as its semantic reasoning prevents the combinatorial explosion of redundant features that plagues traditional automated searches. 

C.7 Effectiveness of Meta-Feature for BO Surrogate Modeling 

In the collaborative optimization process of CoFEH, the performance ğ‘£ is jointly determined by the FE pipeline state ğ‘‡ 

and the hyperparameter configuration ğ€ . Since FE pipelines generated by LLMs lack an explicit, continuous search space, we utilize meta-features ğœ™ (ğ‘  ) to vectorize the dataset state after transformations. This allows the surrogate model to map discrete tree nodes into a representative feature space. As detailed in Section 3.2.1, we formalize the training dataset for the surrogate model ğ‘€ as: 

DBO =   [ğœ™ (ğ‘  ğ‘– ), ğ€ ğ‘–,ğ‘— ], ğ‘£ ğ‘–,ğ‘— 

 | ğ‘  ğ‘– âˆˆ V tree , âˆ€ğ‘— , (10) where Vtree is the set of nodes in the MCTS tree, and ğ€ ğ‘–,ğ‘— , ğ‘£ ğ‘–,ğ‘— denote the ğ‘— -th ML configuration and its corresponding score evaluated on node ğ‘  ğ‘– . We employ Random Forest (RF) as the surrogate model for its efficiency and robust handling of categorical variables. 

The role of meta-features . Without meta-features, the surrogate model is unable to perceive the impact of FE transformations, reducing the input to only hyperparameters ğ€ . In such a scenario, the training data becomes Dbase =

{( ğ€ ğ‘–,ğ‘— , ğ‘£ ğ‘–,ğ‘— )} , which introduces significant noise since identical hyperparameter configurations can yield vastly different performances across different FE pipelines. 

Evaluation of modeling capability . To verify the effectiveness of our joint modeling, we compare the predictive power of the RF surrogate with and without meta-features by collecting the 200 (FE pipeline, ML configuration) pairs generated by CoFEH in the Joint FE+HPO scenario. For each dataset, we generate out-of-fold (OOF) predictions via 5-fold cross-validationâ€”where the model is trained on four subsets to predict the fifthâ€”to ensure evaluation on unseen configurations and reflect true generalization. The Spearman rank correlation coefficient between the predicted and actual performance is then calculated as the primary metric. This is because the effectiveness of BO depends more on the surrogateâ€™s ability to correctly rank potential candidates than on absolute error minimization. The comparative results of surrogate modeling are illustrated in Figure 11, where datasets are sorted by their Spearman correlation in the "w/o meta-feature" setting. We observe several key trends: (i) Gains in FE-sensitive tasks : A significant performance gap is evident on datasets where the baseline ( ğ‘¤ /ğ‘œ meta) fits poorly. For instance, the most extreme case shows a leap from 0.20 to 0.76. This suggests that these tasks are highly sensitive to feature transformations; ignoring the FE pipeline state renders the surrogate model unable to capture the true performance landscape. (ii) Convergence in HPO-sensitive tasks : On the right side of the plot, where the baseline already achieves high correlation, the improvement from meta-features is relatively marginal. In these scenarios, the model performance is likely dominated by hyperparameter configurations, allowing the surrogate to perform reasonably well even without explicit FE information. (iii) Robustness and potential : Overall, our joint modeling approach is robust across diverse tasks. It provides substantial gains for FE-sensitive datasets while maintaining or slightly improving performance for HPO-sensitive ones, raising the mean Spearman correlation from 0.587 to 0.691. The few points where both settings 

> Manuscript submitted to ACM

38 Beicheng Xu et al. 1 6 11 16 21 26 

# Dataset 

# 0.2 

# 0.4 

# 0.6 

# 0.8 

> Spearman correlation

# w/ meta 

# w/o meta 

# mean w/ (0.691) 

# mean w/o (0.587) 

Fig. 11. Spearman correlation between surrogate model predictions and true performance. 0 25 50 75 100 

Number of Evaluations 

0.0 

0.5 

1.0 

> Normalized Performance

HPO 

FE 

(a) â€œDiamondsâ€: FE prop. = 0.7. 0 25 50 75 100 

Number of Evaluations 

0.0 

0.5 

1.0 

> Normalized Performance

HPO 

FE (b) â€œJannisâ€: FE prop. = 0.37. Fig. 12. Comparison between standalone FE and HPO. 

yield low correlation may stem from inherent task complexity that exceeds the modeling capacity of the RF surrogate. Notably, as CoFEH is orthogonal to the specific choice of meta-features, the integration of higher-quality meta-features in the future could further elevate this performance ceiling. 

C.8 Case Study of FE vs. HPO 

To validate the rationality of the dynamic optimizer selector, we examine two representative cases from Figure 5b exhibiting contrasting resource allocations: one dataset (â€œdiamondsâ€) with an FE proportion of 0.7 (FE-intensive) and another (â€œjannisâ€) with 0.37 (HPO-intensive). To verify if these allocations align with the datasetsâ€™ inherent sensitivities, we conducted independent 100-iteration runs of pure FE and pure HPO on both tasks. 

> Manuscript submitted to ACM

CoFEH : LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization 39 As illustrated in Figure 12, the normalized validation curves reveal distinct performance drivers for each task: (i) Diamonds (FE-Sensitive): The pure FE trajectory demonstrates a substantially higher performance ceiling and faster convergence compared to pure HPO. This confirms that the primary bottleneck for this task lies in the feature representation, validating the selectorâ€™s decision to prioritize FE with a 0.7 budget share. (ii) Jannis (HPO-Sensitive): Conversely, the pure HPO curve surpasses the FE results, indicating that hyperparameter configuration has a more profound impact on accuracy than further feature transformations. The selector correctly identifies this sensitivity, skewing the budget toward the Bayesian HPO module (FE ratio 0.37). These results empirically substantiate that the dynamic optimizer selector effectively detects the marginal utility of FE versus HPO. By adaptively shifting focus based on the inherent properties of the dataset, CoFEH ensures the search budget is concentrated where it yields the highest performance gains.