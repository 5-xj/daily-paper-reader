Title: Reward-Guided Discrete Diffusion via Clean-Sample Markov Chain for Molecule and Biological Sequence Design

URL Source: https://arxiv.org/pdf/2602.09424v1

Published Time: Wed, 11 Feb 2026 01:30:05 GMT

Number of Pages: 18

Markdown Content:
# Reward-Guided Discrete Diffusion via Clean-Sample Markov Chain for Molecule and Biological Sequence Design 

Prin Phunyaphibarn 1 Minhyuk Sung 1

## Abstract 

Discrete diffusion models have recently emerged as a powerful class of generative models for chem-istry and biology data. In these fields, the goal is to generate various samples with high rewards (e.g., drug-likeness in molecules), making reward-based guidance crucial. Most existing methods are based on guiding the diffusion model using intermediate rewards but tend to underperform since intermediate rewards are noisy due to the non-smooth nature of reward functions used in scientific domains. To address this, we propose Clean-Sample Markov Chain (CSMC) Sampler, a method that performs effective test-time reward-guided sampling for discrete diffusion models, en-abling local search without relying on intermedi-ate rewards. CSMC constructs a Markov chain of clean samples using the Metropolis-Hastings algo-rithm such that its stationary distribution is the tar-get distribution. We design a proposal distribution by sequentially applying the forward and back-ward diffusion processes, making the acceptance probability tractable. Experiments on molecule and biological sequence generation with various reward functions demonstrate that our method consistently outperforms prior approaches that rely on intermediate rewards. 

## 1. Introduction 

Discrete diffusion models have recently emerged as a pow-erful generative framework for discrete data, showing par-ticular promise in chemistry and biology for generating complex structures such as molecules and DNA sequences. Unlike autoregressive models that assume a canonical left-to-right ordering on the data, discrete diffusion models are more naturally suited for scientific domains whose data (e.g., molecules or DNA sequences) lack a natural fixed order-ing. For instance, the widely used SMILES (Weininger, 

> 1

KAIST, Daejeon, South Korea. Correspondence to: Prin Phunyaphibarn <prin10517@kaist.ac.kr >, Minhyuk Sung <mh-sung@kaist.ac.kr >.

Preprint. February 11, 2026. 

1988) representation for molecules is based on heuristic rules such as depth-first search which does not use a unified fixed ordering (Lee et al., 2025). In many applications in chemistry and biology, there are well-defined notions of quality for the data; for example, drug-likeness (Bickerton et al., 2012) for molecular struc-tures or enhancer activity (Taskiran et al., 2024; Wang et al., 2025) for DNA sequences. Thus, generative models must not only produce natural samples that resemble the training data but also achieve high-quality scores according to such domain-specific criteria. In the emerging regime of test-time scaling, quality con-siderations are incorporated by defining a reward function and optimizing it during inference through reward-guided sampling. The simplest strategy is Best-of-N sampling, where N samples are generated, and the one with the high-est reward is selected. However, this brute-force method is inefficient since it does not perform any structured guid-ance. Recent works (Kim et al., 2025b; Wu et al., 2023; Yu et al., 2023; Bansal et al., 2023; Chung et al., 2023) have instead proposed guidance using process rewards or 

intermediate rewards , which are computable at intermediate steps of generation. While methods involving intermediate rewards are tech-nically applicable to discrete diffusion models, they pose particular challenges for many types of chemistry and bi-ology data as reward functions in these domains are often non-smooth, meaning that small perturbations in the data can cause large changes in the reward. For example, in molecular structures, modifying even a single element in the string representation can render the entire molecule invalid, collapsing the reward to zero, as shown in Fig. 1 (left). As a result, relying on intermediate rewards does not provide an effective local search strategy in these cases. The key question that arises here is how to enable local search in reward-guided generation without relying on in-termediate rewards. We introduce Clean-Sample Markov Chain (CSMC) Sampler which performs iterative search over clean data samples using the Metropolis–Hastings (MH) algorithm. To propose a new clean sample from an existing one, we use forward–backward combinations— 1                       

> arXiv:2602.09424v1 [cs.LG] 10 Feb 2026 Clean-Sample Markov Chain Sampler
> Uniform Masked Clean Reward Trajectory Guidance BoN ✓✓✓✗
> SMC ✓✓✗✓
> SVDD ✓✓✗✓
> SGDD ✓✗✗✓
> CSMC ✓✓✓✓
> Table 1. Comparison of discrete diffusion inference-time scaling methods for reward-guided sampling. Our CSMC applies to all discrete diffusion frameworks and leverages the clean reward while guiding the sampling trajectory.

applying the forward process to corrupt clean data followed by running the reverse process to obtain a new sample. While the acceptance probability for the MH algorithm is intractable due to intractable clean sample probabilities, we show that the forward-backward proposal distribution makes the acceptance probability tractable, enabling effi-cient sampling. We validate CSMC on molecule and biological sequence generation across four different reward functions. CSMC achieves the highest reward in all settings, even for SMILES string generation where other methods fail due to inaccurate intermediate rewards. 

## 2. Related Work 

In continuous diffusion models, reward-guided sampling is conventionally performed using gradient-based meth-ods (Dhariwal & Nichol, 2021; Ho & Salimans, 2022; Chung et al., 2023; Song et al., 2023; Rozet et al., 2024; Bansal et al., 2023; Yoon et al., 2025; Kim et al., 2025b; Wu et al., 2023) which offer strong guidance towards high-reward regions. However, gradient-based approaches cannot be applied to discrete diffusion models as gradients are ill-defined in discrete spaces and it is not theoretically valid to add a continuous gradient to a discrete objective. 

Training-Free Reward-Guided Sampling for Discrete Diffusion. Recently, inference-time scaling methods for discrete diffusion models have been proposed to tackle reward-guided sampling. A comparison of these methods and our method is shown in Tab. 1. The simplest method is Best-of-N (BoN) sampling (Stien-non et al., 2020), which generates N samples independently and selects the one with the highest reward. Due to its simplicity, BoN is applicable to all types of discrete diffu-sion models. However, BoN does not guide the denoising trajectory using the reward, resulting in inefficient search, especially when high-reward samples lie in low-density re-gions unlikely to be sampled by the model. On the other hand, particle-based methods (Ma et al., 2025; Kim et al., 2025a; Singhal et al., 2025; Li et al., 2025), such as SMC (Doucet et al., 2001; Naesseth et al., 2019; Moral, 2004) and SVDD (Li et al., 2025), can be applied to dis-crete diffusion and incorporate reward signals during the denoising process through a reward-based resampling step. SMC (Doucet et al., 2001; Naesseth et al., 2019; Moral, 2004), takes N particles (samples) at each step and samples a subset of the particles to keep while throwing the rest away. By adjusting the probability of keeping each particle based on its expected reward, SMC encourages local exploration around potentially promising samples. SVDD (Li et al., 2025) exploits local search around high-reward samples by generating multiple candidate samples at each timestep and retaining only one sample by sampling the candidates with probability proportional to the reward. A key challenge with these methods is that they require computing intermediate rewards on noisy samples rather than on fully denoised samples. To address this, these approaches exploit the dif-fusion model’s ability to predict an approximation of the clean sample x0 given a noisy sample xt, and compute the rewards on this x0 prediction instead. This enables reward-guidance during the denoising process, but also inherently assumes that the predicted x0 is a good approximation of the true clean sample since exploration will be performed locally around samples with high intermediate rewards, an assumption which does not hold in many scientific applica-tions. Recently, Chu et al. (2025) proposed SGDD which uses the split Gibbs sampler (Vono et al., 2019) by alternating between denoising steps and running MCMC to optimize intermediate noisy samples. However, SGDD applies only 

to uniform diffusion models where intermediate noisy sam-ples can be treated as clean samples. The characteristics of uniform discrete diffusion is discussed in more detail in Sec. 4. Furthermore, SGDD still relies on computing the re-ward on the intermediate noisy samples during the MCMC optimization step. In this work, we bypass the need for intermediate rewards by using the Metropolis-Hastings algorithm (Robert & Casella, 2009) to construct a Markov chain of clean samples that converges to the desired target distribution. Our method is applicable to both uniform and masked discrete diffusion models, and only requires the computation of clean rewards 

on clean samples, which can then be used to efficiently guide the Markov chain (Tab. 1). 

Training-Based Reward-Guided Sampling for Discrete Diffusion. Unlike continuous diffusion, discrete diffu-sion models cannot leverage gradient signals due to the non-differentiable nature of discrete spaces. As such, only gradient-free approaches can be applied to discrete diffusion models. Guidance methods (Nisonoff et al., 2025; Schiff et al., 2025) have also been developed for diffusion models but require training a classifier on noisy data. Due to its non-differentiability, RLFT approaches are often used in discrete diffusion (Rector-Brooks et al., 2025; Wang et al., 2025; 2Clean-Sample Markov Chain Sampler Iterations          

> Noise
> Clean
> Samples
> . . .
> Forward Process
> Reverse Process
> Reverse
> Diffusion
> Sequential
> Monte Carlo (SMC)
> CSMC
> (Ours)
> Accept Sample
> Reject Sample
> C1C2C3C2C2C 4C1C2N3 4
> •QED: 0.444
> •Ring Count: 10
> •SA: 0.353
> C1C2C3C2C2C 4C1C2N3 4
> •QED: 0.0
> •Ring Count: 0
> •SA: 0.0
> C1C2C3C2C2C CC1C2N3 4
> •QED: 0.522
> •Ring Count: 5
> •SA: 0.423
> INVALID
> SMILES
> String
> (-0.444)
> (-10)
> (-0.354)
> (+0.078)
> (-5)
> (+0.070)

Figure 1. Left: In scientific applications, the rewards defined on discrete spaces are highly sensitive to small perturbations. A one-character change to a SMILE string can result in an invalid string with zero reward. Properties such as QED, ring count, and synthetic accessibility (SA) can also vary significantly even when changing only one or two tokens. Right: Reverse diffusion and typical inference-time scaling methods (Kim et al., 2025b; Li et al., 2025) such as SMC (Kim et al., 2025b) rely on guiding samples through noise levels by constructing a Markov chain beginning with pure noise and ending with clean samples. Our CSMC constructs a Markov chain consisting of only clean samples by successively applying the forward and reverse processes sequentially at each step. This formulation bypasses the need for intermediate rewards by evaluating the reward directly on clean samples while leveraging information from past samples for guidance. 

Zekri & Boull ´e, 2025) but are difficult to train since non-differentiability forces fine-tuning to be done using policy gradients based on highly complex reward landscapes (Ue-hara et al., 2025). Crucially, modifying the model weights also runs the risk of deviating from the pretrained prior, af-fecting the naturalness of the generated samples and further complicating training. 

## 3. Background 

Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) learn to reverse a forward Markov process. Although originally designed for continuous spaces, diffusion models have also been successfully ap-plied to discrete state spaces (Austin et al., 2021; Campbell et al., 2022; Lou et al., 2024; Sahoo et al., 2024; Schiff et al., 2025; Shi et al., 2024). 

3.1. Discrete and Continuous-Time Discrete Diffusion Discrete-Time Discrete Diffusion. These discrete diffu-sion models are characterized by forward transition matrices 

Qt. Let Qt = Q1Q2 · · · Qt. These transition matrices de-fine the forward process: 

pt|t−1(xt|xt−1) = Cat (xt; p = xt−1Qt) ,pt(xt|x0) = Cat  xt; p = x0Qt

 ,

where Cat (·; p) denotes the categorical distribution with probabilities given by p.While the reverse process probability p(xt−1|xt) is not di-rectly tractable, by additionally conditioning on x0, the reverse process can be derived in closed form as 

p(xt−1|xt, x 0) = Cat xt−1; p = xtQ⊤ 

> t

⊙ x0Qt−1

x0Qtx⊤

> t

!

.

A neural network pθ (x0|xt) ≈ p(x0|xt) is learned and de-noising is performed by the following parameterization: 

pθ (xt−1|xt) ∝ X

> x0

q(xt−1, x t|x0)pθ (x0|xt).

Notably, the learned neural network pθ (x0|xt) predicts a distribution over x0 given xt which enables sampling x0-predictions ˆx0(xt) ∼ pθ (x0|xt).

Continuous-Time Discrete Diffusion. Campbell et al. (2022) proposed a continuous-time framework based on Continuous-Time Markov Chains (CTMC) where state tran-sitions can occur at any time. Instead of defining transition matrices, CTMC-based discrete diffusion defines a forward transition rate matrix Rt which defines the infinitesimal transition probability between two timesteps: 

p(xt|˜xt−∆t) = δxt,˜xt−∆t + Rt(˜ xt−∆t, x t)∆ t + o(∆ t).

By using a continuous-time framework, advanced sampling strategies such as predictor-corrector methods (Campbell et al., 2022; Zhao et al., 2025) and planning (Liu et al., 2025; Peng et al., 2025) and score-based approaches (Meng et al., 2022; Sun et al., 2023; Lou et al., 2024) have been proposed. 

3.2. Masked and Uniform Discrete Diffusion 

Discrete diffusion models allow the user to choose the tran-sition matrix. The two most common choices of transition 3Clean-Sample Markov Chain Sampler 

matrices result in masked diffusion models (MDMs) (Austin et al., 2021; Sahoo et al., 2024; Shi et al., 2024) and uni-form state models (USMs) (Austin et al., 2021; Schiff et al., 2025). 

Masked Diffusion Models (MDMs). MDMs define the forward process by progressively replacing tokens with a special mask token. The denoising model learns to recover the original sequence from these masked inputs. Notably, samples at intermediate time steps are not valid samples because of the mask tokens, which are not present in the dataset. 

Uniform State Models (USMs). USMs replace each to-ken with a randomly chosen token from the vocabulary as noise increases. This creates a uniform corruption process in which every other possible token substitution is equally likely. Unlike MDMs, samples at intermediate time steps may constitute valid samples. 

## 4. Clean-Sample Markov Chain Sampler 

Let the reward function be denoted by r(·) : X → R where 

X is the domain, and ∆( X ) denote the set of all probability distributions defined on X . Given a reward function r(x),our objective is to generate samples with high rewards while maintaining naturalness by leveraging a pretrained genera-tive model ppre (·). To accomplish this, we sample from the reward-weighted distribution pβ (x):

pβ (x) := arg max  

> p∈∆( X)

Ex∼p(·) [r(x)] − βD KL (p(·)∥ppre (·)) 

∝ exp( r(x)/β )ppre (x),

where β is a hyperparameter controlling the “naturalness” of the samples through KL-regularization with the pretrained model. Most previous methods such as particle-based methods per-form guidance by utilizing intermediate rewards r(ˆ x0(xt)) 

computed from the x0-prediction ˆx0(xt) at xt as an approx-imation to the clean sample. However, intermediate rewards are often inaccurate and noisy in many scientific applica-tions due to the non-smooth reward functions. In scientific applications, even a slight perturbation can significantly im-pact the reward. For instance, when generating molecules based on the SMILES (Weininger, 1988) representation, modifying a single token on a high-reward molecule can result in an invalid molecule with zero reward or a molecule with very different properties, as shown in Fig. 1 (left). This is in stark contrast to rewards defined on continuous space where a slight perturbation smoothly affects the re-ward (small perturbations to some pixel values of an image leave the semantics of the image unchanged). Due to the sensitivity of the reward, the x0 prediction must be perfectly accurate, otherwise the intermediate rewards will be unin-formative. A sample whose intermediate reward is zero may be prematurely removed by a particle-based method even though changing a single token may yield a high-reward sample (Fig. 1, left). In these cases, it is more beneficial to leverage only clean reward r(x0) computed on a clean sample x0 rather than on an approximation ˆx0(xt). One such method leveraging clean rewards is Best-of-N (BoN) sampling (Stiennon et al., 2020). However, BoN does not provide guidance during the denoising process, resulting in inefficient exploration. Its effectiveness is therefore limited to what the pretrained model can already generate: if high-reward samples lie in low-density regions of the model’s distribution, they are unlikely to ever be produced, even when many samples are drawn. A natural question arises: How can we leverage clean rewards while using information from past samples for guidance? 

Our proposed method, Clean-Sample Markov Chain 

(CSMC) Sampler, answers this question by using the Metropolis-Hastings (MH) algorithm to construct a Markov chain of clean samples that converges to the reward-weighted distribution pβ (x0). By using only clean samples, CSMC leverages accurate clean rewards. By using the MH algorithm to iteratively refine the samples, the clean rewards of past samples can be used for guidance. 

4.1. Bypassing the Intermediate Rewards 

Previous methods require intermediate rewards because the denoising trajectory of diffusion models output clean sam-ples only at the very end. A natural solution is to instead explore the clean data space by constructing a chain of 

clean samples {x(i)0 }i, which converges to the target dis-tribution pβ (x0). At each iteration, rewards can then be computed directly on clean samples, yielding clean rewards 

r(x0) which can then be used to guide the chain towards the target distribution. To accomplish this, we propose to use the Metropolis-Hastings (MH) algorithm. Suppose we want to sample from the (potentially unnormal-ized) target distribution pβ (x0). We first define a proposal distribution q(x′

> 0

|x0) to generate the next candidate sample 

x′ 

> 0

∼ q(x′

> 0

|x0) given the current sample x0. After gener-ating the proposal candidate, we decide whether or not to accept this proposal with probability A(x′

> 0

|x0) defined as 

A(x′

> 0

|x0) = min (1 , α ) , α = pβ (x′

> 0

)q(x0|x′

> 0

)

pβ (x0)q(x′

> 0

|x0) .

With a slight abuse of notation, we use A(x′

> 0

|x0) inter-changeably with A when the choice of x′ 

> 0

and x0 are not important. The MH algorithm can be shown to construct a Markov chain whose stationary distribution is the target distribution. For more details and proof of convergence on the MH algorithm, please refer to App. A. To gener-ate samples from the target reward-weighted distribution 4Clean-Sample Markov Chain Sampler 

pβ (x0) ∝ exp( r(x0)/β )p(x0), the acceptance probability is as follows: 

A(x′

> 0

|x0) = min(1 , α ), (1) 

α = exp 

 r(x′

> 0

) − r(x0)

β

 p(x′

> 0

)q(x0|x′

> 0

)

p(x0)q(x′

> 0

|x0) . (2) The difficulty in applying the MH algorithm to diffusion models is the intractibility of the acceptance probability. Since p(x0) is intractable, the acceptance probability cannot be efficiently computed. 

4.2. Forward-Backward Proposal Distribution 

To bypass the computation of p(x0), we define the proposal distribution in such a way that the proposal probabilities 

q(x0|x′

> 0

) and q(x′

> 0

|x0) cancel out p(x0) and p(x′

> 0

). We define the proposal distribution q(x′

> 0

|x0) using a forward-backward diffusion process as follows. Starting from the current clean sample x0, we choose a random time t ∼U(tlo , t hi ) (where tlo and thi are user-defined parameters) and apply the forward process to obtain a noisy auxiliary sample xt ∼ pt(·| x0). Then, we run the reverse process to obtain a new clean sample x′ 

> 0

∼ p(·| xt) This proposal distribution is defined by 

q(x′

> 0

|x0) := pt(xt|x0)p(x′

> 0

|xt), (3) where t ∼ U (tlo , t hi ) and xt ∼ p(·| x0) are resampled at each step. However, we find that running a one-step reverse pro-cess results in an inaccurate x′ 

> 0

in practice. Instead, we run an M -step reverse process on xt to obtain 

(xtM , x tM −1 , . . . , x 1, x ′

> 0

) ∼ QMi=1 p(xti−1 |xti ) where t =

tM > · · · > t 0 = 0 . We then discard the xti and retain only x′

> 0

. This proposal distribution can be interpreted as exploring locally around x0.Assuming that the marginal distributions of the learned re-verse process matches that of the forward process, we have 

α = exp 

 r(x′

> 0

) − r(x0)

β

 p(x′

> 0

)p(xt|x′

> 0

)p(x0|xt)

p(x0)p(xt|x0)p(x′

> 0

|xt)= exp 

 r(x′

> 0

) − r(x0)

β

 p(x′

> 0

)p(xt|x′

> 0

)p(x0|xt, x ′

> 0

)

p(x0)p(xt|x0)p(x′

> 0

|xt, x 0)= exp 

 r(x′

> 0

) − r(x0)

β



.

Thus, the acceptance probability simplifies to 

A(x′

> 0

|x0) = min(1 , α ), α = exp 

 r(x′

> 0

) − r(x0)

β



,

(4) which can now be efficiently computed. The assumption that the marginals of the reverse and forward process match 

Algorithm 1 CSMC (Clean-Sample Markov Chain Sam-pler) 

1: x(1) 0 ∼ p(xT ) QTt=1 p(xt−1|xt)

2: for k = 1 , . . . , K do 

3: t0 ∼ U (tlo , t hi )

4: x(k) 

> t0

∼ pt0 (·| x(k)0 )

5: ˜x(k+1) 0 ∼ QMi=1 p(x(k) 

> ti−1

|x(k) 

> ti

)

6: α ← exp(( r(˜ x(k+1) 0 ) − r(xk 

> 0

)) /β )

7: A ← min(1 , α )

8: ρ ∼ U (0 , 1) 

9: x(k+1) 0 ← ˜x(k+1) 0 if ρ < A else x(k)0

10: end for 

11: {x(K/ 2) 0 , . . . , x (K)0 }

is mild and is enforced by the training objective of discrete diffusion models (Austin et al., 2021). The detailed deriva-tion of the acceptance probability can be found in App. B. CSMC begins by running the full reverse process to obtain a clean initial sample x(1) 0 . At each step, we generate a candidate sample using our forward-backward diffusion process and choose to accept or reject the candidate based on the MH acceptance probability given in Eq. 4, as shown in Fig. 1 (right). This results in a chain of clean samples whose stationary distribution is the reward-weighted distribution. While MH is guaranteed to construct a Markov chain whose stationary distribution is pβ (x0), initial samples may not come from the stationary distribution. Thus, we discard the first half of the chain and take as many samples as desired from the latter half at equal intervals. The practice of “throwing away” the initial samples of a Markov chain is known as burn-in (Robert & Casella, 2009; Murphy, 2023). The full algorithm for CSMC is shown in Alg. 1. 

## 5. Experiments 

We conduct experiments on molecule generation with QM9 (Ramakrishnan et al., 2014) and ZINC250K (Irwin et al., 2012), and biological sequence design using the MPRA (Gosai et al., 2023) dataset. We pretrain four discrete diffusion models on each of the dataset: • MDM (Sahoo et al., 2024): A masked discrete diffusion model using discrete-time ancestral sampling. • USM (Schiff et al., 2025): A uniform state discrete diffu-sion model using discrete-time ancestral sampling. • SEDD-M (Lou et al., 2024) : A score-based CTMC masked discrete diffusion model. • SEDD-U (Lou et al., 2024): A score-based CTMC uni-form discrete diffusion model. We compare the following training-free reward-guided sam-pling methods for discrete diffusion: 5Clean-Sample Markov Chain Sampler 0.0    

> 0.2
> 0.4
> 0.6
> Reward
> QM9:QED
> 0
> 2
> 4
> 6
> 8
> 10 QM9:Rings
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> Reward
> ZINC250K:QED
> 0
> 2
> 4
> 6
> 8
> ZINC250K:Rings
> 2
> 1
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> Reward
> MPRA:HepG2
> Figure 2. Reward distributions of the pretrained USM. The red dotted line represents the average reward achieved by CSMC. For ring count and HepG2, the pretrained model reward distribution has low density at higher rewards, resulting in degraded performance for BoN sampling.

• Pretrained models: Samples are generated using the pretrained model. • Best-of-N (BoN): N samples are generated and the one with the highest reward is selected. • SMC (Doucet et al., 2001): The representative derivative-free particle-based sampling method which approximates the target distribution by updating and resampling a set of 

N particles at each step using the intermediate reward. • SVDD (Li et al., 2025): N candidate samples are gen-erated at each step and a sample is selected by randomly choosing a sample with probability proportional to its intermediate reward. • SGDD (Chu et al., 2025): A posterior sampling method specifically tailored for uniform CTMC discrete diffusion models which uses split Gibbs sampling (Vono et al., 2019). Since this method is based on the forward process of uniform CTMC discrete diffusion models, we only evaluate SGDD on SEDD-U and not on the discrete-time model USM. • CSMC(Ours): We run Alg. 1 to construct a Markov chain converging to pβ (x0) and draw multiple samples from the chain. • CSMC-B (Ours): CSMC with batching by running B

chains in parallel while keeping the total NFE fixed by reducing the number of iterations in each Markov chain, resulting in faster sampling times. We match the total diffusion model NFE for each method. More experiment details can be found in App. C. 

Molecule Generation. We test our method on two molecule datasets: QM9 (Ramakrishnan et al., 2014) and ZINC250K (Irwin et al., 2012). QM9 is a dataset consisting of ∼133,000 small organic molecules, and ZINC250K is a dataset of 250,000 commercially available compounds. The molecules in both datasets are represented as SMILES strings (Weininger, 1988). For our reward functions, we use QED (Bickerton et al., 2012), ring count (Rings) , and synthetic accessibility (SA) (Ertl & Schuffenhauer, 2009). QED measures the drug-likeness of a compound based on eight widely used molecular properties (number of hydro-gen bond donors/acceptors, molecular polar surface area, number of aromatic rings, etc.). Higher QED values indi-cate higher drug-likeness. Ring count measures the number of rings in the symmetrized smallest set of smallest rings (SSSR). Finally, SA measures the ease of synthesis of drug-like molecules through a combination of known common structural features in known synthesized molecules, and a penalty based on complex structural features of the molecule. SA takes on a value bewteen 1 and 10 where higher values indicate that the molecule is harder to synthesize. In this work, SA is converted to a reward function by applying the renormalization (10 − SA )/9 so that higher values indicate better performance. For all rewards, higher is better. 

Biological Sequence Generation. We pretrain the dis-crete diffusion models on the DNA dataset provided by Gosai et al. (2023) (which we refer to as MPRA) which mea-sures the enhancer acitivity of ∼700,000 DNA sequences using massively parallel reporter assays (MPRA). An En-former model (Avsec et al., 2021) is trained to predict the enhancer activity level in the HepG2 cell line which is used as the reward function. We use the model trained on two different subsets of data and use one exclusively to provide the guidance signal during sampling and the other for eval-uation. Higher predicted HepG2 activity indicates better performance. 

Results. The quantitative results of reward-guided genera-tion are shown in Tab. 2. As shown, CSMC or CSMC-B achieves the best reward in all cases. We also compute the diversity in Tab. 8 in App. D.1, showing that the samples remain sufficienctly diverse despite achieving high reward. SMC and SVDD require intermediate rewards which are often inaccurate. As such, these methods sometimes only yield slightly improved results over the pretrained model and, in some cases, slightly worse results due to inaccu-rate exploration. The performance of these methods is also highly reliant on both the smoothness of the reward func-tion and the diffusion model’s accuracy in x0 prediction. CSMC, on the other hand, only use accurate clean rewards for guidance, resulting in accurate and efficient guidance. While BoN performs well on QED and SA for QM9 and ZINC250K, its performance is not as good on the HepG2 activity reward for MPRA and the ring count reward for QM9 and ZINC250K. This is due to the lack of trajectory guidance: BoN performs well only for settings where high-reward samples are likely to be sampled by the pretrained model and performs worse in cases where high-reward sam-6Clean-Sample Markov Chain Sampler 

QM9 ZINC250K MPRA 

QED Rings SA QED Rings SA HepG2 MDM Pretrained 0.461 ±0.138 2.755 ±3.432 0.558 ±.0227 0.663 ±0.323 2.020 ±2.488 0.742 ±0.323 0.442 ±1.767 

BoN 0.580 ±0.073 5.479 ±1.286 0.818 ±0.129 0.854 ±0.121 3.854 ±1.923 0.862 ±0.121 1.842 ±2.093 

SMC 0.512 ±0.144 2.803 ±1.713 0.759 ±0.271 0.637 ±0.310 2.128 ±2.594 0.769 ±0.310 3.087 ±2.975 

SVDD 0.567 ±0.117 2.849 ±1.462 0.836 ±0.174 0.776 ±0.255 2.490 ±1.848 0.754 ±0.255 2.319 ±2.654 

CSMC 0.610 ±0.085 10.00 ±0.933 0.913 ±0.077 0.910 ±0.060 8.091 ±2.328 0.905 ±0.060 5.259 ±0.849 

CSMC-B 0.610 ±0.083 8.678 ±2.531 0.911 ±0.095 0.914 ±0.049 6.032 ±1.918 0.903 ±0.073 5.127 ±1.428 

USM Pretrained 0.461 ±0.152 2.032 ±1.341 0.620 ±0.223 0.739 ±0.270 2.598 ±1.883 0.768 ±0.189 0.351 ±1.541 

BoN 0.600 ±0.054 5.044 ±1.267 0.839 ±0.097 0.901 ±0.065 4.184 ±1.307 0.889 ±0.048 1.753 ±1.965 

SMC 0.485 ±0.170 1.957 ±1.212 0.660 ±0.203 0.750 ±0.270 2.633 ±1.828 0.785 ±0.184 0.743 ±2.048 

SVDD 0.454 ±0.157 2.135 ±1.460 0.624 ±0.217 0.750 ±0.260 2.562 ±1.861 0.773 ±0.195 0.695 ±1.920 

CSMC 0.610 ±0.085 9.570 ±0.791 0.908 ±.063 0.917 ±0.050 8.703 ±3.454 0.898 ±0.060 5.897 ±1.499 

CSMC-B 0.609 ±0.081 9.240 ±2.401 0.915 ±0..088 0.910 ±0.063 6.310 ±2.494 0.895 ±0.073 5.292 ±1.876 

SEDD-M Pretrained 0.460 ±0.143 2.305 ±3.318 0.588 ±0.240 0.669 ±0.328 2.350 ±2.648 0.731 ±0.243 0.373 ±1.631 

BoN 0.582 ±0.069 5.241 ±2.441 0.831 ±0.123 0.852 ±0.119 4.057 ±2.022 0.857 ±0.088 1.874 ±2.243 

SMC 0.461 ±0.162 2.425 ±3.365 0.599 ±0.263 0.669 ±0.338 2.251 ±2.856 0.745 ±0.224 0.386 ±1.639 

SVDD 0.450 ±0.159 2.378 ±3.243 0.575 ±0.244 0.666 ±0.326 2.362 ±2.707 0.739 ±0.245 0.456 ±1.824 

CSMC 0.619 ±0.096 9.792 ±3.402 0.894 ±0.113 0.875 ±0.150 10.026 ±4.305 0.885 ±0.088 7.153 ±1.439 

CSMC-B 0.594 ±0.098 8.977 ±3.041 0.821 ±0.262 0.846 ±0.189 5.677 ±5.179 0.863 ±0.162 5.991 ±1.643 

SEDD-U Pretrained 0.458 ±0.154 1.654 ±2.355 0.637 ±0.230 0.741 ±0.266 2.524 ±1.753 0.771 ±0.189 0.455 ±1.755 

BoN 0.583 ±0.066 3.862 ±1.996 0.843 ±0.112 0.904 ±0.045 4.056 ±1.241 0.889 ±0.049 1.729 ±2.216 

SMC 0.546 ±0.113 2.718 ±2.790 0.811 ±0.196 0.850 ±0.174 2.610 ±1.684 0.847 ±0.136 3.334 ±3.337 

SVDD 0.518 ±0.120 2.482 ±2.624 0.779 ±0.234 0.809 ±0.217 2.759 ±1.784 0.817 ±0.148 3.189 ±3.330 

SGDD 0.536 ±0.121 2.644 ±2.791 0.684 ±0.211 0.844 ±0.152 2.535 ±1.627 0.847 ±115 9.240 ±2.050 

CSMC 0.619 ±0.080 7.488 ±5.376 0.886 ±0.075 0.922 ±0.073 5.499 ±2.614 0.898 ±0.074 10.09 ±2.637 

CSMC-B 0.572 ±0.068 7.213 ±4.325 0.908 ±0.120 0.894 ±0.112 3.625 ±0.870 0.883 ±0.088 9.350 ±1.762 

Table 2. Average reward (with 95% confidence intervals) for each method and pretrained discrete diffusion model. Higher is better. 

Bold indicates the best method, and underline denotes the second best. CSMC achieves the best average reward across all datasets and discrete diffusion models. 

ples lie in low-density regions of the pretrained model. As shown in Fig. 2, for ring count and HepG2 rewards, the high-reward samples generated by CSMC (red dashed line) lie in low-density regions unlikely to be sampled from by the pre-trained model. CSMC, which uses the forward-backward diffusion process to guide the samples toward high-reward regions, is able to achieve high rewards across all datasets and reward functions even when high-reward samples lie in low-density regions of the pretrained model. Although SGDD achieves the second-best result on the MPRA dataset, it is only applicable to uniform CTMC dis-crete diffusion models whereas our CSMC is applicable to all discrete diffusion models. Furthermore, SGDD still relies on computing intermediate rewards, limiting its per-formance on datasets such as QM9 and ZINC250K where the intermediate rewards are inaccurate. 

Wall-Clock Time. We include comparisons of wall-clock time and performance in Tab. 3. Under matched wall-clock time, CSMC-B significantly outperforms all other base-lines in ring count and HepG2. Additional wall-clock time comparisons are included in App. D.2. 

5.1. Comparison with Training-based Methods 

We provide comparison with standard training-based guid-ance methods. In Tab. 4, we compare CSMC against discrete classifier-free guidance (D-CFG) (Nisonoff et al., 2025) on the QM9 dataset. CSMC outperforms D-CFG across all rewards when using MDM as the base model. When using USM as the base model, CSMC outperforms D-CFG on QED and ring count while achieving comparable SA. Furthermore, CSMC can be applied on top of D-CFG, resulting in further improvements except in ring count for ZINC250K MDM. This is due to the MDM CFG model collapsing to narrow modes when trained on ZINC250K ring count, resulting in insufficient exploration or the space when applying CSMC. 

ZINC250K: Rings 

CSMC-B BoN BoN SMC SVDD Batch Size 8 78 8 14 886 NFE 1024 5750 1024 1024 65536 Reward 5.315 4.945 4.312 2.630 2.607 Time (s) 321 331 359 320 276 

Table 3. ZINC250K rewards under matched wall-clock time. 

Rewards and time are computed from 128 drawn samples. CSMC-B significantly outperforms all other baselines in ring count while being comparable with BoN in QED and SA. 

QM9 

QED Rings SA MDM D-CFG 0.820 5.886 0.888 CSMC 0.910 8.091 0.905 D-CFG + CSMC 0.927 7.214 0.928 

USM D-CFG 0.903 5.613 0.908 CSMC 0.917 8.703 0.898 D-CFG + CSMC 0.927 8.943 0.920 

Table 4. Comparison with training-based guidance on ZINC250K. CSMC outperforms D-CFG. CSMC can also be combined with D-CFG by adopting D-CFG-based proposals, re-sulting in further improvements. 

7Clean-Sample Markov Chain Sampler 

5.2. Analysis and Hyperparameter Studies 

In this section, we analyze the effects of the various compo-nents of CSMC. 

Convergence. Since CSMC requires running sufficiently many iterations until the Markov chain converges to its stationary distribution, we empirically analyze the speed of convergence as well as the impact of the chain length. The autocorrelation function shown in Fig. 3 quickly decays to zero within the first 2000 iterations and remains small until the end, indicating that the chain converges quickly. We also report additional autocorrelation plots as well as the acceptance rate in App. D.3. 0 10000 20000   

> Iteration
> 0.0
> 0.5
> 1.0
> Autocorrelation
> QED
> 010000 20000
> Iteration
> Rings
> MDM
> USM
> ZINC250K

Figure 3. Autocorrelation plots for ZINC250K MDM and USM. 

The autocorrelation function quickly vanishes to zero within the first 2000 iterations, indicating fast mixing. 

NFE. We also provide additional experiments varying the NFE in App. D.4, showing that CSMC outperforms the baseline across various NFEs. 

Initialization. Since CSMC constructs a Markov chain of clean samples, the choice of initialization could impact the convergence rate of the chain. We have already shown in Fig. 3 that the Markov chain shows fast mixing. We now provide further evidence that the initialization x(1) 0 of the chain does not significantly affect the convergence. In Fig. 4, we plot the reward trajectories for CSMC with 64 different initializations. The reward distribution across the 64 chains are shown as histograms on top (corresponding to the iterations marked by the gray dashed lines). Although each initial point starts with a different reward, many of which are zero, as shown in the histogram, the rewards of each chain quickly converge to the same high-reward region. 

Results with Varying Number of Reverse Steps M .

CSMC enables fast sampling by leveraging an M -step re-verse sampler in the forward-reverse proposal to produce a new candidate sample. As shown in Tab. 5, varying M

does not significantly affect the rewards achieved by CSMC, except when using large M for the ring count reward. We hypothesize that this is due to limited compute since using for ZINC250K tasks results in only 88 iterations of CSMC, which may not be sufficient for harder tasks such as ring count where high reward samples lie in low density regions of the pretrained model’s distribution. This hypothesis is also supported by the autocorrelation plot in Fig. 3 which shows that the autocorrelation function for USM (green) requires more iterations to converge for ring count. 0 25 50 75 100 125 150 175 200 

> Iteration
> 0.0
> 0.2
> 0.4
> 0.6
> Reward

Figure 4. Reward trajectories with different initializations. We plot the reward trajectories of CSMC for 64 different initializa-tions using MDM on QM9 QED (bottom), along with the reward distribution of the 64 different chains (top). Trajectories with lower reward also quickly converge to high reward regions of the distri-bution, demonstrating robustness to the initialization. 

ZINC250K 

M QED Ring SA 5 0.917 8.703 0.898 10 0.910 8.477 0.901 15 0.917 8.256 0.895 20 0.917 6.885 0.897 

Table 5. Results with varying number of reverse steps M for ZINC250K USM. We vary the number of reverse steps M in the forward-reverse proposal. Varying does not significantly affect the rewards achieved by CSMC. 

Time Parameters tlo and thi . We plot the reward while varying the time parameters tlo and thi in Fig. 7 in App. D.5. As shown, the reward and diversity achieved by CSMC is robust to the choice tlo and thi .

## 6. Conclusion 

We propose CSMC , a novel training-free reward-guided sampler for discrete diffusion which avoids reliance on noisy intermediate rewards based on constructing a Markov Chain of clean samples via the Metropolis-Hastings algorithm. Our proposal distribution, modeled through a forward–backward combination, makes the acceptance probability tractable. Experiments on molecular and biological sequence gener-ation with various reward functions demonstrate superior performance of our method compared to previous methods that rely on intermediate rewards. 

Limitations and Future Work. While the use of the MH algorithm allows CSMC to sample from the target distri-bution, theoretical convergence guarantees depend on the existence of a perfect denoiser. This assumption is mild since discrete diffusion models are trained to match the marginal distributions of the forward process (Austin et al., 2021). Exact quantification of the discrepancy in the learned marginal distributions is an open area of research and is left for future works. 8Clean-Sample Markov Chain Sampler 

## References 

Adam, K. D. B. J. et al. A method for stochastic optimiza-tion. arXiv preprint arXiv:1412.6980 , 1412(6), 2014. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. NeurIPS , 2021. Avsec, ˇZ., Agarwal, V., Visentin, D., Ledsam, J. R., Grabska-Barwinska, A., Taylor, K. R., Assael, Y., Jumper, J., Kohli, P., and Kelley, D. R. Effective gene expression predic-tion from sequence by integrating long-range interactions. 

Nature methods , 18(10):1196–1203, 2021. Bansal, A., Chu, H.-M., Schwarzschild, A., Sengupta, S., Goldblum, M., Geiping, J., and Goldstein, T. Universal guidance for diffusion models. In CVPR , 2023. Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S., and Hopkins, A. L. Quantifying the chemical beauty of drugs. Nature chemistry , 4(2):90–98, 2012. Campbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G., and Doucet, A. A continuous time framework for discrete denoising models. NeurIPS , 2022. Chu, W., Wu, Z., Chen, Y., Song, Y., and Yue, Y. Split gibbs discrete diffusion posterior sampling. NeurIPS , 2025. Chung, H., Kim, J., Mccann, M. T., Klasky, M. L., and Ye, J. C. Diffusion posterior sampling for general noisy inverse problems. In ICLR , 2023. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. NeurIPS , 2021. Doucet, A., De Freitas, N., and Gordon, N. An introduction to sequential monte carlo methods. In Sequential Monte Carlo methods in practice , pp. 3–14. Springer, 2001. Ertl, P. and Schuffenhauer, A. Estimation of synthetic acces-sibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of chem-informatics , 1(1):8, 2009. Gosai, S. J., Castro, R. I., Fuentes, N., Butts, J. C., Kales, S., Noche, R. R., Mouri, K., Sabeti, P. C., Reilly, S. K., and Tewhey, R. Machine-guided design of synthetic cell type-specific cis-regulatory elements. bioRxiv , 2023. Ho, J. and Salimans, T. Classifier-free diffusion guidance. 

arXiv preprint arXiv:2207.12598 , 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-bilistic models. NeurIPS , 2020. Irwin, J. J., Sterling, T., Mysinger, M. M., Bolstad, E. S., and Coleman, R. G. ZINC: a free tool to discover chem-istry for biology. Journal of chemical information and modeling , 52(7):1757–1768, 2012. Kim, J., Yoon, T., Hwang, J., and Sung, M. Inference-time scaling for flow models via stochastic generation and rollover budget forcing. NeurIPS , 2025a. Kim, S., Kim, M., and Park, D. Test-time alignment of diffusion models without reward over-optimization. In 

ICLR , 2025b. Lee, S., Kreis, K., Veccham, S. P., Liu, M., Reidenbach, D., Peng, Y., Paliwal, S. G., Nie, W., and Vahdat, A. Genmol: A drug discovery generalist with discrete diffusion. In 

ICML , 2025. Li, X., Zhao, Y., Wang, C., Scalia, G., Eraslan, G., Nair, S., Biancalani, T., Ji, S., Regev, A., Levine, S., et al. Derivative-free guidance in continuous and discrete dif-fusion models with soft value-based decoding. NeurIPS ,2025. Liu, S., Nam, J., Campbell, A., Stark, H., Xu, Y., Jaakkola, T., and Gomez-Bombarelli, R. Think while you generate: Discrete diffusion with planned denoising. In ICLR , 2025. Loshchilov, I. and Hutter, F. Decoupled weight decay regu-larization. In ICLR , 2019. Lou, A., Meng, C., and Ermon, S. Discrete diffusion mod-eling by estimating the ratios of the data distribution. In 

ICML , 2024. Ma, N., Tong, S., Jia, H., Hu, H., Su, Y.-C., Zhang, M., Yang, X., Li, Y., Jaakkola, T., Jia, X., et al. Scaling inference time compute for diffusion models. In CVPR ,2025. Meng, C., Choi, K., Song, J., and Ermon, S. Concrete score matching: Generalized score matching for discrete data. 

NeurIPS , 2022. Moral, P. Feynman-Kac formulae: genealogical and in-teracting particle systems with applications . Springer, 2004. Murphy, K. P. Probabilistic machine learning: Advanced topics . MIT press, 2023. Naesseth, C. A., Lindsten, F., Sch ¨on, T. B., et al. Elements of sequential monte carlo. Foundations and Trends in Machine Learning , 12(3):307–392, 2019. Nisonoff, H., Xiong, J., Allenspach, S., and Listgarten, J. Unlocking guidance for discrete state-space diffusion and flow models. In ICLR , 2025. Peng, F. Z., Bezemek, Z., Patel, S., Rector-Brooks, J., Yao, S., Bose, A. J., Tong, A., and Chatterjee, P. Path planning for masked diffusion model sampling. arXiv preprint arXiv:2502.03540 , 2025. 9Clean-Sample Markov Chain Sampler 

Ramakrishnan, R., Dral, P. O., Rupp, M., and Von Lilienfeld, O. A. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data , 1(1):1–7, 2014. Rector-Brooks, J., Hasan, M., Peng, Z., Liu, C.-H., Mittal, S., Dziri, N., Bronstein, M. M., Chatterjee, P., Tong, A., and Bose, J. Steering masked discrete diffusion mod-els via discrete denoising posterior prediction. In ICLR ,2025. Robert, C. P. and Casella, G. Metropolis–hastings algo-rithms. In Introducing Monte Carlo Methods with R , pp. 167–197. Springer, 2009. Rozet, F., Andry, G., Lanusse, F., and Louppe, G. Learning diffusion priors from observations by expectation maxi-mization. NeurIPS , 2024. Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. NeurIPS ,2024. Schiff, Y., Sahoo, S. S., Phung, H., Wang, G., Boshar, S., Dalla-torre, H., de Almeida, B. P., Rush, A., Pierrot, T., and Kuleshov, V. Simple guidance mechanisms for discrete diffusion models. ICLR , 2025. Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. Simplified and generalized masked diffusion for discrete data. NeurIPS , 2024. Singhal, R., Horvitz, Z., Teehan, R., Ren, M., Yu, Z., McK-eown, K., and Ranganath, R. A general framework for inference-time scaling and steering of diffusion models. In ICML , 2025. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequi-librium thermodynamics. In International conference on machine learning , pp. 2256–2265. pmlr, 2015. Song, J., Vahdat, A., Mardani, M., and Kautz, J. Pseudoinverse-guided diffusion models for inverse prob-lems. In ICLR , 2023. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-mon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. ICLR , 2021. Stark, H., Jing, B., Wang, C., Corso, G., Berger, B., Barzi-lay, R., and Jaakkola, T. Dirichlet flow matching with applications to dna sequence design. In ICML , 2024. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. NeurIPS ,2020. Sun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. Score-based continuous-time discrete diffusion models. In ICLR ,2023. Taskiran, I. I., Spanier, K. I., Dickm ¨anken, H., Kempynck, N., Pan ˇc´ıkov ´a, A., Ek s¸ i, E. C., Hulselmans, G., Ismail, J. N., Theunis, K., Vandepoel, R., et al. Cell-type-directed design of synthetic enhancers. Nature , 626(7997):212– 220, 2024. Uehara, M., Zhao, Y., Wang, C., Li, X., Regev, A., Levine, S., and Biancalani, T. Inference-time alignment in diffu-sion models with reward-guided generation: Tutorial and review. arXiv preprint arXiv:2501.09685 , 2025. Vono, M., Dobigeon, N., and Chainais, P. Split-and-augmented gibbs sampler—application to large-scale in-ference problems. IEEE Transactions on Signal Process-ing , 67(6):1648–1661, 2019. Wang, C., Uehara, M., He, Y., Wang, A., Lal, A., Jaakkola, T., Levine, S., Regev, A., Biancalani, T., et al. Fine-tuning discrete diffusion models via reward optimization with applications to dna and protein design. In ICLR , 2025. Weininger, D. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences , 28(1):31–36, 1988. Wu, L., Trippe, B., Naesseth, C., Blei, D., and Cunning-ham, J. P. Practical and asymptotically exact conditional sampling in diffusion models. NeurIPS , 2023. Yoon, T., Min, Y., Yeo, K., and Sung, M. Ψ-sampler: Initial particle sampling for smc-based inference-time reward alignment in score models. NeurIPS , 2025. Yu, J., Wang, Y., Zhao, C., Ghanem, B., and Zhang, J. Free-dom: Training-free energy-guided conditional diffusion model. In ICCV , 2023. Zekri, O. and Boull ´e, N. Fine-tuning discrete diffusion models with policy gradient methods. NeurIPS , 2025. Zhao, Y., Shi, J., Chen, F., Druckmann, S., Mackey, L., and Linderman, S. Informed correctors for discrete diffusion models. NeurIPS , 2025. 10 Clean-Sample Markov Chain Sampler 

## Appendix A. Metropolis-Hastings Algorithm 

In this section, we present a brief overview of the Metropolis-Hastings (MH) algorithm. For more details, we refer the reader to chapter 12 section 2 of Murphy (2023). The MH algorithm constructs a Markov chain which converges to a target distribution p⋆(x). In order to do so, a proposal distribution q(·| x) proposes to move from the current state x to a new state x′ ∼ q(·| x). After proposing the new state x′,MH decides whether to accept or reject the new state with the acceptance probability 

A(x′|x) = min (1 , α ) , α = p⋆(x′)q(x|x′)

p⋆(x)q(x′|x) .

Intuitively, the accept-reject step is necessary as the proposal distribution may not match the target distribution. Samples closer to the target distribution are accepted with higher probability whereas samples further from the target distribution are rejected. The MH algorithm is summarized in Alg. 2. The Markov chain constructed by the MH algorithm has the following transition matrix: 

p(x′|x) = 

(

q(x′|x)A(x′|x) if x′̸ = xq(x|x) + P  

> x′̸=x

q(x′|x)(1 − A(x′|x)) otherwise (5) 

Algorithm 2 Metropolis-Hastings Algorithm (MH) 

1: for k = 1 , . . . , K do 

2: ˜x(k+1) ∼ q(·| x(k )

3: α ← p⋆(x′)q(x|x′)

> p⋆(x)q(x′|x)

4: A ← min(1 , α )

5: ρ ∼ U (0 , 1) 

6: x(k+1) ← ˜x(k+1) if ρ < A else x(k)

7: end for 

8: {x(1) , . . . , x (K)}

Theorem A.1 (Theorem 12.2.1 from (Murphy, 2023)) . If the transition matrix defined by Eq. 5 defined by the MH algorithm is ergodic and irreducible, p⋆ is its unique limiting distribution. Proof. Consider two states x′ and x. A Markov chain is said to satisfy the detailed balance equation if the following holds: 

p⋆(x)p(x′|x) = p⋆(x′)p(x|x′). (6) It is known that if a Markov chain satisfies the detailed balance equation, then p⋆ is its stationary distribution (Theorem 2.6.3 from Murphy (2023)). To show that p⋆ is the unique limiting distribution of the Markov chain defined by Eq. 5, it suffices to show that it satisfies the detailed balance condition in Eq. 6. Without loss of generality, assume p⋆(x)q(x′|x) ≥ p⋆(x′)q(x|x′). Then, α = p⋆(x′)q(x|x′) 

> p⋆(x)q(x′|x)

< 1 and thus A(x′|x) = α.Similarly, by switching the arguments, A(x|x′) = 1 .To move from x to x′, x′ must be proposed and accepted. Hence, 

p(x′|x) = q(x′|x)A(x′|x) from Eq. 5 

= p⋆(x′)q(x|x′)

p⋆(x)

11 Clean-Sample Markov Chain Sampler 

It suffices to show that q(x|x′) = p(x|x′):

q(x|x′) = q(x|x′)A(x|x′) ∵ A(x|x′) = 1 = p(x|x′) from Eq. 5 Since the MH Markov chain satisfies the detailed balance equation, p⋆ is its stationary distribution. 

## B. Derivation of the Acceptance Probability 

Recall from Sec. 4 that the proposal distribution q(·| x0) is defined in Eq. 3 as: 

q(x′

> 0

|x0) := pt(xt|x0)p(x′

> 0

|xt) (7) We calculate α which is used to compute the acceptance probability A. First, we draw and fix a noisy sample xt ∼ pt(·| x0).Once xt has been drawn, α simplifies to 

α = exp 

 r(x′

> 0

) − r(x0)

β

 p(x′

> 0

)q(x0|x′

> 0

)

p(x0)q(x′

> 0

|x0)= exp 

 r(x′

> 0

) − r(x0)

β

 p(x′

> 0

)p(xt|x′

> 0

)p(x0|xt)

p(x0)p(xt|x0)p(x′

> 0

|xt)= exp 

 r(x′

> 0

) − r(x0)

β

 p(x′

> 0

)p(xt|x′

> 0

)p(x0|xt, x ′

> 0

)

p(x0)p(xt|x0)p(x′

> 0

|xt, x 0)= exp 

 r(x′

> 0

) − r(x0)

β

 p(x0, x t, x ′

> 0

)

p(x′

> 0

, x t, x 0)= exp 

 r(x′

> 0

) − r(x0)

β



,

where the third line follows from the conditional independence of x0 and x′ 

> 0

given xt due to the Markov property: 

p(x0|xt) = p(x0|xt, x ′

> 0

).Although it may appear that this Markov chain is time-inhomogeneous since t ∼ U (tlo , t hi ) and xt ∼ p(·| x0) are resam-pled at each step, it is secretly time-homogeneous . To see this, we simplify the proposal distribution to an equivalent time-homogeneous proposal distribution. The probability of proposing x′ 

> 0

given that we are currently at state x0 is 

pt(xt|x0)p(x′

> 0

|xt) with probability 1 

> thi −tlo

. To obtain the probability q(x′

> 0

|x0), we simply condition the proposal probability on t and xt and integrate over all possible t and xt:

q(x′

> 0

|x0) = 

Z thi 

> tlo

Z

> Xt

pt(xt|x0)p(x′

> 0

|xt)f (t)dx tdt 

= Et∼U (tlo ,t hi )Ext∼pt(·| x0)[p(x′

> 0

|xt)] ,

which is time-homogeneous. Thus, we still have an equivalent time-homogeneous Markov chain and standard convergence proofs in App. A apply. 

## C. Experiment Details 

C.1. Pretrained Models 

We provide details on the training setup and hyperparameters of each diffusion model on each dataset in Tab. 6. For QM9 and ZINC250K, we use the transformer architecture for the diffusion, whereas for MPRA we use the CNN architecture, following Stark et al. (2024) and Wang et al. (2025). Note that the SEDD-U model pretrained on MPRA is taken from the publicly available checkpoint provided by Chu et al. (2025). All models except for SEDD-U was trained using Adam (Adam et al., 2014) while SEDD-U was trained using AdamW (Loshchilov & Hutter, 2019). 12 Clean-Sample Markov Chain Sampler                                                              

> MDM, USM, and SEDD-M SEDD-U QM9 ZINC250K MPRA QM9 ZINC250K Train steps 25,000 50,000 131,500 40,000 100,000 Context size 32 74 200 32 74 Batch size 1024 384 512 512 512 LR 3e−43e−42e−33e−43e−4
> Optim. ADAM ADAM ADAM ADAM WADAM W(0.9, 0.999) (0.9, 0.999) (0.9, 0.999) (0.9, 0.999) (0.9, 0.999) LR sched. Constant Warmup Constant Warmup Cosine Decay Constant Warmup Constant Warmup --3e−6min. --LR warmup steps 2,500 2,500 3,000 2,500 2,500 GPU count 22284GPU type RTX3090 RTX3090 RTX3090 RTX3090 RTX3090
> Table 6. Training setup and Hyperparameters. Training setup and hyperparameters on QM9, ZINC250K, and MPRA using MDM (Sahoo et al., 2024), USM (Schiff et al., 2025), SEDD-U (Lou et al., 2024), and SEDD-M (Lou et al., 2024).

C.2. Reward-Guided Sampling 

We provide experimental details on each evaluation setup. We sample 1024 molecules for QM9 and ZINC250K, and 640 DNA sequences for MPRA. We fix 32 denoising steps for QM9, 74 denoising steps for ZINC250K, and 128 denoising steps for MPRA. For CSMC-B, we use a batch size of 8 for QM9 and ZINC250K, and a batch size of 4 for MPRA. For experiments on the QM9 (Ramakrishnan et al., 2014) and ZINC250K (Irwin et al., 2012) datasets, we fix the total diffusion model NFE per sample to be 1024. For MPRA (Gosai et al., 2023), we fix the NFE as 1000 as done by Chu et al. (2025). Since one run of CSMC generates multiple samples, we draw S samples from the resulting chain by discarding the first half of the chain (burn-in) and taking S equally-spaced samples from the latter half. Since S samples are generated, we scale the NFE by S for CSMC. This scaling highlights one of the key benefits of CSMC: after the initial burn-in period, samples can be generated quickly using a few steps. For all experiments, we fix β = 0 .02 . To obtain an x0 prediction from SEDD, we run sampling using one step to jump to time t = 1 (clean sample). Hyperparameters used by our method is shown in Tab. 7. Initial denoising steps refer to the number of steps used to obtain the initial clean sample x(1) 0 whereas denoising steps M refer to the number of steps used during the M -step reverse process in every MH iteration. Note that M is much smaller than the initial denoising steps.                         

> QM9 ZINC250K MPRA Samples per Iteration S128 128 64 MH Iterations K26208 26199 6390 Initial Denoising Steps 32 74 100 Denoising Steps M5510
> tl0.2 0.2 0.2
> th0.5 0.5 0.7
> Table 7. Hyperparameters for CSMC.

## D. Additional Experimental Results 

D.1. Diversity 

We compute the diversity as follows: • For molecule tasks (QM9 and ZINC250K), we compute the mean pairwise Tanimoto similarity based on the Morgan2 fingerprint, and subtract it from 1. • For the DNA task (MPRA), we subtract the mean pairwise cosine similarity of the one-hot encodings from 1. 13 Clean-Sample Markov Chain Sampler 

The results are shown in Tab. 8. BoN and CSMC have a slight decrease in diversity while SMC suffers from significant degradation in diversity. This can be attributed to samples clustering around the high-reward modes. CSMC consistently achieves a diversity score of over 0.8 on molecule tasks which suggests that the samples are indeed diverse, with an average Tanimoto similarity of less than 0.2. On DNA tasks, the cosine similarity between the sequences is less than 0.3, indicating that the generated sequences are diverse. 

QM9 ZINC250K MPRA 

QED Rings SA QED Rings SA HepG2 MDM Pretrained 0.922 0.922 0.922 0.910 0.910 0.910 0.749 BoN 0.904 0.884 0.894 0.876 0.874 0.876 0.749 SMC 0.834 0.916 0.802 0.929 0.918 0.876 0.747 SVDD 0.920 0.920 0.920 0.900 0.896 0.900 0.747 

CSMC 0.885 0.804 0.862 0.874 0.797 0.874 0.742 

CSMC-B 0.891 0.854 0.868 0.872 0.877 0.839 0.729 USM Pretrained 0.921 0.921 0.921 0.879 0.879 0.879 0.748 BoN 0.895 0.890 0.912 0.862 0.866 0.841 0.749 SMC 0.923 0.920 0.927 0.874 0.875 0.874 0.748 SVDD 0.922 0.918 0.921 0.875 0.875 0.875 0.748 

CSMC 0.880 0.827 0.868 0.856 0.837 0.841 0.734 

CSMC-B 0.881 0.856 0.881 0.853 0.861 0.838 0.732 SEDD-M Pretrained 0.926 0.926 0.926 0.905 0.905 0.905 0.748 BoN 0.903 0.889 0.926 0.874 0.874 0.877 0.749 SMC 0.925 0.914 0.927 0.902 0.902 0.902 0.748 SVDD 0.928 0.928 0.928 0.907 0.907 0.907 0.748 

CSMC 0.885 0.847 0.890 0.865 0.880 0.837 0.749 

CSMC-B 0.898 0.858 0.898 0.887 0.888 0.870 0.732 SEDD-U Pretrained 0.922 0.922 0.922 0.879 0.879 0.879 0.631 BoN 0.900 0.900 0.915 0.866 0.867 0.851 0.661 SMC 0.901 0.905 0.910 0.872 0.873 0.866 0.661 SVDD 0.906 0.907 0.914 0.873 0.877 0.870 0.666 SGDD 0.906 0.913 0.908 0.868 0.866 0.859 0.650 

CSMC 0.880 0.864 0.863 0.855 0.868 0.825 0.709 

CSMC-B 0.869 0.865 0.848 0.859 0.863 0.843 0.662 

Table 8. Diversity metrics for each method and reward. Higher is better. 

D.2. Wall-Clock Time Comparisons 

We provide additional wall-clock time comparisons in Tab. 10. We also provide a comparison of the wall-clock times for CSMC and CSMC-B in Tab. 9, showing that CSMC-B provides significantly accelerated sampling compared to CSMC.          

> CSMC CSMC-B Time (s) 3029 334 Batch Size 18NFE 1024 1024 Reward 0.916 0.917

Table 9. Wall-clock time comparisons. Wall-clock time measured CSMC applied to USM guided by ZINC250K QED reward. Batching significantly improves the speed of CSMC. 

D.3. Autocorrelation Plots and Acceptance Rate 

We provide autocorrelation plots for ZINC250K MDLM and USM in Fig. 5 to further investigate the burn-in time. The autocorrelation function quickly vanishes to zero within the first 2000 iterations and remains small until the end, indicating that the chain converges quickly. Since these metrics are only heuristics for measuring convergence, we conservatively discard the first half of the chain. However, the autocorrelation plots suggest that it may be possible to burn fewer samples. We provide acceptance rates for CSMC with MDLM and USM in Tab. 11. Although the acceptance rate for MDLM is quite low due to MDLM often generating invalid molecules, the acceptance rate is still acceptable as we draw 128 samples from 13k samples after the burn-in period. On the other hand, USM has much higher acceptance rates which can be attributed to its higher validity rate due to the ability to correct errors from previous diffusion steps 14 Clean-Sample Markov Chain Sampler                                                                             

> ZINC250K QED Rings SA CSMC BoN BoN SMC SVDD CSMC BoN BoN SMC SVDD CSMC BoN BoN SMC SVDD Time (s) 334 334 359 359 264 321 331 359 320 276 337 328 300 341 279 Batch Size 878 814 886 878 814 886 878 814 886 NFE 1024 5750 1024 1024 65536 1024 5750 1024 1024 65536 1024 5750 1024 1024 65536 Reward 0.917 0.934 0.9133 0.766 0.719 5.315 4.945 4.312 2.630 2.607 0.912 0.919 0.892 0.792 0.755

Table 10. USM ZINC250K rewards under matched wall-clock time. Rewards and time are computed from 128 drawn samples. CSMC-B significantly outperforms all other baselines in ring count while being comparable with BoN in QED and SA. 0 5000 10000 15000 20000 

> Iteration
> 0.0
> 0.5
> 1.0
> Autocorrelation

## QED     

> 05000 10000 15000 20000
> Iteration

## Rings     

> 05000 10000 15000 20000
> Iteration

## SA 

> MDM
> USM

## ZINC250K 

Figure 5. Autocorrelation plots for ZINC250K MDM and USM. 

D.4. NFE Comparison 

In this section, we compare the performance of each method across various NFEs for QM9 and ZINC250K. We use NFEs 

∈ { 512 , 1024 , 2048 , 4096 } for molecule tasks and NFEs ∈ { 500 , 1000 , 2000 , 4000 } for MPRA. The results are shown in Fig. 6. As shown in the figure, CSMC consistently and significantly outperforms all other methods in ring count and HepG2 activity across all diffusion models and datasets. CSMC also outperforms all other methods in SA except for BoN when using USM on ZINC250K, and outperforms all other methods in QED except for BoN when using USM. We note that for the hardest reward ring count where the high-reward samples lie in extremely low density regions, CSMC consistently outperforms all other methods by a large margin. 

D.5. Time Parameters tlo and thi 

We provide additional analysis on the time parameters tlo and thi . Larger tlo and thi lead to increased exploration as the samples from the proposal distribution become more uncorrelated due to the larger noise scale. Smaller values lead to increased exploitation as samples are more correlated due to the smaller noise scale. To test the sensitivity of CSMC with respect to these time parameters, we fix tlo = 0 .2 and vary thi ∈ [0 .2, 0.7] . We plot the reward and diversity for ZINC250K USM samples with respect to these parameters. We also include a plot where we fix 

thi = 0 .8 and vary tlo ∈ [0 .3, 0.8] . The results are shown in Fig. 7. As shown in the figure, our method is not sensitive to the time parameters in general. However, For ring count, the reward decreases as tlo increases. Intuitively, as tlo increases, each CSMC step changes the current x0 sample significantly, resulting in greater exploration but less exploitation. For simpler rewards such as QED and SA, this does not significantly impact the average reward. However, for ring count where high-reward samples lie in extremely low density regions of the pretrained model’s learned distribution (refer to Fig. 2), it is necessary to choose smaller times to exploit and explore around the current x0.

## E. Qualitative Results 

Qualitative results are shown in Fig. 8. 15 Clean-Sample Markov Chain Sampler 500 1000 1500 2000 2500 3000 3500 4000 

NFE 

0.46 

0.48 

0.50 

0.52 

0.54 

0.56 

0.58 

0.60 

0.62 

> Reward

QED 

500 1000 1500 2000 2500 3000 3500 4000 

NFE 

2

4

6

8

10 

Rings 

500 1000 1500 2000 2500 3000 3500 4000 

NFE 

0.55 

0.60 

0.65 

0.70 

0.75 

0.80 

0.85 

0.90 

SA 

Pretrained 

BoN 

SMC 

SVDD 

CSMC 

## QM9 - MDM 500 1000 1500 2000 2500 3000 3500 4000 

NFE 

0.450 

0.475 

0.500 

0.525 

0.550 

0.575 

0.600 

0.625 

> Reward

QED 

500 1000 1500 2000 2500 3000 3500 4000 

NFE 

2

4

6

8

10 

Rings 

500 1000 1500 2000 2500 3000 3500 4000 

NFE 

0.60 

0.65 

0.70 

0.75 

0.80 

0.85 

0.90 

SA 

Pretrained 

BoN 

SMC 

SVDD 

CSMC 

## QM9 - USM 500 1000 1500 2000 2500 3000 3500 4000 

NFE 

0.65 

0.70 

0.75 

0.80 

0.85 

0.90 

> Reward

QED 

500 1000 1500 2000 2500 3000 3500 4000 

NFE 

2

3

4

5

6

7

8

Rings 

500 1000 1500 2000 2500 3000 3500 4000 

NFE 

0.70 

0.75 

0.80 

0.85 

0.90 

SA 

Pretrained 

BoN 

SMC 

SVDD 

CSMC 

## ZINC250K - MDM 500 1000 1500 2000 2500 3000 3500 4000 

NFE 

0.725 

0.750 

0.775 

0.800 

0.825 

0.850 

0.875 

0.900 

0.925 

> Reward

QED 

500 1000 1500 2000 2500 3000 3500 4000 

NFE 

4

6

8

10 

Rings 

500 1000 1500 2000 2500 3000 3500 4000 

NFE 

0.78 

0.80 

0.82 

0.84 

0.86 

0.88 

0.90 

0.92 

SA 

Pretrained 

BoN 

SMC 

SVDD 

CSMC 

## ZINC250K - USM 500 1000 1500 2000 2500 3000 3500 4000 

NFE 

> 1
> 2
> 3
> 4
> 5
> 6
> Reward

HepG2 

Pretrained 

BoN 

SMC 

SVDD 

CSMC 

MPRA - MDM 500 1000 1500 2000 2500 3000 3500 4000 

NFE 

> 1
> 2
> 3
> 4
> 5
> 6
> 7
> Reward

HepG2 

Pretrained 

BoN 

SMC 

SVDD 

CSMC 

MPRA - USM 

Figure 6. Performance across various NFEs. We run each method on molecule generation using NFEs ∈ { 512 , 1024 , 2048 , 4096 } for molecule tasks and NFE ∈ { 500 , 1000 , 2000 , 4000 } for MPRA. 16 Clean-Sample Markov Chain Sampler 0.3 0.4 0.5 0.6 0.7 0.8

thi 

0.0

0.2

0.4

0.6

0.8

1.0Reward 

tlo = 0.2 

0.2 0.3 0.4 0.5 0.6 0.7

tlo 

0.0

0.2

0.4

0.6

0.8

1.0Reward 

thi = 0.8 

0.00.20.40.60.81.0Diversity 

0.00.20.40.60.81.0Diversity 

# QED 0.3 0.4 0.5 0.6 0.7 0.8

thi 

6

8

10 

12 

14 Reward 

tlo = 0.2 

0.2 0.3 0.4 0.5 0.6 0.7

tlo 

6

8

10 

12 

14 Reward 

thi = 0.8 

0.00.20.40.60.81.0Diversity 

0.00.20.40.60.81.0Diversity 

# RING 0.3 0.4 0.5 0.6 0.7 0.8

thi 

0.0

0.2

0.4

0.6

0.8

1.0Reward 

tlo = 0.2 

0.2 0.3 0.4 0.5 0.6 0.7

tlo 

0.0

0.2

0.4

0.6

0.8

1.0Reward 

thi = 0.8 

0.00.20.40.60.81.0Diversity 

0.00.20.40.60.81.0Diversity 

# SA 

Figure 7. Analysis of time parameters. We plot the reward and diversity for ZINC250K USM samples with respect to (1) fixing tlo = 0 .2

and vary thi ∈ [0 .3, 0.8] , and (2) fixing thi = 0 .8 and vary tlo ∈ [0 .2, 0.7] .

17 Clean-Sample Markov Chain Sampler 

QM9 ZINC250K MPRA QED Rings SA QED Rings SA HepG2 MDM 0.032 0.002 0.009 0.009 0.003 0.011 0.029 USM 0.510 0.330 0.346 0.725 0.597 0.711 0.038 

Table 11. Acceptance rates for MDM and USM. Pretrained CSMC QM9 

QED: 0.378 Rings: 1 SA: 0.501 QED: 0.578 Rings: 9 SA: 0.920 

ZINC250K 

QED: 0.610 Rings: 2 SA: 0.802 QED: 0.934 Rings: 7 SA: 0.915 

Figure 8. Qualitative results. Molecules randomly sampled from discrete diffusion models pretrained on QM9 (Ramakrishnan et al., 2014) and ZINC250K (Irwin et al., 2012). CSMC generates molecules with high rewards as shown on the right. 

18