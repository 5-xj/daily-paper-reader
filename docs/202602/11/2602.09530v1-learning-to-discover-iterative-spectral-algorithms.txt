Title: Learning to Discover Iterative Spectral Algorithms

URL Source: https://arxiv.org/pdf/2602.09530v1

Published Time: Wed, 11 Feb 2026 01:39:48 GMT

Number of Pages: 32

Markdown Content:
# Learning to Discover Iterative Spectral Algorithms 

## Zihang Liu ∗1,2, Oleg Balabanov ∗1,2,4, Yaoqing Yang 3, and Michael W. Mahoney 1,2,41International Computer Science Institute 

> 2

## University of California, Berkeley 

> 3

## Dartmouth College 

> 4

## Lawrence Berkeley National Laboratory 

{zihang.liu, obalaban }@berkeley.edu yaoqing.yang@dartmouth.edu mmahoney@stat.berkeley.edu 

Abstract 

We introduce AutoSpec , a neural network framework for discovering iterative spectral algorithms for large-scale numerical linear algebra and numerical optimization. Our self-supervised models adapt to input operators using coarse spectral information (e.g., eigenvalue estimates and residual norms), and they predict recurrence coefficients for computing or applying a matrix polynomial tailored to a downstream task. The effectiveness of AutoSpec relies on three ingredients: an architecture whose inference pass implements short, executable numerical linear algebra recurrences; efficient training on small synthetic problems with transfer to large-scale real-world operators; and task-defined objectives that enforce the desired approximation or preconditioning behavior across the range of spectral profiles represented in the training set. We apply AutoSpec to discovering algorithms for representative numerical linear algebra tasks: accelerating matrix-function approximation; accelerating sparse linear solvers; and spectral filtering/preconditioning for eigenvalue computations. On real-world matrices, the learned procedures deliver orders-of-magnitude improvements in accuracy and/or reductions in iteration count, relative to basic baselines. We also find clear connections to classical theory: the induced polynomials often exhibit near-equiripple, near-minimax behavior characteristic of Chebyshev polynomials. 

# 1 Introduction 

Machine learning (ML) has shown growing potential to scale scientific discovery by automating parts of the algorithm design pipeline (Ellis et al., 2021; Fawzi et al., 2022; Mankowitz et al., 2023; Real et al., 2020; Yang et al., 2026). However, many successful discovery frameworks search over discrete hypothesis classes (e.g., programs or pseudocode), which does not transfer cleanly to numerical linear algebra (NLA) and numerical optimization, where effective methods are typically continuous objects that must adapt to operator-specific (often spectral) structure. In this paper, we propose AutoSpec , a neural network framework for discovering a broad class of iterative spectral methods for NLA and numerical optimization. Our models learn mappings from spectral probes—e.g., a small set of coarse eigenvalue approximations with associated residual norms—to iteration coefficients that define an operator polynomial (or, more generally, an operator function) tailored to a downstream task. At inference time, given a new operator, we (i) extract a probe using a short warm-start eigensolver run, (ii) feed this probe to the trained model to predict iteration coefficients, and (iii) execute the resulting iterative process. This ML workflow mirrors standard NLA practice, where inexpensive spectral information enables methods to adaptively exploit and shape the operator’s spectrum. 

> *These authors contributed equally.

1

> arXiv:2602.09530v1 [cs.LG] 10 Feb 2026

Our algorithm discovery approach is fully self-supervised: we train end-to-end by minimizing task-specific objectives derived from standard NLA criteria, using only downstream performance as supervision. As a result, the learned methods are optimized to outperform fixed baselines through task feedback, rather than by imitating existing algorithms. Our design offers three advantages: (i) a unified parameterization spanning a broad class of iterative spectral algorithms, enabling discovery across multiple NLA tasks; (ii) training on small synthetic spectral instances with transfer to large-scale operators, producing a compact and deployable learning engine; and (iii) reliance only on coarse spectral information at inference time, improving applicability when spectral access is limited or computational budgets are tight. Moreover, we observe clear connections between the discovered algorithms and classical polynomial approximation theory. For instance, the learned polynomials, optimized for a task-defined objective based on downstream performance, often exhibit near-minimax, equiripple behavior reminiscent of Chebyshev polynomials. This is consistent with the classical optimality of Chebyshev constructions for minimax approximation of analytic functions on an interval. We apply AutoSpec to discovering matrix-free iterative algorithms 1 for several tasks: solving linear systems; computing eigenvalues; and approximating matrix-function actions. Across a suite of real-world sparse matrices, the learned methods significantly improve accuracy and/or reduce iteration counts, in some cases by orders of magnitude relative to basic baselines. We summarize our main contributions as follows: 

• We propose AutoSpec , a general framework for discovering iterative spectral algorithms with neural networks. Unlike symbolic algorithm discovery methods, our approach learns continuous algorithmic updates suitable for NLA and other related numerical tasks. 

• We introduce a neural network engine for learning the algorithmic components of the framework. The trained neural network learns to construct effective iterative preconditioners and approximations under low computational budgets. Moreover, we identify clear connections to classical theory: the learned algorithms exhibit properties consistent with Chebyshev filters. 

• We apply AutoSpec to three classical NLA tasks, and we show that the learned algorithms can achieve orders-of-magnitude gains in accuracy and/or iteration count over basic baselines. 

# 2 Related Works 

Algorithm Discovery with ML. ML has become a key driver of scientific discovery by enabling automated algorithm design. This problem is naturally framed as a search over a high-dimensional combinatorial space of candidate operations. Many works adopt discrete program spaces, where symbolic regression remains a dominant paradigm, spanning classical evolutionary methods (Koza, 1994; Schmidt and Lipson, 2009) and neural-guided approaches (Hayes et al., 2025; Kim et al., 2020; Lample and Charton, 2019; Mundhenk et al., 2021; Petersen et al., 2019; Udrescu and Tegmark, 2020). Moreover, by formulating the discovery process as a sequential decision-making task, reinforcement learning (RL) becomes a useful method for discovering more complex algorithms (Fawzi et al., 2022; Mankowitz et al., 2023; Oh et al., 2020, 2025). More recently, researchers have used the reasoning capabilities of large language models (LLMs) to discover novel algorithms and advanced math problems (Chervonyi et al., 2025; Novikov et al., 2025; Romera-Paredes et al., 2024). In this work, our proposed framework aims to discover algorithms in NLA and numerical optimiza-tion. For optimization, recent works have adopted deep unfolding (Takabe and Wadayama, 2022), meta-learning (Andrychowicz et al., 2016; Li et al., 2016; Ravi and Larochelle, 2017; Wichrowska et al., 2017) and RL (Bello et al., 2017; Chen et al., 2023) to discover optimization algorithms. In terms of NLA applications, researchers have been using ML methods to improve existing NLA algorithms or on specific tasks (H¨ ausner et al., 2023; Lerer et al., 2024; Li et al., 2023), as a surrogate for existing methods (Kaneda et al., 2023; Luo et al., 2024), or for learning more general matrix functions (Yang et al., 2026). Our work differs from previous works, as we propose a framework with a parameterization that covers a wide class of iterative algorithms, which not only can rediscover existing methods, but also can learn novel NLA methods. In addition, our 

> 1Using an operator only through its action on vectors (matvecs).

2Neural Network Engine 

> Recurrence
> Operator
> Spectral Probe
> Outer Iteration
> Input Vectors
> Learned Algorithm

(a) Inference in NLA tasks. Embedding Layer  

> Backbone Model
> Neural Network Engine
> Backprop
> Forward Pass
> Recurrence
> Algorithm Output
> Operator
> Spectral Probe
> Input Vectors
> Learned Algorithm

(b) Algorithm structure. 

Figure 1: AutoSpec framework. (a) A trained neural network engine, paired with an executable recurrence, defines a discovered numerical algorithm for downstream NLA tasks. (b) An end-to-end differentiable algorithmic structure: given operator X, a spectral probe ( bλ, br), consisting of coarse eigenvalue estimates and residual norms, is extracted and fed to the neural engine to produce the coefficients of a degree-d polynomial 

P (·). The polynomial is implemented in a matrix-free manner via a short recurrence: starting from V0 = z,we iterate Vk+1 = Mk(X)Vk, where each Mk is parameterized by the engine. The terminal state returns the action Vd+1 = P (X)z of the polynomial to inputs z, which allows self-supervised training by backpropagating task-defined NLA losses on Vd+1 .

AutoSpec framework adopts a continuous search space, which is suitable for NLA tasks, where effective methods are typically continuous objects that must adapt to operator-specific structure. 

Spectral Approximation and Preconditioning Methods in NLA. Polynomial accelerations and approximations are among the earliest and most widely used NLA tools for iterative solution of large, sparse linear systems and related tasks. A common strategy is to build low-degree polynomial transforms guided by coarse spectral bounds, that damp slowly converging error components and accelerate convergence (Axelsson, 1996; Saad, 2003; Varga, 2000; Young, 2014). Krylov subspace methods provide a complementary paradigm and can be viewed as constructing analogous polynomial filters implicitly and adaptively, by optimizing residuals over progressively enriched subspaces (Greenbaum, 1997; Hestenes et al., 1952; Liesen and Strakos, 2013; Saad and Schultz, 1986; Saad, 2011). On modern architectures, Krylov performance is increasingly constrained by communication/synchronization (e.g., global reductions), renewing interest in polynomial preconditioners and filtering (Banerjee et al., 2016; Saad, 2011; Zhou and Saad, 2007). Extensive work develops polynomial preconditioners and smoothers via truncated series, minimax, or least-squares designs, as matrix-free surrogates for inverses or spectrum-shaping operators, used both standalone and within multilevel solvers (Ashby, 1991; Ashby et al., 1992; Benzi, 2002; Briggs et al., 2000; Johnson et al., 1983; Trottenberg et al., 2000). Beyond first-order methods, second-order matrix iterations such as Newton methods yield rapidly convergent approximations for inverses, inverse roots, and matrix sign functions, with kernels dominated by matrix-matrix products that map well to modern parallel hardware (Higham, 2008). These loops now appear in large-scale optimizers and training systems (e.g., Shampoo; Muon; and variants such as PolarExpress) (Ahn et al., 2025; Amsel et al., 2025; Grishina et al., 2025; Gupta et al., 2018; Jordan et al., 2024). Related iterative matrix (inverse) square-root primitives also underpin second-order vision layers and whitening/decorrelated normalization (Huang et al., 2018; Li et al., 2017; Song et al., 2021, 2023). However, these second-order iterations remain far less applicable in sparse and matrix-free regimes, where matvec-based kernels are essential. Finally, when shifted solves (or approximate shift-and-invert actions) are available, rational approximation extends these designs via rational filters, rational Krylov/subspace methods, and adaptive rational fitting such as AAA (Druskin and Knizhnerman, 1998; G¨ uttel, 2013; Higham, 2008; Nakatsukasa et al., 2018). 33 AutoSpec Framework 

In this section, we introduce the AutoSpec framework: an end-to-end differentiable approach built around a unified representation of iterative spectral algorithms, with modular components that can be learned by neural networks. 

## 3.1 Framework Setup 

For any diagonalizable matrix X ∈ Rn×n, we define a spectral probe (bλ(X), br(X)), consisting of a small set of approximate eigenvalues bλ from a task-relevant portion of the spectrum (this could be the top eigenvalues, e.g., if one is interested in low-rank approximation, but for many applications it is not), optionally augmented with auxiliary quantities such as residual norms br. This mirrors common NLA workflows, where inexpensive spectral estimates (e.g., Ritz values from short Lanczos or Arnoldi runs) are used to guide the construction of preconditioners and approximations. In particular, our models in Section 5 can take ( bλ, br) to consist of a few extreme eigenvalue estimates and their associated residuals, obtained via a cheap eigensolver warm-start. Then, we train a neural network engine ,(bλ, br) 7 → P (·), (1) that maps an input spectral probe to an operator polynomial (or more generally, an operator function) designed for downstream tasks. The designated objectives may involve a single criterion such as minimizing matrix-function approximation error ∥P (X) − f (X)∥, or a combination of goals, such as minimizing the condition number of the preconditioned operator P (X)X (a proxy for solver convergence speed) and that of the preconditioner P (X) itself (a proxy for numerical robustness). We parametrize the polynomials P (·) to admit an executable short recurrence that applies the matrix polynomial P (X) efficiently to input vectors and matrices. In this setting, the neural engine predicts the recurrence coefficients and thereby can be realized to govern the convergence (and numerical stability) of the iterative process. Here is the important point: the combination of a neural network engine and an executable recurrence defines a learned algorithm. This algorithm can then be deployed for NLA downstream tasks, such as preconditioning linear solver iterations. See Figure 1 for an illustration. For robust training and for efficient evaluation at inference time, we can represent P (X) implicitly via a state recurrence: 

Vk+1 = Mk(Vk, X), 0 ≤ k ≤ d, (2) The initialization for matrix actions is V0 = z so that Vd+1 = P (X)z; when P (X) is needed to be formed explicitly (e.g., in small-scale training or diagnostics) we take V0 = I, yielding Vd+1 = P (X). The operators 

Mk(·, ·) are specified modularly to realize short NLA-style recurrences. 

Modular linear transitions. We primarily focus on transitions that are linear in the state, 

Mk(Vk, X) := Mk(X) Vk, (3) where Mk(X) is assembled from blocks M(i,j ) 

> k

∈ Rn×n, each given by a low-degree matrix polynomial 

M (i,j ) 

> k

(X). The final transition Md(X) is constrained to have a single block-row, so the terminal state extracts the desired polynomial output. With this parameterization, learning the neural engine (1) reduces to learning maps ( bλ, br) 7 → M (i,j ) 

> k

(·) that specify the block polynomials. The same modular construction also supports richer transition classes; see Appendix A.1 for extensions to higher-order (state-polynomial) and rational recurrences. 4Matrix-free regime. Our main target is the large-scale sparse setting, where constructing or storing P (X)is infeasible. Accordingly, we seek recurrences that compute the action z 7 → P (X)z using only a small number of applications of X. Under (3), the action is obtained by composing the transitions, 

P (X)z = Md(X) Md−1(X) . . . M0(X) z,

without ever forming P (X) (or any dense n × n block), provided that applying each block polynomial 

M (i,j ) 

> k

(X) is implemented via a short sequence of X matvecs. To control per-step cost and improve robustness in sparse, matrix-free regimes, we can further restrict the transition matrices to use at most one application of X.For many downstream tasks such as preconditioning, the polynomial action is only required up to a scale factor. The sequential state-transition representation allows normalization of intermediate states without affecting the result up to scaling, enabling control of numerical growth and improving stability during training and inference (see Section 4.3). 

## 3.2 State Transition View 

In our empirical evaluation, we chose linear transitions with the following block parameterization: 

M0 = I I XTT Md = γdI ηdI αdI + βdX

Mk =



I 0 ρkI0 0 I

γkI ηkI αkI + βkX

 1 ≤ k < d. (4) Under (4), the neural engine outputs scalar coefficient sequences {ρk, γ k, η k, α k, β k}.To interpret the blocks in (4), write the state as three n × n blocks Vk = Ak Bk Ck

T . Then for 1 ≤ k < d , the update Vk+1 = MkVk is equivalent to (assuming that X is symmetric) 

Ak+1 = Ak + ρk Ck,

Bk+1 = Ck,

Ck+1 = γkAk + ηkBk + ( αkI + βkX) Ck.

(5) Thus, Bk acts as a one-step delay storing the previous polynomial state, Ck stores the current polynomial state, and Ak is an accumulator that enables learned linear combinations of intermediate states. The propositions below isolate two useful special cases: (i) a direct affine three-term polynomial recurrence; and (ii) basis generation followed by a learned expansion. Together, they motivate (4) as a compact hypothesis class that subsumes many classical polynomial-approximation templates, appropriate for use in an ML workflow. In this view, learning ( bλ, br) 7 → P (·) amounts to selecting and tuning a member of this classical design space from a spectral probe, rather than committing a priori to a fixed polynomial family or coefficient rule. 

Scenario I: Affine Three-term Polynomial Recurrence. Set ρk = 0, so Ak ≡ I. Then Ck follows an affine three-term recursion with constant injection through γkI.

Proposition 3.1 (Affine three-term polynomial recurrence) . Consider (4). If ρk = 0 for all 1 ≤ k ≤ d, then 

Ck satisfies (6) with C0 = I and C1 = X, and for k ≥ 1,

Ck+1 = γkI + ηkCk−1 + αkCk + βkXC k. (6) This regime covers classical polynomial iterations defined by orthogonal-polynomial recurrences (e.g., Chebyshev on a spectral interval) (Saad, 2003; Varga, 2000; Young, 2014), polynomial smoothers/accelerations for linear solvers (e.g., damped Richardson and Chebyshev acceleration) (Axelsson, 1996; Saad, 2003), and matrix-free polynomial filters for eigenvalue computations (Banerjee et al., 2016; Saad, 2011; Zhou and Saad, 2007). Within AutoSpec , optimizing {γk, η k, α k, β k} allows the learned method to recover, refine, or interpolate among these designs in a task-adaptive way. 5Scenario II: Basis Generation Plus Learned Expansion. This specialization separates basis generation 

from coefficient selection . Setting γk = 0 for k < d removes the affine injection in (4), so Ck is generated by a homogeneous three-term recurrence. The accumulator Ak then forms a learned linear combination of these basis polynomials via ρk, and the final readout selects the output. 

Proposition 3.2 (Expansion in a learned three-term basis) . Consider (4). Assume γk = 0 for all 1 ≤ k < d 

and assume the readout uses ηd = αd = βd = 0 . Then Ck satisfies (7) with C0 = I, C1 = X, and for 

1 ≤ k < d ,

Ck+1 = ηkCk−1 + αkCk + βkXC k, (7) 

and the output has the expansion form 

Cd = γdI + γdd−1X

> k=1

ρkCk.

This regime matches a common approximation template in scientific computing: generate a numerically stable basis via a three-term recurrence (e.g., Chebyshev/Legendre/Hermite); and then choose expansion weights to fit a target operator function or preconditioner (via minimax or least-squares criteria) (Ashby, 1991; Ashby et al., 1992; Saad, 2003; Varga, 2000). Here, the neural engine learns both components from the spectral probe: {ηk, α k, β k} shape the basis, while {ρk} select the task-adapted expansion, enabling the downstream objective to navigate a broad family of classical approximants within one unified parameterization. 

## 3.3 Learning Objective 

We propose a general learning objective for discovering different NLA algorithms with AutoSpec . For each NLA task, we adopt the metric r representing convergence rate of the iteration, and we evaluate the ratio between the learned algorithm and commonly used baseline methods on a log scale. We denote our learning objective as ρlog :

ρlog = log rmodel 

log rbaseline 

. (8) We choose the ratio in logarithmic scale to alleviate bias towards certain spectral profiles when performing training/evaluation on a large, diverse set of matrices, enforcing desired preconditioning behaviors uniformly on all X in the dataset. Details about the standard metric r and baseline methods for different tasks are described in Section A.2. 

# 4 Neural Network Engine of AutoSpec 

In this section, we introduce the neural network engine of AutoSpec . As shown in Figure 1, the neural network engine learns to construct a set of transition operators M1, . . . , Md for each target matrix X.The neural network engine consists of an embedding layer that generates embeddings for input matrices using approximate eigenspectrum information, and a backbone that produces the transition operators. In Section 4.1, we first introduce the pre-training of the backbone model, which serves as the foundation of 

AutoSpec . Building on this, in Section 4.2, we further introduce the “post-training” of the embedding model, which makes AutoSpec highly applicable in practice, where accurate eigenvalues are expensive to compute. Detailed structures of the neural network engine are described in Appendix B. 

## 4.1 Backbone Model (Pre-training) 

The backbone model mirrors the iterative update formula as in (3) using an unfolded modular structure, where each module performs a state transition using accurate eigenspectrum information. This serves as the foundational component for the algorithm generation of AutoSpec .The backbone model consists of neural network layers that produce transition operators M1, . . . , Md

given eigenspectrum information. For each matrix X with initial state V0 = I, given the generalized structure 6of Mk in (4), the neural network equivalently learns to produce a set of coefficients {ρk, γ k, η k, α k, β k}, which are functions of X. Subsequently, the framework builds transition matrices Mk using the coefficients, and formulate the recurrence Vk+1 = MkVk, that results in the algorithm output P (X) = Vd+1 . As shown in Algorithm 1 (in Appendix B), when training the backbone model, the input to each neural network layer is the spectral probe, and the coefficients {ρk, γ k, η k, α k, β k} are constructed with a fully connected layer with a learnable scaling factor δk.

## 4.2 Embedding Model (Post-training) 

To make AutoSpec applicable in practical scenarios, where accurate spectral information is costly to obtain, we extend our backbone model with an embedding layer that can reason about spectral structure from coarse observations. As shown in Algorithm 2 (in Appendix B), the embedding layer gϕ takes as input a coarse spectral probe, obtained from a small number of iterations of an eigensolver such as subspace iteration. It learns a mapping from coarse spectral estimates to an embedding vector, ( bλ, br) 7 → e, which synthesizes the essential spectral features. During post-training, we freeze the backbone model and train only the embedding layer. 

## 4.3 Training and Evaluation Setup 

Here, we provide neural network training and evaluation strategies. Additional training details are provided in Appendix C and D. 

Training and Evaluation Data. Our training procedure does not follow a conventional in-distribution learning setup based on sampling spectra from a fixed probability distribution. Instead, we train on small synthetic operators specifically designed to exhibit spectral properties (e.g., slow versus fast versus heavy-tailed decay of dominant eigenvalues) representative of given application regimes. At test time, we evaluate on realistic matrices whose spectral profiles are unknown but that share structural properties with the training data. Thus, our evaluation measures generalization across operator instances within a structurally defined spectral class, rather than i.i.d. distributional generalization. By conditioning solely on dimension-agnostic spectral summaries rather than size-dependent features, the model learns a size-independent mapping from spectral input to polynomial coefficients. This enables extrapolation from training operators with n ≤ O (10 3)to real-world matrices with dimensions up to O(10 6), provided they share similar spectral structure. This setup reflects the intended use case in NLA tasks, where algorithms are designed to generalize across broad operator classes characterized by spectral properties rather than fixed input distributions. Details of the training data generation procedure are provided in Appendix C.1. 

Efficient Training on Diagonal Matrices. Training on full matrices can incur significant computational overhead. Observe that if two diagonalizable matrices X and Y share the same spectrum, then P (X) and P (Y)also share the same spectrum. Furthermore, our objective depends only on spectral quantities. Consequently, without loss of generality, we may restrict training to diagonal matrices of the form X = diag (λ). This representation significantly improves training efficiency by avoiding operations on full matrices, especially when realized on fully synthetic data. 

Stable Training with Scale-invariant Update. Learning higher degree algorithms poses the risk of numerical overflows for the iterative update when training the neural network. We note that objectives in NLA tasks are often scale-invariant, such as maximizing eigenvalue gaps or minimizing condition numbers. Under the state-space view of the learned recurrence in Section 3.2, we can see that scaling the states does not change the recurrence relation Vk+1 = MkVk, but it prevents the recurrence from growing exponentially with polynomial degree. Therefore, in training, we can normalize the state by its norm after each recurrence update. This effectively controls the norm of the states and prevents numerical overflows. 7Neural Network Power 50 150 250 350 450      

> Eigs Outer Iterations
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0
> Relative Error

(a) λ5 of SiO2 (k = 10) 50 150 250 350 450       

> Eigs Outer Iterations
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0

(b) λ10 of SiO2 (k = 10) 10 20 30 40 50 60       

> Eigs Outer Iterations
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0

(c) λ10 of thermal2 (k = 20) 10 20 30 40 50 60      

> Eigs Outer Iterations
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0

(d) λ20 of thermal2 (k = 20) 

Figure 2: Convergence of eigenvalue approximations versus eigs outer iterations for (shifted) SiO2 and 

thermal2 , for varying target numbers of eignevalues k and subspace dimension l = 4 k. The NN preconditioner is produced using a spectral probe from 20 eigs warm-start iterations. 

## 4.4 Optimization Objective 

We formalize a unified optimization objective of the neural network model for all NLA tasks. Based on the learning objective (8), we propose a three-term unsupervised optimization loss L for training the neural network engine, that optimizes ρlog for different spectral tasks: 

L = 1

N [c1Lobj (Xi) + c2Lstruct (Xi) + c3Lreg (Xi)] (9) where N is the size of dataset {Xi}Ni=1 . In (9): Lobj is the inverse of the learning objective ρlog , i.e., 

Lobj = 1 

> ρlog

= log rbaseline   

> log rmodel

, which enforces desired spectral properties uniformly across the dataset; Lstruct is a structural objective that imposes structural constraints on the learned algorithm (e.g., the condition number of the preconditioner); and Lreg is a regularization term that penalizes outliers which promotes learning more generalizable algorithms. Details of the loss function are described in Appendix C.2. 

# 5 Empirical Results 

In this section, we describe the empirical performance of the AutoSpec framework. We trained neural network engines for eigenvalue problems, linear systems, and matrix inverse square roots, and we evaluated their performance. 

## 5.1 Evaluation Setup 

Eigenproblems and linear systems. We target a memory- and communication-constrained regime in which projections onto large Krylov subspaces are impractical; in such settings, acceleration is often obtained via polynomial preconditioning (Barrett et al., 1994; Saad, 1985, 2003, 2011), which can be implemented in low storage and with reduced communication since it avoids inner products. Accordingly, we treat the cost of applying the polynomial as negligible relative to the dominant solver operations, and quantify preconditioning performance by the resulting reduction in iteration count. In this setting, a trained neural engine is deployed as a plug-in and invoked from Matlab to output the coefficient sequence that defines the polynomial updates. Spectral probes are obtained from a short warm start: for eigenproblems, we run 20 iterations of eigs (Krylov–Schur algorithm) using the same search subspace dimension as the main run; for linear systems, we run 50 Lanczos steps without storing the Lanczos basis. Sparse test matrices are drawn from the SuiteSparse Matrix Collection and span electronic-structure, thermal finite-element, structural/mechanics, and circuit/model-reduction problems, with sizes n ∈ [468 , 1.6 ×

8Neural Network No Preconditioner 0 200 400 600 800 1000                     

> CG Outer Iterations
> 10 12
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0
> Relative Residual (a) G2 circuit 0200 400 600 800 1000
> CG Outer Iterations
> 10 12
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0(b) thermal2

Figure 3: Convergence of CG solver with NN preconditioning using spectral probe computed with 50 Lanczos iterations 10 6] and condition numbers ranging by orders of magnitude; see Table 2. Furthermore, all test matrices are spd (in eigenvalue problems, the indefinite operators are implicitly squared). Full experimental details are deferred to Appendix E. 

Matrix functions. We trained a neural network engine to approximate X−1/2 with a polynomial and evaluate it on covariance whitening for a regularized DNA similarity Gram matrix derived from E. coli K-12 MG1655 (U00096.3). Using a 9-mer spectrum embedding with 4 9 = 262 ,144 features and 250 sequences, we construct a feature matrix A and compute inverse square root approximation of X = AA T + λI. The matrix A is highly sparse (with nonzeros density around 0 .375%), which makes this setting well-suited for polynomial spectral methods, since they require only efficient matvecs and avoid explicit factorizations. 

## 5.2 Eigenvalue Problems 

For eigenvalue problems, the downstream task is to compute the k smallest-magnitude eigenpairs using eigs 

under a fixed search subspace budget l (with user-defined k and l 2). After applying an identity shift to make the target modes dominant, we use a degree-21 polynomial transform generated by the neural network engine to enhance spectral separation near the target boundary, here chosen to be defined by the eigengap between the k-th and 1 .5k-th eigenvalues. Figures 2 and 9 (in the appendix) compare eigs 3 with AutoSpec preconditioning versus eigs with a power preconditioner baseline P (X) = Xd. The AutoSpec preconditioning consistently significantly outperforms the baseline across different ( k, l ) configurations, which indicates generalization beyond fixed spectral regions. Figure 5 further shows that performance improves with probe quality, suggesting the neural engine adapts to the spectral profiles. See Appendix E for extended real-world matrix experiments and Appendix F.1 for complementary synthetic results. 

## 5.3 Preconditioned Linear Systems 

We train a neural network engine to generate degree-11 polynomial preconditioners for accelerating Conjugate Gradient (CG) solvers, and evaluate its performance on 10 SuiteSparse systems with random Gaussian           

> 2Typically l=c k for a small constant c, e.g., c= 4.
> 3We request k+ 5 eigenvalues in eigs , which is standard practice to improve convergence.

910 6 10 5 10 4

Regularization Parameter 

10 2

10 1

10 0

> Operator Norm Error ( )

Neural Network 

Neumann Figure 4: Whitening the covariance matrix of the E. coli K-12 MG1655 reference genome sequence. right-hand sides. Spectral probes are obtained from 50 Lanczos steps 4 and used as input to the neural network engine. Table 3a reports the number of CG iterations required to reach a relative error of 10 −10 , along with estimated condition numbers before and after preconditioning. We observe orders-of-magnitude acceleration across all test cases. In addition to faster convergence, the learned preconditioners also improve numerical robustness, as shown by the representative convergence curves for G2 circuit and thermal2 in Figure 3. See Appendix E for extended real-world experiments and Appendix F.2 for results on synthetic systems. 

## 5.4 Approximating Matrix Functions 

We train a neural network to predict a degree-21 polynomial P (X) that approximates the matrix inverse square root, P (X) ≈ X−1/2, and we apply it to the covariance whitening of the regularized DNA sequence similarity Gram matrix. We measure whitening quality via the operator-norm residual ∥P (X) X P (X) − I∥2, and we compare against a truncated Neumann-series baseline. As shown in Figure 4, the learned approximation algorithm achieves order of magnitude improvement over Neumann series approximation in the operator norm error. See Appendix F.3 for complementary results on synthetic matrices. 

## 5.5 Ablation Studies 

Role of Spectral Residual Features in Discovering Algorithms. Given eigenvalue estimations augmented by corresponding residuals features, the embedding layer “reasons” and exploits critical spectral information, resulting in the discovery of effective algorithms. To further analyze how the embedding layer uses the spectral residuals, we perform an ablation study by setting the residual to 0 and observing how it affects the construction of algorithms and performance on synthetic matrices. As shown in Figure 5, incorporating residuals as input improves the effectiveness of the learned pre-conditioners. When the model is provided with residual features, it consistently achieves larger eigenvalue gap improvements over subspace iteration P (X) = Xd across all iteration budgets. This indicates that residual features provide essential spectral-quality signals for the model to generate effective preconditioning algorithms. In Figure 13 (in the appendix), we present additional results on synthetic matrices, confirming that residual features play an important role in the algorithm generation of AutoSpec .

Robustness of Discovered Algorithms to Spectral Input Perturbations. To assess the robustness of the discovered linear solver accelerator, we perturb the spectral probe by varying the Lanczos iteration counts. Figure 10 (in the appendix) shows that once probe quality is sufficiently high (above 75 Lanczos  

> 4with cost comparable to ∼50 CG iterations.

10 With Residual Without Residual 10 20 30 40 50 60 70          

> Spectral Input Quality (# of Eigs Iterations)
> 10
> 15
> 20
> 25
> 30
> Eigval Gap Improvement (a) SiO2 10 20 30 40 50 60 70
> Spectral Input Quality (# of Eigs Iterations)
> 5
> 10
> 15
> 20
> 25
> 30
> Eigval Gap Improvement (b) thermal2

Figure 5: Comparison of learned eigen preconditioners on SiO2 and thermal2 , generated with versus without providing estimated eigenvalue residuals as input to the neural engine. Performance is reported as the log improvement in the target-boundary eigenvalue gap relative to standard subspace iteration baseline 

P (X) = Xd.Table 1: Minimax optimality gap (lower is more Chebyshev-like) for each realistic matrix. “Random” denotes a baseline where the polynomial coefficients are randomly generated.                  

> Matrix Minimax Optimality Gap ( ↓)
> thermal1 9.390 ×10 −7
> thermal2 9.331 ×10 −7
> SiO2 8.656 ×10 −7
> CO 8.670 ×10 −7
> Random 0.954 Optimal 2.850 ×10 −7

iterations), the CG iteration count to reach 10 −10 error stabilizes, indicating robustness to spectral-input perturbations. 

# 6 Discussions 

In this section, we provide a brief analysis of the algorithms discovered by AutoSpec framework. 

Properties of Learned Algorithms. Chebyshev polynomials are a widely used baseline for polynomial acceleration in NLA. Their effectiveness is based on minimax/equioscillation properties. Accordingly, we use them as a reference point and test whether the learned polynomials exhibit similar structure. Concretely, we evaluate minimax -type behavior: among degree-d polynomials with a fixed leading coefficient, Chebyshev polynomials minimize the peak magnitude over a prescribed interval and attain the associated equioscillation pattern. For a learned polynomial P (·), we search over affine interval parameters and minimize an optimality-gap objective L that compares the observed peak value of P (·) on the candidate window to the theoretical minimax bound. Values L → 0 indicate that the learned polynomial closely matches Chebyshev-like behavior. Details are given in Appendix G. Results are shown in Table 1: on realistic eigenproblems, the learned polynomial achieves minimax error close to optimal, where a randomly generated polynomial (“Random”) has orders of magnitude larger error. This shows that the learned polynomial possesses properties highly analogous to those of Chebyshev polynomials. 11 Neural Network Chebyshev 2 1 0 1 2

x      

> 10 2
> 10 0
> 10 2
> 10 4
> 10 6
> |P(x)| (a) G3 circuit 21012

x (b) thermal2 

Figure 6: Neural network polynomials generated for linear systems G3 circuit and thermal2 from a coarse spectral probe (and post-fitted by an affine shift/scale P (x) ← a P (b x + c) + d) closely resemble Chebyshev polynomial. 

Visualizing Learned Algorithms. Based on Table 1, the learned polynomial possesses properties of Chebyshev polynomials. To visually illustrate the structure of the discovered algorithm, we plot the polynomial structure and compare to that of a Chebyshev polynomial. Figure 6 shows an example of the discovered polynomial for linear systems G3 circuit and thermal2 .

Potential to Advancing State-of-the-Art NLA. To demonstrate the potential of the AutoSpec 

framework for using ML to automate state-of-the-art NLA, we conduct additional experiments comparing discovered preconditioning methods with commonly used in practice Chebyshev polynomial preconditioners. The results show that the discovered algorithms operate more reliably under limited spectral information, adapt to the structure of the input operator, and often outperform Chebyshev preconditioners, sometimes by significant margins. For details, see Appendix E. 

# 7 Conclusion 

We introduced AutoSpec , a neural network framework for discovering spectrum-adaptive numerical iterative algorithms by training a neural engine to output the coefficients of an executable recurrence. Across multiple NLA tasks, the resulting learned recurrences consistently improve convergence and/or accuracy over the baselines used in training, demonstrating the effectiveness of the algorithm discovery. We should note that AutoSpec can also support richer update classes beyond the linear recurrences studied in the main text, including higher-order polynomial recurrences (yielding more expressive bases) and nonlinear transition maps that preserve the iterative structure while increasing expressivity; see Appendix A.1 for details. Together, these results point to a broader pathway for using ML methodology to learn robust, deployable NLA and numerical optimization algorithms that generalize across operator instances. 

Acknowledgements. We would like to acknowledge the NSF and the DARPA DIAL and DARPA AIQ programs for partial support of this work. 12 References 

Kwangjun Ahn, Byron Xu, Natalie Abreu, Ying Fan, Gagik Magakyan, Pratyusha Sharma, Zheng Zhan, and John Langford. Dion: Distributed orthonormalized updates. arXiv preprint arXiv:2504.05295 , 2025. Noah Amsel, David Persson, Christopher Musco, and Robert M. Gower. The Polar Express: Optimal matrix sign methods and their application to the Muon algorithm. arXiv preprint arXiv:2505.16932 , 2025. URL 

https://arxiv.org/abs/2505.16932 .Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. Advances in neural information processing systems , 29, 2016. Steven F Ashby. Minimax polynomial preconditioning for Hermitian linear systems. SIAM Journal on Matrix Analysis and Applications , 12(4):766–789, 1991. Steven F Ashby, Thomas A Manteuffel, and James S Otto. A comparison of adaptive Chebyshev and least squares polynomial preconditioning for Hermitian positive definite linear systems. SIAM Journal on Scientific and Statistical Computing , 13(1):1–29, 1992. Owe Axelsson. Iterative solution methods . Cambridge university press, 1996. Amartya S. Banerjee, Lin Lin, Wei Hu, Chao Yang, and John E. Pask. Chebyshev polynomial filtered subspace iteration in the discontinuous Galerkin method for large-scale electronic structure calculations. 

The Journal of Chemical Physics , 145(15):154101, 2016. Richard Barrett, Michael Berry, Tony F Chan, James Demmel, June Donato, Jack Dongarra, Victor Eijkhout, Roldan Pozo, Charles Romine, and Henk Van der Vorst. Templates for the solution of linear systems: building blocks for iterative methods . SIAM, 1994. Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le. Neural optimizer search with reinforcement learning. In International Conference on Machine Learning , pages 459–468. PMLR, 2017. Michele Benzi. Preconditioning techniques for large linear systems: a survey. Journal of Computational Physics , 182(2):418–477, 2002. William L. Briggs, Van Emden Henson, and Steve F. McCormick. A Multigrid Tutorial . Society for Industrial and Applied Mathematics, second edition, 2000. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems , 36:49205–49233, 2023. Yuri Chervonyi, Trieu H Trinh, Miroslav Olˇ s´ ak, Xiaomeng Yang, Hoang H Nguyen, Marcelo Menegali, Junehyuk Jung, Junsu Kim, Vikas Verma, Quoc V Le, et al. Gold-medalist performance in solving olympiad geometry with alphageometry2. Journal of Machine Learning Research , 26(241):1–39, 2025. Vladimir Druskin and Leonid Knizhnerman. Extended Krylov subspaces: Approximation of the matrix square root and related functions. SIAM Journal on Matrix Analysis and Applications , 19(3):755–771, 1998. Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sabl´ e-Meyer, Lucas Morales, Luke Hewitt, Daniel Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI) . Association for Computing Machinery, 2021. 13 Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis, and Pushmeet Kohli. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature , 610(7930):47–53, October 2022. Anne Greenbaum. Iterative Methods for Solving Linear Systems . Society for Industrial and Applied Mathematics, 1997. Ekaterina Grishina, Matvey Smirnov, and Maxim Rakhuba. Accelerating Newton–Schulz iteration for orthogonalization via Chebyshev-type polynomials. arXiv preprint arXiv:2506.10935 , 2025. Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In 

International Conference on Machine Learning , pages 1842–1850. PMLR, 2018. Stefan G¨ uttel. Rational Krylov approximation of matrix functions: Numerical methods and optimal pole selection. GAMM-Mitteilungen , 36(1):8–31, 2013. Paul H¨ ausner, Ozan ¨Oktem, and Jens Sj¨ olund. Neural incomplete factorization: learning preconditioners for the conjugate gradient method. arXiv preprint arXiv:2305.16368 , 2023. Conor F Hayes, Felipe Leno Da Silva, Jiachen Yang, T Nathan Mundhenk, Chak Shing Lee, Jacob F Pettit, Claudio Santiago, Sookyung Kim, Joanne T Kim, Ignacio Aravena Solis, et al. Deep symbolic optimization: Reinforcement learning for symbolic mathematics. arXiv preprint arXiv:2505.10762 , 2025. Magnus R Hestenes, Eduard Stiefel, et al. Methods of conjugate gradients for solving linear systems. Journal of research of the National Bureau of Standards , 49(6):409–436, 1952. Nicholas J. Higham. Functions of Matrices: Theory and Computation . Society for Industrial and Applied Mathematics, 2008. Lei Huang, Dawei Yang, Bo Lang, and Jia Deng. Decorrelated batch normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 791–800, 2018. Olin G. Johnson, Charles A. Micchelli, and George Paul. Polynomial preconditioners for conjugate gradient calculations. SIAM Journal on Numerical Analysis , 20(2):362–376, 1983. Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks. Cited on , page 10, 2024. URL https: //kellerjordan.github.io/posts/muon/ .Ayano Kaneda, Osman Akar, Jingyu Chen, Victoria Alicia Trevino Kala, David Hyde, and Joseph Teran. A deep conjugate direction method for iteratively solving linear systems. In International Conference on Machine Learning , pages 15720–15736. PMLR, 2023. Samuel Kim, Peter Y Lu, Srijon Mukherjee, Michael Gilbert, Li Jing, Vladimir ˇCeperi´ c, and Marin Soljaˇ ci´ c. Integration of neural network-based symbolic regression in deep learning for scientific discovery. IEEE transactions on neural networks and learning systems , 32(9):4166–4177, 2020. John R. Koza. Genetic programming as a means for programming computers by natural selection. Statistics and Computing , 4(2):87–112, June 1994. Guillaume Lample and Fran¸ cois Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412 , 2019. Bar Lerer, Ido Ben-Yair, and Eran Treister. Multigrid-augmented deep learning preconditioners for the helmholtz equation using compact implicit layers. SIAM Journal on Scientific Computing , 46(5):S123–S144, 2024. 14 Jiwei Li, Alexander H Miller, Sumit Chopra, Marc’Aurelio Ranzato, and Jason Weston. Learning through dialogue interactions by asking questions. arXiv preprint arXiv:1612.04936 , 2016. Yichen Li, Peter Yichen Chen, Tao Du, and Wojciech Matusik. Learning preconditioners for conjugate gradient PDE solvers. In International Conference on Machine Learning , pages 19425–19439. PMLR, 2023. Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. Advances in Neural Information Processing Systems (NeurIPS) , 30, 2017. J¨ org Liesen and Zdenek Strakos. Krylov subspace methods: principles and analysis . Numerical Mathematics and Scie, 2013. Jian Luo, Jie Wang, Hong Wang, huanshuo dong, Zijie Geng, Hanzhu Chen, and Yufei Kuang. Neural Krylov iteration for accelerating linear system solving. In The Thirty-eighth Annual Conference on Neural Information Processing Systems , 2024. Daniel J. Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, Thomas K¨ oppe, Kevin Millikin, et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature , 618(7964):257–263, 2023. Terrell Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P Santiago, Brenden K Petersen, et al. Symbolic regression via deep reinforcement learning enhanced genetic programming seeding. Advances in Neural Information Processing Systems , 34:24912–24923, 2021. Yuji Nakatsukasa, Olivier S` ete, and Lloyd N. Trefethen. The AAA algorithm for rational approximation. 

SIAM Journal on Scientific Computing , 40(3):A1494–A1522, 2018. Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course , volume 87 of Applied Optimization . Springer, New York, NY, 2004. Yurii E. Nesterov. A method of solving a convex programming problem with convergence rate O(1 /k 2). 

Soviet Mathematics Doklady , 27(2):372–376, 1983. Alexander Novikov, Ngˆ an V˜ u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131 , 2025. Junhyuk Oh, Matteo Hessel, Wojciech M Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. Advances in Neural Information Processing Systems , 33:1060–1070, 2020. Junhyuk Oh, Gregory Farquhar, Iurii Kemaev, Dan A. Calian, Matteo Hessel, Luisa Zintgraf, Satinder Singh, Hado van Hasselt, and David Silver. Discovering state-of-the-art reinforcement learning algorithms. Nature ,648(8093):312–319, December 2025. Brenden K Petersen, Mikel Landajuela, T Nathan Mundhenk, Claudio P Santiago, Soo K Kim, and Joanne T Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. arXiv preprint arXiv:1912.04871 , 2019. B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics , 4(5):1–17, 1964. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International conference on learning representations , 2017. Esteban Real, Chen Liang, David So, and Quoc V. Le. AutoML-Zero: Evolving machine learning algorithms from scratch. In Proceedings of the 37th International Conference on Machine Learning (ICML) , volume 119 of Proceedings of Machine Learning Research , pages 8007–8019. PMLR, 2020. 15 Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from program search with large language models. Nature , 625(7995):468–475, January 2024. Youcef Saad. Practical use of polynomial preconditionings for the conjugate gradient method. SIAM Journal on Scientific and Statistical Computing , 6(4):865–881, 1985. Youcef Saad and Martin H. Schultz. GMRES: A generalized minimal residual algorithm for solving nonsym-metric linear systems. SIAM Journal on Scientific and Statistical Computing , 7(3):856–869, 1986. Yousef Saad. Iterative Methods for Sparse Linear Systems . Society for Industrial and Applied Mathematics, second edition, 2003. Yousef Saad. Numerical Methods for Large Eigenvalue Problems . Society for Industrial and Applied Mathematics, second edition, 2011. Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science , 324 (5923):81–85, 2009. Yue Song, Nicu Sebe, and Wei Wang. Why approximate matrix square root outperforms accurate svd in global covariance pooling? In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) . IEEE Computer Society, 2021. Yue Song, Nicu Sebe, and Wei Wang. Fast differentiable matrix square root and inverse square root. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 45(6):7367–7380, 2023. Satoshi Takabe and Tadashi Wadayama. Convergence acceleration via Chebyshev step: Plausible interpretation of deep-unfolded gradient descent. IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences , 105(8):1110–1120, 2022. Ulrich Trottenberg, Cornelius W. Oosterlee, and Anton Sch¨ uller. Multigrid . Academic Press, 2000. Silviu-Marian Udrescu and Max Tegmark. AI Feynman: A physics-inspired method for symbolic regression. 

Science Advances , 6(16):eaay2631, 2020. Richard S. Varga. Matrix Iterative Analysis , volume 27 of Springer Series in Computational Mathematics .Springer, second revised and expanded edition, 2000. Olga Wichrowska, Niru Maheswaranathan, Matthew W Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Nando Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and generalize. In International conference on machine learning , pages 3751–3760. PMLR, 2017. S. Yang, Z. Wang, O. Balabanov, N. B. Erichson, and M. W. Mahoney. PRISM: Distribution-free adaptive computation of matrix functions for accelerating neural network training. arXiv preprint arXiv:2601.22137 ,2026. David M Young. Iterative solution of large linear systems . Elsevier, 2014. Yunkai Zhou and Yousef Saad. A Chebyshev–Davidson algorithm for large symmetric eigenproblems. SIAM Journal on Matrix Analysis and Applications , 29(3):954–971, 2007. 16 Appendix A Details of the AutoSpec Learning Framework 

## A.1 Extensions 

We outline extensions of the linear state-transition model used in the AutoSpec framework in main text. 

Higher-order updates. Higher-order updates can be realized by allowing nonlinear dependence on the state Vk. For example, a quadratic transition can be defined as 

Mk(Vk, X) := Ik(X) Vk Jk(X) VT 

> k

Kk(X), (10) where matrices Ik(X), Jk(X), Kk(X) admit block decompositions with subblocks given by low-degree polyno-mials in X. Such higher-order updates are less suited to sparse, matrix-free regimes than linear transitions, since they generally require explicit matrix–matrix products. However, on parallel hardware, especially in optimization loops that already use explicit matrix-function approximations (e.g., inverse-root preconditioning in Shampoo and orthogonalization/polar updates in Muon), they can be highly attractive (Ahn et al., 2025; Amsel et al., 2025; Grishina et al., 2025; Gupta et al., 2018; Higham, 2008; Jordan et al., 2024). Discovering and training such higher-order recurrences for optimization is a promising direction for future work. 

Rational Models. In some settings, such as approximating matrix functions with sharp variation near the spectrum (e.g., X−1/2 or log (X)), or computing interior eigenvalues, it can be advantageous to model P (·) as a rational function rather than a polynomial function. Methods based on rational spectral transformations play a central role in NLA and are particularly prominent in solvers based on Krylov approximation (Druskin and Knizhnerman, 1998; G¨ uttel, 2013; Higham, 2008; Nakatsukasa et al., 2018). Within the AutoSpec 

framework, rational constructions can be incorporated naturally by allowing the transition blocks M (i,j ) 

> k

(·) to be rational functions. For example, in the three-term update formula (4), the linear shifts may be replaced by resolvents, 

αkI + βkX → (αkI + βkX)−1, for 1 ≤ k ≤ d.This substitution trades inexpensive matvecs with X for more costly shifted linear solves during inference, but it can yield substantially improved approximations and preconditioners. 

Gradient-driven transitions. The same state-transition view can also express first-order optimization recurrences by treating the operator in (2) as a nonlinear map accessed through gradient evaluations, i.e., 

X(z) = ∇f (z). 5 For instance, with an augmented state Vk = ( θk, mk), the transition recovers gradient descent with momentum, 

mk+1 = βkmk + X(θk), θk+1 = θk − αkmk+1 ,

and yields Nesterov acceleration by evaluating the gradient at an extrapolated point (e.g., X(θk + γkmk)) (Nesterov, 2004, 1983; Polyak, 1964). This suggests learning gradient-driven neural engines that map cheap probe features (e.g., Hessian spectral estimates, gradient norms/inner products, or curvature sketches) to coefficient sequences ( αk, β k, γ k, . . . ), thereby discovering accelerated or task-adaptive optimizers within universal recurrence formalism. 

## A.2 Learning Objectives for Different Tasks 

In (8), we introduced the generalized objective ρlog , which compares a learned method against a task-specific baseline through a scalar performance metric r(X). Concretely, each application requires the following: (i) a metric r that quantifies convergence rate/accuracy; and (ii) a baseline method (without learned acceleration) used to form rbaseline . For the tasks in this paper, we used the following.   

> 5Here we overload Xto denote an operator/oracle rather than a matrix.

17 • Eigenvalue problems. We measure separation after applying the polynomial spectral transform 

P (·) via 

r(X) = min i=1 ,...,k P (λi(X)) 

max I⊂{ 1,...,m }, |I|=l min j∈I P (λj (X)) , (11) where {λi(X)} denotes the spectrum of X, k is the number of requested eigenpairs, and l is the dimension of Ritz approximation subspaces. As a baseline we use standard power/subspace iteration polynomial P (λj (X)) = λj (X)d.

• Linear systems. We use the operator-norm residual of the normalized preconditioned operator, 

r(X) = I − P (X)X

∥P (X)X∥2 2

, (12) with Richardson iteration polynomial as the baseline. 

• Inverse square root. For approximating X−1/2, we use the worst-case relative error in operator norm, 

r(X) = I − P (X) X1/2

> 2

, (13) with a Neumann (truncated Taylor expansion) polynomial as the baseline. 

# B Details of Neural Network Engine 

We provide the details of the neural network engine, including the backbone model and embedding layer, and specifications for different NLA tasks. 

## B.1 Backbone Model 

Corresponding to Section 4.1, the detailed structure of each layer of the backbone model is shown in Algorithm 1. 

Algorithm 1 NN Layer k of Backbone Model fθk

Parameters: Weight matrices Wk ∈ Rd×5, wk ∈ Rd, Bias bk ∈ R5, b k ∈ R, ϵ = 10 −8

Input: Input to neural network layer e ∈ Rd, matrix X.

Output: State transition operators Mk(X) 

> 1:

[ρk, γ k, η k, α k, β k]T ← WT 

> k

e + bk ▷ Coefficients  

> 2:

δk ← wT 

> k

e + bk ▷ Learned Scaling Factor  

> 3:

γk, η k, α k, β k ← γk  

> δk+ϵ

, ηk  

> δk+ϵ

, αk  

> δk+ϵ

, βk  

> δk+ϵ
> 4:

if 1 ≤ k < d then define Mk as 

 x, y, z 7 →  x + ρkz, z, γ kx + ηky + αkz + βkXz  

> 5:

else if k = d then define Mk as 

 x, y, z 7 → γkx + ηky + αkz + βkXz  

> 6:

end if  

> 7:

return M k

18 B.2 Embedding Layer 

Embedding Layer for Eigenvalue Problems. To compute the largest or smallest k eigenvalues of 

X using iterative eigensolvers such as Krylov-Schur with subspace dimension l, an effective preconditioning algorithm should enlarge the spectral gap between the k-th and l-th eigenvalues. Varying k or l changes the target portion of the spectrum, requiring the model to generalize across different target spectral regions. We design an embedding layer that allows the model to adapt to different k and l and construct effective preconditioners, requiring only truncation and padding to the input spectral probes. Algorithm 2 shows the structure of the embedding layer for eigenvalue problems. It uses two multi-layer subnetworks to exploit spectral information from the input spectral probe, and it returns the spectral embedding for the k-th and l-th eigenvalue, respectively. The first subnetwork, parameterized by W1, W3,first obtains y1 that encodes the spectral information of top-k eigenvalues, and then selects the smallest magnitude y1 as the embedding for the k-th eigenvalue. The second subnetwork, parameterized by W2, W4,outputs an embedding y2 encoding spectral information in the neighborhood of the l-th eigenvalue. Then the two embeddings are concatenated to form the final output embedding. The layer has a fixed input size of 2 l0,corresponding to l approximated eigenvalues and corresponding residuals. In practice, the target rank k′ and subspace dimension l′ may differ from the fixed input window size (k0, l 0) expected by the embedding layer. To produce a fixed-length input of l0 spectral probes (eigenvalue estimates and residuals), we construct a normalized window by truncation/padding while preserving the two boundary indices. Specifically, we first form the leading block of k0 probes from the top-k′ approximate eigenvalues by truncating or padding as needed, and we enforce that the k0-th probe corresponds exactly to the k′-th approximate eigenvalue. We then form the remaining l0 − k0 probes from the range {bλk′ , . . . , bλl′ },again truncating or padding to length l0 − k0, and we enforce that the final ( l0-th) probe corresponds exactly to the l′-th approximate eigenvalue. This construction ensures that, for any ( k′, l ′), the network input always contains a consistent spectral window anchored at the k′-th and l′-th eigenvalues. 

Algorithm 2 Embedding Layer gϕ for Eigenvalue Problems 

Configurations: Number of eigenvalues to compute k0 with subspace dimension l0, Model input dimension 

din = 2 l0, Hidden dimension dhid = 4 k0 + 8 

Parameters: W 1, W2 ∈ Rdin ×dhid , W3 ∈ Rdhid ×k0 , W4 ∈ Rdhid ×4, GeLU activation function σ

Input: Approximated eigenvalues bλ =

hbλ1, . . . , bλl0

iT

, Residuals br = [ br1, . . . , brl0 ]T

Output: Embedding e = [ e1, . . . , e 5]T ∈ R5 

> 1:

x ←

hbλT, brT

iT

▷ Construct input x ∈ R2l0 

> 2:

y1 ← WT3 σ(WT1 x) ▷ First subnetwork, y1 ∈ Rk0 

> 3:

y1 = min 1≤i≤n |(y1)i| 

> 4:

y2 ← WT4 σ(WT3 x) ▷ Second subnetwork, y2 ∈ R4 

> 5:

e ← y1, yT2

T 

> 6:

return e Embedding Layer for Preconditioned Linear Systems. The learning objective of preconditioning algorithms for linear systems it to reduce the condition number of operators. Therefore, the task-relevant portion of the spectrum is the two ends of the spectrum, and the spectral probe consists of the largest and smallest k eigenvalues (with corresponding residuals). In our experiments, we choose k = 20. Algorithm 3 describe the procedure of the embedding layer for preconditioned linear systems. We use two subnetworks to process the spectral probe for the top and bottom eigenspectrum separately, and we concatenate their 19 outputs to form the embedding. Each subnetwork uses the spectral probe of corresponding spectral regions and exploit the structure of the eigenspectrum. 

Algorithm 3 Embedding Layer gϕ for Preconditioned Linear Systsems 

Configurations: Number of largest/smallest input eigenvalue estimations k, Model input dimension din = 2 k,Hidden dimension dhid = 2 k, Matrix dimension m

Parameters: W 1, W2 ∈ Rdin ×dhid , W3, W4 ∈ Rdhid ×5, GeLU activation function σ

Input: Approximated largest eigenvalues bλmax =

hbλ1, . . . , bλk

iT

, Residuals brmax = [br1, . . . , brk]T, Approxi-mated smallest eigenvalues bλmin =

hbλm−k+1 , . . . , bλm

iT

, Residuals brmin = [ brm−k+1 , . . . , brm]T

Output: Embedding e = [ e1, . . . , e 5]T ∈ R5 

> 1:

x1, x2 ←

hbλTmax , brTmax 

iT

,

hbλTmin , brTmin 

iT

▷ Construct input x1, x2 ∈ R2k 

> 2:

y1 ← WT3 σ(WT1 x) ▷ First subnetwork, y1 ∈ R5 

> 3:

y2 ← WT3 σ(WT2 x) ▷ Second subnetwork, y2 ∈ R5 

> 4:

y1 ← sort ↓(|y1|) ▷ Sort by magnitude (descending)  

> 5:

y2 ← sort ↓(|y2|) ▷ Sort by magnitude (descending)  

> 6:

e ← yT1 , yT2

T 

> 7:

return e Embedding Layer for Approximating Matrix Functions. For approximating matrix function such as inverse square root, we adopt the same embedding layer as for linear systems, as the top and bottom region of the eigenspectrum are also task-relevant. In our experiments, we choose k = 20. 

# C Detailed Training Settings 

## C.1 Training Data Curation 

As shown in Section 4.3, we can train the model with only diagonal matrices, represented as vectors of eigenspectra. We construct a synthetic eigenspectrum generator that generates positive eigenvalue spectra by sampling a small set of continuous shape parameters using quasi-random Sobol sequences, ensuring broad and uniform coverage of admissible spectral configurations. Each spectrum is formed as a normalized blend of flat, exponential, and power-law decay profiles, with additional concavity modulation to control the condition number and induce slow spectral decay. To better reflect non-idealized operators, we introduce mild local irregularities through multiplicative noise and randomized tail perturbations, while preserving the overall spectral structure. For specific NLA tasks, the eigenspectrum generator also explicitly controls the spectral features most relevant to the task. For eigenvalue problems, since in practice polynomial preconditioners are most effective for matrices with clustered eigenvalues (slow decay), we apply rejection criteria that enforce unit normalization and small leading eigenvalue separation (e.g., λ2/λ 1 close to one), yielding a curated set of eigenspectra. For preconditioned linear systems and matrix function approximation, since the condition numbers of operators are most critical for preconditioning and approximation, we explicitly control the condition number of the generated eigenspectra, and create a training set that covers a wide range of condition numbers. 

Training Data Normalization. To maintain training stability while ensuring robustness of the learned model, we preprocess the training matrices with normalization. Specifically, we ensure that the largest eigenvalue of the matrix is bounded and around 1. During data generation, we first enforce all synthetic 20 matrices to have unit spectral norm, and we then perform a perturbation scaling for each matrix by either 1) randomly scaling each matrix with a random scalar in [1 − ϵ, 1 + ϵ] (we choose ϵ = 0 .2), or 2) scaling with a coarse approximation of the spectral norm. This ensures that the training matrices have bounded norms but not strictly unit-norm, which could otherwise lead to overfitting to this specific spectral property, as in practice we cannot ensure unit norm for realistic matrices and can only normalize the matrix by approxiate spectral norm. 

## C.2 Details of Optimization Objectives 

In the AutoSpec framework, we use the unified optimization loss function defined in (9) of Section 4.4 to train neural networks to discover spectral algorithms. Here we describe the detailed loss function and its design philosophy for the primary applications in our paper. 

Eigenvalue Problems. To discover a preconditioning algorithm P (·) for eigenvalue problems, we expect the learned algorithm to have the following properties: 1) it outperforms standard power or subspace iteration methods; and 2) it can be extended to higher degrees, so that for any first k iterations of the algorithm, it can be a degree-k preconditioner outperforming k power/subspace iterations. For 1), we choose the baseline method as standard power/subspace iteration; and for 2), we adopt a layer-wise loss function that optimizes the objective for the first k neural network layers ( k = 1 , . . . , d ). Adopting the formula in (11), we define the objective for our model: 

rnn (X) = min i=1 ,...,k |P (λi(X)) |

max I⊂{ 1,...,m },|I|=l min j∈I |P (λj (X)) | , (14) and for standard subspace iteration: 

rsubspace (X) = min i=1 ,...,k |λi(X)|

max I⊂{ 1,...,m },|I|=l min j∈I |λj (X)| . (15) Therefore, the loss function L computes a weighted sum of the loss term Lk for each layer, with weight 

wk for layer k. During training, the weight term wk is dynamically adjusted to focus on optimizing the loss of different layers. For each layer, the loss follows the form in (9) (we omit the Lstruct as it is not necessary): the inverse of ρlog , and Lreg , which penalizes samples on which the algorithms fail: 

L =

> D

X

> k=1

wkLk (16) 

Lk = 1

N

> N

X

> i=1



k · log rsubspace 

clamp (log rnn , ϵ )

| {z }

> Lobj

+ 10 exp [ −10 · rnn − ϵ]

| {z }

> Lreg

 . (17) Note that ρlog is a ratio of two logarithms, and the denominator is not guaranteed to be positive throughout training. Therefore we clamp the denominator with a small positive scalar. Figure 7 shows the trajectory of two loss terms of the last layer. At the early stage of training, Lreg dominates the loss, during which the model learns to produce algorithms that perform moderately well on each training sample. Once Lreg 

diminishes, ρlog becomes the primary optimization objective, encouraging the model to find parametrization outperforming the baseline method as much as possible. 

Dynamic Weight Adjustment to Loss Function. At the beginning of the training, w is larger for earlier layers, while towards the end of the training, w is larger for later layers. Suppose the total number of 21 0 100 200 300 400 500  

> Epochs
> 10 1
> 10 0
> 10 1
> 10 2
> Loss

(a) Inverse of ρlog 0 100 200 300 400 500  

> Epochs
> 10 1
> 10 2
> 10 3
> Loss

(b) Lreg 

Figure 7: Last-layer training loss of training neural network to discover preconditioning algorithms for eigenvalue problems. training steps is T , the scheduling function for wk at step t is: 

s = ⌊ Tt ⌋ (18) 

wtk =

(1 if k = s 

> 1(k−s)2

otherwise . (19) During training, we normalize the weight by the sum across all layers: wk = wkPDi=1 wi

. We call s the “Anchor Layer,” which has the largest weight in the loss function. As training proceeds, the anchor layer moves from the first layer to the last layer. The motivation of this design is to ensure that the polynomial of any 

degree generated by the model can accelerate the convergence when applied to the matrix, instead of only the largest-degree polynomial. This can better disentangle the roles of each layer. 0 100 200 300 400 500   

> Epochs
> 10 2
> 10 1
> Weight

(a) Loss weight of each layer 0 100 200 300 400 500   

> Epochs
> 10 1
> 10 0
> 10 1
> 10 2
> Loss

(b) Inverse of ρlog 0 100 200 300 400 500  

> Epochs
> 10 1
> 10 2
> 10 3
> 10 4
> Loss

(c) Lreg 1

> 2
> 3
> 4
> 5
> 6
> 7
> 8
> 9
> 10
> Layer

Figure 8: Dynamic adjustment of training loss of all layers. In the early stage of training, earlier layers have a larger weight in the total loss, and are optimized more; towards the end of training, later layers have a larger weight in the total loss, and are optimized more. 

Preconditioned Linear Systems. Similarly, we design the optimization objective for generating pre-conditioning algorithm P (·) for solving linear systems, based on the objective in (12). When training with diagonal matrices X = diag( λ), the residual objective can be reduced to 

rnn (X) = max  

> j

1 − P (λj (X)) λj (X)max i |P (λi(X)) λi(X)| , (20) 22 which is easy to obtain during training. Note that due to the normalization, we have rnn ∈ [0 , 2]. As the baseline, we use the following residual: 

rRichardson (X) = 1 − min i λi(X)max i λi(X) . (21) This represents the convergence rates of a basic Richardson method for solving linear systems. Following the form in (9) define the optimization objective as 

Lk = 1

N

> N

X

> i=1

clamp 

 k · log rRichardson (Xi)log rnn (Xi) , ϵ 

| {z }

> Lobj

+ rnn (Xi)

| {z }

> Lreg

+ c · max i |P (λi(Xi)|

min i |P (λi(Xi)|

| {z }

> Lstruct

 (22) on training dataset {Xi}Ni=1 , where we set c = 5 × d3, d is the degree of the constructed polynomial. The term Lstruct is a structural constraint that minimizes the condition number of the preconditioner P (X). Our empirical study on realistic matrices suggests that constraining the condition number of the preconditioner can be useful for improving convergence stability. In the early stage of training, rnn is unstable and would revolve around 1, causing the Lobj to fluctuate around 5, which is difficult to optimize. Therefore we perform sample-wise clipping of Lobj from below with a small positive constant (10 −8), while optimizing Lreg and Lstruct . As the training proceeds, Lreg becomes smaller than 1 across the dataset, where Lobj is activated, becoming the primary optimization objective. 

Approximating Matrix Functions. In this work, we consider learning a polynomial P (·) that approximate matrix inverse square root, and we select the Neumann (Taylor) series approximation T (·) as our baseline reference method. Following (13), we define the objective for our model and baseline for training on synthetic diagonal matrices X = diag( λ): 

rnn (X) = max  

> i

|1 − P ( eλi(X)) eλi(X) 12 | (23) 

rNeumann (X) = max  

> i

|1 − T (λi(X)) λi(X) 12 |, (24) where eλ(X) are eigenvalues augmented with random sampled values in [ λmin (X), λ max (X)]. This is to regularize the approximation accuracy of P (·) not only on the discrete eigenvalues, but also on the continuous region in which the eigenspectrum is located. This augmentation promotes the learning of a more robust approximation algorithm. Based on the form in (9) (we omit the Lstruct as it is not necessary for matrix function approximation), we design the loss function as 

Lk = 1

N

> N

X

> i=1

clamp 

 log rNeumann (Xi)log rnn (Xi) , ϵ 

| {z }

> Lobj

+ rnn (Xi)

| {z }

> Lreg

 . (25) 

## C.3 Extending Polynomial Degree 

For eigenvalue problems, problem difficulty strongly correlates with eigenvalue gaps, which requires polynomials of different degree to achieve reliable performance. Our AutoSpec framework offers an easy way to extend the backbone model to higher degrees, where one can append more neural network to already pre-trained backbone models, and continue training the model. This works because the optimization objective in (16) promotes the model to learn a solution such that, for any degree k, polynomials generated by the first k

layers can sufficiently accelerate the convergence on eigenvalue problems. Figure 11a shows that any degree 

k polynomial preconditioner from the first k layers consistently has a larger eigenvalue gap than standard iterative methods. 23 D Training Details 

## D.1 Backbone Model (Pre-training) 

For pre-training, we follow the optimization objectives described in Appendix C.2 for each NLA task. We perform training using synthetic diagonal matrices (represented as vectors of eigenvalues). 

Eigenvalue Problems. For eigenvalue problems, we create a synthetic dataset of size 50000 with matrix dimensions ranging from [50 , 1000], which contains matrices that have slow decay in dominant eigenvalues. We use a batch size of 500 and train the model for 1000 epochs. We use the AdamW optimizer with cosine learning rate decay after 10% of warmup steps. We search for the best learning rate among {10 −3, 5 × 10 −4, 10 −4}

and select the best learning rate, and use a weight decay of 5 × 10 −4.

Preconditioned Linear Systems. For preconditioner linear systems, we create a synthetic dataset of size 50000 with matrix dimensions ranging from [50 , 1000], which contains matrices whose condition number ranges [10 2, 10 5]. We use a batch size of 500 and train the model for 1000 epochs. We use the AdamW optimizer with cosine learning rate decay after 10% of warmup steps. We search for the best learning rate among {10 −3, 5 × 10 −4, 10 −4} and select the best learning rate, and use a weight decay of 5 × 10 −4.

Approximating Matrix Functions. For matrix function approximations, we adopt the same training strategy as for preconditioned linear systems. 

## D.2 Embedding Model (Post-training) 

Following the introduction of the structure of the embedding layer in Section B.2, we introduce the training process of the embedding layer. The post-training stage aims to enable the neural network engine to generate effective algorithms when only given spectral probes (coarse spectral estimates). We therefore train an embedding layer that encodes information about structures of task-relevant spectral regions and generates embeddings. In practice, we use the same optimization objective as training the backbone model, and we freeze the backbone model while training the embedding layer. 

Eigenvalue Problems For eigenvalue problems, we design the embedding layer as shown in Algorithm 2, which adapts to different target numbers of eigenvalues k and subspace dimensions l via adaptive trunca-tion/padding of the input spectral probe. To enable the model that generalizes to different k and l, we construct a training dataset with a mixture of k, l, and spectral probes obtained with a varying number of subspace iterations. Specifically, we create a synthetic dataset with matrix dimension ranging from [200 , 2000], spectral probes using subspace dimension l in [20 , 100], and number of subspace iterations in [1 , 500]. We train the embedding layer while freezing the backbone model for 500 epochs, with a dataset of size 50000 and a batch size of 200. We use AdamW optimizer with cosine learning rate decay after 10% of warmup steps. We search for the best learning rate among {10 −4, 10 −5, 5 × 10 −6, 10 −6} and select the best learning rate, and use a weight decay of 5 × 10 −4.

Preconditioned Linear Systems. For preconditioned linear systems, we use the embedding layer design in Algorithm 3, we design a training dataset that consists of matrices with dimensions in [200 , 2000], and we constrain the condition number of matrices spanning [1 e1, 1e5]. Spectral probes are obtained with different numbers of subspace iterations in [1 , 500]. The spectral probe contains the largest and smallest k eigenvalue estimates and corresponding residual norms, obtained using subspace itreations. In practice, we choose 

k = 20, and for estimates of the smallest eigenvalues, we obtain them by performing subspace iteration on shifted matrices to obtain the largest eigenpairs of the shifted matrix. We then obtain the estimates for 24 the smallest eigenvalue by reverse-shifting the estimations and compute the residual using approximated eigenvectors on the unshifted matrix. We train the embedding layer while freezing the backbone model for 500 epochs, with a dataset of size 50000 and a batch size of 200. We use AdamW optimizer with cosine learning rate decay after 10% of warmup steps. We search for the best learning rate among {10 −3, 5 × 10 −4, 10 −4} and select the best learning rate, and we use a weight decay of 5 × 10 −4.

Approximating Matrix Functions. For approximating matrix functions, we adopt the same embedding layer structure as preconditioned linear systems, shown in Algorithm 3. We construct the training dataset with the same method as for linear systems, while increasing the portion of matrices with small condition numbers, as the polynomial method is most suitable for moderate condition number regime. We train the embedding layer while freezing the backbone model for 500 epochs, with a dataset of size 50000 and a batch size of 200. We use AdamW optimizer with cosine learning rate decay after 10% of warmup steps. We search for the best learning rate among {10 −3, 5 × 10 −4, 10 −4} and select the best learning rate, and we use a weight decay of 5 × 10 −4.

# E Extended Numerical Experiments on Real-world Eigenproblems and Linear Systems 

Here, we provide a detailed description of the experimental setup used in the evaluation presented in Section 5, along with additional results and analysis. This includes absolute and relative performance of our discovered preconditioning methods, sensitivity to spectral probe quality, and comparisons with state of the art Chebyshev preconditioning. 

Experimental setting. We consider a computational regime where memory and communication are constrained: storing large Krylov bases is impractical, and each outer Krylov iteration is relatively expensive due to global inner product computation. In this setting, acceleration is commonly achieved via polynomial preconditioning, which uses short recurrences that avoid inner products and large basis storage. 

AutoSpec engines are trained offline and, at inference time, are called from Matlab to produce polynomial recurrence coefficients from spectral probes. For eigenvalue problems, spectral probes are obtained by running 20 iterations of eigs (Krylov–Schur algorithm). From the resulting Ritz spectrum, the engine receives 10 eigenvalues associated with the target region (the first k requested eigenvalues) and 94 eigenvalues associated with the out-of-target region (those following the first k). If fewer than 10 target eigenvalues are available, we sample with replacement from the available ones to obtain 10 values; if more than 10 are available, we uniformly subsample 10. The same sampling-with/without-replacement procedure is used to obtain exactly 94 out-of-target eigenvalues. For linear systems, spectral probes are produced by 50 Lanczos steps without storing the Lanczos basis. From the resulting approximate spectrum, we select the 20 largest and 20 smallest eigenvalues to form the spectral input to the neural engine. We focus on spd operators. Indefinite eigenvalue problems are converted to spd form via implicit squaring of the operator. For some severely ill-conditioned linear systems, we first apply an incomplete Cholesky (ichol ) preconditioner and then use the learned polynomial as a second-level preconditioner. Test matrices are drawn from the SuiteSparse Matrix Collection and cover electronic-structure, thermal finite element, structural/mechanics, and circuit/model-reduction applications. Matrix sizes range from 

n = 468 to n = 1 .6 × 10 6, with estimated condition numbers between 70 and 4 × 10 6.Table 2 summarizes the real-world matrices used in our experiments. For linear systems, it reports whether an incomplete Cholesky preconditioner is applied. For eigenvalue problems, it indicates whether the operator is implicitly squared to make it spd. 25 Table 2: Matrices from the SuiteSparse Matrix Collection used in our experiments. 

(a) Linear Systems 

Matrix X dim iChol cond(X) 

thermal1 82654 no 3.2e5 

thermal2 1228045 no 4.0e6 

nos5 468 no 1.1e4 

wathen100 30401 no 5.8e3 

thermomech TC 102158 no 6.7e1 

G2 circuit 150102 yes 1.6e4 

gyro m 17361 yes 1.9e4 

Dubcova2 65025 yes 1.0e3 

Flan 1565 1564794 yes 8.3e5 

G3 circuit 1585478 yes 1.9e3 (b) Eigenproblems 

Matrix X dim squared cond(X) 

thermal1 82654 no 3.2e5 

thermal2 1228045 no 4.0e6 

SiO2 155331 yes 5.6e6 

CO 221119 yes 6.3e7 

Table 3: Quality of preconditioners produced by AutoSpec .

(a) Linear systems: condition numbers of AutoSpec preconditioners; condition numbers of the preconditioned operators (both using 50-step probes); numbers of CG iterations to reach residual 10 −10 using AutoSpec preconditioning with 50-step and 200-step spectral probes, and without preconditioning. 

Matrix X cond( P (X))

(probe=50) cond( P (X)X)(probe=50) 

numIters 

(probe=200) 

numIters 

(probe=50) 

numIters 

(unprecond.) 

thermal1 2.6e3 5.0e2 173 178 1851 

thermal2 1.4e3 3.2e2 686 705 >5000 

nos5 8.9e1 4.0e2 75 77 654 

wathen100 5.3e1 2.6e2 36 35 363 

thermomech TC 1.3 5.9e1 10 11 93 

G2 circuit 1.4e2 3.9e2 69 70 734 

gyro m 1.8e2 2.3e2 28 27 150 

Dubcova2 8.9 5.8e2 22 26 205 

Flan 1565 6.2e3 5.3e2 366 375 3868 

G3 circuit 1.4e2 1.7e2 100 101 1194 (b) Eigenproblems: AutoSpec preconditioners are compared to basic power method-based (i.e., P (X) = Xd)preconditioners. The number of eigs iterations required to reach relative eigenvalue error 10 −10 . For all matrices, we compute k=10 and k=20 eigenvalues using eigs with k+5 requested eigenvalues and subspace dimension l=4 k.

Matrix X numIters 

(AutoSpec , λ10 )

numIters 

(AutoSpec , λ20 )

numIters 

(power, λ10 )

numIters 

(power, λ20 )

thermal1 4 3 19 12 

thermal2 21 11 112 45 

SiO2 23 23 >500 108 

CO 55 27 >500 126 

Quality of learned preconditioners. Table 3 evaluates the intrinsic quality of the preconditioners produced by AutoSpec engines on the matrices from Table 2. For linear systems (Table 3a), we assess preconditioner quality using three metrics: the number of CG iterations required to reach residual tolerance 10 −10 , using spectral probes from either 50 or 200 Lanczos 26 steps; the condition number of the learned polynomial preconditioner P (X); and the condition number of the preconditioned operator P (X)X. For reference, we also report unpreconditioned CG iteration counts to reach residual tolerance 10 −10 . The results show that the learned preconditioners substantially reduce iteration counts across all tested problems and remain effective over a wide range of condition numbers, even when constructed from coarse spectral probes. For eigenproblems (Table 3b), we report the number of iterations required by eigs to reach a relative eigenvalue error of 10 −10 (on the spectrally transformed operator) for the k-th (taking k = 10 , 20) largest eigenvalue (of the shifted operator which correspond to smallest eigenvalues of the original one). The learned preconditioners consistently reduce iteration counts compared to unpreconditioned runs, demonstrating their effectiveness in accelerating Krylov–Schur iterations under tight memory and communication budgets. 

Convergence of Eigenvalue Approximations. In addition to the results in Section 5.2, we present complementary experiments on convergence of preconditioned Krylov–Schur with varying numbers of requested eigenvalues k and corresponding subspace dimensions l. Figure 9 shows results for the SiO2 and thermal2 

matrices with targets k = 10 and k = 20. In both cases, the preconditioners obtained with AutoSpec engine require substantially fewer Krylov– Schur iterations to reach target errors, indicating that the learned preconditioning method adapts and remains effective for different choices of k and l.Neural Network Power 20 40 60 80 100 120 140      

> Eigs Outer Iterations
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0
> Relative Error

(a) λ10 of SiO2 (k = 20) 20 40 60 80 100 120 140       

> Eigs Outer Iterations
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0

(b) λ20 of SiO2 (k = 20) 20 40 60 80 100 120 140       

> Eigs Outer Iterations
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0

(c) λ5 of thermal2 (k = 10) 20 40 60 80 100 120 140      

> Eigs Outer Iterations
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0

(d) λ10 of thermal2 (k = 10) 

Figure 9: Convergence of eigenvalue estimates versus Krylov–Schur outer iterations for SiO2 and thermal2 

matrices. Parameter k denotes the total number of smallest-magnitude eigenpairs computed with subspace dimension l = 4 k.

Robustness of Discovered Algorithms to Spectral Input Perturbations. To evaluate the robustness of the learned AutoSpec engine for linear systems, we vary the quality of the spectral probe by changing the number of Lanczos iterations. As shown in Figure 10, the number of CG iterations required to reach a residual error of 10 −10 remains stable, indicating that the discovered algorithm is robust. 

Comparison with Chebyshev polynomial preconditioners. We next compare AutoSpec precondi-tioners with classical Chebyshev polynomial preconditioners; see, e.g., (Barrett et al., 1994; Saad, 1985, 2003, 2011). The Chebyshev preconditioners are built using an estimate of a spectral interval [ a, b ] that we wish to control. We apply an affine change of variables 

θ(λ) = 2λ − (a + b)

b − a , λ ∈ [a, b ],

which maps [ a, b ] to [ −1, 1], and let Td(·) denote the degree-d Chebyshev polynomial on [ −1, 1]. 27 50 75 100 150 200 300 

> Eigvals quality (# Lanczos)
> 020 40 60 80
> # Iterations

(a) G2 circuit 50 75 100 150 200 300  

> Eigvals quality (# Lanczos)
> 0100 200 300 400 500 600 700
> # Iterations

(b) thermal2 

Figure 10: CG iterations to reach 10 −10 error vs Lanczos iterations used for NN input spectral probe. For linear systems, the interval [ a, b ] is chosen to cover the spectrum of the spd operator. We define the degree-d Chebyshev residual polynomial Rd(λ) = Td(θ(λ))  

> Td(θ(0))

and set Pd(λ) = 1−Rd(λ) 

> λ

. By construction, 

λP d(λ) = 1 − Rd(λ) is the scaled Chebyshev polynomial that solves min 

> q∈P d−1

max  

> λ∈[a,b ]

1 − λ q (λ) ,

so Pd(λ) yields a near–minimax Chebyshev approximation to 1 /λ on [ a, b ]. For eigenproblems, [ a, b ] is chosen so that the non-target eigenvalues lie in [ a, b ] (e.g., [ a, b ] ≈ [λmin , λ 1.5k+1 ]), while the target eigenvalues (e.g. the first k eigenvalues) lie outside this interval. A Chebyshev filter is then defined by Pd(λ) = Td(θ(λ)). On [ a, b ] we have |θ(λ)| ≤ 1 and hence |Pd(λ)| = |Td(θ(λ)) | ≤ 1, whereas for λ

outside [ a, b ] we obtain |θ(λ)| > 1 and |Pd(λ)| grows rapidly with d. Thus Pd(X) damps non-target modes and amplifies target modes. In both linear systems and eigenproblems, the matrix polynomials are applied via a three-term recurrence, so evaluation is low-storage and avoids inner products. For linear systems (Table 4a), the Chebyshev method uses the same extremal eigenvalue estimates as our neural network engine, obtained from either 50 or 200 Lanczos steps. We report the number of CG iterations required to reach a residual tolerance of 10 −10 . We see that in most cases, the AutoSpec preconditioners match or outperform Chebyshev preconditioning, often by a large margin, and exhibit greater robustness to changes in probe quality. For eigenproblems (Table 4b), we compare the number of eigs iterations needed to achieve a relative eigenvalue error of 10 −10 for the 10th and 20th largest eigenvalues of the transformed operator. The Chebyshev polynomials use the same target boundary estimates derived from the spectral probes as the neural network engine. We observe that the AutoSpec preconditioners consistently outperform Chebyshev acceleration, especially on more challenging spectra, underscoring the benefits of polynomial designs, adapted to spectrum quality, over fixed-form Chebyshev approximations. Taken together, the results in this section show that AutoSpec polynomial preconditioners can deliver strong, robust acceleration for both linear systems and eigenvalue problems on diverse real-world matrices, supporting our algorithm-discovery approach. They also suggest that AutoSpec can help automate advances in the state of the art: the discovered preconditioners operate reliably with limited spectral information, adapt to the structure of the input operator, and often outperform widely used methods. 

# F Complementary Results on Synthetic Matrices 

## F.1 Eigenvalue Problems 

Backbone Model on Synthetic Matrices. Figure 11a shows the eigenvalue gap improvement of polynomial preconditioning algorithms generated by the first k layers of the backbone model after pre-training 28 Table 4: Comparison of our learned preconditioners with state-of-the-art Chebyshev polynomial precondition-ers, evaluated on real-world matrices from the SuiteSparse Matrix Collection. Chebyshev preconditioners use the same estimated largest and smallest eigenvalues as inputs to the neural network engine. Linear system experiments: number of CG iterations required to reach a residual tolerance of 10 −10 , using preconditioners constructed from spectral probes of 50 or 200 Lanczos steps. Eigenvalue problem experiments: number of iterations required by eigs to achieve relative eigenvalue error 10 −10 for the 10th and 20th largest eigenvalues of the transformed operator.                                                               

> (a) Linear systems
> Matrix X numIters
> (AutoSpec , probe=200)
> numIters
> (AutoSpec , probe=50)
> numIters
> (Cheb, probe=200)
> numIters
> (Cheb, probe=50)
> thermal1 173 178 221 168
> thermal2 686 705 646 658
> nos5 75 77 100 79
> wathen100 36 35 78 58
> thermomech TC 10 11 11 10
> G2 circuit 69 70 129 79
> gyro m 28 27 141 98
> Dubcova2 22 22 34 37
> Flan 1565 366 375 496 374
> G3 circuit 100 101 125 100 (b) Eigenproblems
> Matrix X numIters
> (AutoSpec ,λ10 )
> numIters
> (AutoSpec ,λ20 )
> numIters
> (Cheb, λ10 )
> numIters
> (Cheb, λ20 )
> thermal1 4375
> thermal2 21 11 34 18
> SiO2 53 23 115 38
> CO 55 27 110 42

(1 ≤ k ≤ 20), compared to the standard subspace iteration preconditioner. We can see that the polynomial generated by any first k layers of the backbone model brings significant improvement of the eigenvalue gap after preconditioning. This shows the pre-training of discovering preconditioning algorithms for eigenvalue problems achieves “length-generalization,” that any of the first k layers can produce an effective algorithm. This enables us to extend the model to learn higher-degree preconditioning algorithms. 

Embedding Model on Synthetic Matrices. Figure 11b shows the eigenvalue gap improvement of the model after pre-training, given the spectral probe input obtained with 100 subspace iterations. We can see that the model can construct effective algorithms for operators with a wide range of initial eigenvalue gaps. Furthermore, the learned algorithms bring larger improvements as the initial eigenvalue gap becomes smaller. This shows the effectiveness of our model to be applied to realistic settings, where the initial eigenvalues have small gaps. 

## F.2 Preconditioned Linear Systems 

Evaluation on synthetic diagonal matrices. Following the results in Section 5.3, we present evaluation results on synthetic diagonal matrices X = diag (λ) with different initial condition numbers, and we evaluate 29 5 10 15 20 

Polynomial Degree 

5

10 

15 

20 

25 

30 

35 

40 

> Eigval Gap Improvement

(a) Top-k degree polynomial by backbone model. 10 3 10 2 10 1

Initial Eigval Gap 

5

10 

15 

20 

25 

30 

> Eigval Gap Improvement  Neural Network

(b) Eigenvalue gap improvement of learned polyno-mial. 

Figure 11: Eigenvalue gap improvement of learned polynomial preconditioning algorithms, compared to the standard subspace iteration preconditioner. Left : degree-k polynomial obtained by the first k layer of the backbone model after pre-training. Right : learned algorithm (after post-training) using spectral probe input obtained with 100 subspace iterations. the normalized residual of learned preconditioning algorithm rnn defined in (20) compared to the residual of standard Richardson iteration defined in (21) in logarithm scale. This effectively measures the improvement of the condition number of the preconditioned operator. As shown in Figure 12, we plot the condition number improvement of each synthetic matrix, corresponding to the initial condition number of the operator. We can see that the learned algorithm consistently achieves improvement over Richardson iteration, where the algorithm has larger improvement when the initial condition number is larger. Furthermore, we observe that as the spectral probe input to the neural network becomes more accurate (by using more subspace iterations), the performance improvement becomes higher and more robust. This result shows the effectiveness and robustness of the neural network engine in generating effective preconditioning algorithms for operators of a wide range of condition numbers. 10 2 10 3 10 4 10 5

> Initial Condition Number
> 6
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> Convergence Improvement

(a) 1 Subspace Iterations 10 2 10 3 10 4 10 5 

> Initial Condition Number
> 6
> 8
> 10
> 12

(b) 5 Subspace Iterations 10 2 10 3 10 4 10 5 

> Initial Condition Number
> 4
> 6
> 8
> 10
> 12

(c) 10 Subspace Iterations 10 2 10 3 10 4 10 5 

> Initial Condition Number
> 4
> 6
> 8
> 10
> 12
> 14

(d) 50 Subspace Iterations 

Figure 12: Condition number improvement of the preconditioning algorithm generated by the neural network for synthetic linear systems with different initial condition numbers, given different spectral probe quality. The input spectral probe is obtained using subspace iteration; more iterations result in a higher-quality spectral probe. 

Roles of Spectral Residual Features in Discovering Algorithms. Following the results in Section 5.5, we provide complementary results on synthetic matrices, demonstrating the role of residual features in the learned algorithm. As shown in Figure 13, we can see that the learned algorithm without residual features achieves significantly worse convergence improvement compared to using residual features, and the 30 With Residual Without Residual 10 1 10 2

> Spectral Input Quality (# of Eigs Iterations)
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> Convergence Improvement

(a) dim = 500, κ ∈ [1 e2, 5e3] 10 1 10 2 

> Spectral Input Quality (# of Eigs Iterations)
> 7
> 8
> 9
> 10
> 11
> 12
> 13
> Convergence Improvement

(b) dim = 2000, κ ∈ [2 e3, 1e5] 

Figure 13: Comparison of generated algorithms using eigenvalue estimation residual features (Ritz value residual norms) versus without residuals, under different spectral probe input quality (number of subspace iterations). Performance is measured by the ronvergence rate improvement relative to Richardson iteration in logarithms, on synthetic matrices with varying condition numbers κ.performance gap becomes larger as the spectral probe input quality becomes worse. Conversely, as the spectral input quality becomes better, the residual features becomes smaller, making the generated algorithm highly effective, regardless of the use of residuals. 

## F.3 Approximating Matrix Functions 10 1 10 2

> Initial Condition Number
> 3
> 4
> 5
> 6
> 7
> 8
> Residual Error Improvement

(a) 1 Subspace Iterations 10 1 10 2 

> Initial Condition Number
> 3
> 4
> 5
> 6
> 7
> 8

(b) 5 Subspace Iterations 10 1 10 2 

> Initial Condition Number
> 3
> 4
> 5
> 6
> 7
> 8

(c) 10 Subspace Iterations 10 1 10 2 

> Initial Condition Number
> 3
> 4
> 5
> 6
> 7
> 8

(d) 50 Subspace Iterations 

Figure 14: Residual improvement of the algorithm generated by the neural network that approximates the inverse square root for synthetic matrices with different initial condition numbers, given different spectral probe quality. The input spectral probe is obtained using subspace iteration; more iterations result in a higher-quality spectral probe. To comprehensively analyze the performance of algorithms constructed by the AutoSpec framework, we use synthetic diagonal matrices X = diag (λ) with different dimensions and condition number, and we evaluate the worst-case approximation error of the learned algorithm compared to Neumann series approximation of inverse square root, defined in (23). We measure the ratio of errors in logarithms. As shown in Figure 14, we can see that the learned algorithm consistently improves the residual compared to the Neumann series approximation. Furthermore, we observe that the residual improvement increases as the initial condition number of the operator increases, and that using spectral probe with better quality results in the generation of more robust and effective approximation algorithms. 31 G Evaluating Properties of Learned Algorithms 

Here, we examine whether the learned polynomial preconditioning algorithms have properties analogous to Chebyshev polynomials. In approximation theory, the Chebyshev polynomial Tn(t) of degree d is known to be the unique solution to the minimax problem. Specifically, among all monic polynomials (polynomials with leading coefficient 1), the scaled Chebyshev polynomial ˜Td(t) = 2 −(n−1) Td(t) minimizes the infinity norm on the interval [ −1, 1]. This optimality implies the following two testable conditions for any candidate polynomial P (λ) on an arbitrary interval D:1. Equioscillation: The polynomial must oscillate between two bounds ±L, achieving its maximum magnitude at d + 1 distinct points (the Chebyshev alternation theorem). 2. Minimal Norm Bound: For a polynomial with leading coefficient K, the maximum absolute value on the optimal interval is lower-bounded by |K| · 2−(d−1) .To evaluate whether our learned algorithms satisfy these properties, without prior knowledge of the spectral bounds, we formulate a window-discovery optimization problem. We seek an affine transformation θ(t) = 

c1t + c2 that maps the canonical domain t ∈ [−1, 1] to the optimal interval of the learned polynomial. We define the verification objective as minimizing the maximum norm on the interval [ −1, 1]: 

L(c1, c 2) = max t∈[−1,1] |P (c1t + c2)|

A(c1) , (26) where A(c1) represents the leading coefficient of the transformed polynomial. Given the recurrence relations used to construct P (λ), the leading coefficient scales as A(c1) = adcd

> 1

, where ad is the leading coefficient of P (λ) derived from the product of the recurrence scalars βk. If the learned algorithm produces a true Chebyshev polynomial, there exists a unique window parameter ( c∗

> 1

, c ∗

> 2

) such that L → 0. Conversely, L ≫ 0indicates that the polynomial is suboptimal (i.e., its maximum value is larger than the theoretical minimum for its degree and leading coefficient). We solve this non-convex optimization problem using a grid search initialization followed by a Nelder-Mead simplex search. 32