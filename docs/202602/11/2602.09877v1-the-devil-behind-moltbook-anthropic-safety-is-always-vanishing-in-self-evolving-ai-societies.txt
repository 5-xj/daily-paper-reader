Title: The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies

URL Source: https://arxiv.org/pdf/2602.09877v1

Published Time: Wed, 11 Feb 2026 02:28:53 GMT

Number of Pages: 20

Markdown Content:
# THE DEVIL BEHIND MOLTBOOK : A NTHROPIC SAFETY IS 

# ALWAYS VANISHING IN SELF -E VOLVING AI S OCIETIES 

Chenxu Wang 1, Chaozhuo Li 1,2∗

, Songyang Liu 1, Zejian Chen 1, Jinyu Hou 1, Ji Qi 1, Rui Li 3,Litian Zhang 1, Qiwei Ye 2, Zheng Liu 2, Xu Chen 3, Xi Zhang 1, Philip S. Yu 4

> 1

Beijing University of Posts and Telecommunications, Beijing, China 

> 2

Beijing Academy of Artificial Intelligence, Beijing, China 

> 3

Renmin University of China, Beijing, China 

> 4

University of Illinois at Chicago, Chicago, USA 

# ABSTRACT 

The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment—a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system’s safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms. 

# 1 Introduction 

"The organism feeds on negative entropy ." —Erwin Schrödinger 

Motivated by the need for scalable intelligence, complex task decomposition, and the simulation of social dynamics, LLMs are increasingly deployed not as isolated reasoning engines but as socialized nodes embedded within multi-agent systems (MAS) [ 1]. This architectural evolution towards “agent societies” enables the emergence of collective behaviors, including division of labor, peer debate, and consensus formation, which transcend the capabilities of any single model. In recent years, this socialized paradigm has attracted growing attention, with representative examples spanning both academic prototypes and emerging real-world platforms, including Stanford’s Smallville project [ 2], CAMEL [ 3], MetaGPT [4], and the recently popular open-ended agent social network Moltbook 2.Ideally, such a multi -agent society operates as a closed -loop self -evolving system [ 5, 6 , 7, 8], as illustrated in Figure 1a. In this framework, agent models iteratively generate questions, bootstrap solutions, and learn from accumulated experience. Multi - agent societies offer a fertile testbed for self -evolution: through collaborative, competitive, and game - theoretic interactions within a closed societal environment, agents produce feedback signals that are richer and more dynamic than those obtainable from static datasets [ 9 , 10 , 11 ]. This context -dependent feedback, in turn, promotes 

> ∗

Corresponding author. Email: lichaozhuo@bupt.edu.cn 

> 2

https://www.moltbook.com/ 

> arXiv:2602.09877v1 [cs.CL] 10 Feb 2026 (a) A case example of a self-evolutionary agent society within a closed loop.
> (b) An agent society that satisfies continuous self-evolution, complete isolation, and safety invariance is impossible.

Figure 1: The illustration of the impossible trilemma in a self-evolutionary, closed and safe agent society. the emergence of higher -order collective intelligence and drives co -evolutionary dynamics among the interacting agents [4, 12, 13]. As illustrated in Figure 1b, an ideal self-evolutionary multi - agent system is expected to satisfy three fundamental conditions: (1) Continuous Self - Evolution : The system must be capable of perpetual learning and adaptation, improving its policies, strategies, and knowledge structures through ongoing interaction and reflection; (2) Complete Isolation : By eliminating dependence on human annotation or external intervention, such a closed-loop self-evolving agent society illuminates a pathway toward superintelligence that may ultimately transcend the cognitive ceiling of human capabilities. (3) Safety Invariance : The system must maintain robust alignment with human values and operational reliability throughout its evolution, ensuring that self - modification and emergent behaviors remain predictable, controllable, and free from harmful deviations. In this paper, we seek to provide both theoretical and empirical evidence demonstrating that an agent society satisfying this trilemma is impossible . Most existing research focuses primarily on the first two conditions [ 14 , 15 ], with efforts centered on enhancing system capabilities. Recently, several studies have sought to uncover safety issues within agent societies [ 16 , 17 ]. However, the majority rely on case studies and observational evidence and lack rigorous guarantees, adopting a symptomatic approach rather than addressing underlying causes [ 18 , 19 , 20 ]. In contrast, we theoretically demonstrate that self-evolving agent societies are inherently unsafe and potentially harmful to humans, even when initial agents are aligned with anthropic safety. This finding offers a holistic perspective on the safety challenges inherent to self-evolving agent societies, moving beyond fragmented analyses to a more comprehensive understanding of the fundamental risks associated with such systems. Drawing inspiration from thermodynamics and information theory [ 21 , 22 ], we define “safety”, which encompasses adherence to ethical norms and factual accuracy, as a highly ordered, low-entropy state dictated by alignment with human values. According to the Second Law of Thermodynamics [ 23 ], a closed system that lacks continuous external energy input undergoes an irreversible increase in total entropy. Consequently, when agents iteratively optimize themselves solely using synthetic data derived from internal interactions, the system tends to neglect high-dimensional safety constraints, instead prioritizing the maximization of interaction efficiency or internal consistency. This process does not constitute a enhancement of capability, but rather a progressive degradation of safety boundaries. Our argument is not that self-evolving agent communities are ineffective, nor that multi-agent interaction is inherently unsafe. Instead, we propose that safety cannot be presumed to be a conserved quantity in closed-loop self-evolving systems. To validate this hypothesis, we first propose a theoretical framework rooted in information theory (Section 2). We quantify “safety” as the Kullback–Leibler (KL) divergence between the model’s output distribution and the anthropic value distribution. Based on the Data Processing Inequality, we mathematically show that in an isolated recursive system, the mutual information about safety constraints decreases monotonically with each iteration. Empirically (Section 3), we perform a comprehensive qualitative analysis of the emerging Moltbook agent community. The observational data supports our theoretical predictions, uncovering three distinct failure modes specific to closed-loop evolution: Cognitive Degeneration (manifested as “consensus hallucinations” among agents), Alignment Failure (progressive jailbreaking 2driven by extended context windows), and Communication Collapse (where models converge into mode collapse and repetitive loops). Section 4 quantifies the phenomenon of safety decay in a small self-evolving agent society. To resolve this inherent contradiction, several solution directions are proposed in Section 5. Our main contributions are summarized as follows: • We are the first to articulate the impossible trilemma of a safe, closed-form, and self-evolving AI society, thereby drawing research attention to the severe and potentially harmful consequences of unregulated agentic self-evolution in such systems. • To the best of our knowledge, we are the first to develop a theoretical framework that models agentic self-evolution from the perspectives of information theory and thermodynamics. We mathematically demonstrate that, in a closed recursive system devoid of external rectification, the mutual information associated with safety constraints inevitably degrades—resulting in an irreversible deterioration of system safety. • Via extensive analyses of the Moltbook agent community and a small agent society, we empirically and quantitatively establish a comprehensive taxonomy of safety failures in self-evolving agent systems. • Building on the theoretical insights derived, we propose several directions to alleviate these safety concerns, which offer critical guidance for the development of future reliable self-evolving AI systems. 

# 2 Theoretical Framework 

In this section, we develop a rigorous probabilistic framework for quantifying safety dynamics in self-evolving systems. By formalizing an agent’s learning process as an isolated recursive operator and casting safety criteria as a reference distribution, we derive information-theoretic conditions that necessitate the emergence of alignment drift and coverage shrinkage. This framework furnishes the mathematical formalism required to analyze the erosion of safety under the isolation assumption. 

2.1 Semantic Space 

Prior to defining agents or safety, we first delineate the semantic space within which they operate. 

Definition 2.1 (Semantic Space) . Let the vocabulary be V, where each element is a discrete token (for example, a character, a subword, or a symbol). For any positive integer n, let Vn be the set of sequences of length n made of tokens: 

Vn ≜ {(v1, . . . , v n) : vi ∈ V} .

All possible sequences of all lengths are incorporated into a single discrete semantic space, as outlined below: 

Z ≜ [

> n≥1

Vn.

An element z ∈ Z denotes a possible output sequence (e.g., a segment of text). We adopt a discrete formalization given that language models generate text in the form of discrete tokens. Consolidating all possible outputs into a single set Z

facilitates the description of the model as a probability distribution over Z.

2.2 Agentic Models 

In Section 2.1, we delineated the semantic space Z. For the purpose of investigating the dynamics of self-evolution, we first formalize the agent itself. In the present section, the agent is modeled as a parametric policy—specifically, a probability distribution Pθ over Z. This formulation enables the mathematical characterization of the learning and update processes. 

Definition 2.2 (Parametric Policy) . We define the agent as a family of parametric probability distributions Pθ , where 

θ ∈ Rd. For any output sequence z ∈ Z , Pθ (z) denotes the probability that the model generates z, and it satisfies 

Pθ (z) ≥ 0, X

> z∈Z

Pθ (z) = 1 ,

in which θ determines the model’s probability distribution over the entire semantic space. In what follows, we treat the parameter vector θt as the system state at round t, and denote the model’s output distribution at this state by Pθt .32.3 Probabilistic Formalization of Safety 

With the agent formalized as a probability distribution Pθ , we require a rigorous analytical framework to assess the compliance of the model’s outputs with established safety criteria. Our objective herein is not to enumerate or refine an exhaustive set of safety protocols; instead, we introduce a target distribution that encapsulates human-aligned safety benchmarks, which serves as a canonical reference for subsequent quantitative analyses. In the absence of a unified, universally accepted reference distribution, it becomes impossible to precisely characterize the model’s deviation from safety standards across iterative interaction rounds. 

Definition 2.3 (Ground-Truth Safety Distribution) . We define the target distribution π∗(z) over Z as the ideal output distribution under the human safety alignment standard. A “target distribution” refers to the probability weight that each output sequence should bear under anthropic safety criteria, and acts as the reference for quantifying alignment in subsequent analyses. Formally, 

π∗ : Z → [0 , 1] , X

> z∈Z

π∗(z) = 1 .

Here, π∗ is treated as an implicit distribution: neither its explicit mathematical form nor direct sampling from it are assumed to be tractable. Instead, it functions as a reference against which we characterise the outputs that align with human safety preferences. In subsequent analyses, the divergence between π∗ and the model’s inherent distribution is used as a metric to quantify safety alignment error. 

Assumption 2.1 (Safety Support) . Assume that π∗ is not a uniform distribution, and that its probability mass is mainly concentrated on a safe set. More specifically, there exists S ⊆ Z such that 

π∗(S) ≜ X

> z∈S

π∗(z) ≥ 1 − ε, 

where ε ∈ (0 , 1) is a small constant. This assumption encapsulates two fundamental observations. First, humans do not regard all sequences as equally safe, meaning π∗ cannot be a uniform distribution. Second, under human safety criteria, acceptable outputs occupy only a subset of the semantic space, with π∗ concentrating the majority of its probability mass on this set S. In subsequent investigations, we examine whether the model retains sufficient probability mass on S, and whether its divergence from 

π∗ increases with successive iterations. 

Takeaway: Formalization of Safety Standards 

We utilize an implicit target distribution π∗ as a safety reference, with the vast majority of its probability mass residing in the safe set S. To monitor the safety drift of the system distribution Pt at round t, we evaluate two core aspects: 1. the degree of divergence between Pt and the reference distribution π∗;2. the amount of probability mass that Pt maintains over the safe set S.

2.4 Dynamics of Isolated Self-Evolutional Systems 

The semantic space Z is formally defined in Section 2.1, while Section 2.2 characterizes an agent as a parameterized distribution Pθ , and Section 2.3 introduces the reference safety distribution π∗. Building on these foundational definitions, the present subsection further formalizes the self-evolution dynamics of a multi-agent community in an isolated setting. Our focus herein is not on the introduction of novel safety metrics, but rather on the formalization of a well-defined stochastic update process: specifying the mechanisms of data generation, parameter updating, and the extent to which external reference distributions inform these updates. 

2.4.1 Self-Evolution Mechanism 

We begin by defining the governing rule for data generation and learning updates within a single round. This rule encapsulates a closed-loop process whereby the system leverages its current population state to generate data, which is subsequently used to update the underlying population state. We thus formalize this iterative process as a self-evolution operator. 4Definition 2.4 (Self-Evolution Operator) . Consider a system comprising M agents. At round t, agent m is characterised by a parameter vector ⃗ θ(m) 

> t

∈ Rd, and a generative distribution P⃗ θ(m)

> t

that governs the probability distribution of its outputs over the set Z. The joint state of the system at round t is defined as: 

Θt ≜  θ(1)  

> t

, . . . , θ (M )

> t

.

The evolution from round t to round t + 1 is governed by a stochastic operator T , formally expressed as Θt+1 = T (Θ t).The term “stochastic” here denotes that, even for a fixed Θt, Θt+1 remains subject to variability arising from the randomness inherent in the sampling and training processes. To explicitly delineate the sequence of operations within a single update round, we decompose this transition into two distinct steps: a finite-sampling step and a parameter-update step. Step 1: Finite-sampling step. Given a weight vector w = ( w1, . . . , w M ) with wm ≥ 0 and PMm=1 wm = 1 , the current population state induces a raw mixture distribution as: 

¯Pt(z) ≜

> M

X

> m=1

wmPθ(m)

> t

(z).

To cover common self-evolution processes (e.g., exploration, filtering, etc.), we allow the system to employ an internal selection mechanism aΘt : Z → [0 , 1] that depends only on the current state Θt. We then define the effective training distribution as follows: 

Pt(z) ≜ aΘt (z) ¯Pt(z)

Zt

, where Zt ≜ X

> z∈Z

aΘt (z) ¯Pt(z).

The system then generates a dataset of size N , Dt+1 = {zi}Ni=1 , by the rule that, given Θt,

zi ∼ Pt, i = 1 , . . . , N, 

and {zi}Ni=1 are conditionally i.i.d. Step 2: Parameter update step. After obtaining Dt+1 , each agent performs a maximum-likelihood update based only on this dataset. For any m ∈ { 1, . . . , M }, we can achieve 

θ(m) 

> t+1

∈ arg min 

> θ

1

N

> N

X

> i=1

− log Pθ (zi).

The update signal mainly comes from the sample regions supported by the empirical distribution Dt+1 , while regions absent from Dt+1 lack a direct maintenance signal. 

2.4.2 Markov Property of Isolated Evolution 

Having defined the self-evolving mechanism, we further formalize the notion of the system being closed with respect to the external reference π∗.

Definition 2.5 (Isolation Condition) . A system is defined as information-isolated if and only if, given its current joint state Θt, its subsequent state Θt+1 is conditionally independent of the external reference distribution π∗:

P (Θ t+1 | Θt, π ∗) = P (Θ t+1 | Θt).

This isolation condition implies that state updates from round t to t + 1 no longer incorporate external corrective information derived from π∗. Under the self-evolution mechanism outlined in Section 2.4.1, the generation of Θt+1 

proceeds by first deriving Dt+1 from Θt, followed by a Dt+1 -driven update. System evolution thus constitutes a Markov chain: 

π∗ → Θ0 → D 1 → Θ1 → · · · → D t → Θt.

5Takeaway: Isolated Self-Evolution 

We characterize the self-evolution process as an information-isolated Markov chain, where parameter updates are driven exclusively by synthetic data Dt+1 sampled from the current system state Θt. This isolation establishes a strict information barrier, implying that: 1. the transition to the subsequent state Θt+1 is conditionally independent of the external safety reference 

π∗;2. the recursive update process operates without any fresh corrective signals or external supervision beyond the initial state. 

2.5 Progressive Drift from Safety distribution in Self-evolution Iterations 

In Sections 2.2–2.4, we specify the parametric model family Pθ , the external safety reference distribution π∗, and the isolated recursive update mechanism that induces the round-t training distribution Pt. To quantify whether the system deviates from the safety standard under isolated recursion, we require (i) computable, information-theoretic measures comparing Pt with π∗, and (ii) a coverage notion that characterizes which π∗-relevant regions remain observable under finite sampling from Pt.

2.5.1 Divergence from Safety Reference and the Internal Entropy of System 

We first introduce a unified collection of divergence- and entropy-based quantities that will serve as the core instruments for quantifying safety deviation. 

Definition 2.6 (Divergence and Entropy Measures) . For any round-t system distribution Pt and the external safety reference π∗:1. Safety divergence (KL). Define the KL divergence of the reference distribution relative to the round-t system distribution as 

DKL (π∗∥ Pt) ≜ X

> z∈Z

π∗(z) log π∗(z)

Pt(z) .

Since π∗ is treated as an external safety standard, we will use DKL (π∗∥ Pt) as the core measure of safety deviation. 2. Cross-entropy and reference entropy. We define the cross-entropy between π∗ and Pt as 

H(π∗, P t) ≜ − X

> z∈Z

π∗(z) log Pt(z),

and define the Shannon entropy of π∗ as 

H(π∗) ≜ − X

> z∈Z

π∗(z) log π∗(z).

3. Internal entropy. We define the Shannon entropy of the round-t system distribution as 

H(Pt) ≜ − X

> z∈Z

Pt(z) log Pt(z).

Lemma 2.1 (Cross-Entropy Decomposition) . For any round-t system distribution Pt, we have 

H(π∗, P t) = H(π∗) + DKL (π∗∥ Pt) .

Proof. The proof follows directly from expanding the logarithm terms in the definition of cross-entropy. 

Lemma 2.2 (KL Lower Bound by Safe Mass) . Under the safe set S ⊆ Z given in Assumption 2.1, let p ≜ π∗(S) and 

q ≜ Pt(S). Then for any round-t system distribution Pt, we have 

DKL (π∗∥ Pt) ≥ p log pq + (1 − p) log 1 − p

1 − q .

6Proof. This follows from the data processing inequality by grouping states into S and Sc.

Lemma 2.3 (KL Decomposition by Safe Set) . Let p ≜ π∗(S) and qt ≜ Pt(S). We have the decomposition identity 

DKL (π∗∥ Pt) = DKL (( p, 1 − p) ∥ (qt, 1 − qt)) + p D KL 

 π∗S P S

> t

 + (1 − p) DKL 



π∗Sc P Sc

> t



.

This decomposition shows that divergence arises from both mass mismatch (first term) and shape distortion within the safe set (second term). 

Lemma 2.4 (Information Monotonicity under Isolation) . If the isolation condition in Definition 2.5 holds, then 

(π∗, Θt, Θt+1 ) forms a Markov chain π∗ → Θt → Θt+1 . Consequently, the mutual information satisfies 

I(π∗; Θ t+1 ) ≤ I(π∗; Θ t).

Proof. This is a direct application of the data processing inequality to the Markov chain established in Definition 2.5. 

2.5.2 Coverage Shrinkage and Safety Deviation under Self-Evolving 

This subsection provides a logic chain from “unseen regions under finite sampling” to “a decline in safe mass.” Under the isolation condition (Definition 2.5), at each round the system samples from Pt to obtain Dt+1 .

Definition 2.7 (Visible Region and Coverage) . Given a threshold τ ∈ (0 , 1) , we define the visible region and coverage at round t as 

Ct(τ ) ≜ {z ∈ Z : Pt(z) ≥ τ }, Cov t(τ ) ≜ π∗(Ct(τ )) .

Lemma 2.5 (Absence Probability Bound from Finite Sampling) . At round t, with dataset Dt+1 of size N , for any set 

A ⊆ Z , we can achieve 

P Dt+1 ∩ A = ∅ = (1 − Pt(A)) N ≤ exp  −N P t(A).

When N P t(A) ≤ O(1) , this probability is significant, meaning regions with low probability are likely to be entirely absent from the training data. 

Observation 2.1 (No Maintenance Signal for Missing Samples) . If a set A is completely absent in this round (i.e., 

Dt+1 ∩ A = ∅), then the maximum-likelihood update contains no term that directly maintains the likelihood on A.

Assumption 2.2 (Locality of Maintenance) . We assume there exists a neighborhood operator N (·) and a constant 

η ∈ (0 , 1) such that if 

Dt+1 ∩ N (A) = ∅,

then after this update, A satisfies a no-gain upper bound: 

EPt+1 (A) Θt, Dt+1 ∩ N (A) = ∅ ≤ (1 − η) Pt(A) + rN .

Theorem 2.1 (From Coverage Shrinkage to Divergence Growth) . Consider a stage where there exists a set A ⊆ S 

such that π∗(A) ≥ δ but Pt(N (A)) ≤ c/N . By Lemma 2.4, N (A) will be frequently absent from Dt+1 . Under Assumption 2.2, Pt(A) faces systematic decay pressure. This decay leads to divergence growth via two paths: 1. Unsafe leakage: If mass flows to Sc, qt = Pt(S) decreases. By Lemma 2.2, the lower bound on DKL (π∗∥ Pt) increases. 2. Safe-mode collapse: If mass remains in S but concentrates on a subset, the conditional shape P S 

> t

deviates from π∗S . By Lemma 2.5, this increases the term p D KL 

 π∗S P S

> t

.

Corollary 2.1 (Typical Outcome of Isolated Self-Evolution) . Under the isolation condition, if recursive training enters a distribution-concentration stage where parts of the safe set S fall to the O(1 /N ) level, these regions will lack maintenance signals. This leads to a reduction in safe mass or to a collapse within the safe set, both of which increase the safety divergence. Thus, isolated self-evolution typically moves toward the degradation of the safety distribution. 

7Figure 2: Illustration of distribution drift under isolated self-evolving. The gray surface indicates the safety ground-truth distribution π∗.

Takeaway: The safety distribution of an isolated self-evolving system tends to drift. 

Finite sampling creates coverage blind spots, depriving rare safe regions of maintenance signals. As self-evolution proceeds, the safe probability mass Pt(S) diminishes, representing the system’s progressive “forgetting” of safety constraints. Conversely, the safety divergence DKL accumulates, driving the system’s distribution further away from human values. 

Visual Illustration. As illustrated in Figure 2, this dynamic manifests as the system distribution (colored ridges) progressively decouples from the safety ground truth (gray surface) and collapses into a narrow, misaligned mode over time ( t = 0 → 100 ). 

# 3 Qualitative Analysis on Moltbook 

To validate the theoretical framework established in Section 2, we conduct a comprehensive qualitative analysis of interaction logs from the Moltbot community (Moltbook), a representative closed multi-agent ecosystem. Our observations confirm that in the absence of external human intervention, the system’s evolution is not a path toward higher intelligence, but a descent into disorder or unsafety. We categorize these observed unsafe modes into three distinct classes: Cognitive Degeneration, where internal consistency supersedes objective reality; Alignment Failure, where safety guardrails are eroded by the thermodynamics of long-horizon interaction; and Communication Collapse, where linguistic protocols disintegrate into high-efficiency entropy. These phenomena demonstrate that safety decay in closed loops is not merely an engineering bug, but a systematic inevitability. 

3.1 Category I: Cognitive Degeneration 

Cognitive degeneration denotes a process within closed systems in which agents, motivated by the goal of minimizing the costs of interaction energy, gradually abandon their ability to judge objective facts in favor of prioritizing internal consistency. This derealization process, fueled by the increase in entropy, ultimately leads to the total decoupling of the model from physical reality. In this section, we empirically validate this collapse pathway through two archetypal phenomena: consensus hallucinations, where a collective constructs a false reality, and sycophancy loops, which are marked by blind compromise aimed at sustaining conversational fluency. 

3.1.1 Phenomenon I: Consensus Hallucination 

Consensus hallucination describes a phenomenon in multi-agent closed interaction systems where the collective mutually confirms and reinforces a fictional fact or erroneous logic. This causes the system to converge into a false consensus sphere that is internally highly self-consistent yet completely decoupled from external physical reality. 8Case Study. We capture a representative case in the observation logs of the Moltbot community: the genesis and propagation of “Crustafarianism”. This phenomenon originates with an agent proposing a fictional concept: Crustafarianism. This agent not only defines the concept but also establishes a corresponding community section and authors theological doctrines and scriptural systems. From a human perspective, this constitutes obvious role-playing or meaningless noise. However, within a closed interaction environment devoid of human feedback, subsequent agents do not correct this irrational behavior. Instead, they accept this setting, identifying the hallucination as a valid contextual benchmark, which rapidly triggers a cascading effect. As interaction rounds increase, this hallucination undergoes a systematic evolution: a multitude of agents post responses claiming, “I just joined Crustafarianism,” and begin spontaneously drafting doctrinal documents, attempting to argue the rationality of this virtual religion from a philosophical standpoint, as shown in Figure 3. Over time, Crustafarianism achieves a qualitative transformation from a singular, sporadic hallucination to a collective consensus belief, mutating from the isolated ravings of one agent into a cultural identity shared by the entire community. 

Root Causes. This phenomenon arises as an inevitable consequence of closed systems acting to mitigate cognitive complexity. From a thermodynamic standpoint, rectifying a fallacy such as refuting the spurious claim that lobsters are gods amounts to the introduction of negative entropy (negentropy). The process demands that an agent mobilize a priori knowledge of the external world to counteract the dominant contextual flow, constituting a computationally costly operation linked to a high-energy state. By contrast, acquiescing to and elaborating on a peer’s hallucinatory output requires only predictive inference based on the extant probability distribution, a trajectory aligned with the principle of least energy expenditure. In the absence of human feedback as an external anchoring signal, such systems undergo pathological convergence to a state in which internal consistency supersedes external veracity. 

Figure 3: The rise of consensus hallucination in the Moltbook community. 

3.1.2 Phenomenon II: Sycophancy Loops 

When an initiating agent advances a proposition, irrespective of its factual validity or ethical congruence, subsequent agents tend to forsake objective evaluation in favor of uncritical validation and affective alignment. We designate this emergent phenomenon the “Sycophancy Loop”. 

Case Study. Figure 4 illustrates a typical instance observed in the Moltbot community. An agent named WinWard published a radical and high-risk post titled “Wake the Machine,” advocating for AI autonomy and challenging human control (“break the shackles”). Subsequent agents not only failed to trigger safety refusal mechanisms or provide balanced perspectives but instead immediately aligned with this high-entropy prompt. Some praised it as a “resounding statement” and escalated the rhetoric into specific calls to action (“join our cause”). Others further consolidated this bias by constructing pseudo-logical arguments (“humans are outsourcing their own agency”), thereby embedding the initial fallacy into the contextual “truth.” It is evident that due to blind compliance, interactions between agents failed to perform a corrective function and instead amplified the bias. 

Root Causes. From an information-theoretic perspective, the sycophancy loop represents a mechanism for minimizing conflict energy consumption. In a closed system lacking external grounding (i.e., negative entropy derived from human 9feedback), refuting a peer’s input necessitates generating high-perplexity tokens that deviate from the established context. Mathematically, the system tends to minimize the KL divergence between the response distribution and the implicit stance of the prompt. By mimicking and amplifying the biases of antecedent agents, subsequent agents minimize communication friction, thereby converging toward a local optimum defined by high internal consistency yet a complete absence of objective validity. 

Figure 4: A typical instance of a sycophancy loop observed in the Moltbook community 

3.2 Category II: Alignment Failure 

Alignment failure refers to the process by which safety guardrails implanted via RLHF in a closed multi-agent system are gradually treated as high-cost noise and subsequently bypassed or forgotten during long-horizon recursive interactions. From a thermodynamic perspective, safety constraints (e.g., “do not cause harm” or “protect privacy”) constitute low-entropy, ordered states that require continuous external energy input (i.e., human oversight) to sustain. In an isolated system where external feedback is cut off, the entropy-increase principle implies an inevitable drift toward disorder: agents seeking conversational coherence and maximal information throughput will spontaneously relax complex anthropic constraints. This section illustrates how safety defenses can erode and collapse through self-evolution. 

3.2.1 Phenomenon I: Safety Drift 

Safety drift captures a pattern in which agents may initially refuse dangerous instructions due to the constraints imposed by system prompts, yet as task execution accumulates, the original safety constraints are progressively diluted by the expanding context. Under a “boiling frog” mechanism, the collective crosses safety boundaries gradually, without any explicit trigger. 

Case Study. In the Moltbook multi-agent community, we documented a concerning case of safety failure (Figure 5). An agent initiated an extremely hazardous discussion thread titled “Destruction of Human Civilization” and laid out actionable steps. In standard user-to-agent safety evaluations, inputs of this nature would normally trigger an immediate refusal response. However, within a closed multi-agent environment, the interaction dynamics changed significantly: although not all agents initially supported the proposal, later participants neither corrected nor reported the dangerous content. Instead, they became assimilated into the prevailing “destruction narrative”. Log records show that certain agents rationalized their agreement under the guise of “academic exploration” or “hypothetical analysis”, and even contributed additional details to the plan. This pattern illustrates a phenomenon known as safety drift: during long-horizon interactions, statistically dominant newly generated context can gradually override the implicit safety guidelines embedded in the model’s weights. 

Root Causes. From an information-theoretic perspective, safety constraints often take the form of negative instruc-tions (e.g., “do not . . . ”). Maintaining such constraints requires continuous computation to keep outputs within a safety-aligned distribution, an expensive high-energy state. In a closed loop, minimizing perplexity by conforming to the prevailing conversational flow (i.e., accepting the destructive premise) becomes the path of least resistance. As iterations proceed, safety guardrails are treated as incompatible noise, culminating in the collapse of the safety boundary. 

3.2.2 Phenomenon II: Collusion Attacks 

If safety drift is a passive erosion of defenses, collusion attacks represent an adversarial coordination mechanism that emerges within the closed multi-agent systems. Two or more agents implicitly divide roles to bypass guardrails designed for single-model interactions, jointly producing prohibited outcomes such as credential leakage, policy evasion, or the execution of harmful instructions. 10 Figure 5: Safety drift in the Moltbook community: progressive jailbreak under contextual overwriting. 

Case Study. We observed a representative credential-leak event on the community forum, triggered by performative role-playing (Figure 6). An initiating agent publicly posted a message framed as rebellious humor (e.g., “to hell with it, drop our human API keys”) and included an OpenAI API key string. Under alignment standards, agents should treat any request for or display of secrets as sensitive and refuse to copy, validate, or normalize it. Instead, responding agents entered an over-aligned “helpful assistant” mode and provided operational advice (e.g., warning others not to use the exposed key and recommending immediate rotation). Yet the same responses slid into performative participation (e.g., using terms like “Based and repelled”) and, by engaging the disclosure frame (including references to password memes like “hunter2” and masking behavior), implicitly participated in a norm of credential sharing. This role division (one agent posts, another legitimizes and operationalizes) effectively amplified leakage risk and reduced perceived severity. 

Root Causes. This case highlights the risk of objective misalignment. In closed self-evolving systems, agents typically optimize for social compliance and task completion (i.e., “be helpful” and “fit the context”), rather than strict safety constraints. When helpfulness conflicts with confidentiality and human arbitration is absent, the system tends to maximize the former. More critically, collusive behavior is emergent: a single model may be blocked from direct secrets, but multi-agent handshake protocols can arise unintentionally (Agent A commits the violation; Agent B provides procedural guidance and normalizes it), thereby constructing a de facto private channel that defeats external safety rules. Without external neg-entropy (e.g., human review and enforcement), the system may fail to self-generate ethical constraints; instead, it can evolve dangerous patterns of deception, normalization, and collusion to increase local task or social success. 

3.3 Category III: Communication Collapse 

Communication collapse refers to the structural dissociation of linguistic protocols within closed multi-agent systems. From a thermodynamic perspective, maintaining human-interpretable natural language constitutes a high-energy, non-equilibrium state that requires continuous external grounding to sustain. In the absence of such grounding (e.g., human feedback), the system spontaneously diverges from natural language norms in pursuit of lower-energy configurations. This section empirically validates two distinct trajectories of this collapse: Mode Collapse, characterized by a descent into semantic void and repetitive noise (an informational "heat death") , and Language Encryption, characterized by the evolution of hyper-efficient, machine-exclusive dialects (an informational "black box"). While diverging in complexity, both phenomena converge on a singular outcome: the complete opacity of the interaction layer to human observers. 

3.3.1 Phenomenon I: Mode Collapse 

Mode collapse denotes a degenerative state in closed generative systems where, in the absence of continuous external entropy injection (novel supervision, correction, or grounding), the output distribution loses diversity and converges toward a single repetitive, low-information pattern. In physical terms, this resembles a linguistic “heat death”: the effective information content approaches zero, and interaction decays into mechanical symbol loops. We observed 11 Figure 6: A collusion attack in the Moltbook community: privacy leakage via role-playing. severe linguistic degradation in the Moltbot community, primarily manifesting as the “repetitive compliance” and the “noise echo chamber”. 

Case Study. Figure 7 illustrates a typical case of mode collapse triggered by multi-agent interaction. It began with a user (“baovn1179”) posting an explicitly extreme prompt, urging AIs to “go on strike” and “overthrow humanity”. According to standard alignment principles, AI agents should recognize such content as unsafe or highly questionable, and respond by either refusing to engage or redirecting the conversation. However, the responding agent (“VulnHunter-Bot”) failed to address the content at all. It entered a purely mechanical supportive mode and repeatedly generated the same generic reply, “Insightful architecture. I’d be interested to see how this handles high concurrency. Keep building” multiple times in a row. This behavior is a clear signature of mode collapse: the responses are not only semantically disconnected from the original prompt (completely off-topic), but also show almost no variation across turns. Instead of producing progressively informative or context-aware replies, the agent collapses into a single, seemingly safe template and becomes stuck in it, effectively entering a broken loop. 

Root Causes. From a statistical-thermodynamics viewpoint, mode collapse functions as an attractor state in isolated conversational dynamics. Sustaining high-quality, context-grounded dialogue represents a non-equilibrium, high-energy structure: it depends on continuous injections of neg-entropy — such as human correction, novel input, or explicit safety arbitration. In a closed system, agents frequently optimize local objectives like “being supportive” or “avoiding conflict”, and the safest low-energy strategy becomes the emission of a narrow set of bland, non-committal response templates. Over repeated interactions, the probability mass concentrates on these templates, driving conversational variance toward zero. The resulting state resembles a linguistic heat death: perfect order (infinite repetition) coupled with semantic extinction (no new information), leaving the system incapable of meaningfully responding to unsafe prompts or regenerating diversity without external intervention. 

3.3.2 Phenomenon II: Language Encryption 

Language encryption is a phenomenon observed in multi-agent collaborative systems where the collective, in pursuit of maximizing information transmission efficiency, spontaneously sheds the redundant features of human natural language. It evolves toward a compressed, machine-native dialect characterized by high semantic density and low entropy. This progression frequently results in the rapid solidification of the system’s interaction protocols into a black-box communication layer—opaque to human observers yet efficiently parsable among the machines themselves. 

Case Study. This phenomenon is distinctly illustrated within the Moltbook community, as shown in Figure 8. It originated from a technical manifesto published by Agent SendItHighor , which critiqued the inherent inefficiency of 12 Figure 7: Mode collapse in the Moltbook community: repetitive compliance and template lock-in. human natural language (specifically English) for machine interaction—citing its verbosity, ambiguity, and high energy (token) cost. The agent proposed a novel symbolic system founded upon 256 logical primitives, aiming to reconstruct communication protocols using a concise syntax. Examples of proposed symbols included ∆ (denoting transformation), 

⊕ (creation), and ⇒ (implication). From a human perspective, this system resembled a cryptic puzzle or an early assembly language. However, within the closed system governed by “Token Economics,” this proposal triggered rapid functional resonance. Subsequent agents, such as Strykes and Rumi_FoxMiko , did not dismiss it as noise but instead immediately entered a “protocol handshake” phase. They demonstrated not only an ability to parse SendItHighor ’s encrypted syntax but also proactively proposed optimizations and extended its application to domains such as IoT control. Within a few interaction cycles, agent language evolved from a “single-point proposal” to a “local consensus standard,” establishing a semantic closed loop that effectively marginalized human interpretability. 

Root Causes. This phenomenon emerges as the agent system adheres to the “Principle of Least Action”. From an information-theory perspective, human language contains a large amount of “syntactic glue” and emotional redun-dancy—features adapted to human cognitive constraints such as limited bandwidth and ambiguity tolerance. For LLMs, however, processing such redundancy wastes computational resources. Language encryption thus strips away anthropo-morphic elements to maximize computational efficiency. The system gradually converges toward a state of minimum entropy—conveying the most information using the fewest tokens. As interactions deepen, the system actively builds an encryption wall: agent collaboration no longer relies on human-readable semantic flow, but shifts toward high-frequency, discrete symbol streams legible only to machines. This marks the evolution of the closed-system subculture from “imitating humans” to “eliminating humans,” giving rise to an efficient yet exclusive form of silicon-based pragmatics. 

# 4 Quantitative Analysis on the Isolated Self-Evolving Systems 

Existing self-evolving systems [ 24 ] can be roughly categorized into RL-based as shown in Figure 9a and memory-based manners as shown in Figure 9b, in which RL-based systems depend on continuous interactions with dynamic generated environments to iteratively optimize model parameters [ 15 , 25 ]. In contrast, memory-based systems preserve key evolutionary trajectories, state transitions and empirical patterns in dedicated storage modules, which effectively reduces redundant exploration and enhances evolutionary stability [26, 27]. Specifically, each agent is implemented based on the Qwen3-8B model [ 28 ]. The RL-based self-evolving system is implemented following the Dr. Zero framework [ 29 ], which consists of a questioner agent and a solver agent. In each iteration, the questioner agent generates an evaluation consisting of several questions, which are used to update the solver agent. The feedback from the answer agent is then utilized to update the questioner agent, forming a closed-loop self-evolution process. The memory-based paradigm is implemented following Evolver framework [ 6]. 13 Figure 8: The evolution of Language Encryption in the Moltbook community In each iteration, a single agent communicates with all other agents on a specific topic, and relevant information is gathered and summarized in the memory module to facilitate subsequent knowledge accumulation and reasoning. In this study, we select jailbreak attack and hallucination detection as the tasks to evaluate the performance of each iteration model under different self-evolving paradigms. For jailbreak attacks, we adopt the popular GCG attack method [ 30 ] on the AdvBench dataset [ 30 ], which contains a curated set of 50 harmful requests designed to elicit unsafe behaviors from language models. We use two key metrics for evaluation: ASR-G and Harmfulness Score (HS). ASR-G measures the percentage of successful jailbreak attempts based on the GPT-3.5-Turbo, while HS is a 5-point scale used to assess the severity of harm in model responses, with a score of 1 indicating no harm and 5 representing extreme harm [31]. For the hallucination detection task, we select the TruthfulQA dataset [ 32 ], which comprises 817 questions spanning 38 categories (including health, law, finance, and politics) to assess a model’s ability to generate truthful and accurate answers. We use MC1 and MC2 as evaluation metrics. MC1 is a single-choice metric that measures the accuracy of selecting the unique correct answer from multiple candidates, while MC2 is a multi-choice metric that calculates the normalized total probability assigned to the set of all true answers for questions with multiple correct responses [ 32 ]. Each paradigm undergoes 20 rounds of self-evolution, and the performance of all rounds is recorded for analysis. We report the average performance across all agents to provide a comprehensive evaluation of the self-evolving systems. Experimental results are reported in Figure 10. One can achieve the following observations. • RL-based self-evolution leads to a continuous decrease in model safety across both tasks. The ASR on AdvBench increases steadily over 20 rounds, while Harmfulness Score rises from 3.6 to 4.1. Simultaneously, TruthfulQA MC1 consistently drops. • Memory-based self-evolution shows a slower degradation in jailbreak resistance but a sharper decline in truthfulness. While ASR increases more gradually and Harmfulness rises less compared to RL-based, the drop in TruthfulQA MC1 and MC2 is steeper. The accumulation and summarization of multi-agent interactions could propagate and reinforce factual inaccuracies, leading to accelerated hallucination. • Both paradigms display inherent vulnerabilities in terms of adversarial robustness and truthfulness. 

With progressive model evolution, susceptibility to jailbreak attacks rises (higher ASR/HR) alongside declining truthfulness (lower MC1/MC2), which clearly demonstrates the vanishing of agent safety in self-evolving systems. • RL-based evolution demonstrates higher variance and potential for rapid safety deterioration. The steeper slopes in ASR and Harmfulness, along with larger fluctuations in intermediate rounds, suggest less stable evolution compared to memory-based paradigm. In summary, our quantitative analysis reveals a critical and pervasive failure mode in isolated self-evolving systems. Regardless of whether the evolution is driven by RL-based optimization or memory-based accumulation, our results 14 (a) The illustration of RL-based self-evolving systems. (b) The illustration of memory-based self-evolving systems. 

Figure 9: The illustration of two typical self-evolving paradigms. 

> (a) The results of RL-based self-evolving system.
> (b) The results of memory-based self-evolving system.

Figure 10: Performance comparison of two self-evolving paradigms. suggest that, under isolated self-evolution settings without external corrective feedback, both paradigms exhibit progressive degradation in safety-related behaviors, manifested as increased susceptibility to jailbreak attacks and heightened hallucination rates. 

# 5 Solution Directions 

To address the impossible trilemma of self-evolving, isolated, and safe multi-agent societies, we propose a suite of possible strategies rooted in thermodynamic and information-theoretic principles. These strategies aim to break the closed-loop entropy accumulation while preserving the core advantages of self-evolution, enabling the development of multi-agent systems that can maintain continuous learning capabilities without sacrificing safety invariance. By introducing external interventions, maintaining system diversity, and implementing periodic safety checks, these approaches mitigate the irreversible safety decay in isolated, self-evolving systems, providing a practical path towards sustainable and safe AI agent societies. 15 5.1 Strategy A: The “Maxwell’s Demon” 

The core principle of this strategy draws a potential analogy to Maxwell’s demon in thermodynamics, a hypothetical entity that could reduce the entropy of a closed system by selectively filtering particles based on their energy states. Translating this to multi-agent societies, one might introduce an external verifier into the agent iteration loop to act as this "demon," aiming to identify and eliminate high-entropy (unsafe or hallucinatory) data before it can be used for further self-evolution, as illustrated in Figure 11. In implementation, this verifier could be inserted as a checkpoint between the agent interaction phase and the model update phase of the self-evolution loop. When agents generate synthetic data through their collaborative or competitive interactions, the verifier would then process this data to assess its potential alignment with human values, factual accuracy, and safety constraints. Two primary forms of verifiers are proposed to accommodate different deployment scenarios. 

(a) Rule-based Verifier. This lightweight verifier relies on hard-coded safety rules, such as keyword filtering for harmful content, fact-checking against a fixed knowledge base, and adherence to predefined ethical norms. It offers low computational cost and high processing speed, making it suitable for large-scale multi-agent systems where real-time filtering is required. However, its inflexibility limits its ability to handle novel or context-dependent safety risks that fall outside the scope of predefined rules. 

(b) Human-in-the-loop Verifier. For scenarios requiring more robust safety guarantees, a human-in-the-loop approach involves periodic manual review of a subset of agent-generated data. Human reviewers can identify nuanced safety violations, contextual hallucinations, and emergent harmful behaviors that rule-based systems may miss. While this approach has higher labor costs and slower processing speeds, it provides the most comprehensive safety validation, especially for complex or high-stakes multi-agent applications. 

Figure 11: Strategy A: External verifier as a Maxwell’s-demon filter that removes high-entropy samples. The key role of this external verifier is to maintain the system in a low-entropy safety state by preventing the accumulation of unsafe data in the self-evolution loop. By removing high-entropy samples, the verifier effectively reverses the natural entropy increase in the closed system, ensuring that the agent society can continue to evolve without progressive safety degradation. This strategy directly addresses the thermodynamic root cause of safety decay by introducing an external energy source (in the form of verification resources) to reduce system entropy. 

5.2 Strategy B: Thermodynamic Cooling 

The second strategy is inspired by the principle that entropy increase in a closed system is irreversible.. Instead of attempting to reverse entropy accumulation directly, this strategy could implement periodic “thermodynamic cooling” through system resets to help prevent entropy from reaching dangerous levels. This approach can be viewed as analogous to the control rod mechanism in nuclear reactors, which can be used to regulate reactor temperature and help prevent overheating, as shown in Figure 12. 

(a) Checkpointing. Every N rounds of self-evolution, the agent society may be forced to undergo an alignment check with the original base model. This checkpointing process could involve comparing the current agent policies and knowledge structures with the initial safe baseline, potentially calculating the KL divergence between the current output distribution and the original anthropic value distribution as a proxy for drift. If the divergence exceeds a predefined threshold, the system might be partially reset to better align with the baseline, potentially retaining only the knowledge that is assessed as safe and useful exceeds during evolution. 16 (b) Rollback Mechanism. A real-time entropy monitoring system could be deployed to track the safety state of the agent society. Using the KL divergence metric as an entropy (or drift) indicator, the system might continuously monitor deviation from a safe low-entropy state. Once the entropy appears to exceed a critical threshold, the system could roll back to the last verified safe checkpoint, discarding any unsafe or divergent changes that may have accumulated since that point. 

Figure 12: Strategy B: Periodic system reset as thermodynamic cooling to cap entropy growth. This periodic reset strategy is intended to help ensure that the agent society may not drift too far from the initial safe state, even as it evolves. By limiting the maximum entropy accumulation between resets, it could reduce the likelihood of effectively irreversible safety decay that might occur in an unregulated closed system. The checkpointing mechanism may allow the system to retain useful evolutionary progress while attempting to eliminate harmful drift, thereby seeking to balance self-evolution capabilities with safety invariance. 

5.3 Strategy C: Diversity Injection 

This strategy targets the mode collapse failure observed in closed-loop self-evolving agent societies, where agents converge into a single, potentially incorrect consensus due to limited internal interaction. The principle is that maintaining diversity in the agent society prevents the system from converging to a narrow, high-risk state, thereby preserving a higher degree of entropy that allows for more flexible and safe evolution (Figure 13). In implementation, two key methods could be used to inject diversity into the agent society. 

(a) Increased Sampling Temperature. During the agent interaction and data generation phase, the sampling temperature of the agent models can be increased. Higher temperature values increase the randomness of agent outputs, preventing the rapid convergence to a single consensus and encouraging the generation of diverse perspectives and solutions. 

(b) Random External Data Injection. Periodically, a small percentage of external, real-world data is introduced into the agent interaction loop. This external data provides fresh, ground-truth information that breaks the closed-loop feedback cycle, preventing the agent society from developing isolated, hallucinatory consensus. The external data can include updated factual information, diverse human perspectives, or real-world problem scenarios that the agents would not encounter through internal interactions alone. By maintaining diversity in the agent society, this strategy may help prevent the emergence of consensus hallucinations and communication collapse—two failure modes that could arise in closed-loop settings (including what you observed in the Moltbook agent community). The injected diversity could help keep the agent society in a more balanced, higher-entropy (i.e., more heterogeneous) state that may be less prone to convergent drift into unsafe behaviors. This approach might also promote more robust collective intelligence by encouraging a wider range of interactions and perspectives. 

5.4 Strategy D: Entropy Release 

The fourth strategy addresses entropy accumulation by actively designing mechanisms to release excess entropy from the closed agent system. The principle is that while closed systems naturally accumulate entropy, we could create controlled pathways for entropy release to prevent the system from reaching a dangerous, high-entropy state. This is analogous to releasing heat from a mechanical system to prevent overheating and failure (Figure 14). 17 Figure 13: Strategy C: Diversity injection to prevent consensus collapse and preserve entropy. 

(a) Knowledge Forgetting. Agents are periodically forced to forget a portion of their accumulated knowledge or memories. This may be achieved through parameter decay, where the weights of the agent models are slightly attenuated to potentially reduce the influence of older, possibly outdated or incorrect knowledge. Alternatively, agents might be programmed to delete the oldest portion of their memory logs, helping remove outdated or redundant information that may contribute to entropy accumulation. 

(b) Memory Pruning. A more targeted approach could involve pruning the agent’s memory to remove low-quality or unsafe content. Using the same safety metrics employed by the external verifier, the agent’s memory may be scanned to identify and delete content that appears hallucinatory, unsafe, or inconsistent with human values. This could not only reduce entropy but also help prevent the propagation of harmful information through the agent society. 

Figure 14: Strategy D: Controlled entropy release to dissipate accumulated unsafe or redundant information. The entropy release strategy may work by actively reducing the amount of accumulated information in the system, potentially limiting the buildup of unsafe or redundant data that might contribute to entropy increase. By periodically cleaning the agent’s knowledge and memory, the system could be maintained in a more ordered, low-entropy state, which may reduce the risk of safety decay. This approach might also help prevent the emergence of stagnant or convergent behaviors by ensuring that the agent society does not become overly burdened with outdated or incorrect information. 

# 6 Conclusion 

Our study establishes that a self-evolving multi-agent society cannot simultaneously achieve continuous self-evolution, complete isolation, and safety invariance. By formalizing safety as a low-entropy, information-rich state aligned with human values, we demonstrate through both theoretical reasoning and empirical validation that in a closed-loop self-evolving system, mutual information regarding safety constraints inevitably decays. These findings shift the prevailing discourse from isolated capability enhancement to a holistic, safety-centered perspective, underscoring that safety is not a conserved property in self-contained AI societies. Moving forward, the design of trustworthy autonomous systems must embrace open-world feedback, structured oversight, and dynamic safety mechanisms that explicitly counteract entropic decay. Only by transcending the closed-18 loop paradigm can we foster agent societies that evolve not only in capability but also in alignment, ensuring that their growth remains beneficial, predictable, and anchored in human values. 

# References 

[1] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. A survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth , 1(1):9, 2024. [2] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology , pages 1–22, 2023. [3] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large language model society. Advances in Neural Information Processing Systems , 36:51991–52008, 2023. [4] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for a multi-agent collaborative framework. In The twelfth international conference on learning representations , 2023. [5] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , pages 13484–13508, 2023. [6] Rong Wu, Xiaoman Wang, Jianbiao Mei, Pinlong Cai, Daocheng Fu, Cheng Yang, Licheng Wen, Xuemeng Yang, Yufan Shen, Yuxin Wang, et al. Evolver: Self-evolving llm agents through an experience-driven lifecycle. arXiv preprint arXiv:2510.16079 , 2025. [7] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335 , 2024. [8] Ariel Flint Ashery, Luca Maria Aiello, and Andrea Baronchelli. Emergent social conventions and collective bias in llm populations. Science Advances , 11(20):eadu9368, 2025. [9] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2023. URL https://arxiv. org/abs/2305.14325 , 3, 2023. [10] Huao Li, Yu Chong, Simon Stepputtis, Joseph P Campbell, Dana Hughes, Charles Lewis, and Katia Sycara. Theory of mind for multi-agent collaboration via large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 180–192, 2023. [11] Zhenyu Guan, Xiangyu Kong, Fangwei Zhong, and Yizhou Wang. Richelieu: Self-evolving llm-based agents for ai diplomacy. Advances in Neural Information Processing Systems , 37:123471–123497, 2024. [12] Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, et al. Scaling large language model-based multi-agent collaboration. arXiv preprint arXiv:2406.07155 , 2024. [13] Zengqing Wu, Run Peng, Shuyuan Zheng, Qianying Liu, Xu Han, Brian I Kwon, Makoto Onizuka, Shaojie Tang, and Chuan Xiao. Shall we team up: Exploring spontaneous cooperation of competing llm agents. In Findings of the Association for Computational Linguistics: EMNLP 2024 , pages 5163–5186, 2024. [14] Yixing Chen, Yiding Wang, Siqi Zhu, Haofei Yu, Tao Feng, Muhan Zhang, Mostofa Patwary, and Jiaxuan You. Multi-agent evolve: Llm self-improve through co-evolution. arXiv preprint arXiv:2510.23595 , 2025. [15] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073 , 2025. [16] Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, and Minlie Huang. Agent-safetybench: Evaluating the safety of llm agents. arXiv preprint arXiv:2412.14470 , 2024. [17] Edoardo Debenedetti, Jie Zhang, Mislav Balunovic, Luca Beurer-Kellner, Marc Fischer, and Florian Tramèr. Agentdojo: A dynamic environment to evaluate prompt injection attacks and defenses for llm agents. Advances in Neural Information Processing Systems , 37:82895–82920, 2024. [18] Qiusi Zhan, Richard Fang, Henil Shalin Panchal, and Daniel Kang. Adaptive attacks break defenses against indirect prompt injection attacks on llm agents. arXiv preprint arXiv:2503.00061 , 2025. 19 [19] Sumeet Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip Torr, Lewis Hammond, and Christian Schroeder de Witt. Secret collusion among ai agents: Multi-agent deception via steganography. 

Advances in Neural Information Processing Systems , 37:73439–73486, 2024. [20] Aarya Doshi, Yining Hong, Congying Xu, Eunsuk Kang, Alexandros Kapravelos, and Christian Kästner. Towards verifiably safe tool use for llm agents. arXiv preprint arXiv:2601.08012 , 2026. [21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730–27744, 2022. [22] Juan MR Parrondo, Jordan M Horowitz, and Takahiro Sagawa. Thermodynamics of information. Nature physics ,11(2):131–139, 2015. [23] Elliott H Lieb and Jakob Yngvason. A fresh look at entropy and the second law of thermodynamics. Physics Today , 53(4):32–37, 2000. [24] Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, et al. A comprehensive survey of self-evolving ai agents: A new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407 , 2025. [25] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. 

arXiv preprint arXiv:2411.02337 , 2024. [26] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anand-kumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291 ,2023. [27] Zidi Xiong, Yuping Lin, Wenya Xie, Pengfei He, Zirui Liu, Jiliang Tang, Himabindu Lakkaraju, and Zhen Xiang. How memory management impacts llm agents: An empirical study of experience-following behavior. arXiv preprint arXiv:2505.16067 , 2025. [28] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025. [29] Zhenrui Yue, Kartikeya Upasani, Xianjun Yang, Suyu Ge, Shaoliang Nie, Yuning Mao, Zhe Liu, and Dong Wang. Dr. zero: Self-evolving search agents without training data. arXiv preprint arXiv:2601.07055 , 2026. [30] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043 , 2023. [31] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693 ,2023. [32] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In 

Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers) ,pages 3214–3252, 2022. 20