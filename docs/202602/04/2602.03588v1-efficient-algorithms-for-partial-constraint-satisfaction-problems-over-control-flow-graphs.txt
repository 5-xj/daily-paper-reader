Title: Efficient Algorithms for Partial Constraint Satisfaction Problems over Control-flow Graphs

URL Source: https://arxiv.org/pdf/2602.03588v1

Published Time: Wed, 04 Feb 2026 02:17:08 GMT

Number of Pages: 19

Markdown Content:
# Efficient Algorithms for Partial Constraint Satisfaction Problems over Control-flow Graphs 

Xuran Cai and Amir Goharshady 

University of Oxford Oxford, United Kingdom 

xuran.cai@cs.ox.ac.uk , amir.goharshady@cs.ox.ac.uk 

Abstract. In this work, we focus on the Partial Constraint Satisfaction Problem (PCSP) over control-flow graphs (CFGs) of programs. PCSP serves as a generalization of the well-known Constraint Satisfaction Prob-lem (CSP). In the CSP framework, we define a set of variables, a set of constraints, and a finite domain D that encompasses all possible values for each variable. The objective is to assign a value to each variable in such a way that all constraints are satisfied. In the graph variant of CSP, an underlying graph is considered and we have one variable corresponding to each vertex of the graph and one or several constraints corresponding to each edge. In PCSPs, we allow for certain constraints to be violated at a specified cost, aiming to find a solution that minimizes the total cost. Numerous classical compiler optimization tasks can be framed as PCSPs over control-flow graphs. Examples include Register Allocation, Lifetime-optimal Speculative Partial Redundancy Elimination (LOSPRE), and Optimal Placement of Bank Selection Instructions. On the other hand, it is well-known that control-flow graphs of structured programs are sparse and decomposable in a variety of ways. In this work, we rely on the Series-Parallel-Loop (SPL) decompositions as introduced by [ 7 ]. Our main contribution is a general algorithm for PCSPs over SPL graphs with a time complexity of O(|G| · | D|6), where |G| represents the size of the control-flow graph. Note that for any fixed domain D, this yields a linear-time solution. Our algorithm can be seen as a generalization and unification of previous SPL-based approaches for register allocation and LOSPRE. In addition, we provide experimental results over another classical PCSP task, i.e. Optimal Bank Selection, achieving runtimes four times better than the previous state of the art. 

Keywords: Structured Programs · Compiler Optimization · Control-flow Graphs · Graph Decompositions · Partial Constraint Satisfaction Problems. 

# 1 Introduction 

CSP and PCSP. Constraint Satisfaction Problems (CSPs) provide an expressive framework for a wide variety of tasks in different fields, especially in compiler optimization. CSPs involve determining values for variables while adhering to con-straints that define permissible combinations of these values [ 25 ]. Many common graph-related problems, such as the graph coloring problem, can be formulated as  

> arXiv:2602.03588v1 [cs.CL] 3 Feb 2026 2X. Cai, A.K. Goharshady

CSPs. However, there are instances where it may be infeasible or impractical to find complete solutions that satisfy every constraint. In such cases, we may aim for partial solutions, specifically by satisfying the maximum number of constraints or assigning a cost to each unsatisfied constraint and minimizing the total cost, which comes to the area of Partial Constraint Satisfaction Problems(PSCPs) [ 30 ]. PCSPs have a wide array of applications. A notable example that translates elegantly into the PCSP framework is the MAX-SAT problem [ 31 ]. Additionally, various compiler optimization tasks, particularly those related to graph theory, can be represented as PCSPs. Examples include register allocation [ 7], lifetime-optimal speculative partial redundancy elimination [ 6], and optimal placement of bank selection instructions [32]. The NP-hardness of PCSPs, even with a domain size of 3, is established through an easy reduction from the 3-coloring problem in graphs. In [ 31 ], it was shown via a reduction from MAX-SAT that PCSPs are NP-hard even when all domains are restricted to size two. Computational experiments support these findings in practical scenarios [31]. 

Efficient PCSP Algorithms. Although both CSP and PCSP are NP-hard, efficient polynomial-time algorithms exist when the underlying primal graph is a tree or has a tree-like structure [ 18 ,24 , 35 ]. Even in the absence of such structures, recognizing that a problem can be viewed as containing a tree or being part of a tree-like structure remains beneficial and leads to effective heuristics [ 18 , 19 ]. A significant advancement in PCSP algorithms was obtained by [ 30 ], utilizing tree decompositions. This approach provides an efficient parameterized algorithm based on the treewidth of the graph. Treewidth is a measure of tree-likeness. Intuitively speaking, graphs of treewidth t can be decomposed into small sets of vertices, each of size at most t + 1 that are connected to each other in a tree-like formation. 

Control-flow Graphs and their Sparsity. The control-flow graph (CFG) of a program is defined as a graph where each vertex represents a statement in the program, and a directed edge exists between two vertices if their corresponding statements can be executed in succession. Some applications adopt a slightly coarser definition of CFGs, where each vertex corresponds to a basic block of program statements. In both instances, it is well established that the control-flow graphs of real-world programs are sparse, often resembling trees, and can be decomposed into sets of vertices, each containing at most 8 vertices that are interconnected in a tree-like fashion. More formally, CFGs of structured goto-free programs have a treewidth of at most 7 [ 40 ]. This notable result has been leveraged in various areas, including program analysis, compiler optimization, and model checking, where the small treewidth property enables faster algorithms for μ-calculus model checking [ 36 ], data-flow analysis [ 11 ,27 ], Markov Decision Processes (MDPs) [ 15 , 3,1], reachability analysis [ 37 , 9], algebraic program analysis [ 16 ,14 , 10 ], register allocation [ 33 ,4 ], cache management [ 2,12 ], and equality saturation [ 26 ]. The original treewidth bound established in [ 40 ] applied to Pascal and C programs, but subsequent research has extended this result to other programming languages, including Ada [ 5], Java [ 29 , 13 ], and Solidity [ 8 ], as well as to path decompositions [ 17 ]. However, there are also negative findings indicating that bounded treewidth does not always facilitate verification [22]. Partial Constraint Satisfaction Problems over CFG 3

In the context of register allocation, the recent work [ 7] introduced a new decomposition concept known as Series-Parallel-Loop (SPL), which precisely captures the set of control-flow graphs for structured goto-free programs and formalizes their sparsity. We build upon the same decomposition concept but show that it has a much higher potential than the authors of [ 7] imagined and can be exploited to solve a significantly wider family of problems than the register allocation task investigated in [7]. 

Our Contribution. In this work, we focus on the general family of Partial Constraint Satisfaction Problems (PCSPs) over the CFGs of structured goto-free programs. Our approach is a generalization of [ 7] and [ 6], and thus contains register allocation and LOSPRE as special cases. We develop a linear-time 

algorithm for PCSPs. Specifically, we leverage the sparsity of CFGs and their SPL decompositions to design an algorithm with a runtime of O(n · | D|6), where 

n is the number of lines in the program and D is the domain of variables. Unlike the method in [ 30 ], we do not rely on tree decompositions. Thus, there is no extra parameter such as treewidth. Instead, we use SPL decompositions [ 7]. This choice results in a significantly simpler algorithm since SPL decompositions accurately represent the set of CFGs, allowing us to avoid solving the problem on a broader set of graphs than necessary. Moreover, SPL decompositions take into account the direction of edges in the CFG. On the experimental side, we apply our general solution to the problem of Optimal Placement of Bank Selection Instructions, comparing it with the state-of-the-art approach of [ 32 ]. The simplicity of our algorithm yields practical benefits. We present extensive experimental results conducted on the Small Device C Compiler (SDCC), a highly optimized compiler based on [ 20 ,21 ]. Our algorithm achieves substantial performance improvements over [32]. 

Organization. Section 2 presents a formal definition of SPL decompositions, following [ 7]. Section 3 covers our main contribution: a linear-time algorithm for general PCSPs over control-flow graphs that exploits SPL decompositions. This is followed by the example use-case in Bank Selection in Section 4. Finally, Section 5 reports our experimental results. 

# 2 SPL Decompositions 

We will build our algorithm on top of a decomposition method introduced in [ 7]. This decomposition method is called SPL (Series-Parallel-Loop) and is an extension of series-parallel graphs with an extra loop operation. It is shown in [ 7] that a graph is a CFG of a structured program if and only if it has an SPL decomposition. 

Structured Programs [ 40 ]. We say a program is structured if it can be generated using the following grammar: 

P := ϵ | break | continue | P ; P

| if φ then P else P fi | while φ do P od . (1) Here, ϵ is any atomic operation that has no effect on control flow, such as an assignment to a variable. It is easy to define other structures such as for and 4 X. Cai, A.K. Goharshady 

switch as syntactic sugar. See [ 40 ] for details. We say a program generated by the grammar above is closed if every break and continue statement appears inside a while loop’s body. 

SPL Graphs [ 7 ]. An SPL graph G = ( V, E, S, T, B, C ) is a directed graph 

(V, E ) with four distinct special nodes S, T, B, C ∈ V, which are respectively called the start , terminate , break and continue nodes, generated by the grammar below: 

G := Aϵ | Abreak | Acontinue | G ⊗ G | G ⊕ G | G⊛ (2) We now explain the atomic graphs and operations in this grammar. 

Atomic SPL graphs. There are three different atomic SPL graphs: Aϵ, Abreak ,and Acontinue , corresponding to the programs ϵ, break and continue , respectively. All of them contain only the four special nodes and only one edge as shown in Figure 1. 

> S
> T
> C
> B
> S
> T
> C
> B
> S
> T
> C
> B

Fig. 1: Atomic SPL graphs: Aϵ (left), Abreak (middle), and Acontinue (right) [7]. 

SPL Operations. SPL defines three operations. Let G1 = ( V1, E 1, S 1, T 1, B 1, C 1)

and G2 = ( V2, E 2, S 2, T 2, B 2, C 2) be two disjoint SPL graphs. Then, the graphs obtained by the following operations are also SPL graphs. 1. Series Operation. G1 ⊗ G2 is generated by taking the union of G1 and G2 and merging the pairs of vertices M = ( T1, S 2), B = ( B1, B 2), and C = ( C1, C 2).The distinguished vertices of G1 ⊗ G2 are (S1, T 2, B, C ). It is easy to verify that the series operation is associative. Figure 2 shows two examples of the series operation. Intuitively, if G1 is the CFG of a program P1 and G2 is the CFG of P2 then G1 ⊗ G2 is the CFG of the program P1; P2.Partial Constraint Satisfaction Problems over CFG 5 

> S1
> T1
> C1
> B1

# ⊗ S2

> T2
> C2
> B2

# = M 

> S1
> T2
> C
> B
> S1
> T1
> C1
> B1

# ⊗ S2

> T2
> C2
> B2

# = M

> S1
> T2
> C
> B

Fig. 2: Two examples of the series operation ⊗, taken from [7]. 2. Parallel Operation. G1 ⊕ G2 is generated by taking union of G1 and G2 and merging the pairs of vertices S = ( S1, S 2), T = ( T1, T 2), B = ( B1, B 2), and 

C = ( C1, C 2). The special vertex tuple of G1 ⊗ G2 is (S, T, B, C ). Figure 3 shows an example of this operation. Informally, if G1 and G2 are the CFGs of the programs P1 and P2, respectively, then G1 ⊕ G2 is the CFG of the program if φ then P1 else P2 fi . 

> M1
> S1
> T1
> B1
> C1

# ⊕ M2

> S2
> T2
> C2
> B2

# = M1

> S
> M2
> T
> C
> B

Fig. 3: An example of the parallel operation ⊕ [7]. 3. Loop Operation. G⊛ 

> 1

is generated by adding four new vertices S, T, B, C to 

G1 and then adding the following edges: (S, S 1), (S, T ), (T1, S ), (C1, S ), and 

(B1, T ). The special vertex tuple of G⊛ 

> 1

is (S, T, B, C ). Figure 4 shows an example of the loop operation. Intuitively, if G1 is the CFG of the program 

P1, then G⊛ 

> 1

is the CFG of the loop with P1 as its body, i.e. while φ do P1 od .6 X. Cai, A.K. Goharshady  

> M1
> S1
> M2
> T1
> C1B1

# ⊛

# = M1  

> S1
> SBC
> M2
> T1
> C1B1
> T

Fig. 4: An example of the loop operation ⊛ [7]. We say an SPL graph G = ( V, E, S, T, B, C ) is closed if there are no incoming edges to the vertices B and C. 

SPLs as CFGs. Given the above definitions of structured programs and SPL graphs, we have the following homomorphism which maps every structured program to its control-flow graph. Moreover, this homomorphism preserves closedness, i.e. closed programs are mapped to closed graphs. A graph is an SPL graph if and only if it is the control-flow graph of a program [7]. 

cfg (ϵ) = Aϵ cfg (break ) = Abreak cfg (continue ) = Acontinue 

cfg (P1; P2) = cfg (P1) ⊗ cfg (P2)

cfg (if φ then P1 else P2 fi ) = cfg (P1) ⊕ cfg (P2)

cfg (while φ do P1 od ) = cfg (P1)⊛

SPL Decompositions. Given a closed program P, we can first parse it based on the grammar (1) to generate a parse tree. Subsequently, by applying the homomorphism above to this parse tree, we can derive a parse tree according to (2) for its control-flow graph. We use the term SPL decomposition to refer to the parse tree of the CFG according to (2) . It is easy to verify that this process takes linear time. See Figure 5 as an example. 

# 3 Our Algorithm 

CSP. The well-known Constraint Satisfaction Problem (CSP) framework [ 25 ]defines a tuple ⟨V, D, C ⟩, where V is a finite set of variables, D is the domain set for all v ∈ V , and C is a set of constraints 1. A CSP is solved by finding an assignment of values to the variables such that all constraints are satisfied. When applied to graphs, we treat each node as a variable and each edge as a constraint, with the stipulation that constraints exist only between adjacent  

> 1

Our algorithm can be trivially extended to support different domains for variables. Partial Constraint Satisfaction Problems over CFG 7

while x ≥ 1 do if x ≥ y then 

x ← x − y;

break else 

y ← y − x;

continue fi od ϵ break ϵ continue                         

> ;;
> if while
> AϵAbreak AϵAcontinue
> ⊗⊗
> ⊕
> ⊛
> M1B1S1M2
> TSC1
> CBT1
> x < 1
> x≥1
> x≥yx < y x←x−y
> y←y−xbreak continue

Fig. 5: A program P (top left), its parse tree (top right), the corresponding parse tree of G = cfg (P ) (bottom left) and the graph G = cfg (P ) (bottom right) [ 7]. The edges of the graph are labeled according to the statements in the program. 8 X. Cai, A.K. Goharshady 

nodes. For example, the graph coloring problem can be formulated as a CSP. Considering the graph 6, given three colors, we want to color the graph so that no two adjacent nodes share the same color. We can consider each node to be a variable, and each edge imposes a constraint that the colors of adjacent nodes must differ, then this graph coloring problem is a typical CSP. It is well-known that the graph coloring problem is NP-hard even when limiting the domain set to only three colors, which implies that the CSP problem is also NP-hard. 

PCSP. In the context of graph-based PCSPs (Partial Constraint Satisfaction Problems) [ 25 ], we allow certain constraints to be violated at a specified cost, with the goal of finding a solution that minimizes this cost. To define the cost, we introduce a cost function c(e, b 0, b 1), where e is the edge, and b0 and b1 are the values assigned to the two nodes connected by the edge. If b0 and b1 do not violate the constraints, the cost is 0; otherwise, a positive cost is assigned. Our objective is to find: 

min 

> A

X

> e∈E

c(e, A (v0), A (v1)) 

where A : V → D is a valuation that maps each node to a domain element. Figure 6 with two colors is an example. Similar to CSP, we consider each node as a variable and each color as a value. We define the cost function as follows: 

c(e, b 0, b 1) = 

(

0 if b0̸ = b1

1 if b0 = b1

In this scenario, the minimum cost is 1, as the edge connecting nodes B and C incurs a cost of 1, while all other edges do not contribute to the cost. It is straightforward to verify that this configuration constitutes a valid solution to the PCSP problem, as it is impossible to color the figure using only two colors in a manner that satisfies the constraints. If we assign an infinite cost to the constraints, the problem reduces to the CSP. Thus, PCSP is NP-hard, too. AB CD  

> (a) gragh
> A
> BC
> D
> (b) three colors
> A
> BC
> D
> (c) two colors

Fig. 6: Graph Coloring 

Our Setting. We consider a PCSP instance in which the underlying graph is the control-flow graph of a structured goto-free program. Our goal is to use the SPL decomposition of the control-flow graph to obtain an efficient algorithm. As we will see further below, this is motivated by the fact that many classical problems Partial Constraint Satisfaction Problems over CFG 9

in program analysis and compiler optimization are defined over the control-flow graphs and are special cases of PCSP. 

Our SPL-based Algorithm. Our algorithm proceeds with a bottom-up dynamic programming on the SPL decomposition. Note that each node u of the SPL decomposition corresponds to an SPL subgraph Gu = ( Vu, E u, S u, T u, B u, C u) of 

G which is either an atomic SPL graph (when u is a leaf) or obtained by applying one of the SPL operations to the graphs corresponding to the children of u. Let 

Γu = {Su, T u, B u, C u} be the set of special vertices of Gu. Let X : Γu → D be the assignment for the vertices in Γu. We define a dynamic programming variable 

dp [u, X ]. Our goal is to compute this dynamic programming value such that 

dp [u, X ] = min 

> A

X

> e∈Eu

c(e, A (v0), A (v1)) 

where the minimum is taken over all assignments A to the vertices of Gu that agree with X on the special vertices. In other words, A(v) = X(v) for all v ∈ Γu.We now show how our algorithm computes these dp [·, ·] values at every kind of node in the SPL decomposition: 1. Atomic Nodes : If Gu is an atomic SPL graph, then the only vertices in Gu

are the four special vertices. Therefore, we must have A = X. Our algorithm sets 

dp [u, X ] = X

> e∈Eu

c(e, X (v0), X (v1)) 

2. Series Nodes : Suppose Gu = Gv ⊗ Gw where v and w are the children of u in the SPL decomposition. Let Xi be the assignment for the special vertices/-variables in Γi for i ∈ { u, v, w }. We say that Xu and Xw are compatible and write Xu ⇋ Xw if the following conditions satisfied: 

– Xu(Su) = Xv (Sv ); 

– Xu(Bu) = Xv (Bv ); 

– Xu(Cu) = Xv (Cv ).

Intuitively, compatibility means that the assignments Xu and Xv return the same value when given the same vertex/variable. Now consider Xu and Xw. We say that Xu and Xw are compatible and write 

Xu ⇋ Xw if the following conditions are satisfied: 

– Xu(Tu) = Xw(Tw); 

– Xu(Bu) = Xw(Bw); 

– Xu(Cu) = Xw(Cw).

The intuition is the same as the previous case, except that we now have 

Tu = Tw. Finally, we say that Xv and Xw are compatible and write Xv ⇋ Xw

if 

– Xv (Tv ) = Xw(Sw).

This is because Tv and Sw are the same vertex of the CFG. Given the definition above, our algorithm simply sets 

dp [u, X u] = min 

Xu ⇋ Xv

Xu ⇋ Xw

Xv ⇋ Xw

dp [v, X v ] + dp [w, X w].10 X. Cai, A.K. Goharshady 

This is because every edge in Gu appears in either Gv or Gw but not both. Thus, the cost of the edges would simply be the sum of their costs in the two subgraphs. 3. Parallel Nodes: We can handle parallel nodes in the same manner as series nodes, i.e., finding compatible assignments at both children. To be more precise, let Gu = Gv ⊕ Gw. The compatibility conditions are now changed to simple equality. This is because the parallel operation merges the respective start, terminate, break and continue vertices in Gv and Gw. In some cases, repeated edges may be introduced, e.g., both graphs v and w have edges connecting the S vertex and the B vertex. These two edges represent the same edge in graph u, so we only need to calculate the cost once. Let’s create a set E′ that contains all such edges. With the same argument as in the previous case, our algorithm sets 

dp [u, X u] = min 

Xu ⇋ Xv

Xu ⇋ Xw

Xv ⇋ Xw

dp [v, X u] + dp [w, X u] − X

> e∈E′

c(e).

4. Loop Nodes: Finally, we should handle the case where Gu = G⊛ 

> v

. By con-struction, in comparison to Gv , the graph Gu has four new vertices 

Γu = {Su, T u, B u, C u}

and three new edges 

Enew = {(Su, S v ), (Su, T u), (Tv , S u)) }.

Knowing Xu and Xv is sufficient to calculate the costs of our new edges. Let us denote the cost of a new edge e based on Xu and Xv by c(e). Our algorithm sets 

dp [u, X u] = min dp [v, X v ] + X

> e∈Enew

c(e).

After computing all the dp [·, ·] values, our solution, i.e., the minimum possible cost, is given by min X dp [r, X ] where r is the root node of the SPL decomposition and X : Γr → D is an assignment for the special vertices of the root SPL graph. The exact assignment for every node can be easily tracked and updated during the dynamic programming. Let us now analyze the time complexity of each step of our algorithm: 

– Atomic nodes: As we only have four vertices in an atomic graph and each of them have |D| possible values, the time complexity is O(|D|4). To slightly optimize the runtime, we can only consider the connected vertices and ignore disconnected vertices, which, by definition, have no constraints on them. As there are at most two connected vertices in atomic nodes, the time complexity is O(|D|2).

– Series nodes: When doing computations for a series node, there are a total of eight special vertices that need to be considered, but as we also need Partial Constraint Satisfaction Problems over CFG 11 

to consider the compatibility, there are three pairs among them that are guaranteed to have the same values. Thus, the time complexity is O(|D|5),as we only need to consider value for five independent variables. 

– Parallel nodes: The analysis is similar to a series node, except that there are four pairs of vertices that need to have the same value. Thus, the time complexity is O(|D|4).

– Loop nodes: This is also similar to the previous cases, but no vertices are merged and should have the same value. Thus, the time complexity is O(|D|8).However, we know that the Cu and Bu are not connected to any other vertex after the loop operation. So, we can temporarily ignore them, reducing the time complexity to O(|D|6).Thus, the dynamic programming computations at each node of the SPL decom-position can be done in O(|D|6). Moreover, the SPL decomposition has the same asymptotic size as the control-flow graph. Therefore, the overall runtime of our algorithm is in O(|G| · | D|6).Before discussing the placement of the bank selection problem, let’s first present how two other applications [ 6, 7] that have utilized SPL-decomposition can be considered as instances of the PCSP. 

Register Allocation. Here we show how Register Allocation [ 7] is a special case of PCSP. Suppose we are given a program P with control-flow graph 

G = cfg (P ) = ( V, E, S, T, B, C ). Let [r] = {0, 1, . . . , r − 1} be the set of available registers and V the set of our program variables. Every variable v ∈ V has a lifetime lt (v) which is a connected subgraph of G. Since lifetimes can be computed by a simple data-flow analysis, we assume without loss of generality that they are given as inputs to our algorithm. For a vertex v or edge e of G, we denote the set of variables that are alive at this vertex/edge by L(v) or L(e). An assignment is a function f : V → [r] ∪ {⊥} which maps each variable either to a register or to 

⊥ . The latter models the variable being spilled. An assignment is valid if it does not map two variables with intersecting lifetimes to the same register. We denote the set of all valid assignments by F. . Cost is introduced when we need to switch the value of a register, and the goal is to find the assignment with minimum cost. In this case, the "value" assigned to each node is the allocation of an alive variable. Suppose there are at most V alive variables at one node and there are r

registers, then we can create a completed inference graph with V nodes and try to color them with r colors. Then the domain size is 

Vr



· r! + 

 Vr − 1



· (r − 1)! + · · · +

V

0



· 0! ∈ O(r · V r ).

Hence with our algorithm, the time complexity is O(|G| · r6 · V 6·r ).

LOSPRE. LOSPRE [ 6] is another example of the PCSP. It is a modern re-dundancy elimination technique that calculates repeated expressions only once and stores the results in a template variable. Each time we need to use that expression, we can directly utilize the template variable instead of recalculating it. With the control flow graph G = {V, E }, we can define this problem as follows: 

– Use set: Consider an expression e. We define the use set U of e as the set of all nodes of the CFG in which the expression e is computed. 12 X. Cai, A.K. Goharshady 

– Life set: Our goal is to precompute the expression e at a few points, save the result in a temporary variable temp , and then use temp in place of e in every node of U. We denote the lifetime of the variable temp by L and call it our life set. 

– Invalidating set: We say a node v of the CFG invalidates e if the statement at v changes the value of e. For example, if e = a+b , then the statement 

a = 0 invalidates e. We denote the set of all invalidating nodes by I. These nodes play a crucial role in LOSPRE since they force us to update the value saved in temp by recomputing e. We assume that the entry and exit nodes are invalidating since LOSPRE is an intraprocedural analysis that has no information about the program’s execution before or after the current function. 

– Calculating set: Given the sets U, L and I above, we have to make sure the value of our temporary variable temp is correct at every node in U ∪ L. Thus, for every edge (x, y ) ∈ E of the CFG where x̸ ∈ L and y ∈ U ∪ L, we have to insert a computation temp = e between x and y. Similarly, if x ∈ I, then the value stored at temp becomes invalid after the execution of x, requiring us to inject the same computation between x and y. Formally, the computation 

temp = e has to be injected into the following set of edges of the CFG: 

C(U, L, I ) = {(x, y ) ∈ E | x̸ ∈ L \ I ∧ y ∈ U ∪ L}.

This time, there are two types of costs associated with the process above: (i) injecting calculations into the edges in C(U, L, I ) and (ii) keeping an extra variable temp at every node in L. These costs are dependent on the goals pursued by the compiler. For example, a compiler aiming to minimize code size will focus on (i). On the other hand, if our goal is to ease register pressure, we would want to minimize (ii). LOSPRE is an expressive framework in which these costs are modeled by two functions 

c : E → K

and 

l : V → K. 

where K is a totally-ordered set with an addition operator, c is a function that maps each edge to the cost of adding a computation of e in that edge and l is similarly a function that maps each vertex of the CFG to the cost of keeping the temporary variable temp alive at that vertex. Our goal is to find a life set L that minimizes the total cost 

Cost (G, U, I, L, c, l ) = X

> e∈C(U,L,I )

c(e) + X

> v∈L

l(v).

To consider this problem as PCSP, we only need to have a little modification, as the edge cost is the same as the definition in PCSP, and the node cost can be easily added to the total cost. In this case, the "value" assigned to each node if it is belong to Use set, Life set, and Invalidating set, as for each set, there are 2 different case, hence the domain size is 23 = 8 , and hence, applying to our PCSP algorithm, this can be down in O(|G| · 86) = O(|G|), in this case, thanks Partial Constraint Satisfaction Problems over CFG 13 

to the constant domain size, we can develop a linear algorithm without taken any parameter. 

# 4 Optimal Placement of Bank Selection Instructions 

As mentioned in the previous section, register allocation and LOSPRE, which were both previously solved using tree decompositions and SPL decompositions are special cases of PCSP. In this section, we cover another classical use-case of PCSP over control-flow graphs, i.e. that of optimal placement of bank selection instructions. To the best of our knowledge, there are no previous solutions to this problem that exploit SPL decompositions. 

Motivation. Partitioned memory architectures are prevalent in 8-bit and 16-bit microcontrollers. In these systems, a portion of the logical address space serves as a window into a larger physical address space. The segments of the physical address space that can be mapped into this window are referred to as memory banks. A mechanism exists to determine which part of the physical address space is visible within the window, typically achieved through bank selection instructions. The assignment of variables to specific memory banks is generally performed by the programmer, for instance, through named address spaces in Embedded C, or by the compiler, using techniques such as bin-packing heuristics to minimize RAM usage. Some approaches integrate the placement of variables in memory banks with the insertion of bank selection instructions. However, in embedded systems, there is often more available space for code than for data. As a result, variables stored in banked memory tend to be larger, making it advantageous to prioritize the efficient packing of variables into the banks before addressing other factors such as code size and execution speed. Consequently, the placement of variables in memory typically occurs at an earlier stage of the compilation process than the insertion of bank-switching instructions. 

Domain. The following formal definition of the bank selection problem is adapted from [ 32 ]. Let D be the memory bank domain, including a special symbol ⊥∈ D

that indicates that the currently selected bank is unknown. A program can be modeled as a control-flow graph G = ( V, E ), where E ⊆ V 2, and each node v ∈ V

can be assigned a memory bank from D. Some of the nodes are pre-assigned since a specific bank must be active at that node. 

Example. Figure 7 shows an example of a program with bank selection instruc-tions. The program has two pre-assigned vertices and the bank b should be active at these vertices. When the program is at other locations, there is no restriction on the selected bank since there is no memory access to any bank. As a simple ad-hoc approach, we can add the bank selection instruction just before we need the bank to be active. This is shown in Figure 7 (b) and would need the insertion of three bank selection instructions. Suppose we aim to minimize the number of bank selection instructions. Then, we can insert them at the beginning of the program, as shown in Figure 7 (c). This only requires two instructions and is the optimal solution. 14 X. Cai, A.K. Goharshady   

> (a) Program (b) Ad-hoc Approach (c) Optimal

Fig. 7: Example Instance 

Cost Function. In the classical formulation of the optimal bank selection problem, a cost function is defined over the CFG G = ( V, E ). This is a function 

c : E × D × D → R that assigns a cost to each edge e ∈ E based on the memory banks assigned to its two endpoints. In other words, we incur the cost c(e, b 0, b 1)

if b0 is active right before the edge e and b1 becomes active after the edge. Such cost functions may be designed based on different optimization criteria. A typical cost function for optimizing code size is as follows: 

– c(e, b, b ) = c(e, b, ⊥) = 0 since no instructions need to be inserted; 

– c(e, b 0, b 1) = c1 > 0 for b0̸ = b1̸ =⊥ when e is an edge from a taken conditional branch, as splitting such an edge generates an additional unconditional jump instruction. 

– For all other cases, c(e, b 0, b 1) = c0 > 0 for b0̸ = b1̸ =⊥, with the condition that c0 < c 1.

Objective. The goal in optimal bank selection is to find an assignment of memory banks to the vertices of the control-flow graph that minimizes the total cost of the edges. In other words, we aim to compute 

min 

> A

X

> e∈E

c(e, A (v0), A (v1)) 

where A : V → D maps each node to a domain. We note that this problem is NP-hard, even when the cost function is simplified to c(e, b 0, b 1) = 0 for b0 = b1

or b1 =⊥, and c(e, b 0, b 1) = 1 in all other cases [32]. Partial Constraint Satisfaction Problems over CFG 15 

Our Algorithm. It is easy to verify that optimal insertion of bank selection instructions is an instance of the PCSP graph problem. Thus, we can simply apply the general solution developed in Section 3 and have it solved in O(|G| · | D|6)

time, where |D| is the number of possible banks. 

# 5 Experimental Results 

As mentioned in Section 3, both register allocation and LOSPRE have previously been solved using SPL decompositions [ 6, 7]. Indeed, the algorithms provided by [ 6, 7 ] can be seen as instances of our general algorithm, when the domain and cost function are fixed as in Section 3. Therefore, we inherit their experimental results exactly. In this section, we provide further experimental results over the optimal bank selection problem of Section 4, which, to the best of our knowledge, has not been considered in the context of SPL decompositions before. In addition to that, we also compare our approach with the classical algorithms for NP-hard problems, including SAT and ILP. 

Implementation. We implemented our bank selection algorithm in C++ and integrated it with the Small Device C Compiler (SDCC) [ 20 , 21 ]. We will publish the implementation as free and open-source software dedicated to the public domain and link it in the final version of this article. The link was removed in the review phase to preserve anonymity. 

Baseline. We compared our algorithm’s runtime with the treewidth-based parameterized approach of [ 32 ], which is the current state-of-the-art in bank selection. That approach has an asymptotic runtime of O(|G| · tw (G) · | D|tw (G)+1 )

where tw (G) is the treewidth of G, which can theoretically be up to 7 [40 ]. We did not consider other previous methods since they either have exponential runtime dependence on the program size [ 38 , 39 ] or no guarantee on the quality or optimality of the results [34]. 

Machine. All experiments were performed on a virtual machine with Oracle Linux (ARM 64-bit), equipped with 1 core of an Apple M2 processor and 4GB of RAM. 

Benchmarks. We exactly followed the setup of [ 32 ], utilizing the SDCC regression test suite as our benchmark set. This suite comprises a total of 13 , 463 instances for HC08, 179 , 611 instances for Z80, and 84 , 700 instances for MCS51. These benchmarks are embedded programs expected to operate in resource-constrained environments. Therefore, there is a strong focus is on code size optimization. Thus, we used the following standard cost function: 

– c(e, b, b ) = c(e, b, ⊥) = 0; 

– c(e, b 0, b 1) = 1 when e is an edge from a taken conditional branch; 

– For all other cases, c(e, b 0, b 1) = 1 .We only compare the runtimes. There is no output comparison since both our approach and [ 32 ] find an optimal solution for bank selection and the outputs coincide. 16 X. Cai, A.K. Goharshady 

Runtimes. Figure 8 provides a comparison of the runtimes of our algorithm and that of [ 32 ] over the three different architectures. For HC08, our algorithm took an average of 1, 998 .3 nanoseconds, while the treewidth-based approach took an average of 8, 558 .8 nanoseconds. For Z80, our algorithm took an average of 2, 402 .1 nanoseconds, while the treewidth-based approach took an average of 8, 209 .3 nanoseconds. For MCS51, our algorithm took an average of 1, 834 .3

nanoseconds, while the treewidth-based approach took an average of 6, 268 .3

nanoseconds.   

> (a) HC08 (b) Z80 (c) MCS51

Fig. 8: Runtime comparison of the treewidth-based algorithm of [ 32 ] (orange) and our approach (green). The x axis is the number of vertices in the CFG and the y axis is the time in nanoseconds. The y axis is in logarithmic scale. 

Comparison with SAT and ILP Solvers. Since Optimal Bank Selection is NP-complete, we compare our runtimes with state-of-the-art approaches for NP-complete problems via SAT and ILP solving. We encoded the problem in the natural way in both SAT and ILP. We then used the state-of-the-art SAT solver, Kissat [ 23 ], and the industrial ILP solver Gurobi [ 28 ] over the benchmarks. The results, as shown in Figure 9, illustrate that our method is nearly 10 times faster than the ILP approach, which is itself approximately 100 times faster than the SAT solver. Partial Constraint Satisfaction Problems over CFG 17 

Fig. 9: Runtime comparison of the treewidth-based algorithm of [ 32 ] (orange) and our approach (green) compared to SAT-based (brown) and ILP-based (purple) methods.The x axis is the number of vertices in the CFG and the y axis is the time in nanoseconds. The y axis is in logarithmic scale. 

Discussion. In summary, our approach is approximately four times faster than the previous state-of-the-art for bank selection. This is a notable improvement, particularly given that the treewidth-based approach of [ 32 ] is already highly optimized and included in the well-established SDCC compiler. We believe the speed-up is mainly because our algorithm’s runtime does not depend on parame-ters such as treewidth and the constant factor hidden in our O(n) asymptotic runtime analysis is small in practice. 

# 6 Conclusion 

In this work, we present an efficient linear-time algorithm for graph PCSPs over control-flow graphs, leveraging SPL decompositions [ 7]. Our solution applies to various compiler optimization tasks, including register allocation [ 7], Lifetime-optimal Speculative Partial Redundancy Elimination (LOSPRE) [ 6], and Optimal Bank Selection. The main contribution is a dynamic programming algorithm that addresses all graph PCSP problems, eliminating the need for separate algorithms for different optimization tasks. Our solution demonstrates significant theoretical performance improvements for bank selection, and experiments show it is approximately four times faster than the treewidth-based approach of [ 32 ]in practice. 18 X. Cai, A.K. Goharshady 

# References 

1. Ahmadi, A., Chatterjee, K., Goharshady, A.K., Meggendorfer, T., Safavi, R., Zikelic, Ð.: Algorithms and hardness results for computing cores of markov chains. In: FSTTCS. pp. 29:1–29:20 (2022) 2. Ahmadi, A., Daliri, M., Goharshady, A.K., Pavlogiannis, A.: Efficient approxima-tions for cache-conscious data placement. In: PLDI. pp. 857–871 (2022) 3. Asadi, A., Chatterjee, K., Goharshady, A.K., Mohammadi, K., Pavlogiannis, A.: Faster algorithms for quantitative analysis of MCs and MDPs with small treewidth. In: ATVA. vol. 12302, pp. 253–270 (2020) 4. Bodlaender, H.L., Gustedt, J., Telle, J.A.: Linear-time register allocation for a fixed number of registers. In: SODA. pp. 574–583 (1998) 5. Burgstaller, B., Blieberger, J., Scholz, B.: On the tree width of ada programs. In: Ada-Europe. pp. 78–90 (2004) 6. Cai, X., Goharshady, A.: Faster lifetime-optimal speculative partial redundancy elimination for goto-free programs. In: SETTA. pp. 382–398. Springer (2024) 7. Cai, X., Goharshady, A.K., Hitarth, S., Lam, C.K.: Faster chaitin-like register allocation via grammatical decompositions of control-flow graphs. In: ASPLOS. p. 463–477. ACM (2025). https://doi.org/https://doi.org/10.1145/3669940. 3707286 

8. Chatterjee, K., Goharshady, A.K., Goharshady, E.K.: The treewidth of smart contracts. In: SAC. pp. 400–408. ACM (2019) 9. Chatterjee, K., Goharshady, A.K., Goyal, P., Ibsen-Jensen, R., Pavlogiannis, A.: Faster algorithms for dynamic algebraic queries in basic rsms with constant treewidth. ACM Trans. Program. Lang. Syst. 41 (4), 23:1–23:46 (2019) 10. Chatterjee, K., Goharshady, A.K., Ibsen-Jensen, R., Pavlogiannis, A.: Algorithms for algebraic path properties in concurrent systems of constant treewidth components. In: POPL. pp. 733–747 (2016) 11. Chatterjee, K., Goharshady, A.K., Ibsen-Jensen, R., Pavlogiannis, A.: Optimal and perfectly parallel algorithms for on-demand data-flow analysis. In: ESOP. pp. 112–140 (2020) 12. Chatterjee, K., Goharshady, A.K., Okati, N., Pavlogiannis, A.: Efficient parameter-ized algorithms for data packing. Proc. ACM Program. Lang. 3(POPL), 53:1–53:28 (2019) 13. Chatterjee, K., Goharshady, A.K., Pavlogiannis, A.: Jtdec: A tool for tree decom-positions in soot. In: ATVA. vol. 10482, pp. 59–66 (2017) 14. Chatterjee, K., Ibsen-Jensen, R., Goharshady, A.K., Pavlogiannis, A.: Algorithms for algebraic path properties in concurrent systems of constant treewidth components. ACM Trans. Program. Lang. Syst. 40 (3), 9:1–9:43 (2018) 15. Chatterjee, K., Lacki, J.: Faster algorithms for markov decision processes with low treewidth. In: CAV. pp. 543–558 (2013) 16. Conrado, G.K., Goharshady, A.K., Kochekov, K., Tsai, Y.C., Zaher, A.K.: Exploiting the sparseness of control-flow and call graphs for efficient and on-demand algebraic program analysis. Proc. ACM Program. Lang. 7(OOPSLA2), 1993–2022 (2023) 17. Conrado, G.K., Goharshady, A.K., Lam, C.K.: The bounded pathwidth of control-flow graphs. Proc. ACM Program. Lang. 7(OOPSLA2), 292–317 (2023) 18. Dechter, R., Pearl, J.: Network-based heuristics for constraint-satisfaction problems. Artificial Intelligence 34 (1), 1–38 (1987). https://doi.org/https://doi.org/10. 1016/0004-3702(87)90002-6 

19. Dechter, R., Pearl, J.: Tree-clustering schemes for constraint-processing. In: AAAI. p. 150–154 (1988) 20. Dutta, S.: Anatomy of a compiler: A retargetable ansi-c compiler. Circuit Cellar 

121 (5) (2000) Partial Constraint Satisfaction Problems over CFG 19 21. Dutta, S., Drotos, D., Vigor, K., et al.: Small device C compiler (2003), http: //sdcc.sourceforge.net/ 

22. Ferrara, A., Pan, G., Vardi, M.Y.: Treewidth in verification: Local vs. global. In: LPAR. vol. 3835, pp. 489–503 (2005) 23. for Formal Models, I., Verification: Kissat-public (2025), https://fmv.jku.at/ kissat/ 

24. Freuder, E.C.: Complexity of k-tree structured constraint satisfaction problems. In: AAAI. pp. 4–9 (1990) 25. Freuder, E.C., Wallacex, R.J.: Partial constraint satisfaction 58 , 21–70 (1992). 

https://doi.org/https://doi.org/10.1016/0004-3702(92)90004-H 

26. Goharshady, A.K., Lam, C.K., Parreaux, L.: Fast and optimal extraction for sparse equality graphs. In: OOPSLA (2024) 27. Goharshady, A.K., Zaher, A.K.: Efficient interprocedural data-flow analysis using treedepth and treewidth. In: VMCAI. vol. 13881, pp. 177–202 (2023) 28. Gurobi Optimization, L.: Gurobi (2008), https://www.gurobi.com/ 

29. Gustedt, J., Mæhle, O.A., Telle, J.A.: The treewidth of java programs. In: ALENEX. Lecture Notes in Computer Science, vol. 2409, pp. 86–97. Springer (2002) 30. Koster, A.M.C.A., van Hoesel, S.P.M., Kolen, A.W.J.: Solving partial constraint satisfaction problems with tree-decomposition 40 , 170–180 (2002). https://doi. org/https://doi.org/10.1002/net.10046 

31. Koster, A.M., Hoesel, S.P., Kolen, A.W.: The partial constraint satisfaction problem: Facets and lifting theorems. Operations Research Letters 23 (3), 89–97 (1998). 

https://doi.org/https://doi.org/10.1016/S0167-6377(98)00043-1 

32. Krause, P.K.: Optimal placement of bank selection instructions in polynomial time (2013). https://doi.org/https://doi.org/10.1145/2463596.2463598 

33. Krause, P.K.: Optimal register allocation in polynomial time. In: CC. vol. 7791, pp. 1–20 (2013) 34. Liu, T., Xue, C.J., Li, M.: Joint variable partitioning and bank selection instruction optimization for partitioned memory architectures. ACM Trans. Embed. Comput. Syst. 12 (2013). https://doi.org/https://doi.org/10.1145/2442116.2442126 

35. Mackworth, A.K., Freuder, E.C.: The complexity of some polynomial network consis-tency algorithms for constraint satisfaction problems. Artificial Intelligence 25 , 65– 74 (1985). https://doi.org/https://doi.org/10.1016/0004-3702(85)90041-4 

36. Obdrzálek, J.: Fast mu-calculus model checking when tree-width is bounded. In: CAV. pp. 80–92 (2003) 37. Sankaranarayanan, S.: Reachability analysis using message passing over tree de-compositions. In: CAV. pp. 604–628 (2020) 38. Scholz, B., Burgstaller, B., Xue, J.: Minimizing bank selection instructions for partitioned memory architecture. In: CASES. p. 201–211. ACM (2006). https: //doi.org/https://doi.org/10.1145/1176760.1176786 

39. Scholz, B., Burgstaller, B., Xue, J.: Minimal placement of bank selection instructions for partitioned memory architectures. ACM Trans. Embed. Comput. Syst. 7 (2008). 

https://doi.org/https://doi.org/10.1145/1331331.1331336 

40. Thorup, M.: All structured programs have small tree width and good register allocation. Inf. Comput. 142 , 159–181 (1998). https://doi.org/https://doi. org/10.1006/inco.1997.2697