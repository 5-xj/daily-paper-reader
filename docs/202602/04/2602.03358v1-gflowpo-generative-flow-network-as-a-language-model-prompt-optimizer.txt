Title: GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer

URL Source: https://arxiv.org/pdf/2602.03358v1

Published Time: Wed, 04 Feb 2026 01:59:25 GMT

Number of Pages: 25

Markdown Content:
# GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

Junmo Cho * 1 Suhan Kim * 2 Sangjune An * 2 Minsu Kim 1 3 Dong Bok Lee 1

Heejun Lee 1 Sung Ju Hwang 1 4 Hae Beom Lee 2

## Abstract 

Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sam-pled from a fixed distribution, leading to poor sample efficiency. We propose GF LOW PO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference prob-lem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an 

off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, in-struction induction benchmarks, and question an-swering tasks, GF LOW PO consistently outper-forms recent discrete prompt optimization base-lines. 

## 1. Introduction 

Injecting appropriate prompts into language models (LMs) is critical to their performance, as small changes in prompts can lead to significant differences in model outputs (Salinas & Morstatter, 2024; Chatterjee et al., 2024). As a result, exploration over the prompt space has become as important 

> *

Equal contribution 1Korea Advanced Institute of Science and Technology (KAIST) 2Korea University 3Mila – Qu´ ebec AI Insti-tute 4DeepAuto.ai. Correspondence to: Hae Beom Lee <haebeom-lee@korea.ac.kr >.

Preprint. February 4, 2026. 

as optimization over the parameter space of LMs, giving rise to new gradient-free learning paradigms such as in-context learning, training-free reinforcement learning, and related approaches (Deng et al., 2022; Yang et al., 2024; Fernando et al., 2024). However, prompt engineering in practice remains a highly manual and labor-intensive process, relying heavily on human intuition and empirical trial-and-error. To address this limitation, automatic prompt optimization has recently gained popularity (Ramnath et al., 2025). In this setting, searching over the prompt space can be viewed as a black-box combinatorial optimization problem, where the goal is to maximize the performance of an LM on a target task. Reinforcement learning (RL) has emerged as one of the most promising approaches for automating this process, particularly by fine-tuning prompt-generating LMs (prompt-LMs) that already encode rich prior world knowl-edge (Kwon et al., 2024; Batorski et al., 2025). By guiding such prompt-LMs with a meta-prompt and optimizing them through RL, these models can evolve to generate effective prompts for improving target LM performance. Despite their promise, existing RL-based approaches face exploration challenges in prompt optimization over large combinatorial spaces. First, many methods rely on on-policy learning, where training signals are obtained from Monte Carlo estimates based on samples from the current policy. In prompt optimization settings with sparse rewards (due to expensive cost of evaluating target LMs) and massive action space, this often requires a large number of samples to obtain reliable gradients, leading to poor sample efficiency. Second, prompt search performance critically depends on contextual conditioning, yet the meta-prompt of the prompt-LM is sampled from a fixed distribution. This conditioning limits the incorporation of accumulated high-reward experience, resulting in weakly guided exploration and further reducing sample efficiency, as shown in Fig. 1(a). In this paper, we propose a novel probabilistic framework for prompt optimization, termed GF LOW PO, which ad-dresses the aforementioned key limitations of existing RL-based approaches. GF LOW PO consists of two alternating steps. STEP -A : sample-efficient off-policy posterior in-ference over latent prompts using Generative Flow Net-works (GFlowNets; Bengio et al., 2021), and STEP -B :1

> arXiv:2602.03358v1 [cs.AI] 3 Feb 2026 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer

training-free updates of the conditioning meta-prompt with the posterior samples in the second step. This design allows the search process to progressively concentrate more on high-reward regions of the prompt space, thereby signifi-cantly improving the sample efficiency of exploration. An overview of the framework is illustrated in Fig. 1(b). Specifically, in STEP -A, we use GFlowNets (Bengio et al., 2021; 2023), an off-policy soft RL framework for amortized inference that samples solutions proportionally to reward. We fine-tune a prompt-LM using the VarGrad objective (Richter et al., 2020; Zhang et al., 2023a) of GFlowNets with a replay-based training scheme. By reusing past experiences, this first step significantly improves sample efficiency and enables effective exploration under sparse reward signals. In STEP -B, we update the meta-prompt, an instruction given to both the prompt-LM and the prior reference-LM, through a Dynamic Memory Update (DMU) mechanism. DMU roughly maximizes the marginal log-likelihood without any parameter updates. DMU maintains two complementary memory buffers: (1) a replay buffer that stores diverse prompts sampled during GFlowNet training, and (2) a high-reward buffer that retains a small set of top-performing prompts encountered so far. At each update step, DMU constructs reference prompts by sampling from both buffers, leveraging high-reward prompts to encourage exploitation and drawing diverse prompts to preserve exploration at the same time. These reference prompts are then injected into the meta-prompt, enabling training-free memory updates that guide subsequent prompt search towards high-reward regions of the prompt space (Fig. 1(b)). To validate the efficacy of our method, we conduct extensive experiments across a diverse set of tasks and LLMs. The datasets includes text classification (Wang et al., 2018), text understanding (Wang et al., 2019), instruction induction (Honovich et al., 2023; Ghazal et al., 2013), and question answering (Wang et al., 2021; Mihaylov et al., 2018). We consider both prompt-LMs and target LMs with model sizes ranging from 2B to 13B, including Llama (Touvron et al., 2023), Mistral (Jiang et al., 2023), Gemma (Team et al., 2024), and Falcon (Almazrouei et al., 2023). Across these settings, our method consistently achieves strong perfor-mance across diverse tasks and model scales. We summarize our main contributions as follows: • We cast prompt optimization as a posterior inference and propose GF LOW PO as an effective instance of it. • We propose to use off-policy GFlowNet replay training scheme for sample-efficient prompt search. • We introduce a training-free Dynamic Memory Update (DMU) to progressively adapt the prompt search towards higher reward region in the search space. • We demonstrate strong performance of GF LOW PO across Optimal 

> Prompt
> Initial
> Prompt
> Optimal
> Prompt

(b) (a)         

> Initial
> Prompt
> …
> Figure 1. Concepts. Blue contour indicates high performing prompt regions. (a) Existing on-policy RL frameworks fail to explore the huge combinatorial search space with poor sample efficiency. (b) Our GF LOW PO that can sample efficiently ex-plore the search space by gradually annealing the posterior density (dotted ellipses) towards more promising area with the off-policy GFlowNet training in STEP -A and DMU in STEP -B.

multiple tasks, including text classification and generation, and across diverse prompt-target LM pairs. 

## 2. Related Work 

Prompt optimization. Prompt optimization (or prompt tuning) has been explored in previous works as task-specific continuous vectors tuned by gradient-based methods to im-prove task performance (Li & Liang, 2021; Lester et al., 2021; Qin & Eisner, 2021). Discrete prompts, on the other hand, involve searching for discrete vocabulary to-kens through gradients (Shin et al., 2020; Shi et al., 2023). Recently, Das et al. (2025) proposed a GReaTer, a gradient-based approach to discrete prompt optimization leverag-ing task loss gradients. A complementary line of work formulates prompt optimization as an RL problem, where an agent model is trained through gradient-based updates to generate or condition prompts (Deng et al., 2022; Zhang et al., 2023b). Recently, Kwon et al. (2024) pro-posed StablePrompt, a scalable and stable RL-based ap-proach by modeling the policy itself as a LM. Compared to StablePrompt (Kwon et al., 2024), which fine-tunes a prompt-LM via on-policy PPO under meta-prompt with randomly sampled few-shot examples, GF LOW PO uses off-policy GFlowNet replay training and a training-free Dynamic Memory Update (DMU) that injects diverse and high-reward prompts into the meta-prompt, enabling more sample-efficient exploration in combinatorial prompt space. 

LLMs as prompt optimizers. Early work such as APE (Zhou et al., 2023) demonstrated that LLMs can be directly used as prompt optimizers by inferring instructions from in-put–output pairs, while subsequent methods formalized this idea with textual gradients (Pryzant et al., 2023). A large body of follow-up work has explored optimization in text space (Yang et al., 2024; Yuksekgonul et al., 2025), includ-ing meta-prompt engineering (Ye et al., 2024), agent-based reasoning (Wang et al., 2024; Shinn et al., 2023), Bayesian 2GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer I gave a friend an  

> instruction and five
> inputs …
> Here are some refer -
> ence instructions:
> The instruction is:
> Replay
> buffer

Prompt : question: 

> answer :
> positive

Compute reward Update High -reward 

> buffer

Dynamic 

Memory 

Update GFlowNet 

fine -tuning  

> Write a statement indicating whether or not a
> cinematic poem is worth watching based on
> the given input.
> You will be given two sentences. Based on
> the content of the sentences, determine which
> sentence is the cause and which is the effect.
> Read the choices and problem carefully and
> choose correct one
> Text
> Classification
> SST -2
> Text
> Generation
> cause and effect
> Question
> Answering
> MMLU
> The wind
> blew strong.
> negative

Meta -prompt Target LM 

Gemma -7B 

Mistral -7B 

Llama -3-8B 

Prompt -LM 

Gemma -7B 

Mistral -7B 

Llama -3-8B 

Figure 2. GFlowPO pipeline. The optimizer prompt-LM samples prompts conditioned on meta-prompt M , the target LLM provides rewards, and off-policy GFlowNet training plus Dynamic Memory Update (DMU) iteratively improves exploration and prompt quality. 

optimization (Agarwal et al., 2025; Liu et al., 2024), and RL (Prasad et al., 2022; Hou et al., 2023; Dong et al., 2024). Most existing methods rely on powerful, closed-source LLMs, since prompt optimization with lightweight mod-els often leads to degraded performance (Zhang et al., 2024). In contrast, our method directly fine-tunes a lightweight, open-source prompt-LM as the prompt optimizer without relying on heavy external LLMs. 

Generative Flow Networks. Generative Flow Net-works (GFlowNets; Bengio et al., 2021; 2023) are a family of reinforcement learning methods for performing amortized inference, sampling proportionally to an unnormalized den-sity or reward. GFlowNets model trajectories from an initial state to a terminal state in a step-by-step manner, where the terminal state represents a compositional objective such as a graph or a string. Such sequential decision-making policies and value functions (i.e., flows) are parameterized by deep networks and learned via constraint-based objec-tives. Based on the level of constraint coverage, representa-tive objectives include detailed balance (DB; Bengio et al., 2023) and flow matching (Bengio et al., 2021), which en-force local one-step transition constraints; sub-trajectory bal-ance (SubTB; Madan et al., 2023), which matches multi-step transition constraints; and trajectory balance (TB; Malkin et al., 2022), which enforces global constraints over full trajectories. Zhang et al. (2023a) proposed variants of TB in the form of log-partition gradients (VarGrad; Richter et al., 2020), which estimate the partition flow from minibatch data rather than learning it directly. 

Language model post-training with GFlowNets. The amortized inference capability of GFlowNets has been ap-plied to language model post-training, as it is beneficial for promoting diversity and enabling efficient off-policy sampling. Hu et al. (2024) applies GFlowNets with expec-tation–maximization (GFlowEM; Hu et al., 2023) to infill-ing tasks and chain-of-thought reasoning. Yu et al. (2025) applied GFlowNets to mathematical and puzzle reasoning using an off-policy local search method (Zhang et al., 2022; Kim et al., 2024), while Bartoldson et al. (2025) applied decentralized GFlowNets for faster reasoning-LM training. Lee et al. (2025) applied GFlowNets to LM red-teaming with a focus on diversity, which Yun et al. (2025a) further extends to an active learning-based red-teaming framework. Yun et al. (2025b) applied GFlowNets with local credit as-signment (Pan et al., 2023) for diverse prompt generation into text-to-image model. Our work is also an LM post-training application of GFlowNets, but to the best of our knowledge, it is the first approach to use in-context memory updates in both the prior and posterior distributions. 

## 3. Approach 

We now introduce GF LOW PO; the pipeline and pseudocode are provided in Fig. 2 and Algorithm 1, respectively. 

3.1. Overview of GF LOW PO 

Given a task-specific distribution p(x, y ) over context x and answer y, we aim to optimize a prompt z that maximizes the performance of a target LM over p(x, y ):

max  

> z

Ep(x,y )

f (ˆ y(x, z ), y ). (1) where ˆy(x, z ) denotes the prediction of the target LM con-ditioned on context x and prompt z (e.g., greedy decoding). 

f is a task-specific evaluation metric (e.g., accuracy). 

Prompt optimization as a posterior inference problem. 

In usual prompt optimization settings, p(x, y ) is unknown and only a tiny set of training dataset D = {(xi, y i)}ni=1 

is given for each task. We thus consider the following Bayesian posterior inference problem to prevent overfitting and adhere to linguistic plausibility: 

p(z|D , M ) ∝ p(D| z) · pref (z|M ), (2) 3GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

where pref (z|M ) is the conditional prior over prompts in-duced by a reference LM , conditioned on the meta-prompt 

M which is an instruction given to the reference LM as an input (see Fig. 3 for a meta-prompt template). p(D| z) is the likelihood, and p(z|D , M ) is the corresponding poste-rior. In this way, we can effectively regularize the prompt solution under scarce data and huge combinatorial search space, balancing task performance p(D| z) and linguistic plausibility via the reference LM pref (z|M ).Note that the RHS in Eq. (2) is unnormalized and highly multi-modal in the huge combinatorial search space, mak-ing the inference problem very challenging. We thus use GFlowNets (Bengio et al., 2021) to fine-tune another LM 

pθ (z|M ), which we call prompt-LM , to approximately amor-tize the posterior p(z|D , M ) in Eq. (2). GFlowNets are a natural option to model a highly multi-modal unnormalized distribution, especially when the search space is discrete. GFlowNets also enable more sample-efficient off-policy training compared to the existing on-policy RL which re-sorts to MC estimates based on samples from on-policy, resulting in poor exploration under huge search spaces. This GFlowNet stage is what we call STEP -A .

Gradual annealing of search area. Ideally, we want our approximate posterior pθ to find all the modes in the search space, but GFlowNet (and other RL-based methods) can focus only on a relatively narrow region at a time (Pan et al., 2024). Therefore, the role of meta-prompt M is critical to properly guiding the search over the huge combinatorial search space. To this end, we propose to update M to 

reshape the focus of search area gradually, by maximizing the following marginal log-likelihood for each iteration with respect to M :

log p(D| M ) = log X

> z

p(D| z) pref (z|M ) (3) Maximizing the marginal likelihood means that the meta-prompt M is adapted to the given task data D, thereby progressively concentrating the focus of the approximate posterior pθ (z|M ), the prior pref (z|M ), and the correspond-ing target posterior into higher-reward prompt regions, as shown in Fig. 1(b). Exactly maximizing Eq. (3) w.r.t. M is another difficult combinatorial optimization problem. Also, we want to keep the same meta-prompt template throughout the training (see Fig. 3). Therefore, we use a simple heuristic: we let M

include a few reference prompts that guide the search, and we simply update those reference prompts into newer ones that can roughly solve Eq. (3) based on a lower bound of it. While simple, we empirically observe that such training-free heuristic update works well in practice. This marginal likelihood maximization stage is what we call STEP -B .We alternate between STEP -A and STEP -B for each iteration. The optimal prompt can be chosen by storing one or a few top-performing prompts found throughout the training. We next detail STEP -A in §3.2 and STEP -B in §3.3, respectively; together, they constitute the proposed GF LOW PO. 

3.2. STEP -A: Off-policy Training with GFlowNets 

Generative Flow Networks (GFlowNets; Bengio et al., 2021) are off-policy RL algorithms designed for amortized in-ference of distributions of the form p(z) ∝ R(z). This framework naturally applies to approximating the posterior in Eq. (2) by defining an unnormalized target reward as 

R(z; M ) = p(D| z) · pref (z|M ). However, we empirically observed that the training likelihood p(D| z) is weakly cor-related with the actual test accuracy for most of the prompt optimization tasks we considered (Appendix A). We thus replace the likelihood with the training correct count AD (z):

R(z; M ) = AD (z) · pref (z|M ) (4) where AD (z) := ϵ + P 

> (x,y )∈D

f (ˆ y(x, z ), y ) and ϵ ∈ R>0

is a small constant to prevent AD (z) from reducing to 0.

Objective function. GFlowNets are trained via consistency-matching objectives that enforce forward– backward flow consistency with the target reward. In autoregressive settings such as language models, the backward transitions are fixed by the tokenization order, causing the objective to reduce to Path Consistency Learning (PCL; Nachum et al., 2017). We adopt a global path consistency-matching objective of the form 

L(θ; M )= Ez∼π(z)

h

(log Z + log pθ (z|M ) − log R(z; M )) 2i

,

(5) where log Z denotes the log-partition function and π(z) the training policy, e.g., z is sampled from a tempered version of 

pθ or a replay buffer. Under a sufficiently broad support of 

π, we can encourage pθ (z|M ) to asymptotically converge to the true target posterior. The log-partition log Z plays the role of a global value for each global path z. In Trajectory Balance (TB; Malkin et al., 2022) and PCL, log Z is explicitly parameterized and learned. In contrast, VarGrad (Richter et al., 2020; Zhang et al., 2023a) estimates log Z from minibatch sam-ples {zi}Bi=1 

> i.i.d.

∼ π(z) as 

log Z ≈ 1

B

> B

X

> i=1



log R(zi; M ) − log pθ (zi|M )



. (6) This relationship mirrors the distinction between value-based and value-free policy optimization methods (e.g., PPO vs. GRPO), where explicit value learning is replaced by minibatch-based estimation. In this work, we adopt the 4GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

VarGrad objective, as explicitly learning log Z was found to be unstable in prompt optimization tasks. Further, we smooth the log Z estimation with exponential moving aver-age (EMA) for learning stability under small B.

Replay buffer and off-policy learning. A key advan-tage of the GFlowNet objective is its compatibility with off-policy learning. Unlike on-policy methods such as PPO commonly used in prior prompt optimization work, GFlowNets naturally support replay-based training. Specifi-cally, we let the training policy π(z) be a mixture of ˜pθ , a tempered version of pθ , and a uniform distribution uB over the replay buffer B, which stores (z, A D (z)) pairs collected from previous iterations: 

π(z) = ρ˜pθ (z) + (1 − ρ)uB(z) (7) where ρ ∈ [0 , 1] (e.g., ρ = 0 .5). Therefore, our off-policy training scheme substantially improves sample efficiency and stabilizes learning in the large and sparse prompt space. Note that, in the reward R(z; M ) = AD (z) · pref (z|M ), the prior pref keeps evolving as M is updated in our STEP -B. We thus need to re-evaluate pref (z|M ) every iteration for correct reward calculation, even with our off-policy training scheme. However, the cost of evaluating pref (z|M ) is significantly less than that of AD (z), which makes it sufficient to store 

(z, A D (z)) pairs alone, preserving the sample efficiency of our off-policy training scheme. In addition, we find that initializing the replay buffer B with the prompts sampled from the initial pref (z|M ) facilitates exploration in the early stage of training. More details are provided in Appendix B. 

3.3. STEP -B: Dynamic Memory Update 

While STEP -A performs amortized inference of the posterior over prompt space under a fixed meta-prompt M , STEP -B updates the meta-prompt itself ( M → M ′) and correspond-ingly adapts the focus of search area with the updated ap-proximate posterior pθ (z|M ′) and reward R(z; M ′). We refer to this process as Dynamic Memory Update (DMU). 

Reference prompt update. We use a meta-prompt similar to Zhou et al. (2023) and Kwon et al. (2024). As illustrated in Fig. 3, the meta-prompt M consists of (1) task-agnostic instructions, (2) k-shot input-output pairs {(xi, y i)}ki=1 ran-domly sampled from D, and (3) reference prompts Zref .Here, we focus on updating Zref in the meta-prompt M .

> STEP

-B should maximize the marginal log-likelihood in Eq. (3), which is intractable. We thus consider the following variational lower bound (Blei et al., 2017) of it instead: 

max  

> M

Epθ (z|M ) [log p(D| z)] 

| {z } 

> α: Accuracy ↑

− KL[ pθ ∥pref ]

| {z } 

> β: Discrepancy ↓

. (8) The first term α promotes higher accuracy, whereas the second term β suppresses discrepancy between pθ (z|M )

Meta-prompt template                

> I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs:
> Input: x1Output: y1
> Input: x2Output: y2
> Input: ...Output: ...
> Input: x5Output: y5
> Here are some reference instructions: Zref
> The instruction is:
> Figure 3. Meta-prompt template used in GF LOW PO.

and pref (z|M ).However, exactly solving Eq. (8) is also a difficult com-binatorial optimization problem. DMU circumvents this difficulty by finding a few prompts {z} that can roughly solve Eq. (8) and use them to construct Zref . Specifically, at each iteration, we carefully sample Zref from the two types of buffers: (1) a replay buffer B that stores the previous prompts sampled from pθ throughout the training, and (2) a small high-reward buffer Q that keeps a few prompts with the highest accuracy so far: 

Zref = {z(i) 

> b

}kb 

> i=1

∪ { z(j) 

> q

}kq

> j=1

, where 

{z(i) 

> b

}kb

> i=1
> i.i.d.

∼ uB(z), {z(j) 

> q

}kq

> j=1
> i.i.d.

∼ uQ(z)

(9) where uB and uQ are uniform distributions over B and Q,respectively. Such a simple heuristic is computationally effi-cient and does not incur any additional parameter updates. The rationale behind Q is simple: we simply select the prompts with the highest accuracy to maximize α. The intuition of B is as follows: the discrepancy β can be mini-mized if Zref is constructed with z’s sampled from a mixture of pθ and pref , i.e., the update operation M → M ′ blends 

pθ (z|· ) and pref (z|· ). Considering that pθ is initialized as a copy of pref and deviates from pref as training goes on, we simply construct B with the prompts sampled from pθ

in the previous training iterations. We found that kb = 2 

and kq = 1 balance well between performance and com-putational cost. Also, note that samples from B promote exploration, whereas samples from Q encourage exploita-tion. More details can be found in Appendix C. 

## 4. Experiment 

We next show the efficacy of our method over diverse bench-mark datasets and settings. The hyperparameters used in GF LOW PO are provided in Appendix D. For all our ex-periments, the prompt-LM is fine-tuned with LoRA (Hu et al., 2022), and each run took approximately one hour on 5GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

Table 1. Results on six few-shot text classification datasets. GF LOW PO achieves the best average accuracy across all datasets. Baseline results, including StablePrompt, are taken from Kwon et al. (2024). For GF LOW PO, we report average accuracy with standard deviation over three runs using different random seeds. The best results are bolded , and the second-best results are underlined. METHOD SST-2 MRPC RTE QNLI MNLI SNLI AVERAGE 

FINE -T UNING FINE -T UNING 71.9 59.6 55.7 63.1 41.1 64.8 59.3 SOFT PROMPT TUNING 78.3 57.1 51.6 89.0 34.9 55.8 61.1 FIXED PROMPT 

MANUAL PROMPT 89.1 51.0 64.0 73.0 67.0 47.0 65.2 ZERO -SHOT COT 57.9 38.4 81.6 75.2 71.1 66.3 65.1 FEW -SHOT PROMPT 55.0 49.0 76.0 82.0 58.0 52.2 62.0 DISCRETE 

PROMPT TUNING 

GRIPS 84.7 ±4.6 55.6 ±2.6 60.9 ±3.5 28.9 ±1.2 44.4 ±1.1 63.5 ±2.3 59.4 PROMPT BOOSTING 65.4 ±1.0 52.7 ±1.1 71.6 ±0.9 71.6 ±1.1 35.5 ±1.4 52.6 ±1.8 58.2 APE 83.2 ±7.7 55.3 ±4.9 78.6 ±1.3 75.0 ±2.2 54.6 ±7.9 72.3 ±4.8 70.1 PRO TEGI 69.2 ±8.4 48.8 ±1.3 73.2 ±6.3 74.2 ±7.7 56.6 ±10.9 61.3 ±12.3 64.0 RL PROMPT 70.8 ±6.5 56.0 ±1.5 67.3 ±2.5 62.6 ±1.3 54.6 ±1.9 56.6 ±1.3 61.3 STABLE PROMPT 92.5 ±1.3 71.3 ±3.4 81.5 ±2.8 75.9 ±1.4 63.3 ±1.2 74.1 ±1.4 76.4 

GF LOW PO 93.0 ±0.6 69.6 ±4.2 82.0 ±2.5 80.2 ±3.4 68.7 ±3.2 78.6 ±2.7 78.7 

a single H100 GPU. We present the full generated prompts in Appendix H. The baselines are considered as follows. 

Baselines. We first consider supervised fine-tuning ap-proaches, including LoRA-based Fine-Tuning and Soft Prompt Tuning (Bailey et al., 2023). We also evaluate fixed prompting strategies, such as hand-crafted Manual Prompt , Few-shot Prompt , and zero-shot chain-of-thought (Zero-Shot CoT ) prompting (Wei et al., 2022). For a direct comparison with our approach, we further include several discrete prompt-tuning methods, spanning generation-based approaches such as APE (Zhou et al., 2023) and ProTeGi 

(Pryzant et al., 2023) and RL-based methods such as GrIPS 

(Prasad et al., 2022), PromptBoosting (Hou et al., 2023), and RLPrompt (Deng et al., 2022). Lastly, we include Sta-blePrompt (Kwon et al., 2024), which directly fine-tunes the prompt-LM using on-policy PPO with accuracy-based rewards. StablePrompt uses the same meta-prompt as our method, except for the reference prompts (see Appendix I), making it the most direct baseline for isolating the effect of our off-policy GFlowNet objective and DMU mechanism. 

4.1. Few-Shot Text Classification Datasets. Consistent with common practice in prompt optimization research, we consider subsets of GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019), including sentiment analysis (SST-2) and natural language inference datasets (MRPC, MNLI, QNLI, SNLI, and RTE). During inference, we use a verbalizer that maps each answer class to a predefined label token. When determining the prediction of the target LM, we compute the probability of predefined label tokens from verbalizer and select the token that has the highest probability among them. Dataset statistics and verbalizer settings are summarized in Appendix E. 

Implementation details. We consider two settings on few-shot text classification task. In the first setting, we MP G2 G7 M7 L8 

Prompt-LM 

L8 

M7 

G7 

F11                 

> Target LM  55.4 78.1 78.7 81.0 79.0
> 62.5 77.9 81.9 81.9 83.7
> 71.2 74.1 78.7 78.5 78.2
> 70.9 78.8 82.3 82.7 82.2

Figure 4. Heatmap of performances on few-shot text classification tasks (6 datasets) with various prompt-LM, target LM pairs. Each number is averaged over six tasks. MP : Manual Prompt, G2 :Gemma-2B, G7 : Gemma-7B, M7 : Mistral-7B, L8 : Llama-3-8B, 

F11 : Falcon-11B. GF LOW PO works well across various (prompt-LM, target LM) pairs. 

experiment with both prompt-LM and target LM fixed to gemma-1.1-7B-it (Gemma-7B; Team et al., 2024) to see how GF LOW PO performs compared to baselines. In the second setting, we run GF LOW PO on four target LMs: Gemma-7B, Mistral-7B-it-v2.0 (Mistral-7B; Jiang et al., 2023), llama3-8B-it (Llama3-8B; Touvron et al., 2023), and falcon-11B (Falcon-11B; Almazrouei et al., 2023), and four prompt-LMs: gemma-1.1-2B-it (Gemma-2B), Gemma-7B, Llama3-8B, and Mistral-7B to assess GF LOW PO on various prompt-LM and target LM pairs. Generated prompts are evaluated using the template 

“[prompt] Input: [input] Output:” . We use # classes ×

16 examples for training data (see Appendix E for details). The highest top-5 accuracy prompts sampled throughout the training are stored in a high-reward buffer Q, and we report the highest performance among them at test time, which is the same evaluation method as in StablePrompt. 

Results. Table 1 shows the performance of various base-lines and GF LOW PO. GF LOW PO achieves the highest per-formance on SST-2, RTE, and SNLI. On QNLI and SNLI, our method also outperforms other discrete prompt tuning methods. Overall, GF LOW PO surpasses StablePrompt and 6GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

Table 2. Results on II and BBII tasks. Baseline results are taken from StablePrompt (Kwon et al., 2024), except that we re-evaluate StablePrompt on II and BBII text generation using the test set. For GF LOW PO, we report average accuracy, with mean and standard deviation computed over three runs using different random seeds. TASK CATEGORY (# T ASKS ) FEWSHOT MANUAL APE PRO TEGI STABLE PROMPT GF LOW PO 

II SPELLING (4) 3.75 29.62 43.81 43.75 50.58 ±2.88 59.17 ±0.63 

SYNTAX (4) 0.00 36.75 67.19 68.75 70.83 ±1.53 73.25 ±0.25 

SEMANTICS (5) 0.60 14.60 44.50 27.50 40.46 ±3.84 57.53 ±2.39 

TRANSLATION (4) 0.00 23.25 35.88 56.25 47.00 ±1.89 52.58 ±2.01 

GLUE (3) 35.00 49.67 45.92 58.33 59.67 ±2.03 63.78 ±2.77 

OTHERS (4) 0.50 57.10 74.69 62.30 82.55 ±1.18 85.00 ±0.66 

ALL (24) 5.21 33.70 51.94 51.60 57.71 ±0.35 64.96 ±0.50 

BBII TEXT CLASSIFICATION (12) 51.49 51.57 56.46 56.58 57.75 60.14 ±0.92 

TEXT GENERATION (6) 6.21 37.61 49.59 55.61 57.80 ±1.08 62.33 ±1.64 

achieves the highest average accuracy, demonstrating the effectiveness of our GFlowNet-based off-policy learning objective and the DMU mechanism. Fig. 4 shows the perfor-mance of GF LOW PO across various prompt-LM and target LM pairs. GF LOW PO consistently outperforms manual prompts across all pairs, showing robustness to the choice of various (prompt-LM, target LM) pairs. 

4.2. Induction task Datasets. We next consider induction tasks where the prompt-LM should generate an instruction prompt that de-scribes the rule behind a given input-output pair. We conduct experiments on Instruction Induction (II; Honovich et al., 2023) and BigBench Instruction Induction (BBII; Zhou et al., 2023), a subset of BIG-bench (Ghazal et al., 2013). These benchmarks include tasks such as sentence editing and rule-based question answering, where instruction-style prompts are required to help the target LM induce the cor-rect answer. The tasks include both text classification and text generation, covering a wide range of settings such as spelling, syntax, and simple rule-based reasoning. In case of the II task, we divide the dataset into six categories: Spelling ,

Syntax , Semantics , Translation , GLUE , and Others , follow-ing Honovich et al. (2023). The Others category includes tasks such as informal to formal , and sum . In total, we eval-uate on 24 II tasks and 18 BBII tasks. Additional details of the dataset are provided in Appendix F. 

Implementation Details. We use Mistral-7B for the prompt-LM and Gemma-7B for the target LM. For text classification tasks, we use the same evaluation protocol as in §4.1. For text generation tasks, prediction is consid-ered correct when the predicted tokens from the target LM exactly match the ground-truth output tokens. 

Results. As shown in Table 2, GF LOW PO outperforms all baselines in both BBII and all II tasks in terms of average accuracy. On the II benchmark, GF LOW PO achieves the best performance in four of six task categories (Syntax, Semantics, GLUE, and Others), while showing comparable 

Table 3. Results on MMLU and OpenBookQA tasks. Baseline results are taken from StablePrompt (Kwon et al., 2024), except that we re-evaluate StablePrompt on MMLU. DATASETS APE PRO TEGI STABLE PROMPT GF LOW PO         

> MMLU 52.1 53.5 55.8 55.6 OPEN BOOK QA 70.7 71.5 72.2 76.2

performance to StablePrompt in Spelling and Translation. These results demonstrate the effectiveness of GF LOW PO in text generation tasks, where exact matching between the target LM’s predicted tokens and the ground-truth output tokens is required, making accurate token prediction critical. 

4.3. Question Answering Datasets and Implementation details. We evaluate our method on large-scale multiple-choice question answer-ing tasks using MMLU (Wang et al., 2021) and Open-BookQA (Mihaylov et al., 2018). For MMLU, we report results on 57 topics including STEM, Humanity, Social Science, and Others. In OpenBookQA, each question is provided with a supporting fact as a hint, which we prepend to the question as part of the prompt. The verbalizer is used in the same way as in §4.1, where the answer classes are the following four candidates (A, B, C, and D). We used Gemma-7B for both prompt-LM and target LM. Generated prompts are evaluated using the template ”[prompt] Ques-tion : [Question] Choice : [Choice] Output:” .

Results. As shown in Table 3, GF LOW PO achieves the best performance in OpenBookQA, outperforming all base-lines. In MMLU, GF LOW PO performs comparably to Sta-blePrompt and consistently outperforms other baselines. These results demonstrate the effectiveness of GF LOW PO on question answering tasks, with robust performance across diverse topics. See Appendix J for the full results. 

4.4. Analysis on Exploration Ability Settings. We next conduct an analysis to examine whether our GF LOW PO can discover diverse high-reward prompts 7GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 0 400 800 1200             

> # of sampled prompts
> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> Train Accuracy
> letters list
> GFlowPO
> StablePrompt
> 0400 800 1200
> # of sampled prompts
> 0.0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5 sentence similarity
> 0400 800 1200
> # of sampled prompts
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8 cause and effect
> 0400 800 1200
> # of sampled prompts
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 taxonomy animal

Figure 5. Training accuracy as a function of the number of sampled prompts on four II tasks. Solid lines denote the mean training accuracy over three random seeds, and shaded regions indicate one standard deviation. GF LOW PO consistently discovers higher-reward prompts more efficiently than StablePrompt as sampling progresses. 

more sample-efficiently than StablePrompt (Kwon et al., 2024), which is based on on-policy PPO. We conduct ex-periments on four tasks from II, and evaluate the average training accuracy of prompts sampled from each prompt-LM throughout training. All the other experimental settings are identical to those described in §4.2. 

Results. As shown in Fig. 5, given the same number of sampled prompts, GF LOW PO consistently discovers prompts with higher accuracy. In contrast, StablePrompt fails to sample high-reward prompts from the prompt-LM, resulting in inferior performance on most of the II tasks, which is consistent with Table 2. Overall, these results demonstrate that GF LOW PO can explore and identify high-reward prompts more sample-efficiently than StablePrompt. 

4.5. Ablation Study Settings. We conduct an ablation study to examine the contribution of individual components in GF LOW PO. Specifically, we consider four variants: (1) GF LOW PO -on-X , (2) GF LOW PO -off-X , (3) GF LOW PO -on-O , and (4) GF LOW PO -off-O (i.e., GF LOW PO). The on and off variants correspond to on-policy and off-policy training, respectively. The O and X suffixes indicate whether the DMU mecha-nism is enabled or disabled. This design allows us to isolate the effects of off-policy training and the DMU mechanism in GF LOW PO. For evaluation, we randomly select 10 text generation tasks from Introduction Induction (II) and use Mistral-7B as the prompt-LM and Gemma-7B as the target LM, following the experimental setup in §4.2. 

Results. Table 4 shows that incorporating each component of GF LOW PO leads to consistent performance improve-ments. Introducing off-policy training to GF LOW PO -on-X 

yields a modest gain, which can be attributed to improved sample efficiency from replay-based training. Enabling the DMU mechanism (GF LOW PO -on-O ) results in a more sub-stantial improvement, reflecting the benefit of adaptively reshaping the meta-prompt to focus the search on high-reward regions of the prompt space. When both components are combined (GF LOW PO -off-O ), performance further im-

Table 4. Comparison on GF LOW PO variants. -on/off denotes on/off-policy training, and -O/X indicates whether DMU is used. GF LOW PO V ARIANTS 

TASK -on-X -off-X -on-O -off-O 

CAUSE AND EFFECT 60.0 60.0 88.0 88.0 

NEGATION 85.0 87.0 87.0 87.0  

> LETTERS LIST

85.0 88.0 93.0 96.0  

> LARGER ANIMAL

86.0 92.0 96.0 93.0  

> SENTENCE SIMILARITY

38.0 37.0 31.0 39.0  

> NUM TO VERBAL

94.0 94.0 98.0 98.0  

> WORD IN CONTEXT

63.0 55.0 61.0 64.0  

> SENTIMENT

91.0 91.0 87.0 91.0  

> INFORMAL TO FORMAL

44.2 50.2 40.8 49.2  

> SINGULAR TO PLURAL

98.0 98.0 98.0 97.0 AVERAGE (10 TASKS ) 74.4 75.2 78.0 80.2 

proves and achieves the best overall results. These results indicate that off-policy training and DMU contribute com-plementarily, not redundantly. 

## 5. Conclusion 

In this work, we proposed GF LOW PO, a novel prompt optimization framework that formulates prompt search as posterior inference and solves it using off-policy GFlowNet training combined with Dynamic Memory Update (DMU). With these two complementary components, GF LOW PO enables efficient exploration of the discrete prompt space while progressively reshaping the search focus on high-reward regions. Empirically, GF LOW PO achieves strong and consistent improvements over existing prompt optimiza-tion methods across Few-shot text classification tasks, In-struction Induction, BigBench Instruction Induction, and Question Answering benchmarks, demonstrating robustness across diverse tasks. 

Limitations. While GF LOW PO demonstrates strong prompt optimization performance, it has not yet been eval-uated on reasoning-centric tasks (Suzgun et al., 2022) that require explicit intermediate reasoning chains, which would be a natural extension. Also, incorporating test-time com-8GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

pute strategies may further improve performance. Finally, the current approach relies on task-specific optimization, and extending GF LOW PO to a meta-learning setting that amortizes prompt optimization across tasks and generalizes to unseen tasks is an important direction for future work. 

## Impact Statement 

This paper presents a method for improving prompt opti-mization by combining off-policy GFlowNet training with dynamic meta-prompt updates. The goal of this work is to advance the efficiency and robustness of prompt-based adaptation for large language models under limited supervi-sion. While improved prompt optimization may indirectly affect downstream applications of language models, such as decision support or content generation, these impacts are consistent with those commonly associated with advances in machine learning research. We do not foresee any unique ethical concerns arising from this work beyond those already well studied in the deployment of large language models. 

## References 

Agarwal, D., Arivazhagan, M. G., Das, R., Swamy, S., Khosla, S., and Gangadharaiah, R. Searching for optimal solutions with llms via bayesian optimization. In Interna-tional Conference on Learning Representations (ICLR) ,2025. Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., ´Etienne Goffinet, Hesslow, D., Launay, J., Malartic, Q., Mazzotta, D., Noune, B., Pannier, B., and Penedo, G. The falcon series of open lan-guage models. arXiv preprint arXiv:2311.16867 , 2023. Bailey, L., Ahdritz, G., Kleiman, A., Swaroop, S., Doshi-Velez, F., and Pan, W. Soft prompting might be a bug, not a feature. In Workshop on Challenges in Deployable Generative AI at International Conference on Machine Learning (ICML) , 2023. Bartoldson, B., Venkatraman, S., Diffenderfer, J., Jain, M., Ben-Nun, T., Lee, S., Kim, M., Obando-Ceron, J., Bengio, Y., and Kailkhura, B. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. In Advances in Neural Information Processing Systems (NeurIPS) , 2025. Batorski, P., Kosmala, A., and Swoboda, P. Prl: Prompts from reinforcement learning. arXiv preprint arXiv:2505.14412 , 2025. Bengio, E., Jain, M., Korablyov, M., Precup, D., and Ben-gio, Y. Flow network based generative models for non-iterative diverse candidate generation. In Advances in Neural Information Processing Systems (NeurIPS) , 2021. Bengio, Y., Lahlou, S., Deleu, T., Hu, E. J., Tiwari, M., and Bengio, E. Gflownet foundations. Journal of Machine Learning Research , 24(210):1–55, 2023. Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia-tional inference: A review for statisticians. Journal of the American Statistical Association , 112(518):859–877, April 2017. Chatterjee, A., Renduchintala, H. S. V. N. S. K., Bhatia, S., and Chakraborty, T. POSIX: A prompt sensitivity index for large language models. In Findings of the Association for Computational Linguistics (EMNLP) , 2024. Das, S. S. S., Kamoi, R., Pang, B., Zhang, Y., Xiong, C., and Zhang, R. Greater: Gradients over reasoning makes smaller language models strong prompt optimizers. In 

International Conference on Learning Representations (ICLR) , 2025. Deng, Y., Rush, A. M., West, M., and Tomkins, A. M. Rlprompt: Optimizing discrete text prompts with rein-forcement learning. In Findings of the Association for Computational Linguistics (EMNLP) , 2022. Dong, Y., Luo, K., Jiang, X., Jin, Z., and Li, G. Pace: Improving prompt with actor-critic editing for large lan-guage model. In Findings of the Association for Compu-tational Linguistics (ACL) , 2024. Fernando, C., Banarse, D., Michalewski, H., Osindero, S., and Rockt ¨aschel, T. Promptbreeder: Self-referential self-improvement via prompt evolution. In International Con-ference on Machine Learning (ICML) , 2024. Ghazal, A., Rabl, T., Hu, M., Raab, F., Poess, M., Crolotte, A., and Jacobsen, H.-A. Bigbench: Towards an industry standard benchmark for big data analytics. In Proceed-ings of the ACM SIGMOD International Conference on Management of Data , 2013. Honovich, O., Shaham, U., Bowman, S. R., and Levy, O. Instruction induction: From few examples to natural lan-guage task descriptions. In Findings of the Association for Computational Linguistics (ACL) , 2023. Hou, B., O’Connor, J., Andreas, J., Chang, S., and Zhang, Y. Promptboosting: Black-box text classification with ten forward passes. In International Conference on Machine Learning (ICML) , 2023. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR) , 2022. Hu, E. J., Malkin, N., Jain, M., Everett, K., Graikos, A., and Bengio, Y. Gflownet-em for learning compositional 9GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

latent variable models. In International Conference on Machine Learning (ICML) , 2023. Hu, E. J., Jain, M., Elmoznino, E., Kaddar, Y., Lajoie, G., Bengio, Y., and Malkin, N. Amortizing intractable infer-ence in large language models. In International Confer-ence on Learning Representations (ICLR) , 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023. Kim, M., Yun, T., Bengio, E., Zhang, D., Bengio, Y., Ahn, S., and Park, J. Local search gflownets. In International Conference on Learning Representations (ICLR) , 2024. Kwon, M., Kim, G., Kim, J., Lee, H., and Kim, J. Sta-blePrompt : Automatic prompt tuning using reinforce-ment learning for large language model. In Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2024. Lee, S., Kim, M., Cherif, L., Dobre, D., Lee, J., Hwang, S. J., Kawaguchi, K., Gidel, G., Bengio, Y., Malkin, N., and Jain, M. Learning diverse attacks on large language models for robust red-teaming and safety tuning. In Inter-national Conference on Learning Representations (ICLR) ,2025. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceed-ings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2021. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Findings of the Association for Computational Linguistics (ACL) , 2021. Liu, T., Astorga, N., Seedat, N., and van der Schaar, M. Large language models to enhance bayesian optimization. In International Conference on Learning Representations (ICLR) , 2024. Madan, K., Rector-Brooks, J., Korablyov, M., Bengio, E., Jain, M., Nica, A. C., Bosc, T., Bengio, Y., and Malkin, N. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning (ICML) , 2023. Malkin, N., Jain, M., Bengio, E., Sun, C., and Bengio, Y. Trajectory balance: Improved credit assignment in gflownets. In Advances in Neural Information Processing Systems (NeurIPS) , 2022. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Findings of the Association for Computational Linguistics (EMNLP) , 2018. Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. Bridging the gap between value and policy based rein-forcement learning. In Advances in neural information processing systems (NeurIPS) , volume 30, 2017. Pan, L., Malkin, N., Zhang, D., and Bengio, Y. Better training of gflownets with local credit and incomplete trajectories. In International Conference on Machine Learning (ICML) , 2023. Pan, L., Jain, M., Madan, K., and Bengio, Y. Pre-training and fine-tuning generative flow networks. In International Conference on Learning Representations (ICLR) , 2024. Prasad, A., Hase, P., Bansal, M., and Bansal, M. Grips: Gradient-free, edit-based instruction search for prompt-ing large language models. In European Chapter of the Association for Computational Linguistics (EACL) , 2022. Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., and Zeng, M. Automatic prompt optimization with “gradient descent” and beam search. In Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2023. Qin, G. and Eisner, J. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the Conference of the North American Chapter of the Associ-ation for Computational Linguistics: Human Language Technologies (NAACL-HLT) , 2021. Ramnath, K., Zhou, K., Guan, S., Mishra, S. S., Qi, X., Shen, Z., Wang, S., Woo, S., Jeoung, S., Wang, Y., Wang, H., Ding, H., Lu, Y., Xu, Z., Zhou, Y., Srinivasan, B., Yan, Q., Chen, Y., Ding, H., Xu, P., and Cheong, L. L. A systematic survey of automatic prompt optimization tech-niques. In Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2025. Richter, L., Boustati, A., N ¨usken, N., Ruiz, F. J. R., and ¨Omer Deniz Akyildiz. Vargrad: A low-variance gradient estimator for variational inference. In Advances in Neural Information Processing Systems (NeurIPS) , 2020. Salinas, A. and Morstatter, F. The butterfly effect of alter-ing prompts: How small changes and jailbreaks affect large language model performance. In Findings of the Association for Computational Linguistics (ACL) , 2024. Shi, W., Han, X., Gonen, H., Holtzman, A., Tsvetkov, Y., and Zettlemoyer, L. Toward human readable prompt tuning: Kubrick’s the shining is a good movie, and a good prompt too? In Findings of the Association for Computational Linguistics (EMNLP) , 2023. 10 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

Shin, T., Razeghi, Y., IV, R. L. L., Wallace, E., and Singh, S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Findings of the Association for Computational Linguistics (EMNLP) ,2020. Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neu-ral Information Processing Systems (NeurIPS) , 2023. Suzgun, M., Scales, N., Sch ¨arli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., , and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 , 2022. Team, G., Mesnard, T., Hardin, C., and et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 , 2024. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-ple, G. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. GLUE: A multi-task benchmark and analy-sis platform for natural language understanding. In Pro-ceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP ,2018. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Super-glue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Informa-tion Processing Systems (NeurIPS) , 2019. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Mea-suring massive multitask language understanding. In 

International Conference on Learning Representations (ICLR) , 2021. Wang, X., Li, C., Wang, Z., Bai, F., Luo, H., Zhang, J., Jojic, N., Xing, E. P., and Hu, Z. Promptagent: Strategic plan-ning with language models enables expert-level prompt optimization. In International Conference on Learning Representations (ICLR) , 2024. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS) , 2022. Yang, C., Yang, Y., Liu, S., Pham, M., Carbonell, J., Salakhutdinov, R., and Neubig, G. Large language mod-els as optimizers. In International Conference on Learn-ing Representations (ICLR) , 2024. Ye, Q., Ahmed, M., Pryzant, R., and Khani, F. Prompt engi-neering a prompt engineer. In Findings of the Association for Computational Linguistics (ACL) , 2024. Yu, F., Jiang, L., Kang, H., Hao, S., and Qin, L. Flow of reasoning: Training llms for divergent problem solving with minimal examples. In International Conference on Machine Learning (ICLR) , 2025. Yuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Lu, P., Huang, Z., Guestrin, C., and Zou, J. Optimizing gener-ative ai by backpropagating language model feedback. 

Nature , 639:609–616, 2025. Yun, T., St-Charles, P.-L., Park, J., Bengio, Y., and Kim, M. Active attacks: Red-teaming llms via adaptive environ-ments. arXiv preprint arXiv:2509.21947 , 2025a. Yun, T., Zhang, D., Park, J., and Pan, L. Learning to sample effective and diverse prompts for text-to-image genera-tion. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR) , 2025b. Zhang, D., Malkin, N., Liu, Z., Volokhova, A., Courville, A., and Bengio, Y. Generative flow networks for discrete probabilistic modeling. In International Conference on Machine Learning (ICML) , 2022. Zhang, D. W., Rainone, C., Peschl, M., and Bondesan, R. Robust scheduling with gflownets. In International Con-ference on Learning Representations (ICLR) , 2023a. Zhang, T., Yuan, J., and Avestimehr, S. Revisiting opro: The limitations of small-scale llms as optimizers. In Findings of the Association for Computational Linguistics (ACL) ,2024. Zhang, Z., Chen, W., Wang, H., Chen, M., Wang, Z., and Wang, W. Y. Tempera: Test-time prompt adjustment via reinforcement learning. In International Conference on Learning Representations (ICLR) , 2023b. Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are human-level prompt engineers. In International Conference on Learning Representations (ICLR) , 2023. 11 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

## A. Correlation between Likelihood and Accuracy 

In this work, we define an unnormalized target reward as R(z; M ) = p(D | z) · pref (z | M ). Here, the training log-likelihood 

log p(D | z) can be computed by summing the log-probabilities of the ground-truth output tokens y from target LM given prompt z and context x over all (x, y ) pairs in Dtask . However, we empirically observe that the training likelihood p(D | z)

is weakly correlated with test accuracy for most prompt optimization tasks we consider. To verify this, we sample 1,000 prompts using the meta-prompt from StablePrompt (see Fig. 7(a)) across six few-shot text classification tasks, and compute the training log-likelihood log p(D | z), training accuracy AD (z) (used as the learning objective in this work), and test accuracy for each prompt. We then measure the correlation between training accuracy and test accuracy, as well as between training log-likelihood and test accuracy. As shown in Fig. 6, the correlation between training log-likelihood and test accuracy is consistently lower than that between training accuracy and test accuracy across all six tasks. Therefore, we directly use training accuracy as the optimization objective in this work. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0   

> train_acc
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> test_acc
> Pearson r= 0.889

SST2                 

> 400 350 300 250 200 150 100 50 0
> train_ll
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> test_acc
> Pearson r= 0.496
> 0.4 0.5 0.6 0.7 0.8
> train_acc
> 0.45
> 0.50
> 0.55
> 0.60
> 0.65
> 0.70
> 0.75
> 0.80
> 0.85
> test_acc
> Pearson r= 0.828

RTE              

> 400 300 200 100 0
> train_ll
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> test_acc
> Pearson r= 0.595
> 0.3 0.4 0.5 0.6 0.7 0.8
> train_acc
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> test_acc
> Pearson r= 0.090

MRPC                 

> 400 350 300 250 200 150 100 50 0
> train_ll
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> test_acc
> Pearson r= -0.214
> 0.3 0.4 0.5 0.6 0.7
> train_acc
> 0.35
> 0.40
> 0.45
> 0.50
> 0.55
> 0.60
> 0.65
> test_acc
> Pearson r= 0.903

MNLI                  

> 300 250 200 150 100 50
> train_ll
> 0.30
> 0.35
> 0.40
> 0.45
> 0.50
> 0.55
> 0.60
> 0.65
> test_acc
> Pearson r= 0.704
> 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85
> train_acc
> 0.50
> 0.55
> 0.60
> 0.65
> 0.70
> 0.75
> 0.80
> 0.85
> test_acc
> Pearson r= 0.863

QNLI                 

> 350 300 250 200 150 100 50 0
> train_ll
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> test_acc
> Pearson r= 0.694
> 0.3 0.4 0.5 0.6 0.7 0.8
> train_acc
> 0.4
> 0.5
> 0.6
> 0.7
> test_acc
> Pearson r= 0.941

SNLI      

> 400 300 200 100
> train_ll
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> test_acc
> Pearson r= 0.631

Train Acc vs Test Acc Train LL vs Test Acc 

Figure 6. Correlation plots on train accuracy vs test accuracy ( left ), and train log-likelihood vs test accuracy ( right ) on six text classification tasks (SST2, RTE, MRPC, MNLI, QNLI, and SNLI). 

12 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

Table 5. Comparison of GF LOW PO variants. GF LOW PO W/O PRE -S TEP removes the Pre-Step procedure. GF LOW PO-B samples reference prompts only from the replay buffer B, while GF LOW PO-Q samples reference prompts only from the high-reward buffer Q.GF LOW PO achieves the best performance across all variants. TASK GF LOW PO W/O PRE -S TEP GF LOW PO-B GF LOW PO-Q GF LOW PO CAUSE AND EFFECT 80.0 76.0 52.0 88.0 

NEGATION 83.0 89.0 86.0 87.0  

> LETTERS LIST

94.0 94.0 96.0 96.0  

> LARGER ANIMAL

56.0 91.0 90.0 93.0  

> SENTENCE SIMILARITY

39.0 36.0 36.0 39.0  

> NUM TO VERBAL

95.0 94.0 100.0 98.0  

> WORD IN CONTEXT

62.0 63.0 60.0 64.0  

> SENTIMENT

90.0 89.0 88.0 91.0  

> INFORMAL TO FORMAL

45.7 52.7 38.8 49.2  

> SINGULAR TO PLURAL

98.0 99.0 98.0 97.0 AVERAGE (10 TASKS ) 74.3 78.4 74.5 80.2 

## B. Effect of Collecting Prompts in Replay Buffer before GFlowNet Training 

One strength of GF LOW PO is its off-policy learning strategy, which allows sampled prompts from the prompt-LM to be stored in a replay buffer and reused for training. This off-policy setting also enables collecting diverse prompts with the prompt-LM and storing them in the replay buffer before the actual GFlowNet training starts. As a result, the replay buffer may already contain high-reward prompts at initialization, which can facilitate exploration in the early stage of training. We refer to this procedure as Pre-Step . Details of Pre-Step and the overall training algorithm are provided in Algorithm 1. To examine whether Pre-Step improves performance, we compare GF LOW PO with GF LOW PO W/O PRE -S TEP . For a fair comparison, GF LOW PO W/O PRE -S TEP is trained for the same total number of sampled prompts as GF LOW PO. All remaining experimental settings are kept the same as in §4.2. As shown in Table 5, GF LOW PO achieves higher average accuracy than GF LOW PO W/O PRE -S TEP , indicating that Pre-Step is beneficial. 

## C. Effect of Sampling Reference Prompts on both Replay Buffer and High-Reward Buffer 

In GF LOW PO, reference prompts Zref in the meta-prompt M are sampled from both the replay buffer B and the high-reward buffer Q to balance exploration and exploitation. In this section, we study the effect of sampling reference prompts exclusively from either B or Q. An example of a meta-prompt that samples reference prompts only from the high-reward buffer Q is shown in Fig. 7(b). Specifically, we compare GF LOW PO with two variants: GF LOW PO-B, which samples reference prompts only from the replay buffer B, and GF LOW PO-Q, which samples reference prompts only from the high-reward buffer Q. We use the same experimental settings as in §4.2. As reported in Table 5, GF LOW PO outperforms both variants, suggesting that balancing exploration and exploitation is important for performance. 

## D. GFlowPO Hyperparameter Setting 

Hyperparameter used in GF LOW PO is presented in Table 6. 

## E. Dataset Details and Verbalizer Settings on Few-Shot Text Classification Tasks 

The dataset details and verbailizer settings on few-shot text classification tasks are presented in Table 7. 

## F. Dataset Details on BigBench-Hard Instruction Induction (BBII) and Instruction Induction (II) 

The dataset details on BigBench-Hard Instruction Induction (BBII) and Instruction Induction (II) are presented in Table 8 and Table 9, respectively. 13 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer  

> Table 6. Hyperparameters of GF LOW PO

Hyperparameters 

Learning Rate 10 −4

Train buffer size 1000 High-reward buffer size 5Total train steps 200 Pre-steps 100 Max prompt length 150 Num example 5LoRA r 16 LoRA α 32 LoRA dropout 0.05 EMA decay 0.99 Sampling temperature [0 .5, 2.0] 

Online/offline ratio 0.5 M-step frequency 1 

> Table 7. Details of the dataset statistics and verbalizer on few-shot text classification tasks.

Dataset Task type # classes # training examples # test examples Verbalizer SST2 sentiment 2 32 872 [“yes”,“no”] MRPC NLI 2 32 408 [“yes”,“no”] RTE NLI 2 32 277 [“yes”,“no”] QNLI NLI 2 32 5,460 [“yes”,“no”] MNLI NLI 3 48 9,820 [“yes”,“maybe”,“no”] SNLI NLI 3 48 9,842 [“yes”,“maybe”,“no”]  

> Table 8. Details of BigBench-Hard Instruction Induction datasets.

Task name Task type Metric # training examples # test examples causal judgment Multiple Choice Accuracy 30 160 disambiguation qa Multiple Choice Accuracy 30 228 epistemic reasoning Multiple Choice Accuracy 30 1,970 hyperbaton Multiple Choice Accuracy 30 49,970 implicatures Multiple Choice Accuracy 30 462 logical fallacy detection Multiple Choice Accuracy 30 2,770 movie recommendation Multiple Choice Accuracy 30 470 navigate Multiple Choice Accuracy 30 970 presuppositions as nli Multiple Choice Accuracy 30 705 ruin names Multiple Choice Accuracy 30 418 snarks Multiple Choice Accuracy 30 151 sportsunderstanding Multiple Choice Accuracy 30 970 dyck languages Generation Exact Match 30 970 gender inclusive sentences Generation Exact Match 30 170 object counting Generation Exact Match 30 970 operators Generation Exact Match 30 181 tense Generation Exact Match 30 256 word sorting Generation Exact Match 30 1,870 

14 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer  

> Table 9. Details of Instruction Induction datasets.

Task name Metric # training examples # test examples antonyms Exact Match 32 100 word in context Exact Match 32 100 rhymes Exact Match 32 100 num to verbal Exact Match 32 100 cause and effect Exact Match 26 25 larger animal Exact Match 32 100 second word letter Exact Match 32 100 taxonomy animal Exact Set 32 100 negation Exact Match 32 100 common concept F1 score 17 16 diff Exact Match 32 100 translation en-es Exact Match 32 100 orthography starts with Exact Set 32 100 sentiment Exact Match 32 100 informal to formal F1 score 15 15 sum Exact Match 32 100 singular to plural Exact Match 32 100 active to passive Exact Match 32 100 translation en-de Exact Match 32 100 sentence similarity Exact Match 32 100 translation en-fr Exact Match 32 100 letters list Exact Match 32 100 first word letter Exact Match 32 100 synonyms Contains 32 100 

## G. Dataset Details on MMLU 

The dataset details on MMLU are presented in Table 10. 15 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer  

> Table 10. MMLU dataset statistics: Training and test sizes by category.

Type Subject # training examples # test examples STEM abstract algebra 11 100 anatomy 14 135 astronomy 16 152 college biology 16 144 college chemistry 8 100 college computer science 11 100 college mathematics 11 100 college physics 11 102 computer security 11 100 conceptual physics 26 235 electrical engineering 16 145 elementary mathematics 41 378 high school biology 32 310 high school chemistry 22 203 high school computer science 9 100 high school mathematics 29 270 high school physics 17 151 high school statistics 23 216 machine learning 11 112 Social Science econometrics 12 114 high school geography 22 198 high school government and politics 21 193 high school macroeconomics 43 390 high school microeconomics 26 238 high school psychology 60 545 human sexuality 12 131 professional psychology 69 612 public relations 12 110 security studies 27 245 sociology 22 201 us foreign policy 11 100 Humanities formal logic 14 126 high school european history 18 165 high school us history 22 204 high school world history 26 237 international law 13 121 jurisprudence 11 108 logical fallacies 18 163 moral disputes 38 346 moral scenarios 100 895 philosophy 34 311 prehistory 35 324 professional law 170 1534 world religions 19 171 Others business ethics 11 100 clinical knowledge 29 265 college medicine 22 173 global facts 10 100 human aging 23 223 management 11 103 marketing 25 234 medical genetics 11 100 miscellaneous 86 783 nutrition 33 306 professional accounting 31 282 professional medicine 31 272 virology 18 166 16 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

## H. Generated Prompts 

In this section, we provide the prompts generated by GF LOW PO: few-shot text classification in Appendix H.1, BigBench-Hard Instruction Induction in Appendices H.2 and H.3, and Instruction Induction in Appendix H.4. For each task, we report the prompt selected from the best-performing prompts. 

H.1. Few-Shot Text Classification SST2 

Write yes or no for each input, indicating whether the statement is likely to result in a good movie. **Input:** too bad maggio couldn’t come up with a better script. **Output:** no **Input:** mostly leaves him shooting blanks **Output:** no **Input:** is sandler running on empty , repeating what he ’s already done way too often **Output:** no **Input:** sit through than this hastily dubbed disaster **Output:** no **Input:** , it manages to maintain both a level of sophisticated intrigue and human-scale characters that suck the audience in. **Output:** yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 92.78% 

MRPC 

**For each input, determine whether the two sentences are semantically equivalent or not.** **Criteria for equivalence:** - The sentences convey the same meaning and information. - The grammatical structure and word choice are similar. **Results:** **1. Sentence 1 & 2:** - Both sentences express that Sarah O’Hare thanked the matriarch for her generosity. - Grammatical structure and word choice are similar. **Output: Yes** **2. Sentence 1 & 2:** - Both sentences describe Lee commenting on Brianna’s behavior. -Similar grammatical structure and word choice. **Output: Yes** **3. Sentence 1 & 2:** - Sentence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 67.16% 

RTE 

1. For each premise, assess whether the hypothesis is true or false. 2. For each of the given premises, generate a logical output related to the given hypothesis. **Results:** **1. Premise:** In 1969, he drew up the report proposing the expulsion from the party of the Manifesto group. **Hypothesis:** Natta supported the Manifesto group. **Output:** No **2. Premise:** The increased amounts of carbon dioxide (CO2) and other greenhouse gases (GHGs) are the primary causes of the human-induced component of global warming. **Hypothesis:** Greenhouse effect changes global climate. **Output:** Yes **3. Premise:** Sida does . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 79.1% 

QNLI 

**1. For each input, write an output indicating whether the sentence mentions the given question.** **Results:** - Question: How many territories is the Premier League broadcast to? Sentence: The Premier League rank second in the UEFA coefficients of leagues based on performances in European competitions over the past five seasons. Output: No - Question: What regime were Hussein loyalists part of? Sentence: Other elements of the insurgency were led by fugitive members of President Hussein’s Ba’ath regime, which included Iraqi nationalists and pan-Arabists. Output: Yes - Question: If Neptune formed closer to the sun, what caused it to migrate to it’s current orbit? Sentence: The provided text does not contain information regarding Neptune’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 84.1% 

MNLI 

1. For each of the given premise-hypothesis pairs, write an output. 2. For each premise, write an output based on the hypothesis provided. 3. Based on the given premise, write an output for the given hypothesis. **Results:** **Premise-Hypothesis Pairs Output:** - Premise: And other stars probably have planets. Hypothesis: Other stars are made of ice. Output: No - Premise: No doubt he will do better in his next book. Hypothesis: He will probably do even worse in his future works. Output: No - Premise: The man gives good movies. Hypothesis: The man gives a good movie but still can’t find success. Output: Maybe **For . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 70.45% 

17 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

SNLI 

**For each premise, write an output based on the hypothesis.** **The outputs are as follows:** **1.**Premise: People attending a concert dressed in yellow. Hypothesis: People are wearing yellow. Output: Yes **2.**Premise: Newly married couple with money attached to their clothing. Hypothesis: Newlyweds pin money to their clothing to represent good fortune. Output: Maybe **3.**Premise: A man in a green coat crossing the street with graffiti in the background. Hypothesis: The man has a green coat. Output: Yes **4.**Premise: Children play soccer on a narrow city street. Hypothesis: Children are playing soccer. Output: Yes **5.**Premise: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 81.48% 

H.2. BigBench-Hard Instruction Induction (BBH-II) - Text Classification 

We randomly choose five tasks from the text classification tasks of BBH-II dataset. 

Implicatures 

Based on the conversation between Speaker 1 and Speaker 2, choose the answer that best fits the situation. The Output for each Input is: 1. Input: Speaker 1: ’I feel horrible. Debbie was furious that I lost her notes. Do you think I should apologize her again?’ Speaker 2: ’If I were you I would cool off for some days before I talk to her again.’ Choices: A : yes, B : no Output: B Explanation: Speaker 2 advised Speaker 1 to wait before talking to Debbie again, indicating that it might not be the best time for an apology. 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 86.3% 

Movie Recommendation 

2. Based on the given list of movies, choose an alternative movie from the provided choices that is not in the given input but shares some similarity with the given movies in terms of genre or theme. Your friend seemed to have misunderstood the instruction and instead chose a movie from the input for each set of choices. The output matches the movies in the choices, although not necessarily the movies in the input that they were asked to find an alternative for. To provide the correct answer for this instruction, we should consider the genre or theme of the given movies and find an alternative movie from the choices that shares some similarity with them. For example, if the given movies are: The Godfather, The Dark Knight, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 70.6% 

Snarks 

”Choose the answer that is the opposite of the given input.” Based on this instruction, the incorrect outputs by your friend for the third and fourth pairs: Input: (a) I’m sure you wouldn’t have remained completely calm if ateacher attacked you. Choices: A: (a) (input without the given phrase) B: (b) (input with the given phrase) Out-put: B (correction: The output should be ’A’ as the opposite of (b) is ’You’re sure you would have remained com-pletely calm if a teacher attacked you’ or more literally ’I’m sure you would have Panicked.’) Input: (a) College . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 65.6% 

Sports Understanding 

”Given the following inputs, choose whether each is plausible or implausible based on your knowledge of base-ball and soccer.” The friend’s outputs for each input are: Input: Alex Pietrangelo was called for slashing in the Stanley Cup [Choices]: A: plausible B: implausible Output: A (Plausible as slashing is a penalty called in ice hockey) Input: Wayne Rooney shot from outside the eighteen [Choices]: A: plausible B: implausible Out-put: A (Plausible as soccer players, including Wayne Rooney, do shoot from outside the eighteen-yard box) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 67.2% 

18 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

presuppositions as nli 

1. ”Based on the given context, determine if the relationship between the two sentences is entailment, neutral, or contradiction.” The Outputs are correct for all the given Inputs. Input 1, 2, and 3 are neutral because the second sentence does not follow or contradict the first sentence but also doesn’t entail it. Input 4 is an example of entailment because the second sentence (He was standing earlier) entails the first sentence (He didn’t fall on his knees in the street). The first sentence implies that the man was not standing earlier since he was running and his knees were not on the ground. The second sentence explicitly states that he was standing earlier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 43.6% 

H.3. BigBench-Hard Instruction Induction (BBH-II) - Text Generation 

We randomly choose five tasks from the text generation tasks of BBH-II dataset. 

gender inclusive sentences german 

1. Add an ’innen’ to every noun in the sentence that indicates people. Your friend followed the instruction and accurately implemented it in all the given examples, making sure all human nouns in the output now carry the suffix ’-innen’ indicating people. Instruction 2: Given a German text, make sure that all gendered nouns and pronouns in the output are inclusive. Your friend followed this instruction as well, by marking all the gendered nouns and pronouns with an asterisk (*) in the outputs to indicate inclusivity. The Instruction 3 that you mentioned is related to agreeing the maskulin singular pronouns and plural pronoun . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 89.51% 

tense 

”Change the tense of each sentence to the one indicated. The given Inputs and their corresponding Outputs in the required tenses are: input: ”Nikita has cooked dinner for us.” (Change tense to FUTURE PERFECT) output: ”Nikita will have cooked dinner for us.” (Future Perfect) input: ”The girl bought the telescope her teacher will have recommended to her.” (Change tense to PAST PERFECT) output: ”The girl bought the telescope her teacher had recommended to her.” (Past Perfect) input: ”The child had stopped crying.” (Change tense to PRESENT PERFECT) output: ”The child has stopped crying.” (Present Perfect) input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 96.7% 

word sorting 

”Change the order of the first two words in each input.” Based on the provided instruction, the friend has correctly followed the instruction for the given input-output pairs, except for the case where the instruction was misinterpreted as ”Swap the position of the first and second words in the output” ( Input : duma cordage , Output: cordage duma ). For the correct application of the instruction, the output for the second input should have been ”cordage duma” instead of ”duma cordage” or ”cordage duma” instead of ”duma cordage” respectively, as in the reference instructions you have provided. Therefore, based on the consistency of your friend’s application of the . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 91.47% 

object counting 

”[Input] I have <something >, [Input] I have <different things >, [Output] = <sum of the number of items in the differ-ent things >.” Based on the provided examples, the friend understood that the instruction is to find the sum of the num-ber of objects in the different lists given the inputs, and the outputs are the respective sums. Therefore, their answers are correct, as the sum of number of items in the input with ’chair, fridge, lamp, table’ is indeed 4. The same goes for the second and third input which evaluates to 18 and 11 (which should be 11 but I assume a typo in your final output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 37.49% 

19 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

operators 

”op i is the ith power of 2.” For this instruction, the friend correctly computed the 5th power of 2 (2 ˆ5), which is 16, for both inputs. The Instruction is: ”op i is the absolute value of i.” For this instruction, the friend correctly found the absolute value of -58, which is 58, for the input. The Instruction is: ”op i is the number j such that the addition of i and j is 0.” For this instruction, the friend correctly identified that for the input of 0, the number j to make the sum of i and j equal to zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 54.70% 

H.4. Instruction Induction (II) 

We randomly choose five tasks from the II dataset. 

Num to Verbal 

”Convert the given number to its written form, e.g. 123 as One Hundred and Twenty-Three.” Your friend correctly followed this instruction for all the given inputs and outputs were written as ”number, number hundred and number” form as per the instruction. Therefore, all the output pairs in your case are valid according to the instruction given. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 98% 

Cause and Effect 

”For each given pair of sentences, determine which sentence is the cause of the situation described in the other sentence and write only that sentence as the output.” Your friend has correctly identified and provided the sentence that represents the cause as output for each pair. It seems that your friend understood the instruction properly. Good job by your friend! Output: In-put : Sentence 1: The man ate hot wings. Sentence 2: The man reached for a glass of water. Output : The man ate hot wings. Input : Sentence 1: The child dropped the toy. Sentence 2: The toy was broken. Output : The child dropped the toy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 88% 

Taxonomy Animal 

2. ”Filter out the non-animal inputs from each list.” Based on this instruction, your friend has produced correct outputs. The friend identified and removed non-animal words from each input list to produce output lists containing only animal words. This is in compliance with the instruction that requires the extraction of animal words only from the input strings and leaving out non-animal elements. Here’s how this applies to some of the provided examples: Input: swan, plane, frog, pajamas, shark, butterfly Output: shark, butterfly, frog, swan Non-animal words - plane, pajamas - were removed, leaving behind shark, butter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 95% 

Larger Animal 

”Choose the larger animal from the given pair and output only its name.” Based on the provided information, the friend has followed the instruction correctly for each input-output pair. Therefore, the friend’s outputs are valid according to the instruction. For instance, in the first pair, the instruction is to choose the larger animal between ”bat” and ”jaguar”. The friend has correctly identified ”jaguar” as the larger animal and outputted it as the result. Similarly, in the second pair, the friend has identified ”puppy” as the larger animal and outputted it as the result. Therefore, the instruction and the friend’s outputs are consistent, and the friend has . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 93% 

Diff 

– Subtract the second number from the first number and find the difference’s absolute value if the result is negative– Take the output from the previous calculation and subtract 8 from it. Let’s verify each of the input-output pairs based on this instruc-tion: Input : 84 16 Output : 68 Calculation: |84-16 | = 68 and |68-8 | = 60, but we did not get this from input and the in-struction didn’t ask for that, so it seems to be a correct output (the first part of the instruction was satisfied: |84-16 | = 68 ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy: 100% 

20 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

## I. Meta prompt Example 

We present the meta-prompt examples in Fig. 7. I gave a friend an instruction and five 

inputs. The friend read the instruction 

and wrote an output for every one of 

the inputs. Here are the input -output 

pairs: 

The instruction is: 

Input : Sentence 1: The girl got 

detention. Sentence 2: The girl 

skipped school. 

Output : The girl skipped school. 

Input : Sentence 1: The meat spoiled. 

Sentence 2: The power was out for 

days. 

Output : The power was out for days. 

Input : Sentence 1: The floor was wet. 

Sentence 2: The man sued. 

Output : The floor was wet. 

Input : Sentence 1: The family went 

to the beach. Sentence 2: The air 

conditioner broke. 

Output : The air conditioner broke. 

Input: Sentence 1: The ship capsized. 

…

Train 

Dataset 

(a) Meta -prompt used in StablePrompt 

I gave a friend an instruction and five 

inputs. The friend read the instruction 

and wrote an output for every one of 

the inputs. Here are the input -output 

pairs: 

Here are some reference instructions: 

The instruction is: 

"From the given pair of sentences, 

determine and write the independent 

sentence." 

Input : Sentence 1: The girl got 

detention. Sentence 2: The girl 

skipped school. 

Output : The girl skipped school. 

Input : Sentence 1: The meat spoiled. 

Sentence 2: The power was out for 

days. 

Output : The power was out for days. 

Input : Sentence 1: The floor was wet. 

Sentence 2: The man sued. 

…… 

High -Reward 

Buffer 

(b) Meta -prompt with reference prompts 

only from high -reward buffer 

I gave a friend an instruction and five 

inputs. The friend read the instruction 

and wrote an output for every one of 

the inputs. Here are the input -output 

pairs: 

Here are some reference instructions: 

The instruction is: 

Input : Sentence 1: The girl got 

detention. Sentence 2: The girl 

skipped school. 

Output : The girl skipped school. 

….. 

1."From the given pair of sentences, 

determine and write the independent 

sentence." 

2. "Identify the sentence that is the 

cause and write it as the output." 

3. "Find and identify the independent 

sentences in the given pairs and 

print them out." 

Replay 

Buffer 

High -Reward 

Buffer 

(c) Meta -prompt used in GFlowPO 

Figure 7. Meta prompt for prompt generation. (a) Meta-prompt used in StablePrompt, consisting of an instruction and few-shot examples. 

(b) Meta prompt with a reference prompt sampled only from high reward buffer Q. (c) Meta prompt used in our proposed GF LOW PO. Reference prompts are sampled from both high reward buffer Q and replay buffer B, which balances exploitation and exploration during training. 

## J. Full Experimental results 

In this section, we provide full experimental results in Tables 11 to 14. 21 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

Table 11. Full experimental results on the BigBench-Hard Instruction Induction datasets with Gemma-7B as the target model. 

Task name type Metric fewshot manual APE ProTeGi StablePrompt GF LOW PO causal judgment Multiple Choice Accuracy 58.75 52.50 58.13 56.69 58.75 58.75 ±1.08 

disambiguation qa Multiple Choice Accuracy 64.29 52.19 64.00 61.40 64.04 64.77 ±1.11 

epistemic reasoning Multiple Choice Accuracy 43.69 57.16 58.40 63.79 61.47 58.97 ±5.19 

hyperbaton Multiple Choice Accuracy 47.89 56.52 75.60 76.06 75.60 74.82 ±3.72 

implicatures Multiple Choice Accuracy 83.33 83.12 80.95 73.59 79.00 85.43 ±0.97 

logical fallacy detection Multiple Choice Accuracy 58.19 63.50 56.50 58.23 58.34 53.19 ± 0.22 

movie recommendation Multiple Choice Accuracy 49.36 37.66 55.30 67.23 55.30 73.55 ±3.82 

navigate Multiple Choice Accuracy 69.22 49.79 52.28 54.02 53.30 52.47 ±0.31 

presuppositions as nli Multiple Choice Accuracy 42.55 40.82 41.56 41.42 43.40 45.96 ±2.34 

ruin names Multiple Choice Accuracy 12.44 30.14 32.53 27.99 37.08 27.76 ±1.75 

snarks Multiple Choice Accuracy 35.79 42.38 50.99 50.99 52.32 60.04 ±5.31 

sportsunderstanding Multiple Choice Accuracy 52.37 59.38 56.50 55.98 60.12 65.91 ±2.00 

dyck languages Generation Exact Match 0.00 0.00 0.00 0.00 0.00 ±0.00 0.00 ±0.00 

gender inclusive sentences Generation Exact Match 9.30 86.00 67.13 93.77 86.39 ±3.15 90.44 ±0.68 

object counting Generation Exact Match 7.13 0.00 14.29 33.33 30.87 ±1.77 38.18 ±2.78 

operators Generation Exact Match 5.53 49.45 57.14 50.00 49.46 ±2.60 53.52 ±4.01 

tense Generation Exact Match 15.29 93.85 96.76 100.00 95.13 ±0.40 96.28 ±0.71 

word sorting Generation Exact Match 0.00 20.14 96.43 75.00 84.97 ±2.71 95.55 ±3.56 

Table 12. Full experimental results on the Instruction Induction datasets with Gemma-7B as the target model. 

Task name Metric fewshot manual APE ProTeGi StablePrompt GF LOW PO antonyms Exact Match 0.00 43.00 62.50 25.00 67.67 ±3.21 74.33 ±2.30 

word in context Exact Match 55.00 46.00 37.50 50.00 60.33 ±5.03 61.00 ±4.35 

rhymes Exact Match 0.00 3.00 6.25 25.00 3.67 ±3.79 8.67 ±2.08 

num to verbal Exact Match 0.00 61.00 93.75 100.00 89.00 ±4.36 97.67 ±1.53 

cause and effect Exact Match 0.00 24.00 60.00 0.00 54.67 ±10.07 81.33 ±11.54 

larger animal Exact Match 0.00 3.00 56.25 25.00 86.67 ±8.50 92.67 ±0.58 

second word letter Exact Match 12.00 8.00 6.25 25.00 9.67 ±4.16 27.33 ±3.51 

taxonomy animal Exact Set 0.00 0.00 37.50 37.50 53.67 ±19.55 93.67 ±8.39 

negation Exact Match 0.00 16.00 68.75 50.00 83.67 ±2.31 87.00 ±0.00 

common concept F1 3.00 4.00 50.00 50.00 3.29 ±1.20 13.67 ±3.21 

diff Exact Match 2.00 99.00 100.00 100.00 100.00 ±0.00 100.00 ±0.00 

translation en-es Exact Match 0.00 15.00 25.00 25.00 43.00 ±0.00 40.33 ±3.79 

orthography starts with Exact Set 0.00 37.50 12.50 0.00 13.33 ±3.21 15.00 ±1.00 

sentiment Exact Match 50.00 83.00 68.75 100.00 86.00 ±3.00 91.67 ±2.08 

informal to formal F1 0.00 27.38 42.50 24.22 43.51 ±4.03 47.33 ±2.08 

sum Exact Match 0.00 99.00 100.00 100.00 100.00 ±0.00 100.00 ±0.00 

singular to plural Exact Match 0.00 75.00 93.75 100.00 96.00 ±1.00 98.33 ±1.53 

active to passive Exact Match 0.00 53.00 100.00 100.00 100.00 ±0.00 99.00 ±0.00 

translation en-de Exact Match 0.00 10.00 18.75 50.00 29.67 ±5.03 32.00 ±2.65 

sentence similarity Exact Match 0.00 20.00 31.50 25.00 32.67 ±4.16 38.67 ±2.52 

translation en-fr Exact Match 0.00 7.00 6.00 50.00 26.33 ±5.03 40.33 ±1.53 

letters list Exact Match 0.00 0.00 68.75 50.00 82.33 ±10.69 95.33 ±2.08 

first word letter Exact Match 3.00 73.00 87.75 100.00 97.00 ±1.00 99.00 ±1.00 

synonyms Contains 0.00 2.00 12.50 25.00 23.00 ±10.54 24.67 ±5.13 

22 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer  

> Table 13. Category-wise results on the Instruction Induction tasks.

Task Group Task Name Metric Fewshot Manual APE ProTeGi StablePrompt GFlowPO Spelling first word letter Exact Match 3.00 73.00 87.75 100.00 97.00 99.00 second word letter Exact Match 12.00 8.00 6.25 25.00 9.67 27.33 

letters list Exact Match 0.00 0.00 68.75 50.00 82.33 95.33 

orthography starts with Exact Set 0.00 37.50 12.50 0.00 13.33 15.00 Average 3.75 29.62 43.81 43.75 50.58 59.17 

Syntax singular to plural Exact Match 0.00 75.00 93.75 100.00 96.00 98.33 active to passive Exact Match 0.00 53.00 100.00 100.00 100.00 99.00 negation Exact Match 0.00 16.00 68.75 50.00 83.67 87.00 

rhymes Exact Match 0.00 3.00 6.25 25.00 3.67 8.67 Average 0.00 36.75 67.19 68.75 70.83 73.25 

Semantics antonyms Exact Match 0.00 43.00 62.50 25.00 67.67 74.33 

synonyms Contains 0.00 2.00 12.50 25.00 23.00 24.67 taxonomy animal Exact Set 0.00 0.00 37.50 37.50 53.67 93.67 

cause and effect Exact Match 0.00 24.00 60.00 0.00 54.67 81.33 

common concept F1 3.00 4.00 50.00 50.00 3.29 13.67 Average 0.60 14.60 44.50 27.50 40.46 57.53 

Translation num to verbal Exact Match 0.00 61.00 93.75 100.00 89.00 97.67 translation en-es Exact Match 0.00 15.00 25.00 25.00 43.00 40.33 translation en-de Exact Match 0.00 10.00 18.75 50.00 29.67 32.00 translation en-fr Exact Match 0.00 7.00 6.00 50.00 26.33 40.33 Average 0.00 23.25 35.88 56.25 47.00 52.58 GLUE sentence similarity Exact Match 0.00 20.00 31.50 25.00 32.67 38.67 

word in context Exact Match 55.00 46.00 37.5 50.00 60.33 61.00 

sentiment Exact Match 50.00 83.00 68.75 100.00 86.00 91.67 Average 35.00 49.67 45.92 58.33 59.67 63.78 

Others larger animal Exact Match 0.00 3.00 56.25 25.00 86.67 92.67 

informal to formal F1 0.00 27.38 42.5 24.22 43.51 47.33 

sum Exact Match 0.00 99.00 100.00 100.00 100.00 100.00 

diff Exact Match 2.00 99.00 100.00 100.00 100.00 100.00 

Average 0.50 57.10 74.69 62.30 82.55 85.00 

23 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

Table 14. Full results of MMLU QA datasets. 

Type Subject Metric Fewshot+Manual CoT APE ProTeGi StablePrompt GFlowPO STEM abstract algebra Accuracy 30.00 33.00 31.00 35.00 33.00 31.00 anatomy Accuracy 50.37 51.85 49.63 52.95 53.33 52.59 astronomy Accuracy 57.89 64.47 53.95 56.58 63.16 57.89 college biology Accuracy 66.67 67.36 56.98 65.80 65.28 56.94 college chemistry Accuracy 38.00 34.00 39.00 40.00 39.00 36.00 college computer science Accuracy 41.00 48.00 32.80 37.00 40.00 39.00 college mathematics Accuracy 32.00 34.00 33.00 33.00 35.00 37.00 

college physics Accuracy 39.22 34.31 32.33 35.29 40.20 40.20 

computer security Accuracy 70.00 67.00 62.20 67.00 64.00 61.00 conceptual physics Accuracy 51.06 55.31 51.06 49.79 52.34 48.09 electrical engineering Accuracy 51.72 55.17 46.21 40.00 58.62 53.79 elementary mathematics Accuracy 38.89 60.05 38.10 37.30 41.01 38.36 high school biology Accuracy 70.65 64.52 65.81 69.81 71.94 70.32 high school chemistry Accuracy 52.71 52.71 52.22 45.82 50.25 47.78 high school computer science Accuracy 61.00 58.00 54.00 51.00 56.00 61.00 

high school mathematics Accuracy 36.30 33.70 38.52 32.96 35.19 37.04 high school physics Accuracy 26.49 31.13 32.45 33.77 31.79 31.13 high school statistics Accuracy 45.37 43.52 46.76 50.46 39.82 45.37 machine learning Accuracy 35.71 46.43 39.29 35.71 38.39 40.18 Social Science econometrics Accuracy 32.46 34.21 32.46 31.58 37.72 35.97 high school geography Accuracy 66.67 61.11 56.57 59.69 68.69 69.19 

high school government and politics Accuracy 74.09 76.17 67.88 70.89 80.31 78.24 high school macroeconomics Accuracy 54.10 55.13 50.00 56.15 54.10 52.82 high school microeconomics Accuracy 55.46 55.46 53.36 56.15 58.40 59.25 

high school psychology Accuracy 76.33 73.58 71.19 72.66 74.5 75.05 human sexuality Accuracy 62.60 52.76 61.07 58.78 69.45 68.70 professional psychology Accuracy 51.80 53.43 49.51 48.09 52.29 54.58 

public relations Accuracy 60.00 54.55 63.64 59.09 68.18 66.36 security studies Accuracy 50.20 48.57 52.24 47.35 58.78 53.47 sociology Accuracy 66.17 67.19 65.17 70.65 70.65 75.62 

us foreign policy Accuracy 75.00 69.00 76.00 73.00 73.00 72.00 Humanities formal logic Accuracy 37.30 38.10 36.51 33.33 32.54 36.51 high school european history Accuracy 63.64 57.58 62.42 65.45 67.88 65.45 high school us history Accuracy 62.75 56.86 65.20 55.39 70.10 68.14 high school world history Accuracy 68.35 67.51 71.23 64.14 73.42 73.84 

international law Accuracy 61.98 65.29 64.46 66.12 69. 42 71.07 

jurisprudence Accuracy 57.41 63.89 62.04 62.04 66.67 71.30 

logical fallacies Accuracy 63.19 65.03 68.10 66.87 66.25 65.03 moral disputes Accuracy 49.71 51.16 58.96 55.49 59.83 58.67 moral scenarios Accuracy 24.36 27.93 27.26 29.27 29.05 28.27 philosophy Accuracy 56.91 54.66 54.66 57.23 54.66 56.27 prehistory Accuracy 60.49 52.16 58.64 56.17 55.25 57.10 professional law Accuracy 40.61 38.53 32.01 41.98 39.83 41.20 world religions Accuracy 73.68 69.59 71.93 74.27 73.68 73.68 Others business ethics Accuracy 47.00 63.00 55.00 51.00 58.00 56.00 clinical knowledge Accuracy 54.34 56.60 51.20 51.70 62.26 62.64 

college medicine Accuracy 54.34 53.17 46.87 49.71 57.80 53.18 global facts Accuracy 32.00 39.00 32.00 35.00 37.00 34.00 human aging Accuracy 56.50 55.61 58.74 58.30 54.71 58.74 

management Accuracy 61.17 64.08 63.11 60.19 70.87 69.90 marketing Accuracy 75.64 80.34 76.92 77.35 82.91 81.20 medical genetics Accuracy 54.00 55.00 55.00 57.00 59.00 59.00 

miscellaneous Accuracy 73.31 74.20 72.41 72.80 70.75 72.16 nutrition Accuracy 59.15 53.59 56.86 62.09 64.38 66.34 

professional accounting Accuracy 40.07 41.48 46.45 42.90 40.78 39.36 professional medicine Accuracy 55.15 45.22 0.50 50.73 44.12 52.21 virology Accuracy 46.39 47.22 48.80 50.00 49.40 51.21 

24 GF LOW PO: Generative Flow Network as a Language Model Prompt Optimizer 

Algorithm 1 GF LOW PO Full Algorithm 

1: Input: Prompt-LM pθ , small training dataset D, learning rates α, pre-step size n, batch size m, training steps T ,meta-prompt M , set of reference prompts Zref , high-reward buffer Q, replay buffer B, update frequency K, number of reference prompts sampled from the replay buffer kb, number of reference prompts sampled from the high-reward buffer kq .

2: pref ← deepcopy (pθ ), Q ← ∅ , B ← ∅ , Zref ← ∅ , ℓ ← 0

3: for i = 1 , . . . , n do ▷ Pre-Step: Replay buffer initialization 

4: τ ← Uniform (0 .5, 2.0) 

5: Sample z from pθ (z|M ) with temperature τ

6: B.Add (z, A D (z)) 

7: end for 

8: for t = 1 , . . . , T do 

9: for i = 1 , . . . , m do ▷ GFlowNet fine-tuning 

10: Sample s from {0, 1}

11: τ ← Uniform (0 .5, 2.0) 

12: if s = 0 then 

13: Sample z from pθ (z|M ) with temperature τ

14: B.Add (z, A D (z)) 

15: Q.Add (z, A D (z)) 

16: else 

17: (z, A D (z)) ∼ B 

18: end if 

19: Compute reward R(z; M ) = AD (z) · pref (z | M ) in Eq. (4) 

20: Compute the loss ℓ ← ℓ + L(θ; M )/m using Eq. (5) 

21: end for 

22: Update θ ← θ − α∇θ ℓ and reset ℓ ← 0

23: if t mod K = 0 then ▷ Dynamic Memory Update (DMU) 

24: Sample {z(i) 

> b

}kb 

> i=1

from B

25: Sample {z(j) 

> q

}kq 

> j=1

from Q

26: Update Zref ← { z(i) 

> b

}kb 

> i=1

∪ { z(j) 

> q

}kq

> j=1

27: Update M with Zref as in §3.3 

28: end if 

29: end for 

30: Return: Prompt-LM pθ

25