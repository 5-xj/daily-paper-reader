Title: Multi-Objective Optimization for Synthetic-to-Real Style Transfer

URL Source: https://arxiv.org/pdf/2602.03625v1

Published Time: Wed, 04 Feb 2026 02:42:42 GMT

Number of Pages: 18

Markdown Content:
# Multi-Objective Optimization for Synthetic-to-Real Style Transfer 

Estelle Chigot 1,2 ⋆, Thomas Oberlin 1 , Manon Huguenin 2 , and Dennis Wilson 1 

> 1

Fédération ENAC ISAE-SUPAERO ONERA, Université de Toulouse, Toulouse, France 

{estelle.chigot2, dennis.wilson, thomas.oberlin}@isae.fr  

> 2

Airbus, Toulouse, France 

manon.huguenin@airbus.com 

Abstract. Semantic segmentation networks require large amounts of pixel-level annotated data, which are costly to obtain for real-world im-ages. Computer graphics engines can generate synthetic images alongside their ground-truth annotations. However, models trained on such images can perform poorly on real images due to the domain gap between real and synthetic images. Style transfer methods can reduce this difference by applying a realistic style to synthetic images. Choosing effective data transformations and their sequence is difficult due to the large combi-natorial search space of style transfer operators. Using multi-objective genetic algorithms, we optimize pipelines to balance structural coher-ence and style similarity to target domains. We study the use of paired-image metrics on individual image samples during evolution to enable rapid pipeline evaluation, as opposed to standard distributional met-rics that require the generation of many images. After optimization, we evaluate the resulting Pareto front using distributional metrics and seg-mentation performance. We apply this approach to standard datasets in synthetic-to-real domain adaptation: from the video game GTA5 to real image datasets Cityscapes and ACDC, focusing on adverse conditions. Results demonstrate that evolutionary algorithms can propose diverse augmentation pipelines adapted to different objectives. The contribution of this work is the formulation of style transfer as a sequencing problem suitable for evolutionary optimization and the study of efficient metrics that enable feasible search in this space. The source code is available at: 

https://github.com/echigot/MOOSS .

Keywords: Genetic algorithm · Evolutionary algorithm · Domain adaptation ·

Style transfer · Semantic segmentation  

> ⋆

Corresponding author 

Accepted in International Conference on the Applications of Evolutionary Computation (Part of EvoStar), April 2026 (EvoApplications 2026). 

1 

> arXiv:2602.03625v1 [cs.CV] 3 Feb 2026 2E. Chigot et al.

# 1 Introduction 

Semantic segmentation assigns class labels to each pixel of an image and is an essential task for scene understanding in autonomous driving or robotics [21]. Deep learning methods achieve high performance but require large labeled datasets. Annotating real-world images is expensive, with a single Cityscapes image requiring approximately 90 minutes of human effort [6]. Synthetic data from game engines like GTA5 [30] provide automatic labels, due to the engines’ inherent access to object position and geometry. Yet, they induce a domain gap when models trained on synthetic images are tested on real data. Domain adaptation techniques transfer knowledge from labeled synthetic do-mains to real target domains. Recent approaches use diffusion models for style transfer [39], generating synthetic images that match real data appearance while preserving semantic structure. However, these style transfer pipelines involve nu-merous steps, similar to data augmentation, and the selection and ordering of these operations remains manual. Operators such as image darkening and sharp-ening, or more complex operations like image normalization through AdaIN [20], can help transfer style from one domain to another. As with data augmentation, these operators can be combined to improve data quality for training models on downstream tasks such as image classification. Previous work has shown evo-lutionary algorithms can automate the search for data augmentation strategies in image classification [18,27,28]. In this work, we study style transfer pipeline optimization for domain adaptation, where two distributions of data must be considered. We formulate the construction of style transfer pipelines as a multi-objective optimization task. A pipeline consists of a sequence of transformations applied to source domain images, with each transformation altering visual properties while preserving semantic content. We model this as a modified Traveling Salesman Problem (TSP) [12], where data operators correspond to cities and a special terminal node ends the sequence. This formulation differs from standard TSP by allowing variable-length solutions through early termination, similar to the Orienteering Problem [14]. A main challenge in optimizing style transfer pipelines is evaluation. Metrics like FID [17] or CMMD [22] estimate distances between distributions of data but require costly generation of many images. Training a segmentation network on each candidate dataset would be even more expensive. Instead, we use metrics that operate on individual image pairs: DISTS [11] for structural similarity and DreamSim [13] for perceptual style matching. These metrics allow rapid evalu-ation during genetic algorithm optimization while maintaining correlation with downstream performance. We test on domain adaptation from GTA5 [30] to Cityscapes [6] and ACDC [32], which includes adverse weather conditions. Our contributions are: formulation of augmentation pipeline selection as a combinatorial problem suitable for evolu-tionary optimization; identification of paired-image metrics that enable feasible optimization while correlating with distributional quality; and demonstration Multi-Objective Optimization for Synthetic-to-Real Style Transfer 3

that multi-objective evolution produces diverse solutions representing different trade-offs between content preservation and style adaptation. 

# 2 Context 

2.1 Style Transfer for Domain Adaptation 

Domain adaptation in semantic segmentation aims to train models on source domain data that generalize to target domains with different visual character-istics. Input-level adaptation methods transform source images to resemble the target domain while preserving semantic content. Several works have attempted to tackle the synthetic-to-real domain adaptation problem by using diffusion models to generate realistically looking datasets. Adaptive Instance Normalization (AdaIN) provides a mechanism for style transfer by aligning feature statistics between images [20]: AdaIN (x, y ) = σ(y)

 x − μ(x)

σ(x)



+ μ(y) (1) where μ and σ denote mean and standard deviation computed across spatial dimensions for each channel. AdaIN enables arbitrary style transfer by match-ing global statistics. However, global statistics matching can produce unrealistic results. Dominant classes in the style image may inappropriately influence other semantic classes in the content image. Class-wise Adaptive Instance Normalization with Cross-attenTIon (CACTI) [3] addresses this through class-specific statistical alignment. Rather than com-puting global statistics, CACTI leverages semantic segmentation masks to com-pute separate statistics for each semantic class. This ensures that style charac-teristics transfer appropriately within semantic boundaries. Diffusion models enable conditioning on structural information during gen-eration. ControlNet [37] adds spatial conditioning layers to pretrained diffusion models. During the diffusion process, ControlNet processes control inputs (seg-mentation masks or edge maps) through an encoder that mirrors the denoising network architecture. The control features modulate the main denoising network through zero-initialized connections, enabling structural control while preserving style generation capabilities. DGInStyle [23] first fine-tunes DreamBooth [31] on the source domain, and then trains a ControlNet to produce images guided by segmentation maps. In the final step, they perform a style-swap procedure that replaces the source-specific UNet with a pretrained generalist UNet. This ensures that only the segmentation-based conditioning is preserved, preventing any source-style leak-age during ControlNet training. DATUM [2] leverages DreamBooth as well to fine-tune Stable Diffusion on a single target image for a small number of itera-tions. Afterwards, they prompt the model to synthesize a specific target object rendered in the style of the target image. DoGE [35] computes the mean differ-ence of CLIP embeddings pairs between the source and target dataset (Domain 4 E. Chigot et al. 

Gap Embedding). This embedding is then added to the latent representation of a source image, which is reconstructed using Stable UnCLIP. The approach is compatible with ControlNet, enabling dataset generation conditioned on seman-tic segmentation maps. 

2.2 Data Augmentation Optimization 

The optimization of data augmentation strategies has received attention as a method to improve model generalization without requiring additional labeled data. A set of operators modify data with the goal of improving model training; optimization focuses on choosing the appropriate data modification operations. AutoAugment [7] introduced a reinforcement learning approach where a con-troller network searches over augmentation operations to find policies that max-imize validation accuracy. The method trains thousands of child models to eval-uate policies, making the search computationally expensive. RandAugment [8] simplifies the search space by randomly selecting augmentation operations with uniform magnitude. Fast AutoAugment [26] employs density matching to ap-proximate policy search, while Faster AutoAugment [16] uses backpropagation through differentiable augmentation implementations. However, these methods generally require repeated model training for evaluation. Population-based methods offer an alternative. Population Based Augmenta-tion (PBA) [18] addresses computational cost by learning augmentation sched-ules rather than fixed policies. Pereira et al. [28] apply an evolutionary strategy to discover optimal sequences of transformation functions. Marc et al. [27] con-sider class-specific augmentation strategies using a genetic algorithm. Recent differentiable methods like DADA [25] and FreeAugment [1] enable end-to-end optimization of augmentation policies. These policy-based formulations share a common characteristic: they deter-mine augmentations on a per-image or per-minibatch basis during training. This dynamic approach allows for adaptive augmentation but requires evaluating fit-ness through model training, which becomes prohibitively expensive when gen-erating datasets for domain adaptation. For style transfer in semantic segmen-tation, the requirements differ. The goal is to generate a transformed dataset that can be used to train or test models that generalize to a target domain. Rather than determining augmentations per-image during training, we seek to optimize a single pipeline that can be applied to the source domain to produce a dataset resembling the target domain. This formulation enables the use of power-ful style transfer operators such as AdaIN [20] and diffusion-based methods [39], which have greater impact on domain gap reduction than traditional geometric augmentations. The fixed pipeline approach also enables evaluation using distri-butional metrics before training segmentation models. To our knowledge, this is the first work which aims at optimizing data augmentation strategies for style transfer. Multi-Objective Optimization for Synthetic-to-Real Style Transfer 5

2.3 Data Similarity Metrics 

Paired-image metrics assess similarity between individual images. LPIPS [38] measures perceptual similarity using features from pretrained networks. Deep Image Structure and Texture Similarity (DISTS) [11] measures perceptual sim-ilarity with robustness to texture variations, combining structure and texture similarity measurements: 

Sl(x, y ) = 2μxμy + ϵμ2 

> x

+ μ2 

> y

+ ϵ (2) 

Tl(x, y ) = 2σxy + ϵσ2 

> x

+ σ2 

> y

+ ϵ (3) DISTS (x, y ) = X

> l

wl · (Sl(x, y ) + Tl(x, y )) (4) where Sl and Tl represent structure and texture similarity at layer l by comparing the means μx, μ y , variances σx, σ y , and covariance σxy of corresponding feature maps ( ϵ is added to avoid instability). Lower DISTS values indicate images share similar content. DreamSim [13] provides a metric for evaluating perceptual similarity with focus on mid-level variations such as pose, perspective, and style. The metric is trained on synthetic data generated using diffusion models, with human an-notations of similarity judgments. These pairwise metrics evaluate individual transformations, allowing for the evaluation of style transfer pipelines on a small number of images. Distributional metrics like FID and CMMD provide assessment of entire gen-erated datasets but require generating many samples. Fréchet Inception Distance (FID) [17] computes distance between feature distributions: FID = || μx − μy || 2 + Tr (Σx + Σy − 2( ΣxΣy )1/2) (5) where μx, μy are mean feature vectors of the two datasets and Σx, Σy the corresponding sample covariance matrices. CLIP Maximum Mean Discrepancy (CMMD) [22] addresses FID limitations by using CLIP embeddings and Maxi-mum Mean Discrepancy with a Gaussian RBF kernel. These distributional met-rics have been used to measure the quality of style transfer methods, using distributions of generated data compared to target domain data [4], however such a calculation would be too costly for optimization. 

# 3 Methodology 

The task of synthetic-to-real domain adaptation through style transfer can be formulated as finding a sequence of transformations T = [ t1, t 2, . . . , t n] that, when applied to source domain images S, produces images S′ that maintain semantic structure while matching visual characteristics of the target domain T .Each transformation ti belongs to a set of available operators A.6 E. Chigot et al. Multi-objective genetic algorithm Pareto optimal solutions   

> Population of image pipelines Paired-image metrics evaluation
> Style Content
> Traditional image transformations
> Deep learning based transformations
> Normalize
> Brighten
> …
> AdaIN
> CACTI
> STOP
> …
> Stopping node
> Pipeline 1
> Pipeline 2
> …

Fig. 1. Overview of the proposed multi-objective pipeline optimization framework. Image transformation pipelines, composed of traditional and deep learning–based op-erators with a stopping node, are evolved using a multi-objective genetic algorithm. Each pipeline is evaluated using paired-image metrics that measure content preserva-tion (DISTS) and style similarity to the target domain (DreamSim). Ultimately, the evolutionary process produces a Pareto front of non-dominated pipelines representing different trade-offs between both objectives. 

The problem presents two competing objectives. First, transformed images must preserve semantic content and spatial structure to ensure ground truth annotations remain valid. Second, transformed images should match the visual style of the target domain to reduce the domain gap. Evaluating candidate pipelines during optimization requires balancing accu-racy and computational cost. To enable feasible optimization, we use paired-image metrics that operate on individual samples. For each candidate pipeline, we apply the sequence to a small set of source images and compute metrics on the resulting pairs. This reduces evaluation cost by several orders of magni-tude while maintaining correlation with distributional quality and downstream performance. We use DISTS [11] to measure structural similarity between source and trans-formed images and DreamSim [13] to measure perceptual similarity between transformed images and target domain samples. The problem becomes a multi-objective optimization task where we seek pipelines that minimize both DISTS (to preserve content) and DreamSim (to match target style). Multi-objective optimization produces a Pareto front of non-dominated solutions representing different tradeoffs between objectives. Figure 1 illustrates the method. 

3.1 Traveling Salesman Problem (TSP) Formulation 

We model the augmentation pipeline selection problem as a variant of the TSP where each data operator represents a city. The TSP seeks the shortest route visiting a set of cities exactly once and returning to the start. We consider this suitable as the main style transfer operators used in this study, such as AdaIn [20] and ControlNet [37], only operate as a single application to an image. Other operators, such as sharpening the image or darkening it, could be applied Multi-Objective Optimization for Synthetic-to-Real Style Transfer 7

multiple times, but do not represent the main contributors to style transfer and are thus included as one-off operator choices. The key modification to the TSP is introduction of a terminal node tstop 

which can be visited at any point to end the pipeline. The solution representation is a permutation of augmentation operators including the terminal node: π =[π1, π 2, . . . , π m] where m = |A| + 1 . When evaluating a solution, operators are applied sequentially until the terminal node is encountered. This is similar to the Orienteering Problem [14], which extends TSP by selecting a subset of cities to visit subject to a budget constraint, however, unlike the Orienteering Problem, we do not impose a budget on total travel time. The discrete and combinatorial nature makes this problem appropriate for genetic algorithms. The solution space consists of permutations of operators, a representation studied extensively in TSP contexts. Genetic algorithms have a long and successful history of use for the TSP and variants, including the Orienteering Problem [33], for a multitude of applications [29,24,34,15,9]. 

3.2 Multi-objective Evolutionary Algorithm 

We employ NSGA-II[10] to solve the multi-objective optimization problem. NSGA-II uses non-dominated sorting to classify individuals into fronts based on domi-nance relationships and maintains diversity through crowding distance. The initial population is generated using permutation random sampling, which creates valid permutations of augmentation operators and terminal node. For crossover, we use the edge recombination crossover [36], which preserves ad-jacency information from parent solutions. For mutation, we apply inversion mu-tation, which selects two random positions and reverses the order of all elements between them. Duplicate elimination prevents the population from containing multiple copies of the same solution. 

# 4 Pipeline Optimization 

4.1 Image Operators 

The augmentation pipeline consists of ten operations: nine transformations and one terminal node. The transformations include six traditional image process-ing operations applied using Pillow, a modern version of the Python Imaging Library [5]. The normalize operation adjusts color distribution to match a refer-ence using histogram matching. Blur applies Gaussian smoothing with a radius value of 2.0 pixels. Brighten and darken modify image luminosity by factors of 1.5 and 0.7 respectively. Contrast modifies contrast by a factor 1.5. Sharpen applies unsharp masking with a factor 2.0. Three deep learning-based style transfer operations provide more substan-tial domain adaptation. AdaIN performs style transfer by aligning channel-wise statistics between content and style images. CACTI uses class-specific statistical alignment through segmentation masks for focused style transfer. ControlNet generates images conditioned on segmentation masks and edge maps. 8 E. Chigot et al. 

For each deep learning operation, we use as reference a single set of five images, one for each weather condition considered. This ensures the method is applicable in a wide range of situations, even in the case of limited target data.   

> Content Style Night Snow Fog Rain Clear Day
> Fig. 2. Pairs of images (content, style) used during the optimization process, respec-tively used for the night, snow, fog, rain, and clear day style transfer. Content images are from the GTA5 dataset. Style images are from the ACDC dataset (night, snow, fog, and rain style) and the Cityscapes dataset (clear day style).

4.2 Optimization Configuration 

We use source domain images from the GTA5 [30] dataset. Target domain images are sampled from the Cityscapes [6] training set for clear day conditions and ACDC [32] training set for adverse weather conditions. Selected images from these content and style datasets are respectively shown in Figure 2. For each candidate pipeline during optimization, we evaluate using the five sampled source images. For each source image, we apply the pipeline transfor-mations and measure DISTS relative to the original and DreamSim relative to selected target domain images. Our NSGA-II configuration uses a population size of 20 individuals and gen-erates 20 offspring per generation. Inversion mutation and edge recombination Multi-Objective Optimization for Synthetic-to-Real Style Transfer 9

crossover are used to generate all offspring. The relatively small population is sufficient given the limited size of the search space. The evolutionary process is run for 20 generations, as Pareto front convergence was observed. Optimization was performed on a Linux server with a 32 core Intel i9 CPU and a RTX4090 GPU. The largest time cost was image generation. CACTI is the most expensive operator, with a maximum generation time of 40 seconds, with ControlNet in second, taking up to 2 seconds. All other operators function in less than 1 second for generation. The maximum evaluation time for a single pipeline was therefore roughly 200 seconds, or 3 minutes and 20 seconds on a single machine. Full optimization took 13 hours, meaning 39 minutes per NSGA-II generation on average and 23.4 seconds per generated image. Evaluation of chosen pipelines, requiring generation of 1250 images, also took up to 13 hours per pipeline, or 37.4 seconds per image, for solutions containing CACTI. This demonstrates the benefit of an optimization approach using pairwise metrics over a small number of images; the full optimization took as long as the distributional evaluation of a single pipeline. Model training on generated images using the same hardware took 5.5 hours and was performed 3 times for averages. 

4.3 Optimization Results 

Figure 3 presents the progression of optimization across twenty generations. The evolutionary algorithm successfully drives the population toward the Pareto front, with individuals moving from high DISTS and DreamSim values toward the region where both objectives are minimized. The initial population shows diversity, with solutions distributed across a wide range of combinations. Several solutions from the first generation are al-ready close to what will become the final Pareto front, which is not surprising due to the relatively small search space. By generation 3, the characteristic shape of the Pareto front becomes visible. From generation 5 onward, the population exhibits limited further evolu-tion. Solutions remain clustered in approximately four to five distinct regions along the Pareto front. This stagnation after relatively few generations confirms that the search space is small enough to be thoroughly explored with a modest computational budget. The observed stagnation demonstrates that the problem formulation and res-olution method are capable of quickly identifying high-quality solutions, making the approach computationally feasible. The final Pareto front provides solutions spanning different points along the content-style trade-off curve, enabling prac-titioners to select pipelines according to specific requirements. 

# 5 Analysis of Optimized Pipelines 

5.1 Optimized Pipelines 

Optimization produces a Pareto front containing multiple non-dominated solu-tions. To evaluate these solutions, we select representative pipelines that span 10 E. Chigot et al.      

> Fig. 3. Populations produced at several stages of the optimization process, from gen-eration 1 on the top left to generation 20 on the bottom right. On the xaxis is the DISTS value representing content preservation, and on the yaxis the DreamSim value representing style transfer. Both metrics range from 1.0 to 0.0, with lower values being better.

the front. We manually select one pipeline from each cluster, resulting in five representative solutions. Figure 4 shows these selected pipelines in red among the full set of optimal solutions. The selection ensures even distribution across the trade-off between content preservation and style transfer. For comparison, we also evaluate a baseline pipeline using only ControlNet: C. ControlNet → Stop The five selected pipelines, ordered from high content preservation to high style fidelity, are: 1. Sharpen → Stop 2. Darken → Sharpen → AdaIN → Stop 3. AdaIN → Darken → CACTI → Stop 4. ControlNet → CACTI → Darken → Sharpen → AdaIN → Stop 5. AdaIN → Normalize → Blur → Contrast → ControlNet → CACTI → Stop These pipelines reveal patterns about the relationship between solution com-plexity and objective values. As DreamSim values decrease (indicating better style transfer), pipelines become progressively more complex. Pipeline 1 applies only a single operation, while Pipeline 5 uses six of the nine available augmen-tations. This suggests that achieving strong style transfer while maintaining reasonable content preservation requires composing multiple transformations. The presence of deep learning operators AdaIN, ControlNet, and CACTI in the more style-focused pipelines reflects their effectiveness for this task. Con-trolNet appears in only two of the five selected pipelines, both of which have Multi-Objective Optimization for Synthetic-to-Real Style Transfer 11  

> Fig. 4. The final Pareto front of sequences optimized by NSGA-II. The five selected sequences (in red) are highlighted among all optimal solutions (in blue). A reference sequence using only ControlNet is shown in green. The top-left solution corresponds to the highest content preservation (low DISTS), whereas the bottom-right solution achieves the most realistic style (low DreamSim).

DreamSim values in the lower range. This distribution suggests that Control-Net contributes more to the style transfer objective in this context, which is surprising given its intended effect of content preservation [37]. The visual results from the five pipelines presented in Figure 5 confirm the progression suggested by DreamSim metric values. Visual results for the other style images can be found in Figure 6. Across all three weather conditions, images become progressively more stylized from Pipeline 1 to Pipeline 5, with visual characteristics increasingly matching the style reference images. Pipeline 1, which applies only the sharpen operator, produces images very similar to original GTA5 content. The primary visible change is enhanced edge definition, with little modification to color distribution, lighting, or texture. This minimal transformation reveals a limitation of relying solely on DISTS. DISTS is designed to measure perceptual similarity with emphasis on structural content, making it relatively insensitive to subtle stylistic differences between synthetic and real domains. Images from Pipelines 2 through 5 show increasingly substantial modifica-tions to visual appearance while maintaining recognizable scene structure. Colors shift to match real-world distributions, lighting conditions become less uniform, and texture characteristics change from smooth rendered appearance to more complex patterns of photographic imagery. These changes are what DreamSim is designed to capture: perceptual style similarity that encompasses broader vi-sual characteristics distinguishing synthetic and real images. The balance between DISTS and DreamSim becomes apparent through these visual results. DISTS prevents pipelines from applying transformations so aggres-sive that spatial structure is lost, which would invalidate ground truth annota-tions. DreamSim encourages sufficient style modification to reduce the domain 12 E. Chigot et al. Night Snow Reference 12345ControlNet    

> Fig. 5. Reference content images, then pipelines 1 →5, and finally ControlNet on the Night (left) and Snow (right) reference style images.

gap. Pipelines in the middle of the Pareto front (particularly Pipelines 3 and 4) achieve a reasonable compromise. On the other hand, the ControlNet outputs differ noticeably from those pro-duced by the selected pipelines. Although the overall structure looks well pre-served, the generated images tend to have an overly smooth texture that drifts further from the style reference. In the night scenario, ControlNet achieves a rea-sonably strong style transfer, producing a dark scene with pronounced lighting and a fading of the road markings. In contrast, the snow scenario shows only minimal stylistic modification, with slight adjustments in lighting and color. Compared to Pipeline 3, the ControlNet results appear to have higher percep-Multi-Objective Optimization for Synthetic-to-Real Style Transfer 13 

tual quality, yet they exhibit weaker alignment with the style reference and less faithful preservation of content details such as building structure. 

5.2 Pipeline Evaluation                                              

> DISTS DISTS DreamSim DreamSim CMMD Samples 51250 51250 1250 Pipeline 1 0.0415 0.0396 0.5779 0.5600 5.15 Pipeline 2 0.1563 0.1478 0.5334 0.5241 4.57 Pipeline 3 0.2277 0.2249 0.4266 0.3837 2.67 Pipeline 4 0.2840 0.2777 0.3540 0.3465 2.37 Pipeline 5 0.3185 0.3025 0.3030 0.3177 2.35
> GTA5 0.0000 0.0000 0.5766 0.5577 5.18 ControlNet 0.2646 0.2666 0.4852 0.4653 3.48
> Table 1. Image quality metrics for the five selected pipelines and the baseline, com-puted between content and style pairs of reference images, as well as generated datasets (1250 images) and the target domain.

For each selected pipeline, we generate a complete dataset by applying the augmentation sequence to source domain images. We generate 250 transformed images for each of the five target weather conditions, producing a total dataset of 1250 images. Table 1 reports the DISTS and DreamSim values used as optimization met-rics in Figure 4, computed both on the small evaluation set (5 images) and on the full generated datasets (1250 images per pipeline). Across all pipelines, DISTS and DreamSim remain highly consistent when moving from 5 to 1250 images. This stability indicates that both metrics are robust to dataset scaling and can therefore be reliably estimated from only a few samples during optimization. As DreamSim improves and DISTS gets worse (moving from Pipeline 1 to Pipeline 5), CMMD generally decreases, indicating better distributional match. This cor-relation between DreamSim and CMMD further validates the use of pairwise metrics in the proposed optimization approach. Next, we evaluate the generated data on semantic segmentation model train-ing to evaluate the capacity of these style transfer pipelines to aid in domain adaptation. We train DAFormer [19], a transformer-based domain adaptation ar-chitecture, on each generated dataset. Following standard protocols [19,2,23,3], we train for 40,000 iterations with batch size 2. After training, we evaluate each model on ACDC and Cityscapes validation sets. We report mean Intersec-tion over Union (mIoU), the pixel-wise segmentation accuracy averaged across all semantic classes. This metric directly reflects downstream task performance achieved by training on datasets generated with each optimized pipeline. Table 2 presents downstream task performance of each pipeline. mIoU scores vary across pipelines and weather conditions. Furthermore, the baseline Control-14 E. Chigot et al. Fog Night Rain Snow ACDC Cityscapes Pipeline 1 48.57 16.65 42.50 40.34 37.07 51.41 Pipeline 2 50.65 19.89 45.65 43.54 39.40 51.51 Pipeline 3 49.99 19.32 45.17 42.55 39.79 50.69 Pipeline 4 50.94 19.46 45.94 44.02 40.27 51.36 Pipeline 5 48.91 17.57 44.29 42.69 38.73 50.09 GTA5 48.74 16.35 42.52 40.33 36.64 50.56 ControlNet 55.73 24.84 46.66 44.39 42.42 52.12 Table 2. Evaluation metrics for the five selected pipelines and the baseline. mIoU values are shown for each ACDC weather condition, the mean across ACDC conditions and Cityscapes clear day images. 

Net achieves mIoU values that exceed most optimized pipelines across several conditions. Having been fine-tuned directly on the style images, ControlNet can effectively combine the learned target-domain features with strong class and structural conditioning. Even though its outputs show weaker perceptual sim-ilarity, particularly in textures and lighting, ControlNet generates images that stay close to the target domain in terms of semantic content. This strong se-mantic coherence helps explain its comparatively high downstream performance despite less accurate style transfer. The segmentation results indicate that distributional similarity between gen-erated and target data does not directly correlate with training performance. This finding aligns with recent work on domain generalization, particularly DA-TUM [2], which demonstrates that realistic appearance is not necessary for train-ing segmentation networks. 

# 6 Conclusion 

This work demonstrates that evolutionary algorithms can effectively explore the space of data augmentation pipelines for domain adaptation. The proposed TSP formulation allows multi-objective optimization using pairwise metrics to gener-ate multiple candidate solutions. This formulation constitutes a novel application of evolutionary computation to automate the design of style transfer pipelines for semantic segmentation domain adaptation. As measured by DISTS and Dream-Sim, our approach produces pipelines with good style transfer properties and diverse trade-offs between content preservation and style matching. A key advantage of this method is computational efficiency. Optimizing the complete set of pipelines requires the same time as evaluating a single pipeline for full data generation. This means the quick metrics can be applied to a new target domain before committing to expensive data generation, allowing practitioners to consider multiple pipeline options early in the adaptation process. However, our results reveal a limitation in the choice of objectives. While the evolved pipelines achieve good style transfer, they do not match the down-stream segmentation performance of ControlNet alone when measured by mIoU. Multi-Objective Optimization for Synthetic-to-Real Style Transfer 15 

This indicates that the DISTS and DreamSim combination, while effective for assessing style transfer quality, does not serve as reliable surrogates for down-stream task performance on image segmentation. Future work should focus on identifying metrics that can cheaply estimate downstream segmentation qual-ity. For domain adaptation specifically, an ideal formulation would include three objectives: DreamSim for style matching, a surrogate metric for segmentation performance, and compute time. Additional directions include expanding the set of available augmentation operations and incorporating hyperparameter tuning directly into the genetic algorithm. 

# References 

1. Bekor, T., Nayman, N., Zelnik-Manor, L.: Freeaugment: Data augmentation search across all degrees of freedom. In: European Conference on Computer Vision (ECCV). pp. 36–53. Springer (2024) 2. Benigmim, Y., Roy, S., Essid, S., Kalogeiton, V., Lathuilière, S.: One-shot unsu-pervised domain adaptation with personalized diffusion models. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 698–708 (2023) 3. Chigot, E., Wilson, D.G., Ghrib, M., Oberlin, T.: Style transfer with diffusion models for synthetic-to-real domain adaptation. Computer Vision and Image Un-derstanding 259 , 104445 (2025) 4. Chung, J., Hyun, S., Heo, J.P.: Style injection in diffusion: A training-free ap-proach for adapting large-scale diffusion models for style transfer. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8795–8805 (2024) 5. Clark, A., Contributors: Pillow: Python imaging library. https://python-pillow. org/ (2025) 6. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: Conference on Computer Vision and Pattern Recogni-tion (CVPR). pp. 3213–3223 (2016) 7. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation strategies from data. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 113–123 (2019) 8. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data augmentation with a reduced search space. In: Conference on Computer Vi-sion and Pattern Recognition Workshops (CVPRW). pp. 3008–3017 (2020) 9. Davendra, D.: Traveling salesman problem: Theory and applications. BoD–Books on Demand (2010) 10. Deb, K., Pratap, A., Agarwal, S., Meyarivan, T.: A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation 6(2), 182–197 (2002) 11. Ding, K., Ma, K., Wang, S., Simoncelli, E.P.: Image quality assessment: Unify-ing structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence 44 (5), 2567–2581 (2022) 12. Flood, M.M.: The traveling-salesman problem. Operations research 4(1), 61–75 (1956) 13. Fu, S., Tamir, N., Sundaram, S., Chai, L., Zhang, R., Dekel, T., Isola, P.: Dream-sim: Learning new dimensions of human visual similarity using synthetic data. 16 E. Chigot et al. In: Advances in Neural Information Processing Systems (NeurIPS). vol. 36, pp. 50742–50768 (2023) 14. Golden, B.L., Levy, L., Vohra, R.: The orienteering problem. Naval Research Lo-gistics (NRL) 34 (3), 307–318 (1987) 15. Groba, C., Sartal, A., Vázquez, X.H.: Solving the dynamic traveling salesman problem using a genetic algorithm with trajectory prediction: An application to fish aggregating devices. Computers & Operations Research 56 , 22–32 (2015) 16. Hataya, R., Zdenek, J., Yoshizoe, K., Nakayama, H.: Faster autoaugment: Learn-ing augmentation strategies using backpropagation. In: European Conference on Computer Vision (ECCV). p. 1–16 (2020) 17. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. In: Advances in Neural Information Processing Systems (NeurIPS). vol. 30 (2017) 18. Ho, D., Liang, E., Chen, X., Stoica, I., Abbeel, P.: Population based augmentation: Efficient learning of augmentation policy schedules. In: International Conference on Machine Learning (ICML). pp. 2731–2741. PMLR (2019) 19. Hoyer, L., Dai, D., Van Gool, L.: DAFormer: Improving network architectures and training strategies for domain-adaptive semantic segmentation. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9924–9935 (2022) 20. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance normalization. In: International Conference on Computer Vision (ICCV). pp. 1501– 1510 (2017) 21. Hurtado, J.V., Valada, A.: Semantic scene segmentation for robotics. In: Deep learning for robot perception and cognition, pp. 279–311. Elsevier (2022) 22. Jayasumana, S., Ramalingam, S., Veit, A., Glasner, D., Chakrabarti, A., Kumar, S.: Rethinking fid: Towards a better evaluation metric for image generation. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9307–9315 (2024) 23. Jia, Y., Hoyer, L., Huang, S., Wang, T., Van Gool, L., Schindler, K., Obukhov, A.: Dginstyle: Domain-generalizable semantic segmentation with image diffusion models and stylized semantic control. In: European Conference on Computer Vision (ECCV). pp. 91–109 (2024) 24. Larranaga, P., Kuijpers, C.M.H., Murga, R.H., Inza, I., Dizdarevic, S.: Genetic algorithms for the travelling salesman problem: A review of representations and operators. Artificial intelligence review 13 (2), 129–170 (1999) 25. Li, Y., Hu, G., Wang, Y., Hospedales, T., Robertson, N.M., Yang, Y.: Differen-tiable automatic data augmentation. In: European Conference on Computer Vision (ECCV). p. 580–595 (2020) 26. Lim, S., Kim, I., Kim, T., Kim, C., Kim, S.: Fast AutoAugment (2019) 27. Marc, S.T., Belavkin, R., Windridge, D., Gao, X.: An evolutionary approach to automated class-specific data augmentation for image classification. In: Interna-tional Conference on the Dynamics of Information Systems. pp. 170–185. Springer (2023) 28. Pereira, S., Correia, J., Machado, P.: Evolving data augmentation strategies. In: International Conference on the Applications of Evolutionary Computation (Part of EvoStar). pp. 337–351. Springer (2022) 29. Potvin, J.Y.: Genetic algorithms for the traveling salesman problem. Annals of operations Research 63 (3), 337–370 (1996) 30. Richter, S.R., Vineet, V., Roth, S., Koltun, V.: Playing for data: Ground truth from computer games. In: European Conference on Computer Vision (ECCV). pp. 102–118. Springer (2016) Multi-Objective Optimization for Synthetic-to-Real Style Transfer 17 31. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 22500– 22510 (2023) 32. Sakaridis, C., Dai, D., Van Gool, L.: Acdc: The adverse conditions dataset with correspondences for semantic driving scene understanding. In: Conference on Com-puter Vision and Pattern Recognition (CVPR). pp. 10765–10775 (2021) 33. Tasgetiren, M.F., Smith, A.E.: A genetic algorithm for the orienteering problem. In: Proceedings of the 2000 Congress on Evolutionary Computation. vol. 2, pp. 910–915. IEEE (2000) 34. Tsai, H.K., Yang, J.M., Tsai, Y.F., Kao, C.Y.: An evolutionary algorithm for large traveling salesman problems. IEEE Transactions on Systems, Man, and Cybernet-ics, Part B (Cybernetics) 34 (4), 1718–1729 (2004) 35. Wang, Y.O., Chung, Y., Wu, C.H., De La Torre, F.: Domain Gap Embeddings for Generative Dataset Augmentation. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 28684–28694 (Jun 2024) 36. Whitley, D., Starkweather, T., Shaner, D.: The traveling salesman and sequence scheduling: Quality solutions using genetic edge recombination. Colorado State University, Department of Computer Science Fort Collins (1991) 37. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: International Conference on Computer Vision (ICCV). pp. 3836–3847 (2023) 38. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 586–595 (2018) 39. Zhang, Y., Huang, N., Tang, F., Huang, H., Ma, C., Dong, W., Xu, C.: Inversion-based style transfer with diffusion models. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10146–10156 (2023) 18 E. Chigot et al. 

# 7 Appendix 

Fog Rain Day Reference 12345ControlNet 

Fig. 6. Reference content images, then pipelines 1 → 5, and finally ControlNet on the Fog, Rain, and Day reference style images.