Title: A Multi-objective Evolutionary Algorithm Based on Bi-population with Uniform Sampling for Neural Architecture Search

URL Source: https://arxiv.org/pdf/2602.08513v1

Published Time: Tue, 10 Feb 2026 03:01:31 GMT

Number of Pages: 19

Markdown Content:
> JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1

# A Multi-objective Evolutionary Algorithm Based on Bi-population with Uniform Sampling for Neural Architecture Search 

Yu Xue, Senior Member, IEEE , Pengcheng Jiang, Graduate Student Member, IEEE ,Chenchen Zhu, Yong Zhang, Senior Member, IEEE , Ran Cheng, Senior Member, IEEE ,Kaizhou Gao, Senior Member, IEEE , Dunwei Gong, Senior Member, IEEE 

Abstract —Neural architecture search (NAS) automates neural network design, improving efficiency over manual approaches. However, efficiently discovering high-performance neural net-work architectures that simultaneously optimize multiple objec-tives remains a significant challenge in NAS. Existing methods often suffer from limited population diversity and inadequate exploration of the search space, particularly in regions with extreme complexity values. To address these challenges, we propose MOEA-BUS, an innovative multi-objective evolutionary algorithm based on bi-population with uniform sampling for neural architecture search, aimed at simultaneously optimiz-ing both accuracy and network complexity. In MOEA-BUS, a novel uniform sampling method is proposed to initialize the population, ensuring that architectures are distributed uniformly across the objective space. Furthermore, to enhance exploration, we deploy a bi-population framework where two populations evolve synergistically, facilitating comprehensive search space coverage. Experiments on CIFAR-10 and ImageNet demonstrate MOEA-BUS’s superiority, achieving top-1 accuracies of 98.39% on CIFAR-10, and 80.03% on ImageNet. Notably, it achieves 78.28% accuracy on ImageNet with only 446M MAdds. Ablation studies confirm that both uniform sampling and bi-population mechanisms enhance population diversity and performance. Ad-ditionally, in terms of the Kendall’s tau coefficient, the SVM achieves an improvement of at least 0.035 compared to the other three commonly used machine learning models, and uniform sampling provided an enhancement of approximately 0.07. 

Index Terms —Evolutionary algorithm, neural architecture search, multi-objective optimization, multi-population mecha-nism, surrogate model. 

This work was supported by the National Natural Science Foundation of China (NO. 62376127, NO. 61876089, NO. 61876185), the Guangdong Basic and Applied Basic Research Foundation (No. 2024B1515020019), and the Natural Science Foundation of Shandong Province (NO. ZR2023ZD06). 

(Corresponding author: Yu Xue.) 

Yu Xue, Pengcheng Jiang and Chenchen Zhu are with the School of Software, Nanjing University of Information Science and Technology, Nanjing 210044, China (e-mails: xueyu@nuist.edu.cn; pcjiang@nuist.edu.cn; 202212490283@nuist.edu.cn). Yong Zhang is with the School of Information and Control Engineering, China University of Mining and Technology, Xuzhou 221008, China (e-mail: yongzh401@cumt.edu.cn). Ran Cheng is with the Department of Data Science and Artificial Intel-ligence, and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China, and also with The Hong Kong Polytech-nic University Shenzhen Research Institute, Shenzhen 518057, China (e-mail: ranchengcn@gmail.com). Kaizhou Gao is with the Macau Institute of Systems Engineering, Macau University of Science and Technology, Taipa 999078, Macao SAR, China (e-mail: kzgao@must.edu.mo). Dunwei Gong is with the College of Automation and Electronic Engi-neering, Qingdao University of Science and Technology, Qingdao 266061, Shandong, China (e-mail: dwgong@qust.edu.cn). 

I. I NTRODUCTION 

# DEEP neural networks (DNNs) have achieved remarkable success in various fields, such as image and speech recognition [1], natural language processing [2], autonomous driving [3], game and robotics [4], etc. With further research, DNNs are continuously optimized and improved, and their performance mainly depends on the structures of networks [5]. Traditional neural network architectures are usually designed manually by experts with extensive domain knowledge. Over time, these manually designed approaches have gradually shown limitations, especially when dealing with complex and high-dimensional data. Furthermore, as the size of datasets grows and computational resources increase, the demand for designing deeper and more complex networks increases [6]. In this context, neural architecture search (NAS) has emerged, which aims to use algorithms to search for optimal network architectures, thus reducing human intervention and improving design efficiency [7]. Neural architecture search can not only optimize existing network architectures, but also explore new network architectures through the search process. These new architectures offer enhanced performance and higher gen-eralization ability, thereby promoting the development and application of deep learning in various fields [8]. The research and development of neural architecture search is of great sig-nificance in areas such as real-world applications and industrial production [9]–[13]. Despite the significant progress made by neural archi-tecture search in automating the design of neural network architectures, it still faces several challenges, including the scale of the search space, search efficiency, and model size constraints [14]. Existing NAS methods usually concern them-selves only with the maximization of the classification accu-racy [15], [16]. However, real-world applications often require neural networks to achieve a balance across multiple aspects. For example, models deployed on mobile devices need to maintain high accuracy while having a smaller model size and fast inference speed [17]. With the widespread application of artificial intelligence technologies, the demand for efficient and high-performance models is increasing, which has prompted researchers to explore neural network architectures that can meet multiple performance needs. Therefore, some researchers have begun to conduct in-depth research on multi-objective neural architecture search, attempting to find architectures that  

> arXiv:2602.08513v1 [cs.NE] 9 Feb 2026 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2

can take into account multiple performance indicators [18]. Unlike single-objective optimization, multi-objective optimiza-tion requires considering multiple performance indicators at the same time, which usually means finding the balance among these indicators, rather than a single optimal so-lution [19]. Evolutionary algorithms, by simulating natural selection and genetic mechanisms, maintain a population of candidate solutions and improve these solutions through op-erations such as selection, crossover, and mutation in each generation [20]. Evolutionary algorithms have good global search capabilities and can flexibly and effectively explore and handle Pareto optimization in multi-objective space. In contrast, the two other popular categories of NAS methods: reinforcement learning-based (RL) [21] and gradient-based (GD) [22] methods, have some limitations when dealing with multi-objective problems. Gradient-based methods, such as DARTS [23], usually assume that the optimization problem is differentiable and has only one objective function. However, some indicators of architectures, such as model complexity, are usually non-differentiable and cannot be easily optimized through the loss function. In addition, gradient-based methods may tend to optimize the objectives that contribute the most to the gradient signal, while neglecting other equally important objectives [24]. Reinforcement learning-based methods usually rely on a reward function to guide the search process, but in the case of multi-objective, defining a reward function that fully reflects all objectives is very difficult [25]. More-over, they consume more computational resources and incur higher time costs than the other two methods [26]. Overall, evolutionary algorithms are more suitable for multi-objective neural structure search, as they provide an effective search strategy. In current NAS methods, some research employs multi-objective optimization theory to simultaneously optimize multiple metrics, with network complexity being a common second metric besides classification accuracy. The frequently used approaches to represent network complexity include the number of parameters in the network or “multiplying and accumulating operations (MAdds)”. In multi-objective evolutionary optimization methods, pop-ulation diversity determines the distribution of the population on the Pareto front. A population lacking diversity tends to converge to one or more regions in the objective space while neglecting other parts. During the evolutionary process, a population with insufficient diversity tends to focus solely on exploiting known regions of the objective space, thereby neglecting the exploration of new areas. This results in a final solution set where the trade-off solutions are not representative across each objective. In multi-objective evolutionary neural architecture search (MO-ENAS), this issue is often overlooked. For instance, NSGA-Net focuses more on architectures around a specific MAdds value, resulting in a population that lacks diversity, limiting the breadth of search, and leading to ar-chitectures that are locally optimal in this region. Based on analysis of this problem, population initialization and selection operators during the search process are identified as two critical factors. In the objective space of NAS, medium-sized architectures often have a large number of different representations of encoding, but small and large architectures do not. Therefore, commonly used random initialization is not entirely suitable for the NAS search spaces, which leads to a bias toward small and medium-sized network architectures in terms of MAdds during population initialization. Addition-ally, relying solely on non-dominated sorting-based selection operators makes it difficult to maintain good population diver-sity during the search process. Multi-population mechanisms are common, flexible, and effective methods for enhancing population diversity. Under existing selection operators, multi-population mechanisms can significantly improve population diversity. Another key challenge in NAS stems from the substantial re-sources consumed in evaluating numerous candidate architec-tures. Although there are currently many studies on training-free evaluation, they still do not have significant advantages compared to traditional evaluation acceleration methods [27]. Therefore, during the search process, each architecture re-quires training to obtain accuracy for environmental selection, which consumes considerable resources and requires extensive time. To address this issue, ENAS methods commonly employ surrogate models, weight inheritance, and other techniques. Weight inheritance methods aim to utilize pre-trained weights obtained from supernets to initialize parameters of identi-cal modules in architectures, thereby reducing training time for individual architectures. This approach can significantly shorten the search duration of the original algorithm. However, architectures still require at least one inference time for actual evaluation even when using one-shot methods, which prevents a large number of architectures from being searched. Surrogate models reduce the number of architectures requiring actual evaluation by predicting architecture performance. The resource and time consumption of this prediction process are substantially lower than the inference cost of network architec-tures, thus enabling rapid evaluation of numerous architectures during the search process. To address the above problems, we propose an effective algorithm, called MOEA-BUS, a multi-objective evolution-ary algorithm based on bi-population with uniform sampling for neural architecture search. Firstly, we design a uniform sampling method for initializing the population so that the initial architectures are distributed as uniformly as possible in the objective space. Second, to explore the search space more fully during the search process, we propose a multi-objective bi-population-based evolutionary algorithm where two populations evolve concurrently and exchange individ-uals. The proposed method aims to provide a set of high-performance architectures that take into account multiple optimization objectives. We validate the effectiveness of the proposed algorithm on an image classification task using the standard datasets CIFAR-10, CIFAR-100, and ImageNet. The computational results show that the proposed method outperforms most state-of-the-art NAS methods. In addition, we conduct sufficient ablation studies for each key mechanism to prove the effectiveness of the proposed method. The main contributions are as follows: 1) The proposed method simultaneously optimizes accuracy and network complexity, with MAdds as the complexity metric. During the search process, a surrogate model and JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3

weight inheritance are used to reduce the time and re-sources required to evaluate the architectures. 2) Uniform sampling is proposed to improve the quality of the initial population, in which a two-stage sampling method is designed to sample individuals and initialize an initial population that is uniform on the network complexity, i.e., MAdds. 3) A multi-objective bi-population-based evolutionary algo-rithm is proposed, in which two populations evolve together and genes are exchanged between them to fully explore the search space. It can largely prevent the algorithm from falling into a local optimum while accelerating conver-gence. The remainder of this paper is organized as follows: Section II presents related work and background. Section III describes the proposed method in detail. We present the experimental design to verify the effectiveness and efficiency of the pro-posed method and discuss the results in Section IV. Finally, conclusions and future work are outlined in Section V. II. R ELATED WORK 

A. Multi-objective NAS 

Existing research in NAS concentrates mainly on improving the accuracy of neural networks, but these single-objective methods often ignore the more complex requirements of real-world applications. Although these existing networks perform well on recognition tasks, they are often difficult to deploy in real-world situations due to high computational costs and large model sizes. Researchers have turned to multi-objective optimization for NAS and explore how to more effectively find the optimal balance between these metrics to design neural network architectures that are both efficient and practical. For example, Lu et al . use NSGA-II as the multi-objective optimization method to simultaneously optimize accuracy and computational cost [28], [29]. Subsequently, they further in-vestigate methods to reduce the time consumption of multi-objective optimization by introducing a surrogate model [30], [31]. In addition, Xue et al . propose a multi-objective evolu-tionary algorithm for NAS that focuses on accuracy and time consumption [32]. Wang et al . improve the particle swarm optimization (PSO) algorithm to optimize both classification accuracy and MAdds [33]. Du et al . design an environmental selection operation based on reference points to improve the multi-objective optimization process in NAS [34]. Although these studies have yielded successful results in multi-objective optimization, they usually require evaluation of a large number of architectures, which is time-consuming and inefficient. In addition, among the existing multi-objective NAS methods, there are relatively few studies and improvements on multi-objective evolutionary algorithms, and researchers tend to choose only off-the-shelf algorithms, such as NSGA-II, to han-dle multi-objective optimization problems in NAS. Therefore, an improved multi-objective algorithm is proposed in order to better adapt to the search framework in this work. 

B. Multi-population ENAS 

Multi-population strategies in ENAS are designed to en-hance search diversity and prevent premature convergence. However, these methods encounter a fundamental paradox: while the migration of high-performing individuals between populations is intended to share beneficial traits, it can in-advertently homogenize the gene pool, ultimately converging to a single suboptimal solution. To address this issue, recent research has proposed more sophisticated strategies. These include creating heterogeneity by employing different evo-lutionary algorithms [8], implementing intelligent migration protocols that select for novelty to increase diversity [35], and redefining the search to evolve functionally specialized networks that are combined for superior performance [36]. However, these methods do not further explore the lack of population diversity caused by the uneven distribution of ob-jective space in NAS, nor do they make adjustments according to this characteristic. 

C. Diversity Preservation 

In evolutionary multi-objective algorithms, preserving pop-ulation diversity is crucial for helping the algorithm to avoid falling into a local optimum and explore a globally opti-mal solution, and many scholars have conducted research to balance diversity and convergence. Saad et al . propose a multi-objective artificial bee colony (ABC) algorithm [37]. The algorithm relies on the basic principle of population evolution, which exploits the differences among individuals in the population to generate new candidate solutions, ef-fectively making use of the diversity among individuals and promoting the evolution of the whole population. Wang et al .combine the differential evolution algorithm with the particle swarm optimization, which uses an adaptive mutation strategy, achieving effective preservation of population diversity at the early stage and significantly accelerating the convergence rate at the later stage during the evolution [38]. It can be seen that designing better search strategies can accelerate convergence speed, improve population diversity, and enhance effective interactions between individuals, thereby ultimately enhancing the performance of the multi-objective evolutionary algorithms (MOEAs). Therefore, careful consideration and design of appropriate search strategies are crucial for obtaining satisfactory results [39]. In addition, initialization methods can be adjusted to integrate external information at the outset in the population initialization phase of multi-objective evolutionary algorithms, aiming to approximate the global optimum solu-tion as closely as possible [40]. Evolutionary strategies are crucial for MOEAs to rapidly converge to the Pareto front. Thus, we design a multi-objective evolutionary algorithm for NAS from two perspectives of the initialization and search strategy. III. P ROPOSED METHOD FOR MULTI -OBJECTIVE 

EVOLUTIONARY NEURAL ARCHITECTURE SEARCH 

This section presents the details of a bi-population-based multi-objective evolutionary algorithm with uniform sampling for NAS. We firstly present the framework of the proposed algorithm in Section III-A. Then, the details of the pro-posed search space and encoding are introduced in Section III-B. Subsequently, the proposed uniform sampling method JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4Terminate ? Obtain      

> Architectures
> YES
> NO
> Population
> 1
> Candidates
> Population
> Elite
> Architecture s
> Population
> 2
> Elite
> Architecture s
> Next iteration
> Next iteration
> First
> Pareto Front
> First
> Pareto Front
> Sub -Archive 1
> Sub -Archive 2
> Diversity
> Selection
> Diversity
> Selection
> Surrogate Model Surrogate Model
> Surrogate -
> Assisted Sub -
> Search
> Surrogate -
> Assisted Sub -
> Search
> Surrogate -
> Assisted Sub -
> Search
> Surrogate -
> Assisted Sub -
> Search
> Calculate
> Population
> Division
> Sampling
> Numerous
> Architectures
> Archive
> Evaluation
> Uniform
> Sampling Select Pareto Sub -Search Merg e
> Populations
> Select Pareto Sub -Search Merg e
> Populations
> 2
> 2
> 1
> 1
> 0
> 2
> ...
> 2
> 2
> 1
> 1
> 0
> 2
> ...
> 2
> 2
> 1
> 1
> 0
> 2
> ...
> 2
> 2
> 1
> 1
> 0
> 2
> ...
> 2
> 2
> 1
> 1
> 0
> 2
> ...
> 2
> 2
> 1
> 1
> 0
> 2
> ...
> Construct
> Pairwise
> Dataset

Fig. 1: Overall Framework: A multi-objective evolutionary neural architecture search method based on bi-population with uniform sampling. is described in Section III-C, and proposed multi-objective algorithm with bi-population is described in Section III-D. Finally, surrogate model and the use of supernet are introduced in Section III-E. 

A. Overall Framework 

The existing multi-objective evolutionary neural architecture search methods are prone to the problem of lack of diversity due to conflicting objectives, and the proposed method sug-gests two improvement measures: firstly, a uniform sampling method is designed to initialize the initial population; secondly, two populations jointly perform evolutionary exploration of the search space to improve population diversity during the search. An overview of the proposed overall framework is illustrated in Fig. 1. First, a large number of architectures are sampled and their MAdds is evaluated, with uniform sampling being used to obtain candidate architectures that are uniformly distributed across the MAdds metric. These selected architectures serve as the initial architectures of archive A and undergo real evaluation. Subsequently, these architectures are divided into two archives, one containing medium-sized architectures and another containing large and small architectures. These two archives are respectively used for the search processes of two populations. The core idea of the uniform sampling method is to ensure that the individuals in the initial population are uniformly distributed in the objective space, avoiding archi-tecture concentration in certain regions and improving search space coverage. Uniform sampling helps enhance the diversity and global exploration ability in the early search phase. To strengthen information exchange between populations and so-lution diversity, the proposed method performs an exchange of individuals between populations at the end of each generation. Population 1 shares excellent elite individuals with population 2, thereby promoting comprehensive search space coverage and diversity maintenance, and accelerating the convergence of the entire search process. Meanwhile, the computational cost from the evaluation during the search is reduced with the help of a surrogate model and weight inheritance technique. After several generations, all the searched network architectures are sorted by non-dominated sorting and a set of high quality architectures are chosen based on specific preferences. 

B. Search Space and Encoding 

The quality of evolutionary search results is fundamentally determined by the chosen search space. In this work, archi-tectures are based on MobileNetV3 [41] and are composed of three stages. The initial stage and final stage remain fixed. The main part of architectures consists of a stack of multiple convolutional blocks. Externally, the size of the input image (resolution) also needs to be searched. In the internal structure, each block contains several layers, and the numbers of layers are optional. In addition, each layer uses an inverted bottle-neck structure that contains multiple convolutions, requiring optimization of both convolution kernel size and expansion rate. Fig. 2 illustrates the search space and encoding strategy. The algorithm searches for the appropriate expansion ratios for the initial 1 × 1 convolution and kernel sizes for the depth-wise separable convolution in each layer. The encoding of an architecture is composed of image resolution and other parts representing five blocks. Each block’s encoding spec-ifies the number of layers, expansion rate, and kernel size of its constituent convolution layers. The encoding’s values correspond to indices from predefined considered option sets. Moreover, the absence of a layer is indicated by a padded zero to achieve the fixed length encoding, which is not from considered options. 

C. Uniform Sampling 

During the evolutionary process, the selection and distri-bution of the initial population critically determines both the search efficacy of the method and the performance of the surrogate model. A well-designed initial population provides diverse starting points that enhance the global search capa-bility, while a poor initial population may lead to the search falling into local optimum and limit the exploration of the JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5(a) 

Architecture search space 

Block 1 Input Start 

Stage Output 

Final 

Stage 

Block 2 Block 3 Block 4 Block 5  

> 1× 1
> Conv
> Depth -wise
> Conv
> ……
> Layer 1
> 1× 1
> Conv
> 1× 1
> Conv
> Depth -wise
> Conv
> ……
> Layer L
> 1× 1
> Conv
> ……

(b) 

Example of the encoding  

> +

2 2 0 1 02 0 1 0 1 1 0 21 1 0 22 2 0 1 0 1 1 0 2 1 1 1 2 01 1 2 0 1 1 0 01 1 0 01 1 1 2 0 1 1 0 05 …… + +   

> Block 1 Block 5 Block 2~Block 4
> Resolutions
> { 0, 1, 2, …… ,15, 16 }

192 196 200 252 256 …192 196 200 252 256 …  

> Resolutions
> { 0, 1, 2, …… ,15, 16 }

192 196 200 252 256 … 3 4 63 4 6

> Expansion Rate
> { 0, 1, 2}

3 4 6

> Expansion Rate
> { 0, 1, 2}

2 3 42 3 4

> Layers
> { 0, 1, 2}

2 3 4

> Layers
> { 0, 1, 2}

3 5 73 5 7

> Kernel Size
> { 0, 1, 2}

3 5 7  

> Kernel Size
> { 0, 1, 2}
> Resolutions
> { 0, 1, 2, …… ,15, 16 }

192 196 200 252 256 … 3 4 6

> Expansion Rate
> { 0, 1, 2}

2 3 4

> Layers
> { 0, 1, 2}

3 5 7

> Kernel Size
> { 0, 1, 2}

Fig. 2: Search space and encoding. (a) The architecture search space. (b) An example of the encoding. The encoding is divided into five parts by blocks. The parameters we search include image resolution, the number of layers in each block, the expansion rate, and the kernel size in each layer. 

Fig. 3: The distribution of randomly sampled 5,000 architec-tures. ……  

> Search Space Architectures Distribution
> Sample
> Calculate
> Population 1
> (big and small
> architectures)
> Population 2
> (middle -sized
> architectures)
> Population Division
> Number
> MAdds
> Number
> MAdds

Fig. 4: The illustration of uniform sampling. search space. Furthermore, the initial archive derived from a uniformly distributed initial population proves beneficial for surrogate model training, enabling more precise identification of superior architectures in subsequent search iterations. To investigate this phenomenon, we sampled 5,000 ar-chitectures from the search space using random sampling method and analyzed their distributional characteristics. Fig. 3 illustrates the distribution of these 5000 architectures across the metric of network complexity. The horizontal coordinate is the MAdds metric, and the vertical coordinate is the count of architectures. As can be seen in Fig. 3, randomly sampled architectures exhibit a highly concentrated distribution pat-tern on the MAdds metric, with the overwhelming majority clustering between 300M and 400M MAdds. This means that architectures within this complexity range occupy the majority of the search space, while architectures with higher or lower complexity remain relatively scarce. This concentrated distribution limits the capacity of the population to explore in regions of higher or lower complexity, resulting in that po-tentially valuable architectures may be overlooked at an early stage. Consequently, in subsequent evolutionary iterations, the evolutionary algorithm tends to generate new architectures that closely resemble the current population, further limiting architectural diversity and search effectiveness. In order to obtain high-quality initial populations, we pro-pose a uniform sampling method illustrated in Fig. 4. Specif-ically, the uniform sampling method proceeds as follows: Initially, a substantial number of architectures are randomly sampled from the search space, and their complexity (MAdds) is calculated. Subsequently, all architectures are sorted by MAdds values and divided into several regions with uni-form ranges based on MAdds distribution according to their complexity from smallest to largest. After division, a certain number of architectures are selected from each region. In order to ensure diversity across the search space, regions with JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6

Algorithm 1: Framework of MOEA-BUS 

Input: Supernet Ws, number of iterations T . 

> 1

H ← Initialize numerous architectures;  

> 2

A ← ∅; // Create an empty archive for storing records.  

> 3

P1, P 2 ← Uniform Sampling( H); // The initial populations are constructed using the proposed uniform sampling method. See Section III-C for details.  

> 4

for a in P1 ∪ P2 do  

> 5

Wa ← Ws(a); // Inherit the weights of corresponding pre-trained modules in the supernet according to architecture a. 

> 6

error rate ← SGD (a, W a); 

> 7

A ← A ∪ { (a, error rate )}; 

> 8

end  

> 9

t ← 0; 

> 10

while t < T do  

> 11

predictor ← Construct surrogate model with A;

// See Section III-E for details.  

> 12

P ∗ 

> 1

← Sub-Search (P1, predictor, A); // Search with the small and big architectures.  

> 13

P ∗ 

> 2

← Sub-Search (P2, predictor, A); // Search with the middle-sized architectures.  

> 14

for a in P ∗ 

> 1

∪ P ∗ 

> 2

do  

> 15

Same as lines 4 to 8; // Evaluate a

with supernet and add the records into A 

> 16

end 

// Update the initial populations for two Sub-Search processes.  

> 17

P1 ← P1 ∪ P ∗ 

> 1

; 

> 18

P2 ← P2 ∪ P ∗ 

> 1

∪ P ∗ 

> 2

; 

> 19

t ← t + 1 ; 

> 20

end  

> 21

return Final population (all individuals in A). high and low MAdds values are emphasized, containing ar-chitectures with extreme complexity. Architectures from these extreme regions are selected and merged to form population 1. Meanwhile, architectures with moderate MAdds values are selected and merged to constitute population 2. These two initial populations ensure the diversity and provide a rich architectural pool for subsequent evolution. Through uniform sampling, the initial population covers multiple complexity re-gions, from low to high, achieving a more uniform distribution in the objective space. 

Algorithm 2: Sub-Search ( P op , predictor , A)

Input: Number of generations G. 

> 1

g ← 0; 

> 2

P ← Get first Pareto front of P op ; // Only use the first Pareto front as the initial population.  

> 3

P.F 1 ← Predict the strength for each architecture with 

predictor ; // See Section III-E for details.  

> 4

P.F 2 ← Calculate the MAdds of each architecture;  

> 5

while g < G do  

> 6

Q ← Generate offspring of P with crossover and mutation;  

> 7

P.F 1, Q.F 1 ← Predict the strength for each architecture in P ∪ Q with predictor ; // See Section III-E for details.  

> 8

Q.F 2 ← Calculate the MAdds of each architecture;  

> 9

P ← P ∪ Q; 

> 10

P ← Non-Dominated-Sort( P );  

> 11

P ← Crowded-Selection( P );  

> 12

g ← g + 1 ; 

> 13

end  

> 14

P F ← Non-Dominated-Sort( P );  

> 15

return Diversity Selection( F P , A). 

D. Multi-objective Evolutionary Algorithm Based on Bi-population 

In order to further increase the population diversity and ex-plore the search space more comprehensively, a bi-population evolution framework is proposed. The core idea of the pro-posed framework is to introduce two populations for parallel evolution to increase the diversity of solutions during the search process and to improve the global search capability of the algorithm. The bi-population evolution process is the main loop part of the evolutionary search. Algorithm 1 demonstrates the overall process of bi-population search. First, two populations are obtained ac-cording to the uniform sampling method in Section III-C, where population 1 contains large and small architectures, and population 2 contains medium-sized architectures (lines 1-3). Subsequently, all individuals in both populations are truly evaluated, and the results of real evaluation are recorded in archive A (lines 4-8). During this process, each architecture needs to be trained and validated using image datasets, and the classification error rate on the validation set is obtained. Since the training and validation of architectures in the over-all population are mutually independent processes, the real evaluation of each architecture is divided into sub-tasks that are automatically allocated to multiple available GPUs by a single device for execution. Afterwards, we set T rounds of iterative search while continuously updating the overall archive and training the surrogate model. Specifically, we first train a surrogate model based on the current overall archive A

(line 11), then use the surrogate model to assist the respective evolutionary processes of the two populations (lines 12-13). The evolutionary process of each population is detailed in JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 72    

> 2
> 1
> 1
> 0
> 2
> ...
> Arc 1
> 2
> 2
> 1
> 1
> 0
> 2
> ...
> Arc 2
> 2
> 2
> 1
> 1
> 0
> 2
> ...
> Pairwise
> 2
> 2
> 1
> 1
> 0
> 2
> ...
> Pairwise
> Concatenate Concatenate Input Input Output Output Which is better?
> 0 / 1
> Label 0
> Arc 1 is
> better
> 1
> Arc 2 is
> better
> Surrogate Model Surrogate Model

Fig. 5: The illustration of the proposed surrogate model. Algorithm 2. Subsequently, the two elite populations ( P ∗

> 1

and P ∗ 

> 2

) obtained from the search undergo real evaluation, following the same process as lines 4 to 8. Afterwards, we add individuals from elite population P ∗ 

> 1

to initial populations 

P1 and P2, and add individuals from elite population P ∗

> 2

only to initial population P2 (lines 17-18). When T rounds of iterations are satisfied, the algorithm terminates, and the final overall population (i.e. all individuals in A) are obtained. Algorithm 2 presents the detailed process of sub-search (lines 12-13 in Algorithm 1). First, based on the current sub-population, non-dominated sorting is performed and the Pareto front is obtained, where individuals in the first rank serve as the initial population P for sub-search (line 2). During the surrogate evaluation process for P , the surrogate model is used to predict the strength of each individual as the fitness value for the first objective ( P.F 1), while the computational complexity MAdds of each architecture individual in P is calculated as the fitness value for the second objective ( P.F 2). Subsequently, G generations of search are executed, where new individuals are generated through crossover and mutation operations in each iteration, and the fitness values of offspring (Q.F 1 and Q.F 2) are obtained through the same surrogate evaluation process as in lines 3-4. The only difference is that 

Q.F 1 is calculated using both P and Q, so P.F 1 is also updated. Then, non-dominated sorting and crowding distance selection are performed on the combined population to obtain the next generation population P . After G generations of search, diversity selection is applied to the final population 

P that has undergone non-dominated sorting to obtain the architecture most different from the overall population A. The specific operation of diversity selection involves computing the fitness value differences between individuals in F P and each individual in the overall population layer by layer, and retaining the individual with the maximum difference. 

E. Surrogate-assisted Search and Weight Inheritance 

A key challenge in the field of NAS lies in the substantial computational overhead required to evaluate numerous net-work architectures [42]. This problem is prevalent despite the fact that different search spaces and strategies are used. To improve the search efficiency of the proposed method, we use a surrogate model for sub-search process and weight inheritance for real evaluation to reduce the search time. We construct a surrogate model used to predict performance ranking of architectures during the sub-search process, which costs much less time for evaluation and is able to identify the potential architectures. In this paper, support vector machine (SVM) is chosen to build the surrogate model based on comparison relationships between architectures. First, we concatenate the encoding of each individual in the archive with the encoding of every individual numbered after it to construct training data D, and set the data labels to 0 or 1, where 0 indicates that the preceding architecture in the concatenated encoding is better, and 1 indicates that the following architecture is better. This process is illustrated in Fig. 5. Subsequently, this dataset is used to train a fitted SVM model. During the prediction phase, we apply the same processing to the architectures that need to be predicted. Assuming the prediction result for (Xi, X j )

is P red , then the i-th architecture and the j-th architecture will each receive a score, where the i-th architecture obtains a score of P red , and the j-th architecture obtains a score of (1 − P red ). Finally, we use the accumulated scores as intensity, where higher intensity indicates higher classification error rate of the architecture. The non-dominated sorting for architectures is performed by the predicted strength and the calculated MAdds. Additionally, the construction and training of the surrogate model require already evaluated architectures to be used as training samples, and we use weight inheritance to accel-erate the evaluation of architectures. When evaluating the performance of an architecture, the weights of Once-For-All [43] are used as initialization for the gradient descent algorithm, thereby significantly reducing the time of training and evaluation for candidate architectures. Once-For-All is a well-trained supernet built upon the MobileNetV3 back-bone network, encompassing more than 10 19 candidate sub-networks. In this paper, candidate networks directly inherit their weights from the supernet. During training process of them, the weights of candidate networks are updated, and the corresponding weights in the supernet remain frozen. IV. E XPERIMENTS 

In this section, we conduct a series of experiments to validate the effectiveness of the proposed algorithm. Ini-tially, Section IV-A describes the specific configuration of the experiments. Subsequently, we present and analyze the experimental results on the most commonly used datasets for image classification from different perspectives in Section IV-B. Additionally, we conducted ablation experiments on two key mechanisms in the paper and analyze the results in Section IV-C to demonstrate the effectiveness of the proposed method. Then, we discuss the surrogate model in Section IV-D. Furthermore, the effectiveness of the uniform sampling strategy is shown in Section IV-E. Finally, the ablation study on the bi-population mechanism and analysis are presented in Section IV-F. 

A. Experimental Configurations 

We conduct experiments using three widely recognized image classification datasets: CIFAR-10, CIFAR-100, and Im-ageNet. CIFAR-10 consists of 60,000 32x32 color images across 10 classes. CIFAR-100 is similar but contains 100 classes. ImageNet contains over 1.2 million images belong-ing to 1,000 different classes. ImageNet is known for its vast variety of images and challenging classification tasks, JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8

making it a benchmark for evaluating deep learning models. The performance of architectures discovered by the proposed algorithm is evaluated based on accuracy and MAdds. MAdds provides insights into the computational complexity of ar-chitectures. The summary of parameter settings for MOEA-BUS is presented in Table I. Our experiments are conducted on a single RTX 3090 (24GB) card using PyTorch 2.0 and CUDA 11.7 environment. The code is available on https: //github.com/pcjiang1998/MOEA-BUS. TABLE I: Detailed settings of MOEA-BUS. 

In main framework 

Size of initial population 1 25 Size of initial population 2 75 Number of iterations 25 

In sub-search 

Number of generations 40 Size of population 60 Number of reserved elite individuals 4 for population 1 6 for population 2 Crossover method Two-point crossover Mutation method Polynomial mutation 

In final train 

Number of epochs 200 Optimizer SGD Initial learning rate 0.01 Momentum rate 0.9 Batch size 128 

B. Results on Standard Datasets 

This section presents and analyzes the experimental results on the CIFAR and ImageNet datasets from different perspec-tives. We evaluate the performance of the architectures discov-ered by the proposed algorithm based on accuracy, MAdds, and search time. Under the available computational resources, to ensure the robustness of our experiments, we conduct multi-ple rounds of training evaluation on the architectures searched on CIFAR-10 and CIFAR-100, and present the mean and standard deviation of multiple results. Due to page limitations, the experimental results and analysis on CIFAR-100 are in the 

supplementary material . To further validate the performance of MOEA-BUS, we compare the non-dominated architectures obtained by the proposed algorithm with those discovered by other state-of-the-art NAS methods. The selected peer methods can be broadly divided into three categories: manually designed by human experts, EA-based, and non-EA-based (RL-based and GD-based) methods. The results in Tables II and III are from the original papers and the classification accuracy, MAdds, number of parameters, and search cost are displayed in them. Among them, the search cost is expressed in GPU days, and the supernet training time is excluded for all of the methods. All results of the proposed method in these tables, i.e., MOEA-BUS-S/M/L/XL, are searched on CIFAR-10 and ImageNet datasets separately and the final four architectures on each dataset are trained. 

Results on CIFAR-10: We select four architectures ac-cording to different sizes of MAdds, named MOEA-BUS-S/M/L/XL. Table II summarizes the comparison with other state-of-the-art methods. The proposed algorithm achieves the highest average classification accuracy of 98.39% ±0.03 on TABLE II: Comparison on the CIFAR-10 dataset. This table compares the classification accuracy, computational complex-ity (MAdds), and search cost with other state-of-the-art NAS methods on the CIFAR-10 dataset.                                                                                                                                                                                                                     

> Architecture Accuracy (%) MAdds (M) Params (M) Search Cost (GPU Days) Search Method Year
> MobileNetV2 [44] 95.74 300 2.2 -manual 2018 EfficientNet-B0 [45] 98.1 387 4.0 -manual 2019 NASNet-A [46] 97.35 608 -1800 RL 2018 BNAS [47] 97.03 -4.7 0.19 RL 2021 DBNAS-B [48] 97.33 -3.1 -RL 2025 PC-DARTS [49] 97.43 558 3.6 0.1 GD 2019 P-DARTS [50] 97.5 532 3.4 0.3 GD 2019 FairDARTS [51] 97.46 373 2.8 0.25 GD 2020 NoisyDARTS [52] 97.63 534 3.3 0.4 GD 2021 EoiNAS [53] 97.50 -3.4 0.6 GD 2022 iDARTS [54] 97.47 -3.6 -GD 2023 SWD-NAS [55] 97.49 519 3.17 0.13 GD 2024 PA-DARTS [56] 97.59 578 3.75 0.36 GD 2024 GENAS [57] 97.55 -3.53 0.26 GD 2024 DBNAS-A [48] 97.58 -2.4 -GD 2025 DBNAS-C [48] 97.50 -2.9 -GD 2025 FX-DARTS [9] 95.96 ±0.01 195 1.26 0.11 GD 2025 AmoebaNet-B [58] 97.5 555 -3150 EA 2019 NSGA-Net [29] 97.25 535 3.3 4EA 2019 CARS [59] 97.43 728 3.6 0.4 EA 2020 FairNAS-A [60] 98.2 391 -12 EA 2021 FairNAS-B [60] 98.1 348 -12 EA 2021 FairNAS-C [60] 98.0 324 -12 EA 2021 MPAE-A [61] 97.35 -2.8 0.3 EA 2024 MPAE-B [61] 97.39 -3.2 0.3 EA 2024 MPAE-C [61] 97.51 -3.7 0.3 EA 2024 MPE-NAS [36] 96.53 -6.4 0.78 EA 2024 PEPNAS [62] 97.62 -4.23 0.7 EA 2024 SPNAS [63] 98.20 -6.33 1.4 EA 2025 EmCENAS [64] 97.42 ±0.03 -4.1 0.3 EA 2025 DSGENAS [65] 97.47 -4.8 0.5 EA 2025 MOEA-BUS-S 98.12 ±0.03 281 5.18 1.2 EA -MOEA-BUS-M 98.15 ±0.02 327 6.12 1.2 EA -MOEA-BUS-L 98.25 ±0.03 461 7.37 1.2 EA -MOEA-BUS-XL 98.39 ±0.03 601 6.47 1.2 EA -

the CIFAR-10 dataset. The discovered architecture, MOEA-BUS-S, has a lowest MAdds of 281M among the ENAS methods, and its accuracy of 98.12% exceeds that of most other methods. FairNAS-A [60] achieves the accuracy of 98.2%, comparable to our models, but with significantly higher MAdds of 391M and the search cost of 12 GPU days, highlighting the efficiency of the proposed method. In recent years, graph neural network (GNN)-guided NAS has emerged as a novel approach [64], [65]. Compared to these methods, MOEA-BUS maintains significant advantages. Compared to gradient-based NAS methods, the proposed ap-proach demonstrates significant advantages in accuracy while maintaining similar MAdds to most of them. Although FX-DARTS [9] achieves the lowest MAdds of 195M, its accuracy is significantly inferior to the proposed method. In general, the architectures discovered by MOEA-BUS show lower MAdds compared to other methods with the same accuracy, indicating higher computational efficiency. Besides, compared with other ENAS methods based on multi-population mechanisms, such as MPE-ENAS [36], the proposed method achieves an advan-tage of approximately 2% in classification accuracy. The total search cost is approximately 1.2 GPU days less than most methods. The performance of different metrics performance surpasses several state-of-the-art NAS methods, demonstrating that the proposed algorithm achieves the best balance between accuracy and computational efficiency. 

Results on ImageNet: On the ImageNet dataset, we also provide a set of architectures with different sizes. From Ta-ble III, it can be seen that the proposed algorithm demonstrates significant improvements in both classification accuracy and JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9

TABLE III: Comparison with state-of-the-art image classifiers on the ImageNet dataset. The search cost excludes the supernet training cost.                                                                                                                                                                                                                                                                                                              

> Architecture Top-1 Acc (%) Top-5 Acc (%) MAdds (M) Params (M) Search Cost (GPU Days) Search Method Year
> MobileNetV2 [44] 72.0 91.0 300 3.4 -manual 2018 EfficientNet-B0 [45] 76.3 93.2 390 5.3 -manual 2019 NASNet-A [46] 74.0 91.6 564 -1800 RL 2018 MnasNet [26] 76.13 92.85 391 5.2 -RL 2019 BNAS [47] 74.3 91.5 -3.9 -RL 2021 DBNAS-B [48] 75.0 92.3 385 4.4 0.9 RL 2025 DBNAS-w/o SE [48] 77.6 93.5 386 4.9 0.9 RL 2025 PC-DARTS [49] 75.8 92.7 597 5.3 3.8 GD 2019 P-DARTS [50] 75.6 92.6 557 4.9 0.3 GD 2019
> β-DARTS [66] 76.1 93.0 609 5.5 0.4 GD 2022 NAP [22] 75.5 92.6 574 4.8 4GD 2022 EoiNAS [53] 74.4 91.7 570 5.0 -GD 2022 iDARTS [54] 75.3 92.3 568 5.1 1.9 GD 2023 GENAS [57] 76.1 92.8 -50.26 GD 2024 SWD-NAS [55] 75.5 92.4 -6.3 0.13 GD 2024 DBNAS-A [48] 74.9 92.3 382 3.7 0.6 GD 2025 DBNAS-C [48] 75.6 92.5 428 4.1 0.6 GD 2025 FX-DARTS [9] 76.4 93.4 610 5.1 0.17 GD 2025 NSGANetV2 [30] 77.4 93.5 225 6.1 1EA 2020 CARS [59] 75.2 92.5 591 5.1 0.4 EA 2020 FairNAS [60] 77.5 -392 -12 EA 2021 AutoFormer-Tiny [67] 74.7 92.6 1300 5.7 -EA 2021 AutoFormer-Small [67] 81.7 95.7 5100 22.9 -EA 2021 AutoFormer-Base [67] 82.4 95.7 11000 54 -EA 2021 MixPath [68] 77.2 93.5 378 5.1 10.3 EA 2023 RelativeNAS [69] 75.1 92.3 563 5.1 -EA 2023 MPAE-A [61] 74.1 91.9 -4.2 0.3 EA 2024 MPAE-B [61] 75.1 92.5 -4.8 0.3 EA 2024 MPAE-C [61] 75.7 92.7 -5.2 0.3 EA 2024 PEPNAS [62] 73.75 91.78 -6.71 0.7 EA 2024 T-Razor-Tiny [70] 75.5 92.9 1400 5.9 0.4 EA 2024 T-Razor-Small [70] 82.2 95.9 5100 22.3 0.4 EA 2024 T-Razor-Base [70] 82.3 95.6 11600 53.8 0.4 EA 2024 SPNAS [63] 78.62 94.07 687 6.6 0.37 EA 2025 HENAS [71] 78.69 94.01 580 -0.22 EA 2025 BossNet-S++ [72] 81.4 95.6 3400 --EA 2025 BossNet-M++ [72] 82.0 95.7 5800 --EA 2025 BossNet-L++ [72] 83.2 96.4 10500 --EA 2025 MOEA-BUS-S 77.67 93.71 289 6.17 0.3 EA -MOEA-BUS-M 78.28 94.04 446 6.51 0.3 EA -MOEA-BUS-L 78.71 94.23 461 6.62 0.3 EA -MOEA-BUS-XL 80.03 94.42 610 7.46 0.3 EA -

computational efficiency over existing NAS methods. The best architecture discovered by MOEA-BUS achieves a top-1 accu-racy of 80.03% and a top-5 accuracy of 94.42% with MAdds of 610M. Compared to manually designed architectures like EfficientNet-B0 and B1 [45], our architectures provide higher accuracy with competitive or lower computational costs. When compared to other EA-based methods, some architectures have higher accuracy than ours, but the architectures discovered by the proposed algorithm is smaller in MAdds with the least search cost of 0.3 GPU days. MixPath [68], for instance, achieves a top-1 accuracy of 77.2% with 378M MAdds and a search cost of 10.3 GPU days, while MOEA-BUS-L surpasses this with a top-1 accuracy of 78.71% and a search cost of only 0.3 GPU days. Among all ENAS methods, NAS approaches based on Transformer search space achieve the highest classi-fication accuracy, but result in a significant increase in MAdds. For instance, although BossNet-L++ [72] obtains the highest classification accuracy of 83.2%, its MAdds of 10500M is 

17 .2× that of MOEA-BUS-XL. T-Razor-Base [70] achieves a classification accuracy of 82.3%, but its MAdds of 11000M and parameter count of 54M far exceed the consumption of MOEA-BUS. Compared to AutoFormer-Tiny [67] and T-Razor-Tiny [70] with similar parameter scales, their MAdds are still 2× that of MOEA-BUS-XL, and their classification accuracy is significantly lower than MOEA-BUS-XL. Our architectures also outperform various non-EA-based methods in terms of both accuracy and search efficiency. GENAS [57] achieves a top-1 accuracy of 76.1% with search cost of 0.26 GPU days, whereas our architectures achieve higher accuracy with comparable search costs. In summary, the proposed algorithm consistently delivers high-accuracy architectures with lower computational com-plexity and reduces search cost, outperforming several state-of-the-art NAS methods on three datasets. 

C. Ablation Study of Two Key Mechanisms 

The bi-population mechanism in MOEA-BUS algorithm is designed to enhance the exploration and exploitation capabili-ties during the search process. By allocating different roles to the two populations, the proposed algorithm can explore the search space more comprehensively while simultaneously fo-cusing on high-potential architectures. In the proposed imple-mentation, population 1 focuses on exploring the search space broadly, while population 2 emphasizes exploiting promising JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10 

regions identified by population 1. This division allows for a more balanced search process, combining the strengths of both exploration and exploitation. 100 200 300 400 500 600 700 

> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Architectures selected
> Pareto front

(a) Search result of NSGA-II (initial-ized with random sampling) 100 200 300 400 500 600 700 

> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Architectures selected
> Pareto front

(b) Search result of NSGA-II with multi-population mechanism 100 200 300 400 500 600 700 

> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Architectures selected
> Pareto front

(c) Search result of NSGA-II (initial-ized with uniform sampling) 100 200 300 400 500 600 700 

> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Architectures selected
> Pareto front

(d) Search result of MOEA-BUS 

Fig. 6: Comparison results of population distribution with and without the proposed two mechanisms on ImageNet. To further assess the effectiveness of the proposed algo-rithm, we compare the performance of architectures discovered using the proposed bi-population-based multi-objective algo-rithm with those obtained using the NSGA-II, a well-known multi-objective evolutionary algorithm. In the ablation study, we analyze two key mechanisms employed in the paper: the bi-population and uniform sampling methods. Fig. 6 contains a total of 4 experimental results, and for each experimental result, we present the final result obtained in the objective space. All points in the figure represent the values of the two search objectives finally retain in archive A, and the red line indicates the Pareto front of the final results. Furthermore, we design a metric for evaluating the diversity of architecture distributions based on the architecture entropy proposed by Chu et al. [73], termed architecture distribution entropy: 

Entropy = − X

> i,j

pij log 2

 pij 

, (1) where pij = Hist( a′

> i

, m ′ 

> j

)

P 

> i,j

Hist( a′

> i

, m ′ 

> j

) , (2) where a′ 

> i

and m′ 

> j

are normalized accuracy and MAdds, and 

Hist( a′

> i

, m ′ 

> j

) is the 2D histogram of normalized accuracy and MAdds. We plot the results from Fig. 6 according to search generations, including the proposed architecture distribution entropy and the commonly used hyper-volume (HV) for eval-uating population convergence and distribution diversity. The relevant results are presented in Fig. 7. From Fig. 6a, the architectures searched by NSGA-II are heavily concentrated in the MAdds interval of 200M to 400M. This concentration suggests that the results of NSGA-II are limited, focusing 0 5 10 15 20 25 

> Generation
> 5.4
> 5.6
> 5.8
> 6.0
> 6.2
> 6.4
> Entropy
> NSGA-II (random sampling)
> Multi-pop NSGA-II
> NSGA-II (uniform sampling)
> MOEA-BUS

(a) Result of architecture distribution entropy (Entropy) 0 5 10 15 20 25 

> Generation
> 0.55
> 0.56
> 0.57
> 0.58
> 0.59
> 0.60
> 0.61
> 0.62
> Hyper-volume (HV)
> NSGA-II (random sampling)
> Multi-pop NSGA-II
> NSGA-II (uniform sampling)
> MOEA-BUS

(b) Result of hyper-volume (HV) 

Fig. 7: The architecture distribution entropy (Entropy) and hyper-volume (HV) between MOEA-BUS with and without the proposed two mechanisms on ImageNet. predominantly on a narrow region of the search space. As a result, NSGA-II may miss potentially superior architectures in other regions. The blue line in Fig. 7a illustrates this point, where it can be observed that this benchmark does not have an advantage in diversity during the initial stage, and Fig. 7b re-veals that this benchmark has difficulty converging. When the multi-population mechanism is employed, more architectures with MAdds exceeding 400M are discovered in Fig. 6b, and as shown in Fig. 7b, the two populations can improve search efficiency, enabling more promising architectures to emerge earlier. Uniform sampling is also applied independently to NSGA-II, achieving richer population diversity. As can be ob-served from Fig. 6c, the number of architectures with MAdds less than 200M or greater than 400M increases significantly. The architectures searched by the proposed algorithm are almost uniformly distributed in the objective space, covering a wide range of MAdds and accuracy values. From Fig. 7a and Fig. 7b, it can be observed that this ablation setting exhibits good distributional diversity in the early stages of the search and successfully achieves higher diversity and convergence of the final population. According to Fig. 7a, the proposed method demonstrates the ability to achieve favorable diversity in the early stages of search and improve throughout the search process rapidly, thereby exploring the entire objective space. Based on Fig. 7b, MOEA-BUS can steadily enhance the HV of the entire population, indicating that this method possesses good convergence properties. The population distribution plot of the final results in Fig. 6d also substantiates this point, where it can be observed that the points are more uniformly distributed in the objective space compared to other bench-marks, and the Pareto front is also closer to the dominant region. 

D. Ablation Study and Analysis of Surrogate Model 

The surrogate model is utilized in this paper to rapidly filter 60,000 architectures, thus the prediction accuracy of the surrogate model is crucial for the results of this experiment. To exclude influences from the search process, we design an experiment focused specifically on the surrogate model. The experimental results are presented in Table IV, which shows the Kendall’s tau correlation coefficient (Ktau) between JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11 

TABLE IV: Ablation study on surrogate model prediction performance using four different machine learning models, including the SVM adopted in this paper.                    

> RF SVM MLP Adaboost Regression+Random 0.6257 0.6586 0.5658 0.6748 Regression+Uniform 0.6807 0.7492 0.6276 0.7228 Pairwise+Random 0.6630 0.7052 0.5731 0.7060 Pairwise+Uniform 0.6991 0.7721 0.6524 0.7371

TABLE V: Architecture distribution entropy and Hyper-volume (HV) of different sampling methods for population initialization.          

> Entropy HV Uniform sampling (proposed) 5.8851 0.5717
> Stratified sampling 4.7095 0.5553 Latin Hypercube sampling 4.6774 0.1821 random sampling 5.2927 0.3314

predicted rankings and real rankings for 1000 architectures under different configurations. We collect the execution processes from all our previous experiments on the ImageNet dataset, obtaining historical information for approximately 7,000 architectures in total. We perform sampling among these architectures to simulate the impact of different sampling methods on the surrogate model. We sample 1,300 architectures from these candidates, where 300 architectures are used for training the surrogate model and 1000 architectures are employed to evaluate the performance of the surrogate model. We select four commonly used machine learning models as base models: random forest (RF), support vector machine (SVM), multilayer perceptron (MLP), and AdaBoost. In this paper, the proposed surrogate model employs pairwise prediction methods, therefore in our additional experiments, we compare the performance of pair-wise prediction with regression prediction. Additionally, we investigate the impact of two initialization strategies: random sampling and uniform sampling. According to Table IV, we can intuitively observe the impact of different surrogate patterns (regression prediction and pairwise comparison relationship prediction), initialization methods, and base machine learning models on surrogate per-formance. It can be observed that both uniform sampling and pairwise prediction can stably enhance the prediction perfor-mance of the surrogate model with each base machine learning model. For example, the SVM method used in this paper achieves a 0.0669 Ktau improvement (from 0.7052 to 0.7721) when employing the proposed uniform sampling initialization method combined with pairwise prediction. Comparing the results across different base machine learning models under the Pairwise+Uniform setting, the employed SVM achieves at least a 0.035 Ktau improvement compared to other models (0.7721 vs. 0.7371 for AdaBoost, 0.6991 for RF, and 0.6524 for MLP). Our results demonstrate that the surrogate model employed in the proposed methodology effectively adapts to uniform sampling, enabling accurate performance prediction of candidate architectures. 100 200 300 400 500 600 700                                   

> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Initial architectures
> (a) Distribution of initial architec-tures with uniform sampling 100 200 300 400 500 600 700
> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Initial architectures
> (b) Distribution of initial architec-tures with Stratified sampling 100 200 300 400 500 600 700
> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Initial architectures
> (c) Distribution of initial architec-tures with Latin Hypercube sampling 100 200 300 400 500 600 700
> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Initial architectures
> (d) Distribution of initial architec-tures with random sampling

Fig. 8: Comparison of the distribution of initial architectures using uniform sampling and random sampling. 

E. Ablation Study and Analysis of Uniform Sampling 

The uniform sampling employed in the proposed algorithm is designed to ensure that the initial population is distributed as uniformly as possible in the objective space. By uniformly distributing the initial architectures, we avoid the issue of clustering, where architectures are densely populated in certain regions of the search space while other regions remain unex-plored. This diversity is crucial for effectively exploring the vast and complex search space of neural architectures, leading to a higher likelihood of discovering optimal solutions. To fur-ther assess the effectiveness of the proposed uniform sampling strategy, we compare our results with those obtained using tra-ditional random sampling. To illustrate this, we plot the effect of using uniform sampling and random sampling in Fig. 8. In the figures, blue dots represent the initial architectures. We also calculate the architecture distribution entropy and HV for the initial population, with results presented in Table V. In Fig. 8a, the initial architectures are almost uniformly distributed in the objective space, which can make the newly generated individuals in later generations to be uniformly spread out, promoting comprehensive exploration of the search space. The results in Table V demonstrate that the adopted method can achieve distribution diversity far superior to other methods, and possesses significantly better HV. In contrast, Fig. 8d shows that the initial architectures are concentrated in regions with MAdds between 200M and 400M. This concentration limits the search process and reduces the chances of discovering optimal solutions. There are even no architectures in regions with MAdds larger than 450M, which is possible to lead to no larger architectures being generated in the subsequent evolutionary process, neglecting regions with potential for the higher performance. Therefore, architectures derived from traditional random sampling exhibit lower diversity in the initial population, leading to sub-optimal performance during JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12 

the evolutionary process. The results in Table V show that al-though random sampling can obtain some diversity, it achieves lower HV due to architectures clustering around medium and small sizes, which also makes the subsequent search process challenging. Additionally, we employ two commonly used sampling methods, stratified sampling method and Latin hypercube sampling method. Due to the close interpolation between the upper and lower bounds of each bit in our en-coding scheme, we utilize the encoding approach proposed by Lu et al . for these two sampling methods [74]. Fig. 8b presents the results using the stratified sampling method, revealing that this approach, which samples based on the probability distribution of the encoding region, is not suitable for the specific problem addressed in this paper. Although the initial population obtained by the stratified sampling method achieves a favorable distribution in terms of MAdds, it does not exhibit good distribution characteristics from the perspective of the objective space. The entropy and HV values in Table V also corroborate this observation. Fig. 8c demonstrates the results using the Latin hypercube sampling method, showing that due to the integer encoding nature of this problem and the still relatively close interpolation between upper and lower bounds, the obtained initial architectures exhibit poor distribution in the objective space. The substantially lower HV compared to other methods in Table V illustrates this phenomenon. Overall, the proposed uniform sampling method ensures a more balanced exploration, resulting in higher-quality architectures with bet-ter trade-offs between accuracy and computational complexity. 

F. Ablation Study and Analysis of Bi-population Mechanism 

In this paper, the bi-population mechanism is designed to enhance population diversity and improve search efficiency. To investigate the impact of individual exchange rules in the bi-population mechanism, we design a comparative experi-ment. The results are shown in Figure 9. In this compar-ative experiment, elite individuals from two populations are exchanged with each other. This means that, unlike MOEA-BUS method, elite individuals from population 2 are allowed to enter population 1. We plot the results of this comparative experiment. As can be observed from Figure 9a, in regions where MAdds is less than 250M and greater than 450M, the comparative experiment obtained fewer individuals than the proposed method. Additionally, the Pareto front of the comparative experiment is also significantly worse than the proposed method, achieving architectures with higher error rates under the same MAdds. We believe that medium-sized architectures in population 1 are more inclined to achieve non-dominated frontier positions, thereby preventing population 1 from effectively focusing on extreme architectures. Addition-ally, we calculate the architecture distribution entropy and HV for both approaches. The final population obtained by MOEA-BUS achieves architecture entropy of 6.32 and an HV of 0.62. When the two populations exchange with each other, the final population exhibits architecture entropy of 6.07 and an HV of 0.60. These metrics also demonstrate that population diversity decreases when the two populations exchange with each other. To further analyze the bi-population mechanism of MOEA-BUS, we perform a series of experiments by varying the initial 100 200 300 400 500 600 700           

> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Architectures selected
> Pareto front
> (a) Population 1 and population 2 share elite individuals with each other 100 200 300 400 500 600 700
> MAdds (M)
> 18
> 20
> 22
> 24
> 26
> Error (%)
> Architectures selected
> Pareto front
> (b) Population 1 shares elite individ-uals with population 2

Fig. 9: Comparison results of population distribution with different exchange rules. population size for the sub-search process and present the results in Table VI. First, we maintain the total number of 100 individuals unchanged and altered the ratio between population 1 and population 2, denoted as P1 and P2. In the proposed method, we employ parameter values of P1 = 25 and P2 = 75 ,denoted as the parameter pair (25, 75). We design two other sets of experiments, including (50, 50) and (75, 25). According to the results in Table VI, we found that individuals with medium-sized architectures achieve lower accuracy compared to the proposed configuration. Moreover, the results under the (75, 25) setting are dominated by those of (25, 75) and (50, 50). This indicates that maintaining a larger number of individuals in population 2 is beneficial for medium-sized ar-chitectures. This inference aligns with the quantity distribution in the search space. As illustrated in Figure 3, the number of medium-sized architectures is evidently far greater than that of extreme architectures, thus necessitating the maintenance of a larger population 2. Furthermore, we experiment with different sizes for P1 and P2 with the same ratio but a larger total number of individuals. Three sets of parameter values for P1 and P2 are employed, and the sizes are varied as follows: (15, 45), (35, 105), and (45, 135) marked with “ ∗”. Increasing the population size generally improves the diversity of the architectures and leads to better performance in terms of accuracy. However, larger populations also increase com-putational costs. Table VI also summarizes the results of our ablation study on different population sizes. A population size of (45, 135) gives slightly better results than (25, 75), but the performance improvement is very limited when considering the higher computational cost. When the population size is (15, 45), although this configuration cost the least computational search time, the architectures with lower accuracy are obtained compared to larger populations. The limited diversity in the smaller population size restricts the exploration in the search space, resulting in sub-optimal architectures. We find that the population size of (25, 75) strikes an optimal balance between performance and computational efficiency. To obtain search results with the same number of actual evaluations, we modify the number of search generations for these three experimental settings. Specifically, for (15, 45), we increase generations by four; for (35, 105), we decrease generations by four; for (45, 135), we decrease generations by eight. It can be observed that although increasing the initial population size can reduce the JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13 

overall search time, it yields worse architectures at medium scales. While reducing the initial population can obtain more search generations, the training data for the surrogate model is acquired at a slower rate, resulting in increased search duration without achieving better results. The ablation experiments in Table VI demonstrate the rationality of the employed setting (25, 75). TABLE VI: Comparison results on ImageNet with different sizes/ratios of the two initial populations. 

Size (P1, P2)Top-1 Acc (%) MAdds (M) Search Cost (GPU Days) Number of real evaluations 

(25, 75) 78.71 461 0.30 350 (50, 50) 78.44 447 0.30 350 (75, 25) 78.38 464 0.29 350 (15, 45) 78.35 501 0.40 350 (15, 45) ∗ 78.20 489 0.25 310 (35, 105) 78.42 445 0.28 350 (35, 105) ∗ 78.73 475 0.37 390 (45, 135) 78.15 438 0.25 350 (45, 135) ∗ 78.78 503 0.44 430 

V. C ONCLUSION AND FUTURE WORK 

In this paper, we present MOEA-BUS, a multi-objective evolutionary algorithm based on bi-population with uniform sampling for neural architecture search. The proposed method aims to address the challenges of generating high-performance neural architectures while balancing computational complex-ity. By integrating a uniform sampling strategy for initializing the population and a bi-population mechanism for evolu-tionary search, MOEA-BUS effectively explores the search space, ensures diversity and optimizing multiple objectives. We validate the effectiveness of MOEA-BUS on three image classification datasets: CIFAR-10, CIFAR-100, and ImageNet. Our experimental results demonstrate that MOEA-BUS out-performs several state-of-the-art NAS methods in terms of accuracy and computational efficiency. While this study utilized the MobileNetV3 backbone to align with common mobile deployment scenarios and ensure fair comparison with recent benchmarks, we acknowledge that evaluating a single search space is a limitation. However, the core contributions of this work—specifically the uniform sampling and bi-population mechanisms—are designed to be architecture-agnostic. Future work will aim to demonstrate this broad applicabil-ity by extending the framework to other backbone architec-tures, such as ResNet, EfficientNet, and Vision Transformers. Additionally, we plan to adapt the proposed framework to other domains beyond image classification, including semantic segmentation and object detection, by designing correspond-ing search operators and surrogate models. These efforts, combined with comprehensive cross-task benchmarks, will further validate the generality and robustness of the proposed strategies. REFERENCES [1] E. Picco, A. Lupo, and S. Massar, “Deep photonic reservoir computer for speech recognition,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 4, pp. 7606–7614, 2025. [2] D. Yang, S. Liu, R. Huang, C. Weng, and H. Meng, “InstructTTS: Modelling expressive TTS in discrete latent space with natural language style prompt,” IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 32, pp. 2913–2925, 2024. [3] Z. Gao, Y. Mu, C. Chen, J. Duan, P. Luo, Y. Lu, and S. Eben Li, “En-hance sample efficiency and robustness of end-to-end urban autonomous driving via semantic masked world model,” IEEE Transactions on Intelligent Transportation Systems , vol. 25, no. 10, pp. 13067–13079, 2024. [4] X. Guo, R. Cui, and W. Yan, “Pursuit-evasion games of marine surface vessels using neural network-based control,” IEEE Transactions on Systems, Man, and Cybernetics: Systems , vol. 55, no. 1, pp. 18–27, 2025. [5] Y. Sun, B. Xue, M. Zhang, G. G. Yen, and J. Lv, “Automatically designing CNN architectures using the genetic algorithm for image clas-sification,” IEEE Transactions on Cybernetics , vol. 50, no. 9, pp. 3840– 3854, 2020. [6] J. Lee and B. Ham, “AZ-NAS: Assembling zero-cost proxies for network architecture search,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 5893–5903, 2024. [7] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement learning,” in International Conference on Learning Representations ,2017. [8] Z. Wang, Y. Xue, and F. Neri, “Multi-population co-evolutionary generative adversarial network architecture search for zero-shot learn-ing,” IEEE Transactions on Evolutionary Computation , 2026. DOI: 10.1109/TEVC.2026.3650926. [9] X. Rao, B. Zhao, D. Liu, and C. Alippi, “FX-DARTS: Designing topology-unconstrained architectures with differentiable architecture search and entropy-based super-network shrinking,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 10, pp. 19356– 19369, 2025. [10] J. Liang, G. Liu, Y. Bi, M. Yu, M. Liu, and Y. Jin, “Evolutionary neural architecture search for remote sensing image classification,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 10, pp. 17886–17900, 2025. [11] Q. Tao, R. Cai, Z. Lin, and Y. Tang, “Automatic design of deep graph neural networks with decoupled mode,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 5, pp. 7918–7930, 2025. [12] X. Yan, H. Huang, Y. Jin, Z. Wang, and Z. Hao, “Neural architecture search based on bipartite graphs for text classification,” IEEE Trans-actions on Neural Networks and Learning Systems , vol. 36, no. 6, pp. 10749–10763, 2025. [13] M. Gambella, J. Pomponi, S. Scardapane, and M. Roveri, “NACHOS: Neural architecture search for hardware-constrained early-exit neural networks,” IEEE Transactions on Neural Networks and Learning Sys-tems , vol. 36, no. 10, pp. 19342–19355, 2025. [14] S. Yang, H. Ma, Y. Bi, Y. Tian, L. Zhang, Y. Jin, and X. Zhang, “An evolutionary multi-objective neural architecture search approach to ad-vancing cognitive diagnosis in intelligent education,” IEEE Transactions on Evolutionary Computation , vol. 29, no. 6, pp. 2431–2445, 2025. [15] Z. Ding, Y. Chen, N. Li, D. Zhao, and C. L. P. Chen, “Stacked BNAS: Rethinking broad convolutional neural network for neural architecture search,” IEEE Transactions on Systems, Man, and Cybernetics: Systems ,vol. 53, no. 9, pp. 5679–5690, 2023. [16] Z. Ding, Y. Chen, N. Li, and D. Zhao, “BNAS-v2: Memory-efficient and performance-collapse-prevented broad neural architecture search,” 

IEEE Transactions on Systems, Man, and Cybernetics: Systems , vol. 52, no. 10, pp. 6259–6272, 2022. [17] B. Lyu, Y. Yang, Y. Cao, P. Wang, J. Zhu, J. Chang, and S. Wen, “Ef-ficient multi-objective neural architecture search framework via policy gradient algorithm,” Information Sciences , vol. 661, p. 120186, 2024. [18] C. Garcia-Garcia, H. J. Escalante, and A. Morales-Reyes, “CGP-NAS: Real-based solutions encoding for multi-objective evolutionary neural architecture search,” in Proceedings of the Genetic and Evolutionary Computation Conference Companion , pp. 643–646, 2022. [19] Y. Li, D. Li, W. Gong, and Q. Gu, “Multiobjective multitask optimiza-tion via diversity- and convergence-oriented knowledge transfer,” IEEE Transactions on Systems, Man, and Cybernetics: Systems , vol. 55, no. 3, pp. 2367–2379, 2025. [20] J. Dong, B. Hou, L. Feng, H. Tang, K. C. Tan, and Y.-S. Ong, “A cell-based fast memetic algorithm for automated convolutional neural archi-tecture design,” IEEE Transactions on Neural Networks and Learning Systems , vol. 34, no. 11, pp. 9040–9053, 2023. [21] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean, “Efficient neural architecture search via parameters sharing,” in Proceedings of the 35th International Conference on Machine Learning , pp. 4095–4104, 2018. JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14 

[22] Y. Ding, Y. Wu, C. Huang, S. Tang, F. Wu, Y. Yang, W. Zhu, and Y. Zhuang, “NAP: Neural architecture search with pruning,” Neurocom-puting , vol. 477, pp. 85–95, 2022. [23] H. Liu, K. Simonyan, and Y. Yang, “DARTS: Differentiable architecture search,” in International Conference on Learning Representations , 2018. [24] Z. Cai, L. Chen, T. Ling, and H.-L. Liu, “STO-DARTS: Stochastic bilevel optimization for differentiable neural architecture search,” IEEE Transactions on Emerging Topics in Computational Intelligence , vol. 8, no. 3, pp. 2324–2335, 2024. [25] Y. Lin, Y. Endo, J. Lee, and S. Kamijo, “Bandit-NAS: Bandit sampling and training method for neural architecture search,” Neurocomputing ,vol. 597, p. 127684, 2024. [26] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le, “MnasNet: Platform-aware neural architecture search for mobile,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 2820–2828, 2019. [27] T. Yamasaki, Z. Wang, T. Luo, N. Chen, and B. Wang, “RBFleX-NAS: Training-free neural architecture search using radial basis function kernel and hyperparameter detection,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 6, pp. 10057–10071, 2025. [28] Z. Lu, I. Whalen, Y. Dhebar, K. Deb, E. D. Goodman, W. Banzhaf, and V. N. Boddeti, “Multiobjective evolutionary design of deep convo-lutional neural networks for image classification,” IEEE Transactions on Evolutionary Computation , vol. 25, no. 2, pp. 277–291, 2021. [29] Z. Lu, I. Whalen, V. Boddeti, Y. Dhebar, K. Deb, E. Goodman, and W. Banzhaf, “NSGA-Net: Neural architecture search using multi-objective genetic algorithm,” in Proceedings of the Genetic and Evolu-tionary Computation Conference , pp. 419–427, 2019. [30] Z. Lu, K. Deb, E. Goodman, W. Banzhaf, and V. N. Boddeti, “NS-GANetV2: Evolutionary multi-objective surrogate-assisted neural archi-tecture search,” in Proceedings of the European Conference on Computer Vision , pp. 35–51, 2020. [31] Z. Lu, R. Cheng, S. Huang, H. Zhang, C. Qiu, and F. Yang, “Surrogate-assisted multiobjective neural architecture search for real-time semantic segmentation,” IEEE Transactions on Artificial Intelligence , vol. 4, no. 6, pp. 1602–1615, 2023. [32] Y. Xue, C. Chen, and A. Słowik, “Neural architecture search based on a multi-objective evolutionary algorithm with probability stack,” IEEE Transactions on Evolutionary Computation , vol. 27, no. 4, pp. 778–786, 2023. [33] B. Wang, Y. Sun, B. Xue, and M. Zhang, “Evolving deep neural networks by multi-objective particle swarm optimization for image clas-sification,” in Proceedings of the Genetic and Evolutionary Computation Conference , pp. 490–498, 2019. [34] L. Tong and B. Du, “Neural architecture search via reference point based multi-objective evolutionary algorithm,” Pattern Recognition , vol. 132, p. 108962, 2022. [35] Y. Xue, P. Jiang, C. Zhu, M. Zhou, M. Wahib, and M. Gabbouj, “A pairwise comparison relation-assisted multiobjective evolutionary neural architecture search method with multipopulation mechanism,” IEEE Transactions on Systems, Man, and Cybernetics: Systems , 2026. DOI: 10.1109/TSMC.2025.3647894. [36] C. Song, Y. Ma, Y. Xu, and H. Chen, “Multi-population evolutionary neural architecture search with stacked generalization,” Neurocomputing ,vol. 587, p. 127664, 2024. [37] A. Saad, S. A. Khan, and A. Mahmood, “A multi-objective evolutionary artificial bee colony algorithm for optimizing network topology design,” 

Swarm and Evolutionary Computation , vol. 38, pp. 187–201, 2018. [38] S. Wang, Y. Li, and H. Yang, “Self-adaptive mutation differential evolution algorithm based on particle swarm optimization,” Applied Soft Computing , vol. 81, p. 105496, 2019. [39] L. Tang, Y. Dong, and J. Liu, “Differential evolution with an individual-dependent mechanism,” IEEE Transactions on Evolutionary Computa-tion , vol. 19, no. 4, pp. 560–574, 2015. [40] B. Kazimipour, X. Li, and A. K. Qin, “A review of population initial-ization techniques for evolutionary algorithms,” in Proceedings of the IEEE Congress on Evolutionary Computation (CEC) , pp. 2585–2592, 2014. [41] A. Howard, M. Sandler, B. Chen, W. Wang, L.-C. Chen, M. Tan, G. Chu, V. Vasudevan, Y. Zhu, R. Pang, H. Adam, and Q. Le, “Searching for Mo-bileNetV3,” in Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 1314–1324, 2019. [42] R. Zhang, Y. Sun, and M. Zhang, “GPU-based genetic programming for faster feature extraction in binary image classification,” IEEE Trans-actions on Evolutionary Computation , vol. 28, no. 6, pp. 1590–1604, 2024. [43] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-For-All: Train one network and specialize it for efficient deployment,” in International Conference on Learning Representations , 2020. [44] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mo-bileNetV2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,pp. 4510–4520, 2018. [45] M. Tan and Q. Le, “EfficientNet: Rethinking model scaling for con-volutional neural networks,” in Proceedings of the 36th International Conference on Machine Learning , pp. 6105–6114, 2019. [46] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable architectures for scalable image recognition,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,pp. 8697–8710, 2018. [47] Z. Ding, Y. Chen, N. Li, D. Zhao, Z. Sun, and C. L. P. Chen, “BNAS: Efficient neural architecture search using broad scalable architecture,” 

IEEE Transactions on Neural Networks and Learning Systems , vol. 33, no. 9, pp. 5004–5018, 2022. [48] A. Yang, Y. Liu, C. Li, and Q. Ren, “Deeply supervised block-wise neural architecture search,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 2, pp. 2451–2464, 2025. [49] Y. Xu, L. Xie, X. Zhang, X. Chen, G.-J. Qi, Q. Tian, and H. Xiong, “PC-DARTS: Partial channel connections for memory-efficient architecture search,” in International Conference on Learning Representations , 2019. [50] X. Chen, L. Xie, J. Wu, and Q. Tian, “Progressive differentiable archi-tecture search: Bridging the depth gap between search and evaluation,” in Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 1294–1303, 2019. [51] X. Chu, T. Zhou, B. Zhang, and J. Li, “Fair DARTS: Eliminating unfair advantages in differentiable architecture search,” in Proceedings of the European Conference on Computer Vision , pp. 465–480, 2020. [52] X. Chu and B. Zhang, “Noisy differentiable architecture search,” in 

British Machine Vision Conference , p. 217, 2021. [53] Y. Zhou, X. Xie, and S.-Y. Kung, “Exploiting operation importance for differentiable neural architecture search,” IEEE Transactions on Neural Networks and Learning Systems , vol. 33, no. 11, pp. 6235–6248, 2022. [54] H. Wang, R. Yang, D. Huang, and Y. Wang, “iDARTS: Improving DARTS by node normalization and decorrelation discretization,” IEEE Transactions on Neural Networks and Learning Systems , vol. 34, no. 4, pp. 1945–1957, 2023. [55] Y. Xue, X. Han, and Z. Wang, “Self-adaptive weight based on dual-attention for differentiable neural architecture search,” IEEE Transac-tions on Industrial Informatics , vol. 20, no. 4, pp. 6394–6403, 2024. [56] Y. Xue, C. Lu, F. Neri, and J. Qin, “Improved differentiable architecture search with multi-Stage progressive partial channel connections,” IEEE Transactions on Emerging Topics in Computational Intelligence , vol. 8, no. 1, pp. 32–43, 2024. [57] Y. Xue, X. Han, F. Neri, J. Qin, and D. Pelusi, “A gradient-guided evolutionary neural architecture search,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 3, pp. 4345–4357, 2025. [58] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, “Regularized evolution for image classifier architecture search,” in Proceedings of the AAAI Conference on Artificial Intelligence , vol. 33, pp. 4780–4789, 2019. [59] Z. Yang, Y. Wang, X. Chen, B. Shi, C. Xu, C. Xu, Q. Tian, and C. Xu, “CARS: Continuous evolution for efficient neural architecture search,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 1829–1838, 2020. [60] X. Chu, B. Zhang, and R. Xu, “FairNAS: Rethinking evaluation fairness of weight sharing neural architecture search,” in Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 12239– 12248, 2021. [61] J. Zou, H. Chu, Y. Xia, J. Xu, Y. Liu, and Z. Hou, “Multiple population alternate evolution neural architecture search,” in 2025 International Joint Conference on Neural Networks (IJCNN) , pp. 1–9, 2025. [62] Y. Xue, J. Zha, D. Pelusi, P. Chen, T. Luo, L. Zhen, Y. Wang, and M. Wahib, “Neural architecture search with progressive evaluation and sub-population preservation,” IEEE Transactions on Evolutionary Computation , vol. 29, no. 5, pp. 1678–1691, 2025. [63] P. Jiang, Y. Xue, and F. Neri, “Score predictor-assisted evolutionary neural architecture search,” IEEE Transactions on Emerging Topics in Computational Intelligence , 2025. DOI: 10.1109/TETCI.2025.3526179. [64] X. Zhang, Y. Xue, and F. Neri, “Embedding comparator for evolutionary neural architecture search via contrastive learning,” in 2025 IEEE Congress on Evolutionary Computation (CEC) , pp. 1–8, 2025. [65] Y. Xue, X. Zhang, F. Neri, B. Xue, and M. Zhang, “Graph neural network-based surrogate model for evolutionary neural architecture JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15 

search,” IEEE Transactions on Systems, Man, and Cybernetics: Systems ,vol. 55, no. 12, pp. 9631–9644, 2025. [66] P. Ye, B. Li, Y. Li, T. Chen, J. Fan, and W. Ouyang, “ β-DARTS: Beta-decay regularization for differentiable architecture search,” in Pro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 10864–10873, 2022. [67] M. Chen, H. Peng, J. Fu, and H. Ling, “AutoFormer: Searching transformers for visual recognition,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 12250–12260, 2021. [68] X. Chu, S. Lu, X. Li, and B. Zhang, “MixPath: A unified approach for one-shot neural architecture search,” in Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5972–5981, 2023. [69] H. Tan, R. Cheng, S. Huang, C. He, C. Qiu, F. Yang, and P. Luo, “Rel-ativeNAS: Relative neural architecture search via slow-fast learning,” 

IEEE Transactions on Neural Networks and Learning Systems , vol. 34, no. 1, pp. 475–489, 2023. [70] Q. Zhou, K. Sheng, X. Zheng, K. Li, Y. Tian, J. Chen, and R. Ji, “Training-free transformer architecture search with zero-cost proxy guided evolution,” IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 46, no. 10, pp. 6525–6541, 2024. [71] P. Jiang, Y. Xue, and F. Neri, “Homogeneous architecture augmentation and confidence prediction for evolutionary neural architecture search,” in 2025 IEEE Congress on Evolutionary Computation (CEC) , pp. 1–8, 2025. [72] C. Li, S. Lin, T. Tang, G. Wang, M. Li, X. Liang, and X. Chang, “Boss-NAS family: Block-wisely self-supervised neural architecture search,” 

IEEE Transactions on Pattern Analysis and Machine Intelligence ,vol. 47, no. 5, pp. 3500–3514, 2025. [73] J. Chu, X. Yu, S. Yang, J. Qiu, and Q. Wang, “Architecture entropy sampling-based evolutionary neural architecture search and its applica-tion in osteoporosis diagnosis,” Complex & Intelligent Systems , vol. 9, no. 1, pp. 213–231, 2023. [74] Z. Lu, R. Cheng, Y. Jin, K. C. Tan, and K. Deb, “Neural architecture search as multiobjective optimization benchmarks: Problem formulation and performance assessment,” IEEE Transactions on Evolutionary Com-putation , vol. 28, no. 2, pp. 323–337, 2024. 

Yu Xue (Senior Member, IEEE) received the Ph.D. degree from the School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China, in 2013. He was a Visiting Scholar with the School of Engineering and Computer Science, Victoria University of Welling-ton, Wellington, New Zealand, from August 2016 to August 2017. He was a Research Scholar with the Department of Computer Science and Engineer-ing, Michigan State University, East Lansing, MI, USA, from October 2017 to November 2018. He is currently a Professor with the School of Software, Nanjing University of Information Science and Technology, Nanjing. His research interests include deep learning, evolutionary computation, machine learning, computer vision, and feature map selection. 

Pengcheng Jiang (Graduate Student Member, IEEE) received the B.E. degree from Nanjing Uni-versity of Information Science and Technology, China, in 2020. He is currently pursuing the Ph.D. degree with the School of Software in Nanjing University of Information Science and Technology, China. His current research interests include feature selection, evolutionary computation, neural architec-ture search, and model compression. 

Chenchen Zhu received the B.E. degree from Nan-jing University of Information Science and Tech-nology, China, in 2022, where she is currently pursuing a master’s degree. Her research interests include deep learning, multi-objective optimization and neural architecture search. 

Yong Zhang (Senior Member, IEEE) received the Ph.D. degree in control theory and control en-gineering from China University of Mining and Technology, Xuzhou, China, in 2009. He is cur-rently a Professor at the School of Information and Control Engineering, China University of Mining and Technology. His research interests cover swarm intelligence and machine learning. 

Ran Cheng (Senior Member, IEEE) received the B.Sc. degree from the Northeastern University, Shenyang, China, in 2010, and the Ph.D. degree from the University of Surrey, Guildford, U.K., in 2016. He is currently an Associate Professor with the Department of Data Science and Artifi-cial Intelligence, and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China. He is a recipient of the IEEE Trans-actions on Evolutionary Computation Outstanding Paper Award (2018 and 2021), the IEEE Compu-tational Intelligence Society Outstanding Ph.D. Dissertation Award (2019), the IEEE Computational Intelligence Magazine Outstanding Paper Award (2020), and the IEEE Computational Intelligence Society Early Career Award (2025). He is the Founding Chair of the IEEE Computational Intelligence Society Shenzhen Chapter. He is an Associate Editor of IEEE Transactions on Evolutionary Computation, IEEE Transactions on Artificial Intelligence, IEEE Transactions on Emerging Topics in Computational Intelligence, and IEEE Transactions on Cognitive and Developmental Systems. 

Kaizhou Gao (Senior Member, IEEE) received the B.Sc. degree from Liaocheng University, Liaocheng, China, in 2005, the master’s degree from Yangzhou University, Yangzhou, China, in 2008, and the Ph.D. degree from Nanyang Technological Univer-sity (NTU), Singapore, in 2016. He is currently an Associate Professor with the Macau Institute of Systems Engineering, Macau University of Science and Technology. He has published over 100 refer-eed papers. His research interests include intelligent computation, optimization, scheduling, and intelli-gent transportation. He is an Associate Editor of IEEE Transactions on Intelligent Transportation Systems, Swarm and Evolutionary Computation, and Expert Systems with Applications. 

Dunwei Gong (Senior Member, IEEE) is a professor and the Dean of the School of Microelectronics at Qingdao University of Science and Technology. He was selected as a Shandong Taishan Scholar Distin-guished Expert in 2025, and a Clarivate Highly Cited Researcher in 2022 and 2023. His main research interests include intelligent optimization theory and applications. He investigated seven National Nat-ural Science Foundation of China (including one key project) and one National Key R&D Program project, received five Prizes for Natural Science, and published 75 papers in CAS Zone I journals, accumulating 10824 Web of Science citations and an H-index of 56. JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 1

# Supplementary Material 

# A Multi-objective Evolutionary Algorithm Based on Bi-population with Uniform Sampling for Neural Architecture Search 

Yu Xue, Senior Member, IEEE , Pengcheng Jiang, Graduate Student Member, IEEE ,Chenchen Zhu, Yong Zhang, Senior Member, IEEE , Ran Cheng, Senior Member, IEEE ,Kaizhou Gao, Senior Member, IEEE , Dunwei Gong, Senior Member, IEEE 

Abstract —This is the supplementary material for the paper titled “A Multi-objective Evolutionary Algorithm Based on Bi-population with Uniform Sampling for Neural Architecture Search”. Section I reports the comparison search results on the CIFAR-100 dataset, including accuracy, the number of parame-ters, MAdds and search cost. Section II reports an ablation study on surrogate model performance, using visualization methods to present the effects of different surrogate modes (regression prediction and pairwise comparison relationship prediction), initialization methods, and base machine learning models on sur-rogate performance. Section III reports the surrogate time costs for different base machine learning models using both regression prediction and pairwise comparison relationship prediction. 

Index Terms —Evolutionary algorithm, neural architecture search, multi-objective optimization, multi-population mecha-nism, surrogate model. 

I. S EARCH RESULTS ON CIFAR-100 CIFAR-100 is a standard dataset widely used for neural architecture search (NAS) comparison, along with CIFAR-10 and ImageNet. Similar to CIFAR-10, CIFAR-100 also contains 60,000 images with a size of 32 × 32 pixels. The difference is that CIFAR-100 has 100 categories, thereby increasing the difficulty of classification. Similar to the experimental setting on CIFAR-10, we also conduct experiments on CIFAR-100. Table I presents the 

This work was supported by the National Natural Science Foundation of China (NO. 62376127, NO. 61876089, NO. 61876185), the Guangdong Basic and Applied Basic Research Foundation (No. 2024B1515020019), and the Natural Science Foundation of Shandong Province (NO. ZR2023ZD06). 

(Corresponding author: Yu Xue.) 

Yu Xue, Pengcheng Jiang and Chenchen Zhu are with the School of Software, Nanjing University of Information Science and Technology, Nanjing 210044, China (e-mails: xueyu@nuist.edu.cn; pcjiang@nuist.edu.cn; 202212490283@nuist.edu.cn). Yong Zhang is with the School of Information and Control Engineering, China University of Mining and Technology, Xuzhou 221008, China (e-mail: yongzh401@cumt.edu.cn). Ran Cheng is with the Department of Data Science and Artificial Intel-ligence, and the Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China, and also with The Hong Kong Poly-technic University Shenzhen Research Institute, Shenzhen, China (e-mail: ranchengcn@gmail.com). Kaizhou Gao is with the Macau Institute of Systems Engineering, Macau University of Science and Technology, Taipa 999078, Macao SAR, China (e-mail: kzgao@must.edu.mo). Dunwei Gong is with the College of Automation and Electronic Engi-neering, Qingdao University of Science and Technology, Qingdao 266061, Shandong, China (e-mail: dwgong@qust.edu.cn). 

experimental results of MOEA-BUS on CIFAR-100 and com-pares them with other NAS methods. The results show that the MOEA-BUS method performs well in balancing accuracy and MAdds, and has a significant advantage in search time compared to some other methods. In terms of accuracy, MOEA-BUS shows excellent performance. The accuracy of different sized MOEA-BUS models (MOEA-BUS-S/M/L/XL) all exceed 86%, with MOEA-BUS-XL achieving the highest accuracy of 88.01%. This surpasses most existing NAS meth-ods, especially the multi-objective ENAS and multi-population ENAS [11], [15], [16], and is significantly better than GD-based algorithms like the DARTS series, which usually have lower accuracies of around 83% to 84%. Regarding model complexity, the MAdds values of the architectures found by our method are relatively low. MOEA-BUS-S has a MAdds of 198M, while the largest MOEA-BUS-XL is 657M. These complexity values are much lower than those of NASNet-A [3] series models, such as NASNet-A Large, which has a MAdds of up to 12031M. Although there are certain differences in MAdds values for MOEA-BUS, it maintains a relatively mod-erate computational complexity while ensuring high accuracy, making it highly scalable and computationally efficient in practical applications. In terms of search time, the MOEA-BUS method also has a significant advantage. It takes only 1.2 GPU days to search, while other methods like FairNAS series need 12 GPU days. Compared to FX-DARTS [10], the obtained MOEA-BUS-S can achieve an approximately 8.7% classification accuracy advantage, while the MAdds remains nearly consistent with FX-DARTS. Compared to SPNAS [18], which also uses the MobileNetV3 search space, MOEA-BUS achieves a 0.75% advantage in classification accuracy, while the obtained MOEA-BUS-XL has a smaller number of parameters. II. A BLATION STUDY OF SURROGATE MODEL 

The surrogate model is utilized in this paper to rapidly filter 60,000 architectures, thus its prediction accuracy is crucial for the results of this experiment. To exclude influences from the search process, we design an experiment focused specifically on the surrogate model. The experimental results are presented in Fig. 1, where each subplot has the horizontal axis repre-JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 2

TABLE I: Comparison on the CIFAR-100 dataset. This table compares the classification accuracy, computational complexity (MAdds), and search cost with other state-of-the-art NAS methods on the CIFAR-100 dataset.                                                                                                                                                                                    

> Architecture Accuracy (%) MAdds (M) Params (M) Search Cost (GPU Days) Search Method Year
> MobileNetV2 [1] 80.8 300 3.4 -manual 2018 EfficientNet-B0 [2] 88.1 400 4.0 -manual 2019 NASNet-A Large [3] 86.7 12031 -1800 RL 2018 NASNet-A mobile [3] 83.9 600 -1800 RL 2018 DBNAS-B [4] 84.54 -3.3 -RL 2025 MixNet-M [5] 86.11 200 2.1 -GD 2019 PC-DARTS [6] 83.1 -3.6 0.3 GD 2019 EoiNAS [7] 82.7 -3.4 0.6 GD 2022 GENAS [8] 83.14 504 3.53 0.26 GD 2024 SWD-NAS [9] 83.87 -3.56 0.13 GD 2024 DBNAS-A [4] 85.18 -2.5 -GD 2025 DBNAS-C [4] 84.74 -3.0 -GD 2025 FX-DARTS [10] 77.93 ±0.08 195 1.26 0.11 GD 2025 NSGA-Net [11] 80.17 -11.6 8EA 2019 MUXNet-M [12] 86.11 200 2.1 11 EA 2020 FairNAS-A [13] 87.3 391 -12 EA 2021 FairNAS-B [13] 87 348 -12 EA 2021 FairNAS-C [13] 86.7 324 -12 EA 2021 ZenNet [14] 84.4 487 0.5 EA 2021 MPAE-A [15] 82.74 -4.2 0.3 EA 2024 MPAE-B [15] 83.45 -4.8 0.3 EA 2024 MPAE-C [15] 84.12 -5.2 0.3 EA 2024 MPE-NAS [16] 80.41 -6.6 0.81 EA 2024 PEPNAS [17] 83.95 -4.34 0.85 EA 2024 SPNAS [18] 87.26 -6.7 1.6 EA 2025 MOEA-BUS-S 86.71 ±0.03 198 4.19 1.2 EA -MOEA-BUS-M 87.20 ±0.03 273 4.77 1.2 EA -MOEA-BUS-L 87.53 ±0.04 458 6.24 1.2 EA -MOEA-BUS-XL 88.01 ±0.03 657 6.30 1.2 EA -

TABLE II: Time cost of different machine learning model as the surrogate with regression/pairwise surrogate mode.                            

> Surrogate mode Machine learning model Ktau Training time (seconds) Prediction time (seconds) Regression Random forest 0.6807 0.33 0.02 Support vector machine 0.7492 0.003 0.009 Multilayer perceptron 0.6276 34.16 0.02 AdaBoost 0.7228 0.033 0.003 Pairwise Random forest 0.6991 7.78 4.14 Support vector machine 0.7721 48.33 166.39 Multilayer perceptron 0.6524 657 2.43 AdaBoost 0.7371 4.93 0.97

senting the real ranking of 1000 architectures and the verti-cal axis representing the predicted ranking. The dots closer to the diagonal line indicate better prediction performance. We collect the execution processes from all our previous experiments on the ImageNet dataset, obtaining historical information for approximately 7,000 architectures in total. We perform sampling among these architectures to simulate the impact of different sampling methods on the surrogate model. We sample 1,300 architectures from these candidates, where 300 architectures are used for training the surrogate model and 1000 architectures are employed to evaluate its performance. The results using random sampling are plotted in Figures 1a - 1h, while the results using uniform sampling are depicted in Figures 1i - 1p. We select four commonly used machine learning models as base models, including random forest (RF), support vector machine (SVM), multilayer perceptron (MLP), and AdaBoost. In this paper, proposed sur-rogate model employs pairwise prediction methods, therefore in our additional experiments, we compare the performance of pairwise prediction with regression prediction. In each subplot title, “-R” indicates regression prediction and “-C” indicates pairwise prediction. Additionally, in the parentheses of each subplot title, “ R” represents random sampling, “ U” represents uniform sampling, and the floating-point numbers represent the Kendall’s tau correlation coefficient (Ktau) calculated between predicted rankings and real rankings. According to Fig. 1, we can intuitively observe the impact of different surrogate patterns (regression prediction and pairwise comparison rela-tionship prediction), initialization methods, and base machine learning models on surrogate performance. It can also be observed that both uniform sampling and pairwise prediction can stably enhance the prediction performance of the surrogate model with each base machine learning model. For example, according to Figures 1f and 1n, the SVM method used in this paper can achieve a 0.07 Ktau improvement when employing the proposed uniform sampling initialization method. Accord-ing to Figures 1m - 1p, using different base machine learning models also affects surrogate performance, and the employed SVM can achieve at least a 0.035 Ktau improvement compared to other models. Our results demonstrate that the surrogate model employed in the methodology effectively adapts to uniform sampling, enabling accurate performance prediction JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 3

(a) RF-R ( R, 0.6257) (b) SVM-R ( R, 0.6586) (c) MLP-R ( R, 0.5658) (d) Adaboost-R ( R, 0.6748) 

(e) RF-C ( R, 0.6630) (f) SVM-C ( R, 0.7052) (g) MLP-C ( R, 0.5731) (h) Adaboost-C ( R, 0.7060) 

(i) RF-R ( U, 0.6807) (j) SVM-R ( U, 0.7492) (k) MLP-R ( U, 0.6276) (l) Adaboost-R ( U, 0.7228) 

(m) RF-C ( U, 0.6991) (n) SVM-C ( U, 0.7721) (o) MLP-C ( U, 0.6524) (p) Adaboost-C ( U, 0.7371) 

Fig. 1: Ablation study on surrogate model prediction performance using four different machine learning models, including the SVM adopted in this paper. of candidate architectures. III. S TUDY ON SURROGATE TIME COST 

To further analyze the time consumption of the surrogate process, we conduct statistics on the time consumption for training surrogate models and predicting architecture perfor-mance in the experiments from Section II. We collect the time consumption for each base machine learning model under both regression prediction and pairwise comparison relation prediction modes, and present the results in Table II. Although the proposed method incurs the highest time consumption, the training time remains within 1 minute, and the time overhead for pairwise comparison relation prediction on 1,000 architectures is within 3 minutes. Since the number of individuals that need to be evaluated simultaneously in each iteration of the search process does not exceed 150, the time overhead will be significantly reduced. Therefore, we are confident that this is acceptable for neural architecture search (NAS) problems. REFERENCES [1] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mo-bileNetV2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,2018, pp. 4510–4520. [2] M. Tan and Q. Le, “EfficientNet: Rethinking model scaling for con-volutional neural networks,” in Proceedings of the 36th International Conference on Machine Learning , 2019, pp. 6105–6114. JOURNAL OF L ATEX CLASS FILES, VOL. 18, NO. 9, SEPTEMBER 2020 4

[3] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable architectures for scalable image recognition,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,2018, pp. 8697–8710. [4] A. Yang, Y. Liu, C. Li, and Q. Ren, “Deeply supervised block-wise neural architecture search,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 2, pp. 2451–2464, 2025. [5] M. Tan and Q. V. Le, “MixConv: Mixed depthwise convolutional kernels,” in British Machine Vision Conference , 2019, p. 74. [6] Y. Xu, L. Xie, X. Zhang, X. Chen, G.-J. Qi, Q. Tian, and H. Xiong, “PC-DARTS: Partial channel connections for memory-efficient architecture search,” in International Conference on Learning Representations , 2019. [7] Y. Zhou, X. Xie, and S.-Y. Kung, “Exploiting operation importance for differentiable neural architecture search,” IEEE Transactions on Neural Networks and Learning Systems , vol. 33, no. 11, pp. 6235–6248, 2022. [8] Y. Xue, X. Han, F. Neri, J. Qin, and D. Pelusi, “A gradient-guided evolutionary neural architecture search,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 3, pp. 4345–4357, 2025. [9] Y. Xue, X. Han, and Z. Wang, “Self-adaptive weight based on dual-attention for differentiable neural architecture search,” IEEE Transac-tions on Industrial Informatics , vol. 20, no. 4, pp. 6394–6403, 2024. [10] X. Rao, B. Zhao, D. Liu, and C. Alippi, “FX-DARTS: Designing topology-unconstrained architectures with differentiable architecture search and entropy-based super-network shrinking,” IEEE Transactions on Neural Networks and Learning Systems , vol. 36, no. 10, pp. 19 356– 19 369, 2025. [11] Z. Lu, I. Whalen, V. Boddeti, Y. Dhebar, K. Deb, E. Goodman, and W. Banzhaf, “NSGA-Net: Neural architecture search using multi-objective genetic algorithm,” in Proceedings of the Genetic and Evolu-tionary Computation Conference , 2019, pp. 419–427. [12] Z. Lu, K. Deb, and V. N. Boddeti, “MUXConv: Information multiplexing in convolutional neural networks,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 12 044–12 053. [13] X. Chu, B. Zhang, and R. Xu, “FairNAS: Rethinking evaluation fairness of weight sharing neural architecture search,” in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 12 239–12 248. [14] M. Lin, P. Wang, Z. Sun, H. Chen, X. Sun, Q. Qian, H. Li, and R. Jin, “Zen-NAS: A zero-shot NAS for high-performance image recognition,” in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 337–346. [15] J. Zou, H. Chu, Y. Xia, J. Xu, Y. Liu, and Z. Hou, “Multiple population alternate evolution neural architecture search,” in 2025 International Joint Conference on Neural Networks (IJCNN) , 2025, pp. 1–9. [16] C. Song, Y. Ma, Y. Xu, and H. Chen, “Multi-population evolutionary neural architecture search with stacked generalization,” Neurocomputing ,vol. 587, p. 127664, 2024. [17] Y. Xue, J. Zha, D. Pelusi, P. Chen, T. Luo, L. Zhen, Y. Wang, and M. Wahib, “Neural architecture search with progressive evaluation and sub-population preservation,” IEEE Transactions on Evolutionary Computation , vol. 29, no. 5, pp. 1678–1691, 2025. [18] P. Jiang, Y. Xue, and F. Neri, “Score predictor-assisted evolutionary neural architecture search,” IEEE Transactions on Emerging Topics in Computational Intelligence , 2025, DOI: 10.1109/TETCI.2025.3526179.