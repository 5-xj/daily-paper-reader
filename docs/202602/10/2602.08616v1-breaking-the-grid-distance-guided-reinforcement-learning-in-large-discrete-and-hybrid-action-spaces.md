# Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces
# 突破网格：大规模离散及混合动作空间中的距离引导强化学习

**Authors**: Heiko Hoppe, Fabian Akkerman, Wouter van Heeswijk, Maximilian Schiffer \\
**Date**: 2026-02-09 \\
**PDF**: https://arxiv.org/pdf/2602.08616v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:LNS</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 7.0 \\
**Evidence**: addresses large discrete action spaces using neighborhood-based exploration \\

---

## Abstract
Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.

## 摘要
强化学习正越来越多地应用于物流、调度

---

## 速览摘要（自动生成）

**问题**：标准强化学习难以处理超大规模（达$10^{2