Title: Automatic regularization parameter choice for tomography using a double model approach

URL Source: https://arxiv.org/pdf/2602.08528v1

Published Time: Tue, 10 Feb 2026 03:02:51 GMT

Number of Pages: 6

Markdown Content:
# AUTOMATIC REGULARIZATION PARAMETER CHOICE FOR TOMOGRAPHY USING A DOUBLE MODEL APPROACH 

# Chuyang Wu and Samuli Siltanen 

# Department of Mathematics and Statistics, University of Helsinki 

ABSTRACT 

Image reconstruction in X-ray tomography is an ill-posed in-verse problem, particularly with limited available data. Regu-larization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feed-back control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the small-est parameter that yields sufficient similarity between recon-structions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data. 

Index Terms — Computed tomography, control theory, regularization 

1. INTRODUCTION 

Computed Tomography (CT) is a key application of inverse problems in imaging science [18]. The goal is to recover an unknown object x from indirect and noisy projection data y.For complete datasets, filtered back-projection (FBP) is the standard reconstruction method [14]. Incomplete data such as sparse angular sampling typically require iterative recon-struction methods based on a finite-dimensional linear model 

y = Ax + η. (1) Here, A denotes the discretized projection operator (e.g., Radon transform on a pixel grid), and η is measurement noise [13]. The unknown vector x encodes X-ray attenuation val-ues on a chosen computational grid. Notably, this grid is not canonically defined; its construction is an integral part of the mathematical modeling of the tomographic problem. Such inverse problems are frequently ill-posed [3]. The forward operator A is typically ill-conditioned with strong at-tenuation of high-frequency signal components [20]. Direct inversion amplifies measurement noise and yields reconstruc-tions dominated by artefacts, not physically meaningful struc-ture [25]. Effective stabilization therefore requires balancing 

> This work was supported by the Finnish Ministry of Education and Cul-ture’s Pilot Project Mathematics of Sensing, Imaging and Modelling.

fidelity to the measured data against a priori assumptions on the solution, such as smoothness or sparsity. A standard approach is variational regularization :

ˆxα = arg min 

> x

 12 ∥Ax − y∥22 + αR (x)



, (2) where R(x) is a regularization functional, and the scalar α > 

0 is the regularization parameter that controls the trade-off between data fidelity and a priori assumptions: small values lead to noisy reconstructions, while large values risks over-smoothing and loss of detail [2]. Automatically selecting an appropriate α remains a long-standing challenge. Classical approaches such as the L-curve criterion and generalized cross-validation typically rely on grid searches and unstable feature detection [5]. Discrepancy-based principles provide theoretical guarantees, but require accurate knowledge of the noise level |η| and are restricted to specific classes of regularizers [4]. Data-driven methods, including deep neural networks, can learn regularization pa-rameters or even entire regularizers end-to-end, at the expense of extensive training and limited interpretability [8, 6, 16, 1, 21]. We adopt a different perspective by casting regularization parameter selection as a closed-loop control problem. Instead of estimating a fixed α from first principles, we adjust it dy-namically to enforce a user-specified quality criterion. The approach is based on a double model strategy : reconstructions are done on two geometrically distinct grids using the same sinogram . Because discretization errors differ across grids, insufficient regularization leads to divergent reconstructions. As α increases, noise is progressively suppressed and the two solutions converge toward a common, grid-independent rep-resentation. We exploit this inter-grid consistency as a mea-surable feedback signal for the controller. As shown in Figure 1, the user selects a regularizer, a sim-ilarity measure, and a target consistency level Sref that reflects task-specific requirements. The controller then iteratively up-dates α until the measured consistency reaches the target, and halts the process automatically. Crucially, high similar-ity alone does not imply optimal reconstruction: overly large 

α produces consistent yet trivial solutions. By delegating the choice of Sref to the user, the method explicitly and trans-parently incorporates domain knowledge such as acceptable                

> arXiv:2602.08528v1 [cs.CV] 9 Feb 2026 Init α0, Target Sref
> Recons A&A′
> Sim (xA, x A′)
> Target met if |S−Sref |< ϵ
> Stop: Output xA
> Update α
> Sinogram y
> Yes
> No
> New α

Fig. 1 : Feedback loop using double-model consistency sens-ing; update α until S matches the user target. noise levels or the relative importance of fine texture. Our proposed approach builds on three prior ideas. The first is to exploit a priori sparsity of the unknown as a basis for parameter selection [7]. The second introduces a simple PID controller that dynamically steers the regularization parame-ter toward a desired sparsity level during iterative optimiza-tion [19, 12]. The third is the use of multiple computational grids, such as triple-resolution strategies in tomography [15] and double-grid formulations for image deconvolution [9]. In this work, we synthesize these ideas into novel framework. 

2. METHODOLOGY 2.1. Discrete Inverse Problem 

We consider CT in a discrete computational setting. The task is to estimate an image vector x ∈ Rn from measured projec-tion data (a sinogram) y ∈ Rm, modeled by the linear system in Equation (1). To stabilize the ill-posed inversion of A, we employ variational regularization as in Equation (2), where 

R(x) encodes prior assumptions such as gradient sparsity (TV: total variation [17]) or energy penalization (Tikhonov regularization [22]). The regularization parameter α governs the trade-off between data fidelity and prior enforcement. Rather than seeking a single mathematically optimal value, we aim to regulate α dynamically so as to satisfy a task-dependent quality criterion. 

2.2. Double Model Reconstruction 

To evaluate reconstructions without ground truth, we utilize a double-model strategy. By solving the inverse problem on two distinct grids, we force discretization-induced errors to manifest as uncorrelated artifacts. Given a primary forward operator A, a secondary opera-tor Aθ rotated by angle θ, and a given value of α, we com-pute two reconstructions: x1 = Solve (y, A, α ) and x2 =

Solve (y, A θ , α ). x2 is then mapped back to x1e by derotating 

θ. We then quantify inter-grid consistency using the Struc-tural Similarity Index (SSIM [24]): 

S(α) = Sim (x1, ˜x2). (3) Rather than an objective to be maximized, S(α) serves as a feedback signal for regularizer control. 

2.3. Monotonicity Assumption 

The use of S(α) as a control signal assumes that inter-grid consistency increases monotonically with α. This is driven by two primary effects: 1. Noise orthogonality (low α): Under-regularized re-constructions are dominated by grid-specific noise and artifacts. Because A and Aθ discretize the problem dif-ferently, these errors are largely uncorrelated, resulting in low similarity between x1 and ˜x2

2. Signal coherence (moderate α): As regularization in-creases, grid-dependent artifacts are suppressed. The reconstructions converge toward the common underly-ing signal, increasing the measured similarity. This assumption holds provided that the rotation angle θ

avoids degenerate grid alignments (e.g., multiples of 90 ◦ on square grids). In the limit α → ∞ , both reconstructions col-lapse to trivial, highly regularized states, yielding artificially high similarity at the cost of severe information loss. The con-troller therefore operates on the rising edge of the consistency curve, terminating before trivial over-regularization occurs. 

Algorithm 1 Double-Model Adaptive Regularization 

Require: sinogram y, operators A, A θ , target Sref , gain Kp

Ensure: optimal reconstruction ˆx 

> 1:

Initialize: α0 ← 10 −6, k ← 0 

> 2:

repeat  

> 3:

// 1. Actuation (Dual Reconstruction)  

> 4:

xA ← Solve (y, A, α k) 

> 5:

xB ← Solve (y, A θ , α k) 

> 6:

// 2. Sensing (Geometric Alignment)  

> 7:

˜xB ← T −θ (xB ) 

> 8:

Sk ← SSIM (xA, ˜xB ) 

> 9:

// 3. Error Correction (Log-domain P-Control)  

> 10:

ek ← Sref − Sk 

> 11:

log 10 (αk+1 ) ← log 10 (αk) + Kp · ek 

> 12:

k ← k + 1  

> 13:

until |ek| < ϵ for N consecutive steps  

> 14:

return ˆx = xA

2.4. Closed-Loop Control Formulation 

The reconstruction process is formulated as a closed-loop feedback system in which α is the control variable and Sk

acts as the sensed output (Algorithm 1). The controller up-dates α in the logarithmic domain to ensure scale-invariant, multiplicative adjustments. The iteration terminates when the measured consistency remains within a tolerance band 

ϵ of the user-defined target Sref for a prescribed number of consecutive steps. 3. EXPERIMENTAL SETUP 3.1. Codes and Datasets 

The source code is available 1. Projection operators and for-ward models are implemented with the ASTRA Toolbox[23]. We used two open-access X-ray tomography datasets pro-vided by the Finnish Inverse Problems Society (FIPS): • Walnut: A dataset dominated by smooth organic gra-dients and moderate internal structure [11]. • Pine Cone: A more challenging dataset containing thin, high-frequency scale structures that are particu-larly sensitive to over-regularization [10]. While both datasets are 3D cone-beam acquisitions, we ex-tracted central 2D slices and simulated a fan-beam geometry. All reconstructions were done on a fixed 450 ×450 pixel grid. 

3.2. Control Parameters and Execution 

All experiments followed Algorithm 1. No sweeps or offline searches over α were performed. α was initialized at a small value and updated automatically at each iteration based on the inter-grid SSIM. The angle θ is uniformly drawn from 

(10 , 20) . The proportional controller gain was set to Kp =0.5 in the log 10 (α) domain, yielding multiplicative updates of the regularization strength. To ensure stable termination, convergence was declared only when |Sk − Sref | ≤ 0.05 for 

N = 5 consecutive iterations to suppress premature termina-tion due to transient fluctuations in the SSIM and to enforce stable convergence to the user-defined operating point. 

4. RESULTS 

This section analyzes the controller behavior, focusing on convergence, stability, and robustness across different datasets and regularizer. 

4.1. Experiment 1 (Walnut / TV) 

Figure 2 illustrates the controller response on the Walnut dataset using TV regularization. The initial negligible reg-ularization ( α0 = 10 −10 ) created a noisy reconstruction (Fig. 2a). The low inter-grid SSIM ( S ≈ 0.53 ) generated a large control error, which drove a progressive increase in 

α. SSIM rose monotonically to reach the user-defined target 

Sref = 0.95 . The corresponding reconstruction (Fig. 2b) suppressed noise effectively and preserved sharp boundaries of both the shell and internal septa. Once stable convergence was achieved, the controller terminated automatically.         

> 1https://tinyurl.com/4xturjx8
> (a) Under-reg ( α= 10 −10 )(b) Target ( α≈2.3e-6)
> (c) Over-reg ( α≈2.3e-4)(d) SSIM over α

Fig. 2 : Walnut / TV. (a) Initial noise-dominated state. (b) The converged result at target SSIM 0.95. (c) Over-regularized reference showing loss of detail. (d) The recorded monotonic SSIM vs. α curve. The controller halts in the target band. We then set a large α ≈ 2.3 × 10 −4 to generate an over-regularized reconstruction (Fig. 2c). This suffered from pro-nounced loss of structural detail, illustrating that high simi-larity alone does guarantee a desirable result. The SSIM tra-jectory in Fig. 2d confirms the monotonicity of the similar-ity function S(α) defined in (3). Using the domain-specific knowledge, the controller halts the process on the rising edge of the curve to avoid trivial over-regularization. 

4.2. Experiment 2 (Walnut / Tikhonov) 

To assess solver dependence, we repeated the same experi-ment with unchanged control mechanism using Tikhonov reg-ularization ( R(x) = ∥x∥22). which induces smoother tex-tures and blurred edges compared to TV. As shown in Fig. 3, the controller again transitions from a noise-dominated ini-tial state toward the prescribed consistency target, tracking a monotonic SSIM trajectory (Fig. 3d). Stable convergence is achieved without modification of the control parameters, demonstrating that the control logic is independent of the spe-cific regularization functional. 

4.3. Experiments 3 and 4 (Pine Cone) 

We evaluate the robustness of the controller under increased geometric complexity of the pine cone which is sensitive to over-regularization. The results for TV and Tikhonov are summarized in Figures 4 and 5 respectively. In both cases, the (a) Under-reg ( α = 10 −10 ) (b) Target ( α ≈ 5.1e-2)    

> (c) Over-reg ( α≈5.1e0)(d) SSIM over α

Fig. 3 : Walnut / Tikhonov. The monotonic SSIM trajectories demonstrate that the control method is solver-agnostic.         

> (a) Under-reg ( α= 10 −10 )(b) Target ( α≈4.1e-6)
> (c) Over-reg ( α≈4.1e-4)(d) SSIM over α

Fig. 4 : Pine Cone / TV. The controller preserves the sharp scale structures while removing noise. controller reliably drove the system from a noise-dominated initial state toward the user-defined target ( Sref = 0 .90 ), with monotonic SSIM trajectories throughout (Figs. 4d, 5d). TV regularization expectedly preserved sharp scale bound-aries (Fig. 4b), and Tikhonov produced smoother approxima-tions (Fig. 5b). Importantly, no changes to controller gains or         

> (a) Under-reg ( α= 10 −10 )(b) Target ( α≈5.1e-2)
> (c) Over-reg ( α≈5.1e0)(d) SSIM over α

Fig. 5 : Pine Cone /Tikhonov. The loop stabilizes effectively even with the smoothing L2 prior. stopping criteria were required, underscoring the geometric and solver robustness of the proposed framework. 

5. COMPARATIVE ANALYSIS 

We compare our controller against two widely used parameter choice heuristics on the Walnut dataset using TV regulariza-tion. The goal is not to identify a universally superior method, but to highlight the differences of fundamentally different pa-rameter selection philosophies. 

5.1. Baselines 

We consider the following baseline methods, both performing a one-shot parameter selection without feedback or explicit encoding of task-specific preferences. • L-Curve Criterion: α was selected via a grid search over 20 logarithmically spaced values, choosing the point of maximum curvature in the log-log plot of the residual norm versus the regularization term. • Discrepancy Principle: α was chosen to satisfy 

∥Ax α − y∥2 ≈ τ √mσ with τ = 1 .01 , using the known noise level σ. This is an oracle setting, as accurate noise estimates are typically unavailable in practice. 

5.2. Performance Trade-offs 

Figure 6 summarizes the comparison. Both methods (Figs. 6b and 6c) select relatively large α, yielding highly smooth re-(a) Controller( α ≈ 8.6e-7) (b) L-Curve( α ≈ 2.1e-5)   

> (c) Discrepancy( α≈8.8e-6)(d) Consistency vs. Detail

Fig. 6 : Method Comparison. The Pareto front shows that the Controller (Star) finds a balanced operating point distinct from the conservative heuristics (Square/Diamond). constructions with very high inter-grid consistency (SSIM >

0.98 ). While effective at suppressing noise, they attenuate structural details such as the walnut kernel septa. The con-troller (Fig. 6a) regulates α to track a lower, user-specified consistency target ( Sref = 0 .95 ). By tolerating inter-grid variability, the controller preserves high-frequency structural detail that is suppressed by the more conservative heuristics. This trade-off is quantified in Fig. 6d, which plots inter-grid consistency against a simple proxy for image detail, given by the gradient energy ∥∇ x∥2. The curve exhibits a Pareto-like structure: the L-Curve and Discrepancy Principle cluster in a high-consistency, low-detail regime, while the controller operates near the knee of the curve, where a small SSIM reduction yields a large gain in recovered detail. These results highlight a key distinction: classical heuris-tics implicitly optimize for stability or noise consistency, whereas the proposed controller explicitly enforces a user-defined quality specification. As a result, the controller does not aim to maximize consistency, but to regulate it, enabling principled navigation of the stability–detail trade-off. 

6. CONCLUSION 

We present a control-theoretic framework for automatic reg-ularization in CT, reframing parameter selection as a closed-loop control problem rather than static estimation. By ex-ploiting discretization-induced inconsistencies between two reconstructions, we construct an internal, physically inter-pretable sensing signal that enables stable feedback control without access to ground truth or explicit noise estimates. The central contribution of this work is the use of dis-cretization effects—typically treated as numerical error—as a source of actionable information. This engineering perspec-tive operates directly in the discrete computational regime rel-evant to practical CT reconstruction. Inter-grid consistency provides a reliable proxy for reconstruction stability, allow-ing the regularization strength to be regulated dynamically. We made a crucial design choice to let the user explicitly set a target consistency level. Instead of attempting to com-pute a universally optimal regularization parameter, we sim-ply enforce a user-defined specification that reflects domain-and task-specific priorities, such as how much noise is toler-ated or how important fine texture is relative to smoothing. By shifting the problem from parameter tuning to reference tracking, the method becomes transparent, intuitive, and eas-ily adaptable across different solvers and datasets. The experiments demonstrate stable convergence, solver independence, and robustness to geometric complexity, while comparative analysis highlights a fundamental distinction between closed-loop regulation and traditional open-loop heuristics. By regulating consistency instead of maximiz-ing it, the proposed approach enables principled navigation of the stability–detail trade-off inherent to ill-posed inverse problems. Several directions naturally follow from this work. On the theoretical side, establishing formal conditions under which the monotonicity of the consistency signal holds remains an open problem. From an applied perspective, extending the framework to more severely ill-posed CT scenarios—such as limited-angle acquisition or metal artifact reduction—and ex-ploring alternative sensing metrics beyond SSIM are promis-ing avenues. More broadly, the results suggest that feedback control offers a natural and effective paradigm for manag-ing uncertainty and trade-offs in large-scale computational in-verse problems. 

# References 

[1] Simon Arridge et al. “Solving Inverse Problems Using Data-Driven Models”. In: Acta Numerica 28 (2019), pp. 1–174. DOI : 10.1017/S0962492919000059 .[2] Martin Benning and Martin Burger. “Modern regular-ization methods for inverse problems”. In: Acta numer-ica 27 (2018), pp. 1–111. [3] Mario Bertero, Patrizia Boccacci, and Christine De Mol. Introduction to inverse problems in imaging .CRC press, 2021. [4] Thomas Bonesky. “Morozov’s discrepancy principle and Tikhonov-type functionals”. In: Inverse Problems 

25.1 (2009), p. 015015. [5] Pasquale Cascarano et al. “Constrained regularization by denoising with automatic parameter selection”. In: 

IEEE Signal Processing Letters 31 (2024), pp. 556– 560. [6] Juan Carlos De los Reyes, Carola-Bibiane Sch¨ onlieb, and Tuomo Valkonen. “Bilevel Parameter Learning for Higher-Order Total Variation Regularisation Models”. In: Journal of Mathematical Imaging and Vision 57.1 (2017), pp. 1–25. DOI : 10.1007/s10851- 016-0662-8 .[7] Keijo Hamalainen et al. “Sparse tomography”. In: 

SIAM Journal on Scientific Computing 35.3 (2013), B644–B665. [8] Karl Kunisch and Thomas Pock. “A Bilevel Optimiza-tion Approach for Parameter Learning in Variational Models”. In: SIAM Journal on Imaging Sciences 6.2 (2013), pp. 938–983. DOI : 10.1137/120882706 .[9] Juvonen M et al. “Dual-grid parameter choice method with application to image deblurring”. In: Applied Mathematics for Modern Challenges 5 (2025), pp. 64– 85. DOI : 10.3934/ammc.2025011 .[10] Alexander Meaney. Cone-Beam Computed Tomogra-phy Dataset of a Pine Cone . Version 1.1.0. Aug. 2022. 

> DOI

: 10.5281/zenodo.6985407 . URL : https: //doi.org/10.5281/zenodo.6985407 .[11] Alexander Meaney. Cone-Beam Computed Tomogra-phy Dataset of a Walnut . Version 1.1.0. Aug. 2022. 

> DOI

: 10.5281/zenodo.6986012 . URL : https: //doi.org/10.5281/zenodo.6986012 .[12] Alexander Meaney et al. “Image reconstruction in cone beam computed tomography using controlled gradient sparsity”. In: Applied Mathematics for Modern Chal-lenges 3 (2025), pp. 64–89. DOI : 10.3934/ammc. 2025003 .[13] Jennifer L Mueller and Samuli Siltanen. Linear and nonlinear inverse problems with practical applica-tions . SIAM, 2012. [14] Frank Natterer. The mathematics of computerized to-mography . SIAM, 2001. [15] Kati Niinimaki et al. “Multiresolution parameter choice method for total variation regularized tomography”. In: 

SIAM journal on imaging sciences 9.3 (2016), pp. 938– 974. [16] Ismoilov Nusrat and Sung-Bong Jang. “A comparison of regularization techniques in deep neural networks”. In: Symmetry 10.11 (2018), p. 648. [17] Stanley Osher et al. “An iterative regularization method for total variation-based image restoration”. In: Mul-tiscale Modeling & Simulation 4.2 (2005), pp. 460– 489. [18] Xiaochuan Pan, Emil Y Sidky, and Michael Vannier. “Why do commercial CT scanners still employ tradi-tional, filtered back-projection for image reconstruc-tion?” In: Inverse problems 25.12 (2009). [19] Zenith Purisha et al. “Controlled wavelet domain spar-sity for X-ray tomography”. In: Measurement Science and Technology 29.1 (2017), p. 014002. [20] T Sarkar, D Weiner, and V Jain. “Some mathematical considerations in dealing with the inverse problem”. In: 

IEEE transactions on antennas and propagation 29.2 (2003), pp. 373–379. [21] Ensio Suonper¨ a and Tuomo Valkonen. “Linearly con-vergent bilevel optimization with single-step inner methods”. In: Computational Optimization and Appli-cations 87.2 (2024), pp. 571–610. [22] Andrei Nikolaevich Tikhonov, Aleksandr S Leonov, and Anatolij Grigorevic Yagola. “Nonlinear ill-posed problems”. In: Proceedings of the first world congress on World congress of nonlinear analysts’ 92, volume I .1996, pp. 505–511. [23] Wim Van Aarle et al. “The ASTRA Toolbox: A plat-form for advanced algorithm development in electron tomography”. In: Ultramicroscopy 157 (2015), pp. 35– 47. [24] Zhou Wang et al. “Image quality assessment: from er-ror visibility to structural similarity”. In: IEEE transac-tions on image processing 13.4 (2004), pp. 600–612. [25] Armand Wirgin. “The inverse crime”. In: arXiv preprint math-ph/0401050 (2004).