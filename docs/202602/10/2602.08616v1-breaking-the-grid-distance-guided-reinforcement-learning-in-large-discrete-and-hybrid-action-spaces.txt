Title: Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces

URL Source: https://arxiv.org/pdf/2602.08616v1

Published Time: Tue, 10 Feb 2026 03:09:38 GMT

Number of Pages: 26

Markdown Content:
# Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces 

Heiko Hoppe 1 2 Fabian Akkerman 3 Wouter van Heeswijk 3 Maximilian Schiffer 1 2 

## Abstract 

Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender sys-tems, but standard algorithms struggle with the curse of dimensionality in such large discrete ac-tion spaces. Existing algorithms typically rely on restrictive grid-based structures or computation-ally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregu-larly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), com-bining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10 20 actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumet-ric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a sta-ble regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependen-cies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured envi-ronments, while simultaneously improving con-vergence speed and computational complexity. 

## 1. Introduction 

Reinforcement Learning (RL) has achieved remarkable suc-cess in continuous and small-scale discrete domains. How-ever, many real-world applications – ranging from logistics 

> 1

School of Management, Technical University of Munich, Munich, Germany 2Munich Data Science Institute, Technical University of Munich, Munich, Germany 3Department of High-tech Business and Entrepreneurship, University of Twente, En-schede, the Netherlands. Correspondence to: Heiko Hoppe 

<heiko.hoppe@tum.de >.

Preprint. February 10, 2026. 

planning and scheduling to recommender systems and robot control – require agents to navigate action spaces that ex-hibit both large cardinality and complex structure. These spaces often include high-dimensional discrete components, categorical variables, or combinations of discrete and con-tinuous parameters, presenting a fundamental “curse of car-dinality” that renders traditional approaches ineffective. 

The Scalability Gap in Prior Work. Standard methods fail to scale to these regimes due to three primary bottle-necks: gradient variance, computational tractability, and structural rigidity. Early approaches attempt to reduce dimensionality through factorization (Sallans & Hinton, 2004; Pazis & Parr, 2011) or symbolic representations (Cui & Khardon, 2016), but these require substantial manual tuning. Representation learning methods (Chandak et al., 2019; Whitney et al., 2020) often suffer from high sample complexity as they require learning dedicated representa-tions for every discrete action. A more scalable paradigm leverages continuous actor-critic architectures by mapping continuous proto-actions to discrete counterparts. Sim-ple rounding (van Hasselt & Wiering, 2009; Vanvuchelen et al., 2024) is efficient but suboptimal in complex land-scapes. To improve precision, the Wolpertinger architecture (Dulac-Arnold et al., 2015) employs k-nearest neighbor searches, yet this becomes computationally intractable in high-dimensional spaces. Recently, Akkerman et al. (2024) introduce Dynamic Neighborhood Construction (DNC) to navigate neighborhoods via simulated annealing; while scal-able, it relies heavily on regular structures (e.g., numeri-cal adjacency), making it brittle in irregularly spaced do-mains. Finally, hybrid continuous-discrete action spaces pose unique challenges, as most existing algorithms treat components independently or rely on hierarchical sequenc-ing (Hausknecht & Stone, 2016; Fan et al., 2019), failing to account for the tight coupling required in complex tasks. 

Contributions. We propose Distance-Guided Reinforce-ment Learning (DGRL) , a framework that introduces a volu-metric projection loop composed of two synergistic pillars: • Sampled Dynamic Neighborhoods (SDN): A trust-region constructor that performs stochastic volumetric exploration. By utilizing the L∞ (Chebyshev) metric, SDN maintains a stable search volume as dimensionality 1

> arXiv:2602.08616v1 [cs.LG] 9 Feb 2026 Breaking the Grid: Distance-Guided RL in Large Action Spaces

N → ∞ and reduces search complexity from exponential to linear. • Distance-Based Updates (DBU): A denoised projection mechanism that transforms policy optimization into a stable regression task. We prove that DBU’s gradient vari-ance is independent of action cardinality |A| , addressing the key bottleneck of gradient explosion in large spaces. We establish that DGRL ensures monotonic policy improve-ment in expectation and enables joint optimization in hybrid action spaces, bypassing the commitment bottlenecks of hierarchical methods. Empirical results against state-of-the-art benchmarks across logistics and recommender systems show performance gains of up to 66% in spaces with 10 20 ac-tions, while reducing computational overhead significantly. 

## 2. Problem Setup and Notation 

We consider Markov Decision Processes (MDPs) defined by the tuple (S, A, P, R, γ ), comprising a transition kernel 

P (s′|s, a ), a reward function R(s, a ), depending on states 

s ∈ S and actions a ∈ A , and a discount factor γ. We focus on maximizing the expected return J(π) = E[PTt=0 γtrt].We specifically address two challenging regimes: Large Dis-crete Action Spaces (LDAS) and Hybrid Action Spaces . To efficiently navigate such massive spaces, we need to exploit their underlying topology. We therefore formalize the di-mensionality constraints, metric structures, and continuous relaxations that underpin our framework. 

Dimensionality and Structure. We classify discrete action spaces based on two properties dictating scalability. First, we distinguish univariate actions (a single index) and mul-tivariate actions. An action space A is multivariate if each action a is a vector of N components a = ( a1, . . . , a N ),where each component ad is chosen from a sub-space Ad. In these settings, the cardinality | A | grows exponentially with the dimensionality N , rendering enumeration intractable. Second, we assume the action space possesses an underlying metric. We distinguish three component types: 

Numerical: Actions represent quantitative values (e.g., prices). Distance is natural (Euclidean). 

Ordinal: Actions represent ordered options (e.g., intensity). Distance is defined by rank. 

Categorical: Actions are nominally unordered (e.g., items in a catalog). Building on the manifold hypothesis, we assume these actions can be mapped to a latent metric space, e.g., through pre-trained semantic embeddings or learned dissimilarity functions, where actions with similar semantic roles occupy proximal regions in the embedding space. Unlike algorithms relying on regular grids (Akkerman et al., 2024), our approach solely requires the existence of a valid metric, allowing us to handle irregularly spaced or sparse feasible regions. 

Relaxed Action Space and Latent Structure. To navigate these large spaces, we operate in a continuous relaxation. Let A ⊂ ZN be the set of valid discrete actions. We define the Relaxed Action Space A′ ⊆ RN such that A ⊂ A ′. The actor network φθ : S → A ′ outputs a continuous proto-action ˆa ∈ A ′.Crucially, to ensure that local search in the relaxed space translates to meaningful optimization in the discrete space, we require the existence of a semantic structure. We formal-ize this via the following assumption. 

Assumption 2.1 (Latent Lipschitz Continuity) . We assume a discrete action space (A, d A) with an embedding function 

ϕ : A → Z ⊆ Rd (where Z that corresponds to our relaxed space A′), such that the state-action value function Q(s, a )

admits a continuous extension ˜Q : S × Z → R which is 

L-Lipschitz continuous with respect to a latent metric ∥ · ∥ p :

| ˜Q(s, ϕ (a)) − ˜Q(s, ϕ (a′)) | ≤ L∥ϕ(a) − ϕ(a′)∥p, ∀a, a ′ ∈A. Furthermore, we assume the embedding ϕ is semantic, such that proximity in Z implies functional similarity in A.This assumption is consistent with the manifold hypothesis in representation learning (e.g., Chandak et al., 2019). It is particularly robust in multivariate action spaces, where changing a single component preserves overall vector sim-ilarity, naturally satisfying the requirement for functional proximity in the latent manifold. Moreover, it allows us to treat the latent space as a smooth manifold where gradients provide useful signals, justifying the distance-based loss even when the underlying action space is fundamentally discrete or irregularly structured. See Appendix A.1 for further empirical grounding. 

Hybrid Action Spaces. We further investigate hybrid spaces A ⊂ ZN × RM , where an action a = ( ad, a c)

contains both discrete and continuous components. We are particularly interested in Parameterized Action Spaces 

(Hausknecht & Stone, 2016), where the validity or semantic meaning of the continuous parameter ac depends on the discrete choice ad. This creates a tight coupling between components, often rendering methods that optimize them independently suboptimal. 

## 3. Theoretical Framework: Design Principles 

Before introducing DGRL, we establish the theoretical prin-ciples that underscore our design choices. We specifically address two fundamental questions: how to define a stable optimization objective in relaxed spaces, and how to scale local search to high dimensions without losing efficiency. 

Distance as a Proxy for Value. Directly optimizing dis-crete actions via policy gradients is unstable in massive 2Breaking the Grid: Distance-Guided RL in Large Action Spaces 

spaces due to variance explosion. Instead, we leverage the relaxed space A′. Following Assumption 2.1, we rely on the Lipschitz continuity of the Q-function, implying that actions close in the relaxed metric space possess similar values. This allows us to reframe the RL problem: rather than maximizing potentially noisy Q-values directly, the actor can minimize the distance to a high-value target ¯a.

Proposition 3.1 (Approximation Bound via Lipschitz Con-tinuity) . Let Q(s, ·) : A′ → R be LQ-Lipschitz continuous w.r.t. a metric d. Let a⋆ be the optimal discrete action, 

ˆa be a continuous proto-action, and ¯a be a target action. The value loss of rounding ˆa to its nearest neighbor ann is bounded by the distance loss J(w) = ∥ˆa − ¯a∥2:

Q(s, a ⋆)−Q(s, a nn ) ≤ LQ

p J(w) + d(¯ a, a ⋆) + ϵround 



.

Proof Sketch. The value gap decomposes into the distance from ˆa to the target ¯a, and from ¯a to the true optimum 

a⋆. By Lipschitz continuity, minimizing the distance J(w)

minimizes the upper bound of the value loss. See App. A.2. This proposition provides the theoretical justification for Distance-Based Updates (DBU), motivating the supervised-like regression target formally introduced in Section 4. 

The Geometry of High-Dimensional Search. Inference in large action spaces requires identifying a high-value action 

a in the vicinity of ˆa. In high dimensions ( N ≫ 1), the choice of metric defines the shape of this search volume, which is critical for both theoretical bounds and sampling efficiency. In standard Euclidean ( L2) neighborhoods, the 

L2 radius must grow with √N to encompass a fixed volume of independent coordinate variations, which loosens the Lipschitz bound and leads to vanishing sampling density. We address this by adopting the L∞ (Chebyshev) metric: our hypercubic sampling distribution perfectly matches the geometry of the L∞ trust region, ensuring that the probabil-ity of sampling near-optimal actions remains non-vanishing even as N → ∞ . By maintaining coordinate-wise inde-pendence while remaining invariant to N , the L∞ metric ensures that a low number of samples K remains sufficient for high empirical stability, as formalized in Prop. 3.2. 

Proposition 3.2 (Dimensional Invariance of Chebyshev Neighborhoods) . Consider a search neighborhood N con-structed by independent sampling within a range ±ϵ across 

N dimensions. The geometry of N corresponds to a ball under the L∞ (Chebyshev) metric. While the L2 radius re-quired to encompass the support corners grows as O(ϵ√N ),the L∞ radius remains invariant to N . This ensures the trust region remains tightly coupled to the sampled support, preventing the volume expansion inherent to Euclidean met-rics in high dimensions. Proof Sketch. Independent coordinate-wise sampling forms a hypercube. The distance from the center to a corner under Proto-action ˆa 

> Neighborhood
> B(ˆ a)
> Candidates
> {a′}
> Q-values
> q(a′)
> Figure 1. Schematic representation of SDN.

the L2 norm is ϵ√N , which diverges as N → ∞ , forcing the trust region to include vast areas of zero-probability mass. Under the L∞ norm, the distance is identically ϵ.This invariance allows for a uniform Lipschitz constant for the policy update that is independent of the action space dimensionality. See App. A.3. This insight drives our choice of Sampled Dynamic Neighborhoods (SDN): we construct neighborhoods using coordinate-wise bounds ( L∞) to decouple dimensionality from search depth, ensuring the search remains tractable even as N increases. 

## 4. Methodology 

We propose Distance-Guided Reinforcement Learning 

(DGRL), a new algorithm that decouples the search for high-value actions from the policy update, enabling rein-forcement learning in action spaces with up to 10 20 candi-dates. Designed for integration into off-policy actor-critic architectures – e.g., Deep Deterministic Policy Gradients (DDPG) – our approach introduces a volumetric projection loop composed of two synergistic components: Sampled Dynamic Neighborhoods (SDN) and Distance-Based Up-dates (DBU). SDN constructs a trust region, identifying high-value regions without the axial blindness of prior grid-based methods, while DBU performs a denoised projection within the relaxed action space. The actor generates a con-tinuous proto-action that SDN uses as the starting point of a stochastic, volumetric search to identify the discrete execution action. Instead of backpropagating through high-variance discrete selections, DBU transforms the actor’s objective into a stable regression task. By minimizing the distance to a high-value target constructed from perturbed neighborhood samples, we decouple gradient variance from the total cardinality |A| , ensuring stable convergence fol-lowing the aggregate density of the discovered manifold. 

4.1. Sampled Dynamic Neighborhood (SDN) 

In large action spaces, identifying the optimal action a⋆

effectively requires solving two sub-problems: finding the promising region of the state-action space, and identifying the specific optimal discrete action within that region. Stan-dard methods often conflate these steps or rely on intractable 

k-nearest neighbor lookups (Dulac-Arnold et al., 2015). 3Breaking the Grid: Distance-Guided RL in Large Action Spaces 

Mechanism. Adopting the continuous-discrete mapping paradigm of van Hasselt & Wiering (2009), our actor φθ (s)

operates in the relaxed space A′, outputting a continuous 

proto-action ˆa that serves as the center of mass for a high-value region. As introduced by Dulac-Arnold et al. (2015), the critic then acts as a local selector to refine this coarse es-timate. We construct a dynamic neighborhood B(ˆ a) around 

ˆa bounded by a Chebyshev radius L (as motivated in Sec. 3). To avoid the combinatorial explosion associated with enu-merating all integer points in this hypercube, we approx-imate the neighborhood via probabilistic sampling. We visualize SDN in Fig. 1 and Algorithm 1. 

Independent Coordinate Sampling. To make the search tractable in high dimensions, we exploit the structure of multivariate spaces by sampling each dimension n ∈{1, . . . , N } independently. We interpret the proto-action 

ˆa as the mode of a sampling distribution over B(ˆ a) and draw K candidate actions. The probability of sampling a candidate a′ decreases with its distance from ˆa, effectively creating a soft trust region. To estimate probabilities, we can use a softmax over the negative absolute distances or apply a linear scheme: 

p(a′) = L − ∥ ˆa − a′∥∞ + τs

P

> a′B

L − ∥ ˆa − a′B∥∞ + τs

,

with a′B being options in B(ˆ a) and τs being the sampling temperature parameter. In our experiments, we use the linear scheme with τs = 1 , which depends less on the magnitude of distances than the softmax scheme. By design, indepen-dent coordinate sampling fulfills the Chebyshev Distance constraint. While the total action space |A| is massive, SDN only requires K samples to obtain a local directional es-timate of the objective landscape. Because the L∞ trust region remains volumetrically stable in high dimensions (Prop. 3.2), K does not need to scale with |A| . Instead, 

K is governed by the smoothness of the latent manifold, allowing for high-performance updates with as few as 20 samples in spaces of size 10 20 . As a consistent statistical estimator (App. A.4), SDN maintains this linear complexity 

O(N ·K) regardless of the total action cardinality, providing a tractable alternative to exhaustive or axial search. We utilize the critic for action selection: first, the critic eval-uates all candidate actions, creating a list of Q-values qk =

Qψω (s, a ′

> k

). During testing, we select a = argmax k qk. To ensure exploration during training, we sample a from {a′

> k

}

based on qk, using either a softmax over the Q-values or a rank-based probability metric: 

p(a) = τ rank (q(a)) 

> e

P 

> a′

τ rank (q(a′)) 

> e

,

with rank (q(a)) being the descending rank of the action according to its Q-value and τe being the exploration tem-perature. In our experiments, we use the rank-based scheme with τe = 0 .8, which is less influenced by the magnitude of Q-values than the softmax scheme. To ensure these samples map correctly to the valid action space, we include a specific scaling step. This maps the actor’s continuous output (typically clipped to a feasible range [ˆ amin , ˆamax ]) to the integer coordinates of the discrete action space [Amin , A max ]:

ˆascaled = ˆaclipped − ˆamin 

ˆamax − ˆamin 

(Amax − Amin ) + Amin .

This strategy reduces the search complexity from exponen-tial O((2 L)N ) to linear O(N ·K), enabling scalability to di-mensions where N > 100 while retaining local correlation. Crucially, as a consistent statistical estimator (App. A.4), SDN maintains this linear complexity regardless of the total action cardinality |A|, providing a computationally tractable alternative to exhaustive or axial search in high-dimensional manifolds. 

Algorithm 1 Sampled Dynamic Neighborhood (SDN) 

Input: state s, actor φθ , critic ψω , radius L, samples K

1. Estimation: Compute proto-action ˆa ← φθ (s)

2. Scaling: Map ˆa to action space bounds via scaling formula 

3. Independent Sampling: Sample K a ′ 

> k

∼ p(a′

> k

) from 

BL∞ (ˆ a, L )

4. Evaluation: Compute Q-values qk = Qψω (s, a ′

> k

)

5. Selection: if training then 

Sample a ∼ p(a) from {a′

> k

}

else 

Select a = argmax k qk

end if Benefits of SDN. Compared to Wolpertinger and DNC, SDN offers four key advantages: (i) While Wolpertinger requires the storage of a distance lookup table that grows intractable with action cardinality, SDN constructs neigh-borhoods during inference. (ii) DNC depends on a metric action space to exploit its structure, while SDN samples ac-tions along independent dimensions. (iii) Both Wolpertinger and DNC generate candidate actions sequentially, generally scaling with |A|. In contrast, SDN’s dimension-independent sampling is trivially parallelizable, empirically only scaling with L. (iv) Both Wolpertinger and DNC fail to explore neighborhoods extensively and exhaustively independently of structure, which SDN is able to do. 

4.2. Distance-Based Updates (DBU) 

With the search mechanism established, we require a com-patible update rule. Standard Deep RL losses are ill-suited for this setting: Policy Gradients applied to A suffer from 4Breaking the Grid: Distance-Guided RL in Large Action Spaces Proto-action ˆa 

> Perturbed
> {ˆai}
> Rounded
> {ai}
> Target ¯a
> Figure 2. Schematic representation of DBU.

variance explosion as the probability mass π(a|s) vanishes in large action spaces. When applied to the relaxed action space A′, Policy Gradients face a mismatch between the parametrized probability of choosing a proto-action ˆa given 

s and the probability of choosing an action a given s and the mapping procedure. This adds noise to the gradients and destabilizes learning. Deterministic policy gradients fail because the critic ψω is trained only on valid discrete actions a ∈ A . The evaluation of the gradient ∇aQ(s, ˆa)

for a continuous proto-action lying between valid actions is therefore inaccurate. We propose DBU to circumvent these issues by transforming the policy update into a supervised regression task against a stable, non-stationary target. Our loss function extends the paradigms proposed by Abdolmaleki et al. (2018) and Hoppe et al. (2025), adapting them to the specific geometry of relaxed large discrete spaces. We visualize DBU in Fig. 2 and outline it in Algorithm 2. 

Target Construction: The Softmax Average. To generate a reliable training signal, we do not simply use the best sampled action from SDN, as this argmax a′ q(s, a ′) is non-differentiable and sensitive to critic noise – given learned Q-values may not reflect the ground truth, the argmax can be the result of critic overestimation. Instead, we construct a robust target ¯a in the relaxed space A′: we perturb and round the proto-action ˆa to generate a set of auxiliary candi-dates {ai} (distinct from the execution candidates in SDN to decouple exploration from target estimation). We then compute the softmax-weighted average of these candidates: 

¯a =

P 

> i

exp( Q(s, a i)/τ ) · ai

P 

> i

exp( Q(s, a i)/τ ) .

This operation acts as a denoising filter, producing a target 

¯a that represents the expected location of the optimal action. By averaging over candidates, it pushes the target away from narrow local optima and smoothens the loss landscape. 

Regression Update. We update the actor weights w to minimize the distance to this target: 

J(w) = ∥φθ (s) − ¯a∥22.

This provides a dense, low-variance gradient signal that points directly toward the high-value region, independent of the cardinality of A. The update effectively pulls the proto-action ˆa into the trust region of valid, high-value discrete actions found by the critic. 

Algorithm 2 Distance-Based Updates (DBU) 

Input: state s, actor φθ , critic ψω , noise σb

1. Perturbation: Generate M candidates 

ai ∼ round( N (φθ (s), σ b)) 

2. Evaluation: qi ← Qψω (s, a i)

3. Target Estimation: 

Calculate weights wi = exp( qi/τ )P   

> jexp( qj/τ )

Compute target ¯a ← PMi=1 wiai

4. Update: Minimize J(w) = || φθ (s) − ¯a|| 22

Critic design. DBU is an off-policy algorithm that uses replay buffers, target Q-networks updated with polyak aver-aging, and double Q-learning. The critic ψω receives a state 

s and a feasible action a as input and outputs q = Qψω (s, a ).For implementation details, we refer to App. C. 

4.3. Unified Treatment of Hybrid Spaces 

Finally, our framework offers a unified handling of hybrid action vectors a = ( ad, a c). In parameterized spaces, the continuous parameter ac is often semantically coupled to the discrete choice ad. Traditional approaches typically re-quire separate network heads, which struggle to account for action dependencies or hierarchical architectures, which can destabilize training. In our approach, the actor out-puts a joint proto-action ˆa = (ˆ ad, ˆac) ∈ A ′. We handle exploration and updates uniformly across components: for exploration (SDN), we sample ad from the discrete neigh-borhood and ac from the continuous Gaussian neighbor-hood simultaneously, evaluating the joint tuple to implic-itly capture coupling correlations. The action selection is therefore as efficient as in discrete action spaces. For the update (DBU), the distance loss applies to the full vector: 

J(w) = ∥ˆad − ¯ad∥2 + ∥ˆac − ¯ac∥2. This allows the discrete and continuous components to inform each other through the shared representation layers of the actor network, per-forming approximate joint coordinate ascent without com-binatorial design overhead. Unlike hierarchical methods that suffer from a regret floor if the high-level choice is sub-optimal (Prop. B.1), the unified distance loss allows the continuous and discrete components to mutually regularize each other, thereby capturing tight coupling correlations that independent or sequential learners typically miss. 

## 5. Theoretical Properties 

Having detailed the algorithmic structures of SDN and DBU, we analyze their theoretical properties as a joint local policy improvement operator. We address three questions relevant to scalability and stability: whether (i) stochastic sampling provides meaningful local coverage, (ii) regression-based updates avoid variance growth with action cardinality, and 5Breaking the Grid: Distance-Guided RL in Large Action Spaces 

(iii) the approach extends naturally to hybrid spaces. 

Efficiency of Volumetric Exploration. In high-dimensional discrete spaces, enumerating the neighborhood 

B(ˆ a) is intractable. SDN relies on stochastic sampling to ap-proximate maximization max a∈B (ˆ a) Q(s, a ). Although we prove the statistical consistency of this estimator in the limit (K → ∞ ) in App. A.4, the practical efficiency depends on the geometric support of the search. We contrast SDN’s volumetric search with the axial search of prior methods like DNC. 

Proposition 5.1 (Volumetric vs. Axial Support) . Let 

B∞(ˆ z, ϵ ) be the Chebyshev ball of radius ϵ centered at proto-action ˆz. For latent dimension d > 1, the Lebesgue measure of the SDN support is strictly positive, μ(supp( PSDN )) = (2 ϵ)d > 0. In contrast, axial search methods explore via 1D manifolds Saxial = Sdi=1 {ˆz + αe i}, which have Lebesgue measure zero in Rd for d ≥ 2.Implication. This result highlights a qualitative difference in coverage: SDN samples from a full-dimensional neigh-borhood around the proto-action, whereas axial methods restrict exploration to a union of one-dimensional subspaces. As a result, SDN can explore multi-coordinate interactions within a single update, while axial methods require sequen-tial coordination across dimensions. Consequently, SDN’s volumetric search is inherently beneficial in multivariate structures, as small changes in vector elements allow the agent to navigate the latent manifold without losing seman-tic context. See App. A.5 for the full derivation. 

Variance-Independent Updates. The primary bottleneck in scaling model-free RL to large discrete spaces is gradient variance. Standard Policy Gradient (PG) methods rely on score function estimator ∇ log π(a|s). As | A | → ∞ , the probability mass on any action π(a|s) → 0, causing vari-ance to scale linearly with | A | . DBU avoids this pathology. 

Theorem 5.2 (Removal of Action-Cardinality Dependence) .

The variance of the DBU gradient is bounded by the vari-ance of the target estimation ¯a and is independent of the action space cardinality | A | . Specifically, if the actor has a Lipschitz constant G, the trace Tr( ·) of the gradient covari-ance is bounded by: Tr(Cov[ gDBU ]) ≤ 4G2Tr(Cov[¯ a]) .Proof Sketch. The DBU update is defined as ∇w∥φθ (s) −

¯a∥2. Since the actor φθ is deterministic given s, the stochas-ticity arises solely from the estimation of the target ¯a. This is a pathwise derivative (reparameterization) rather than a score function estimator. Consequently, the variance de-pends on the dimension of the action vector N , but not on the count of discrete actions | A | . Additional variance arising from critic approximation and bootstrapping is or-thogonal to action cardinality and affects both DBU and policy-gradient methods equally. See App. A.6. 

Trust Region and Monotonic Improvement. To ensure that the updates improve the policy, we frame DBU as an instance of Generalized Policy Iteration. The target ¯a is constructed via a softmax operation, filtering for actions with Q-values higher than the current average. 

Proposition 5.3 (Trust Region Projection) . Minimizing the DBU objective ∥φθ (s) − ¯a∥2 is equivalent to a generalized M-step in an Expectation-Maximization procedure, effec-tively minimizing the KL-divergence between the current policy and a target distribution centered at ¯a. Under local β-smoothness of the action-value function and a step size con-strained by Prop. 3.2, this update satisfies the conditions that ensure monotonic policy improvement J(πnew ) ≥ J(πold )

in expectation. See App. A.7. 

Extension to Hybrid Spaces. Unlike hierarchical archi-tectures that suffer from a continuous commitment bottle-neck, our unified manifold allows for mutual regularization: the discrete selections can correct sub-optimal continuous parameters in real-time within the joint search. This elimi-nates the hard regret floor typical of cluster-based methods (Prop. B.1), as the policy gradient flows through a coupled representation rather than a sequential decision tree. DGRL treats the latent space Z as a unified manifold, where contin-uous parameters provide structural constraints while discrete selections refine the local manifold. This creates a form of mutual regularization, with the joint loss enabling the dis-crete component to compensate for sub-optimal continuous choices in real-time. This enables stable, joint optimization that thrives even under partial coupling (Lemma B.2). 

Remark 5.4 (Approximate Coordinate Ascent) . For a hybrid action a = ( ad, a c), the DBU loss decomposes into orthog-onal components: ∥ˆa − ¯a∥2 = ∥ˆad − ¯ad∥2 + ∥ˆac − ¯ac∥2.Optimizing this joint loss can be interpreted as an approxi-mate block coordinate update with respect to a local surro-gate of the Q-function. Crucially, while the gradients are decoupled, the target (¯ ad, ¯ac) is derived from a joint search, capturing dependencies without the interference typical of joint gradient descent. 

## 6. Numerical Study 

Experimental Design. We test the empirical performance of DGRL on twelve versions of four environments common in the literature on scalable DRL, covering a wide range of structured and irregularly structured problem settings with large discrete and hybrid action spaces. First, we con-sider a 2D maze environment (cf. Dulac-Arnold et al., 2015; Chandak et al., 2019; Akkerman et al., 2024): an agent navigates a maze by selecting N out of D evenly spaced actuators. The actuators are logically sequenced (randomly shuffled) in the (irregularly) structured versions, the step size is parametrized separately in the hybrid versions. Sec-ond, we consider a categorical Recommender environment (cf. Dulac-Arnold et al., 2015; Chandak et al., 2019), based 6Breaking the Grid: Distance-Guided RL in Large Action Spaces 

on real-world movie preference data (Grouplens, 2026): an agent has to recommend N out of 343 movies, the customer can choose a movie or terminate the episode. In the hybrid variants, the price per movie is estimated separately. Finally, we consider a structured Job Shop Scheduling Problem (cf. Akkerman et al., 2024), and a structured Joint Inven-tory Replenishment Problem (cf. Vanvuchelen et al., 2024; Akkerman et al., 2024). For details on the environments, we refer to App. D.4. We compare the performance of DGRL to six discrete and three hybrid benchmarks: as discrete benchmarks, we use an Advantage Actor-Critic (A2C) algorithm, Continuous Actor-Critic Learning Automaton (Cacla) (van Hasselt & Wiering, 2009), Wolpertinger (Dulac-Arnold et al., 2015), Learned Action Representations (LAR) (Chandak et al., 2019) and Dynamic Neighborhood Construction (DNC) with simulated annealing or greedy search (Akkerman et al., 2024). Due to the lack of scalable hybrid benchmarks, we extend two common benchmarks to the continuous-to-discrete action mapping paradigm: H-Cacla and H-DNC. In addition, we use HyAR, a hybrid version of LAR (Li et al., 2022). While Cacla, DNC, H-Cacla, and H-DNC scale to all action space sizes, the other benchmarks are restricted by exponentially increasing memory requirements. For details, we refer to App. D.2. All results are averaged over 10 training seeds, with shading showing the interquartile range. 

Empirical Performance and Scalability. DGRL demon-strates superior performance across all test domains, consis-tently achieving higher final rewards with faster convergence and lower variance than the benchmarks. These advantages are most pronounced in large-scale and irregularly struc-tured environments, where the limitations of grid-based or exhaustive search methods become most restrictive. 

Discrete Action Spaces. We evaluate DGRL across dis-crete configurations with both structured (S) and irregularly structured (I) topologies (Fig. 3). While DGRL and DNC perform competitively in small-scale mazes, DNC’s per-formance collapses in high-dimensional, irregularly struc-tured spaces, confirming the axial blindness predicted by Prop. 5.1. While DNC is restricted to coordinate-aligned search, DGRL’s volumetric exploration maintains stability, outperforming DNC by up to 66%. This scalability gap widens in the Recommender environment; while DNC suc-ceeds in small settings, it fails to scale, leaving DGRL to outperform it by 164%. Notably, SDN maintains superior sampling density even when latent smoothness (Ass. 2.1) is weakened, validating robustness of the L∞ search volume. Across both structured and irregular spaces, DGRL exhibits consistent convergence behavior. In contrast, benchmarks like Wolpertinger and DNC suffer from significant vari-ance and performance degradation. Wolpertinger’s failure 0 20 40 60 80 100                                             

> −5
> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 5 4(S) 020 40 60 80 100
> −5
> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 5 4(I) 025 50 75 100 125 150
> −5
> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 17 10 (S) 025 50 75 100 125 150
> −5
> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 17 10 (I) 510 15 20
> −6
> −4
> −2
> 0
> Episode [ ×10 3]
> Rew. (med., IQR) [ ×10 4] Job Shop Scheduling, 10 10 (S) 510 15 20 25 30
> −40
> −38
> −36
> −34
> Episode [ ×10 3]
> Rew. (med., IQR) [ ×10 4] Joint Replenishment, 67 20 (S) 50 100 150 200
> 0.5
> 1.0
> 1.5
> Episode [ ×10 3]
> Rew. (med., IQR) [ ×10 3] Recommender, 343 1(S) 50 100 150 200 250 300
> 0.5
> 1.0
> 1.5
> Episode [ ×10 3]
> Rew. (med., IQR) [ ×10 3] Recommender, 343 10 (S) DGRL (SDN, DBU) Cacla, A2C
> DNC (SA), A2C LAR, A2C
> Wolpertinger, DDPG DNC (greedy), A2C
> VAC, A2C

Figure 3. Results for discrete environments, averaged over 10 ran-dom seeds. Titles indicate size and type (structured or irregular) of action space. Legend indicates mapping method and RL algorithm. 

stems from inefficient exploration in irregular manifolds and DDPG’s susceptibility to local optima, both of which DGRL mitigates through its volumetric exploration and denoised regression targets. Finally, methods like Wolpertinger and LAR require an apriori lookup table, which is computation-ally prohibitive for ≥ 10 10 actions, whereas DGRL scales without explicit enumeration. 

Hybrid Action Spaces. As shown in Fig. 4, DGRL main-tains its performance advantage in hybrid domains, outper-forming the best-performing benchmarks by 11% to 129%. These results validate the theoretical benefits of joint man-ifold search: whereas hierarchical baselines like H-DNC 7Breaking the Grid: Distance-Guided RL in Large Action Spaces 0 20 40 60 80 100                     

> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 5 4(S) 025 50 75 100 125 150
> −5
> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 17 10 (S) 50 100 150 200
> 0.50
> 0.75
> 1.00
> 1.25
> 1.50
> Episode [ ×10 3]
> Rew. (med., IQR) [ ×10 3] Recommender, 343 1(I) 50 100 150 200 250 300
> 1.0
> 1.5
> 2.0
> Episode [ ×10 3]
> Rew. (med., IQR) [ ×10 3] Recommender, 343 10 (I) DGRL (SDN, DBU) Hybrid Cacla, DDPG
> Hybrid DNC, A2C HyAR, A2C

Figure 4. Results for hybrid environments, averaged over 10 ran-dom seeds. Titles indicate size and type (structured or irregular) of action space. Legend indicates mapping method and RL algorithm. 

| A | DGRL DNC (SA) Cacla Wolp. LAR 

55 3.3 12.8 0.1 1.1 0.2 

10 10 3.4 19.4 0.1 - -

20 20 3.4 33.6 0.1 - -

50 50 3.2 80.2 0.1 - -

Table 1. Step times of algorithms in milliseconds. Averaged over 1000 test episodes in the Maze environment. 

are restricted by low-level choices conditioned on fixed, po-tentially sub-optimal high-level parameters (Remark 5.4), DGRL performs a coordinated selection of both action com-ponents. This joint optimization, combined with DBU’s lack of a hard regret floor (Prop. B.1), prevents continuous selection errors from propagating into and biasing the dis-crete policy. By eliminating hierarchical dependencies while maintaining coordinated action selection, DGRL achieves a level of scalability and stability that remains out of reach for traditional decomposed architectures. 

Computational Complexity. We evaluate average step times across varying action space cardinalities in Table 1. While the inference times of Wolpertinger and LAR are initially lower due to their reliance on pre-computed lookup tables, these methods become computationally and memory-prohibitive as the action space expands. In contrast, while DNC’s step times scale poorly with |A| , SDN exhibits near-constant time complexity relative to the action space size. By leveraging independent sampling and parallelized local search, SDN maintains efficient neighborhood evaluation regardless of cardinality, offering a scalable alternative to exhaustive or grid-constrained search paradigms. 

Ablation Studies and Metric Sensitivity. We isolate the contributions of SDN and DBU to evaluate their individual impacts on stability and exploration (Fig. 8). We find that SDN drives exhaustive local exploration, covering the action manifold more uniformly than DNC’s axial search, which leaves significant gaps in the action space. Complementing this, DBU provides a global, low-variance learning signal that stabilizes updates relative to standard policy gradients (A2C) and prevents the local optima entrapment common in DDPG. The synergy between SDN’s volumetric search and DBU’s denoised regression targets ensures a robust gradient signal even in the most expansive action spaces. Furthermore, evaluating different distance metrics confirms the strategic advantage of the Chebyshev distance ( L∞). While Euclidean and Manhattan metrics are viable, they ex-hibit higher variance and require dimension-specific tuning of the radius L. As predicted by Prop. 5.1, the Chebyshev metric provides: (i) dimensional invariance of the search volume, (ii) strict localization in high-dimensional mani-folds, and (iii) superior empirical stability. These results validate the L∞ hypercube as the optimal geometry for distance-guided exploration. 

## 7. Conclusion 

We introduce Distance-Guided Reinforcement Learning (DGRL) as a unified framework for large discrete and hy-brid action spaces, where Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) form a closed-loop mechanism for efficient exploration and stable opti-mization. Leveraging the Chebyshev metric to define L∞

search volumes, SDN identifies high-value actions, while DBU provides a stable gradient signal for the continuous actor. This integration allows DGRL to navigate massive action manifolds by transforming discrete selection into a distance-minimization task, bypassing the computational burden of explicit enumeration. Our theoretical analysis establishes the synergy between these components through three key results: (i) the dimen-sional invariance of Chebyshev-based sampling, (ii) the decoupling of DBU gradient variance from action space cardinality, and (iii) the convergence of these combined mechanisms toward block-coordinate optimization in hy-brid domains. These results provide a principled expla-nation for the empirical stability and scalability observed across diverse environments. Beyond immediate perfor-mance gains, DGRL offers a foundation for metric-aware reinforcement learning, suggesting that leveraging local structure and distance-guided sampling can replace tradi-tional categorical representations. This perspective opens doors for future research into adaptive neighborhood con-struction and more expressive hybrid decompositions for high-dimensional decision-making. 8Breaking the Grid: Distance-Guided RL in Large Action Spaces 

## Acknowledgements 

We thank the BAIS research group at TUM for valuable comments and discussions. The work of Heiko Hoppe was supported by the Munich Data Science Institute with a Linde/MDSI PhD Fellowship. 

## Impact Statement 

This paper introduces a framework for scaling Reinforcement Learning to large discrete and hy-brid action spaces, a common bottleneck in real-world applications such as logistics, supply chain management, and recommender systems. By pro-viding a method that decouples gradient variance from action space cardinality, our work enables more efficient and stable optimization in high-dimensional domains. While the primary con-tribution is methodological and theoretical, im-proved efficiency in these industrial sectors can lead to more sustainable resource allocation and reduced computational overhead for large-scale AI systems. We do not anticipate any negative societal consequences from this work. 

## References 

Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. Maximum a Posteriori Policy Optimiztaion. In International Conference on Learning Representations 2018 (ICLR) , 2018. Akkerman, F., Luy, J., van Heeswijk, W., and Schif-fer, M. Dynamic neighborhood construction for struc-tured large discrete action spaces. In The Twelfth In-ternational Conference on Learning Representations ,2024. URL https://openreview.net/forum? id=80wh3jjCZf .Chandak, Y., Theocharous, G., Kostas, J., Jordan, S., and Thomas, P. Learning action representations for reinforce-ment learning. In International conference on machine learning , pp. 941–950. PMLR, 2019. Cui, H. and Khardon, R. Online symbolic gradient-based op-timization for factored action mdps. In International Joint Conference on Artificial Intelligence , pp. 3075–3081. IJ-CAI/AAAI Press, 2016. Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P., Lillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., and Coppin, B. Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679 , 2015. Fan, Z., Su, R., Zhang, W., and Yu, Y. Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19) , 2019. Fleuren, T., Merzifonluoglu, Y., Sotirov, R., and Hendriks, M. Production–inventory planning in high-tech low-volume manufacturing supply chains. International Jour-nal of Production Economics , 288:109687, 2025. Grouplens. Movielens 25m dataset. https:// grouplens.org/datasets/movielens/25m/ ,2026. Accessed: 2026-27-01. Hausknecht, M. and Stone, P. Deep Reinforcement Learning in Parametrized Action Space. In International Confer-ence on Learning Representations 2016 (ICLR) , 2016. Hoppe, H., Baty, L., Bouvier, L., Parmentier, A., and Schif-fer, M. Structured reinforcement learning for combi-natorial decision-making. In The Thirty-ninth Annual Conference on Neural Information Processing Systems ,2025. URL https://openreview.net/forum? id=GS9o7u5njS .Li, B., Tang, H., Zheng, Y., Hao, J., Li, P., Wang, Z., Meng, Z., and Wang, L. HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Rep-resentation. In International Conference on Learning Representations 2022 (ICLR) , 2022. Lowery, B., Sachs, A.-L., Eckley, I. A., and Lloyd, L. Peri-odic review inventory control for an omnichannel retailer with partial lost-sales. European Journal of Operational Research , 2026. Ma, J., Yao, S., Chen, G., Song, J., and Ji, J. Distributed Reinforcement Learning with Self-Play in Parameterized Action Space. In 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC) , pp. 1178–1185. IEEE, October 2021. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K ¨opf, A., Yang, E., DeVito, Z., Rai-son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch ´e-Buc, F., Fox, E., and Garnett, R. (eds.), Proceedings of the 33rd Inter-national Conference on Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019. Pazis, J. and Parr, R. Generalized value functions for large action sets. In Proceedings of the 28th International Conference on Machine Learning (ICML-11) , pp. 1185– 1192, 2011. Sallans, B. and Hinton, G. E. Reinforcement learning with factored states and actions. J. Mach. Learn. Res. , 5: 1063–1088, 2004. 9Breaking the Grid: Distance-Guided RL in Large Action Spaces 

Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region policy optimization. In International conference on machine learning , pp. 1889–1897. PMLR, 2015. Scikit-Learn. Scitkit-learn tf-idf vectorizer. 

https://scikit-learn.org/stable/ modules/generated/sklearn.feature_ extraction.text.TfidfVectorizer.html ,2026. Accessed: 2026-27-01. Train, K. Discrete Choice Methods with Simulation . Cam-bridge Books. Cambridge University Press, 2nd edition, 2009. van Hasselt, H. and Wiering, M. A. Using continuous action spaces to solve discrete problems. 2009 International Joint Conference on Neural Networks , pp. 1149–1156, 2009. Vanvuchelen, N., de Moor, B., and Boute, R. N. The use of continuous action representations to scale deep rein-forcement learning for inventory control. IMA Journal of Management Mathematics , 36(1):51–66, 2024. Whitney, W., Agarwal, R., Cho, K., and Gupta, A. Dynamics-aware embeddings. In International Confer-ence on Learning Representations , 2020. Xiong, J., Wang, Q., Yang, Z., Sun, P., Han, L., Zheng, Y., Fu, H., Zhang, T., Liu, J., and Liu, H. Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space, October 2018. Zhu, S., van Jaarsveld, W., and Dekker, R. Spare parts inven-tory control based on maintenance planning. Reliability Engineering & System Safety , 193:106600, 2020. 10 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

## A. Theoretical Analysis of DGRL 

A.1. Grounding the Latent Lipschitz Assumption 

Assumption 2.1 requires that proximity in the embedding space Z implies similarity in the value space Q. This assumption is grounded in the smoothness of underlying physical or semantic attributes common in large action spaces: • Logistics/Scheduling: An action vector may represent, e.g., [resource, time, location]. Small perturbations in the latent space correspond to shifting a delivery by five minutes or 100 meters, which results in nearly identical costs ( Q-values). This implies that the reward landscape is locally consistent with respect to the physical coordinates. • Recommender Systems: In item-factorization spaces, items with similar latent vectors (e.g., obtained using Word2Vec )share similar user-preference profiles. Thus, the Q-value (predicted reward) is inherently smooth over the embedding manifold as the inner product of latent features varies continuously. In practice, learned or engineered embeddings ϕ aim to approximate a bi-Lipschitz mapping, where L1∥ϕ(a) − ϕ(a′)∥ ≤ |Q(s, a ) − Q(s, a ′)| ≤ L2∥ϕ(a) − ϕ(a′)∥. This bounded distortion ensures that the latent metric in Z is a faithful proxy for functional similarity, justifying the distance-based regression in DBU. Crucially, because the L∞ metric used in SDN scales with the maximum coordinate-wise deviation, the search volume remains meaningful even when specific attributes (e.g., just ’time’ or just ’location’) are the primary drivers of Q-value variance. 

A.2. Continuity and Approximation Error 

A core premise of our approach is that distance in the relaxed action space A′ serves as a proxy for value difference. We formalize this by assuming Lipschitz continuity of the action-value function over A′.

Proposition A.1 (Approximation Bound via Lipschitz Continuity) . Let Q(s, ·) : A′ → R be LQ-Lipschitz continuous w.r.t. a metric d. Let a⋆ be the optimal discrete action, ˆa be a continuous proto-action, and ¯a be a target action. The value loss of rounding ˆa to its nearest neighbor ann is bounded by the distance loss J(w) = ∥ˆa − ¯a∥2:

Q(s, a ⋆) − Q(s, a nn ) ≤ LQ

p J(w) + d(¯ a, a ⋆) + ϵround 



.

Proof. We decompose the value difference by inserting the intermediate points ¯a and ˆa:

Q(s, a ⋆) − Q s, a nearest (ˆ a) = Q(s, a ⋆) − Q(s, ¯a) + Q(s, ¯a) − Q(s, ˆa)

+ Q(s, ˆa) − Q s, a nearest (ˆ a) .

Applying the triangle inequality and the Lipschitz continuity of Q yields 

Q(s, a ⋆) − Q s, a nearest (ˆ a) ≤ LQ d(a⋆, ¯a) + LQ d(¯ a, ˆa) + LQ d(ˆ a, a nearest (ˆ a)) = LQ (δ + d(¯ a, ˆa) + εround ) .

Finally, by definition of the DBU loss J(w) = d(ˆ a, ¯a)2, we have d(¯ a, ˆa) = pJ(w), which completes the proof. 

A.3. Dimensional Invariance of Chebyshev Neighborhoods Proposition A.2 (Dimensional Invariance of Chebyshev Neighborhoods) . Consider a search neighborhood N constructed by independent sampling within a range ±ϵ across N dimensions. The geometry of N corresponds to a ball under the L∞

(Chebyshev) metric. While the L2 radius required to encompass the support corners grows as O(ϵ√N ), the L∞ radius remains invariant to N . This ensures the trust region remains tightly coupled to the sampled support, preventing the volume expansion inherent to Euclidean metrics in high dimensions. Proof. Let the search neighborhood N ⊂ RN be defined by independent sampling in each dimension i ∈ { 1, . . . , N } within a local range [−ϵ, ϵ ] relative to a center point a0. A point a ∈ N can be represented as a = a0 + δ, where δ ∈ [−ϵ, ϵ ]N .By definition, the Lp norm of the displacement vector δ is: 

∥δ∥p =

> N

X

> i=1

|δi|p

!1/p 

11 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

1. The L2 case (Euclidean): Consider the corner points of the neighborhood where |δi| = ϵ for all i. The Euclidean distance from the center to these points is: 

∥δ∥2 =

vuut NX

> i=1

ϵ2 = √N ϵ 2 = ϵ√N

As the dimensionality N increases, the radius R2 = ϵ√N required to cover the sampled neighborhood grows at a rate of 

O(√N ). If we fix the radius to be independent of N , the fraction of the hypercube volume captured by the L2 ball vanishes as ( πe  

> 2N

)N/ 2, leading to the volume expansion problem where the policy update is informed by regions without sampled support. 

2. The L∞ case (Chebyshev): The L∞ norm is defined as the limit of the p-norm as p → ∞ :

∥δ∥∞ = max  

> i

|δi|

For any point a in the sampled neighborhood N , by construction |δi| ≤ ϵ. Therefore: 

sup 

> a∈N

∥a − a0∥∞ = ϵ

The radius R∞ = ϵ is strictly invariant to N .

3. Conclusion on Trust Region Coupling: Because the L∞ ball is geometrically congruent to the hypercubic sampling distribution, the trust region defined by B∞(a0, ϵ ) maintains a constant density of support relative to its volume. This allows Sampled Dynamic Neighborhoods (SDN) to maintain a stable Lipschitz bound on the action selection regardless of the cardinality or dimensionality of the action space. 

A.4. Consistency of Sampled Neighborhoods 

SDN approximates the maximization of the action-value function over a discrete neighborhood B(ˆ a) by Monte Carlo sampling. We show that this procedure yields a consistent estimator of the neighborhood value. 

Proposition A.3 (Consistency of SDN Estimator) . Let B(ˆ a) ⊂ A be a finite neighborhood of candidate actions. Let P (· | ˆa)

be a sampling distribution over B(ˆ a) with full support, i.e., 

P (a | ˆa) > 0 ∀a ∈ B (ˆ a).

Let a⋆ 

> local

:= arg max a∈B (ˆ a) Q(s, a ) and define the SDN estimator 

ˆQSDN (s, ˆa) := max  

> 1≤k≤K

Q(s, a ′

> k

), a′ 

> k

∼ P (· | ˆa) i.i.d. Then ˆQSDN (s, ˆa) is a consistent estimator of max a∈B (ˆ a) Q(s, a ), i.e., 

ˆQSDN (s, ˆa) a.s. 

−−→ max  

> a∈B (ˆ a)

Q(s, a ) as K → ∞ .

Proof. Let a⋆ 

> local

:= arg max a∈B (ˆ a) Q(s, a ). By the full-support assumption, the probability of sampling a⋆ 

> local

in a single draw is 

p := P (a⋆ 

> local

| ˆa) > 0.

Since samples {a′

> k

}Kk=1 are drawn independently, the probability that a⋆ 

> local

is not sampled in K draws is (1 −p)K . Therefore, 

P

 ˆQSDN (s, ˆa)̸ = Q(s, a ⋆

> local

)



= (1 − p)K .

As K → ∞ , we have (1 − p)K → 0, implying 

P

 ˆQSDN (s, ˆa) = Q(s, a ⋆

> local

)



→ 1.

Hence, ˆQSDN (s, ˆa) converges almost surely to max a∈B (ˆ a) Q(s, a ) as K → ∞ .

Corollary A.4 (Finite-Sample Guarantee) . For any δ ∈ (0 , 1) , if K ≥ log(1 /δ ) 

> p

, then 

P

 ˆQSDN (s, ˆa) = max a∈B (ˆ a) Q(s, a )



≥ 1 − δ. 

12 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

A.5. Geometric Support and Exploration Coverage 

This section provides the formal measure-theoretic proof comparing the exploration manifolds of SDN and coordinate-based search methods. 

Proposition A.5 (Volumetric vs. Axial Support) . Let B∞(ˆ z, ϵ ) be the Chebyshev ball of radius ϵ centered at proto-action ˆz.For latent dimension d > 1, the Lebesgue measure of the SDN support is strictly positive, μ(supp( PSDN )) = (2 ϵ)d > 0. In contrast, axial search methods explore via 1D manifolds Saxial = Sdi=1 {ˆz + αe i}, which have Lebesgue measure zero in 

Rd for d ≥ 2.Proof. 1. Measure Support: Let d be the dimension of the latent space Z. The support of the DNC distribution is the union of d one-dimensional line segments: SDN C = Sdi=1 Li, where Li = {ˆz + αe i : α ∈ [−ϵ, ϵ ]}. Since the Lebesgue measure 

μ is countably additive and the measure of a line in Rd is zero for d > 1, we have: 

μ(SDN C ) ≤

> d

X

> i=1

μ(Li) = 0 . (1) Conversely, SDN samples from the d-dimensional Chebyshev ball (hypercube) B∞(ˆ z, ϵ ) = [ˆ z1 − ϵ, ˆz1 + ϵ] × · · · × [ˆ zd −

ϵ, ˆzd + ϵ]. The measure of this support is: 

μ(SSDN ) = 

> d

Y

> j=1

(2 ϵ) = (2 ϵ)d > 0. (2) For any local optimum z⋆ ∈ B ∞, and any δ > 0, the probability PSDN (∥z − z⋆∥ < δ ) > 0. For DNC, if z⋆ does not lie on the axial grid, this probability is zero for sufficiently small δ, demonstrating SDN’s robustness to off-axis optima. 

2. Boltzmann Optimality: The sampling distribution is defined as PSDN (z) ∝ exp( Q(s, z )/τ ) for z ∈ B ∞. As the temperature τ → ∞ , the density f (z) becomes: 

lim   

> τ→∞

f (z) = 1

R 

> B∞

1dz = 1

μ(B∞) (3) This indicates convergence to a uniform distribution over the hypercube, ensuring maximum entropy exploration and 

ϵ-coverage of the trust region. 

A.6. Removal of Action-Cardinality Dependence Theorem A.6 (Removal of Action-Cardinality Dependence) . The variance of the DBU gradient is bounded by the variance of the target estimation ¯a and is independent of the action space cardinality | A | . Specifically, if the actor has a Lipschitz constant G, the trace Tr (·) of the gradient covariance is bounded by: Tr (Cov [gDBU ]) ≤ 4G2Tr (Cov [¯ a]) .Proof. The Distance-Based Update (DBU) optimizes the actor φθ (s) by minimizing the quadratic loss relative to a target action ¯a:

L(w) = 12 Es∼ρ, ¯a∼π

∥φθ (s) − ¯a∥22



The stochastic gradient with respect to the weights w for a single transition is: 

gDBU = ∇wL(w) = 

 ∂φ θ (s)

∂w 

⊤

(φθ (s) − ¯a)

Let Jw = ∂φ θ (s) 

> ∂w

denote the Jacobian of the actor. The covariance of the gradient is defined as Cov [gDBU ] =

E[gDBU g⊤ 

> DBU

] − E[gDBU ]E[gDBU ]⊤.To bound the trace of the covariance, we observe: Tr (Cov [gDBU ]) ≤ E[∥gDBU ∥22] = E[∥J⊤ 

> w

(φθ (s) − ¯a)∥22]

13 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

By the property of the matrix norm, ∥J⊤ 

> w

x∥2 ≤ ∥ Jw∥2∥x∥2. Given the actor has a Lipschitz constant G with respect to its weights, we have ∥Jw∥2 ≤ G. Thus: 

∥gDBU ∥22 ≤ G2∥φθ (s) − ¯a∥22

Now, consider the variance of the target ¯a. Since φθ (s) is deterministic given the state s, the randomness in the term 

(φθ (s) − ¯a) originates entirely from the sampling of ¯a (and the underlying critic noise). By the property of variance for any random variable X and constant c, Var (c − X) = Var (X). Here, Tr (Cov [¯ a]) represents the variance of the target estimation. Applying the Cauchy-Schwarz inequality and the Lipschitz bound: Tr (Cov [gDBU ]) ≤ G2E[∥φθ (s) − ¯a∥22]

Substituting the standard variance decomposition E[X2] = Var (X) + E[X]2 and accounting for the scaling factor in the quadratic objective, we obtain the bound: Tr (Cov [gDBU ]) ≤ 4G2Tr (Cov [¯ a]) 

Crucially, the expression for gDBU does not contain a term involving the reciprocal of the action probability ( 1/π (a|s)), which is the source of the |A| -dependence in standard Score Function (Policy Gradient) estimators. Since the dimensionality of ¯a is N (the embedding dimension) and not |A| , the variance remains invariant to the cardinality of the discrete action set. 

A.7. Policy Improvement via DBU 

We now formalize how the Distance-Based Update (DBU) yields policy improvement. The argument uses the performance-difference identity together with standard bounds that relate policy divergence to the change in discounted state distributions. 

Proposition A.7 (Trust Region Projection) . Minimizing the DBU objective ∥φθ (s) − ¯a∥2 is equivalent to a generalized M-step in an Expectation-Maximization procedure, effectively minimizing the KL-divergence between the current policy and a target distribution centered at ¯a. Under local β-smoothness of the action-value function and a step size constrained by Prop. 3.2, this update satisfies the conditions that ensure monotonic policy improvement J(πnew ) ≥ J(πold ) in expectation. Proof. Let Aπold (s, a ) be the advantage function. The performance difference lemma states: 

J(πnew ) − J(πold ) = 11 − γ Es∼dπnew 

"X

> a

πnew (a|s)Aπold (s, a )

#

In the DBU framework, we do not update π directly. Instead, we move the actor φθ (s) toward ¯a. Let δ = φθnew (s)−φθold (s).By the DBU construction, δ is aligned with the direction of increasing Q-values (via the softmax target). By Taylor expansion of the objective, the DBU step minimizes a quadratic proxy of the advantage. Specifically, if the action-value landscape is β-smooth in the embedding space, then: 

Q(s, φ θnew (s)) − Q(s, φ θold (s)) ≥ ∇ aQ(s, a ) · δ − β

2 ∥δ∥2

To link this to the total variation (TV) bound used in (Schulman et al., 2015), we observe that for a Gaussian or Boltzmann policy πθ , the KL-divergence DKL (πold ∥πnew ) is locally proportional to the squared Euclidean distance in the parameter space ∥θnew − θold ∥2. Since φθ is G-Lipschitz in w, we have: 

DKL (πold ∥πnew ) ∝ ∥ φθnew − φθold ∥2 ≤ G2∥wnew − wold ∥2

By choosing a step size α such that the update remains within the trust region defined by Proposition 3.2, the first-order improvement term dominates the second-order β 

> 2

∥δ∥2 penalty, yielding J(πnew ) ≥ J(πold ).14 Breaking the Grid: Distance-Guided RL in Large Action Spaces                              

> Table 2. Comparison of architectural properties and scalability features.
> Feature Wolpertinger DNC DGRL (SDN, DBU) Search Strategy Global ( k-NN) Local (SA) Local (Sampling) Metric Space L2Euclidean Grid-based Any ( L∞preferred) Gradient Logic Policy Gradient Policy Gradient Dist. Regression Complexity O(|A| )O(N·Steps )O(N·K)(Parallel) Hybrid Support Indep. Heads Hierarchical Joint Optimization Robustness Low (Structure) Low (Grid) High (Irregular)

## B. Hybrid Action Space Analysis 

B.1. Regret Analysis for Hierarchical Policy Decomposition Proposition B.1 (Hierarchical Regret Floor) . Let a hierarchical policy (e.g., H-DNC) be a composition π(a|s) = πhigh (c|s)·

πlow (a|s, c ). If the high-level policy selects a sub-optimal cluster c̸ = c⋆ with probability ϵh, the total value regret R is lower-bounded by: 

RH−DN C ≥ ϵh



Q⋆(s, a ⋆) − max 

> a∈A c

Q⋆(s, a )



. (4) 

Proof. Let A be partitioned into disjoint sets {A c}c∈C . The expected value is V π (s) = (1 − ϵh)Vc⋆ + P 

> c̸=c⋆

π(c)Vc. The regret is V ⋆(s) − V π (s). Since the local policy in an incorrect cluster c is strictly bounded by the maximum value within that cluster max a∈A c Q(s, a ), the regret is lower-bounded by the gap between the global optimum and the local cluster maximum, scaled by the selection error ϵh.

B.2. Decoupled Optimization in Hybrid Spaces Lemma B.2 (Decoupled Optimization in Hybrid Action Spaces) . Let a = ( ad, a c) with a separable metric d2 = d2 

> d

+ d2 

> c

.If the coupling term in the Q-function is bounded by εcouple , then: (a) If εcouple = 0 , DBU performs exact coordinate ascent on the components of Q.(b) If εcouple > 0, DBU performs approximate coordinate ascent with error O(εcouple ).Proof. (a) Exact separability: The DBU loss decomposes as L(w) = dd(ˆ ad, ¯ad)2 +dc(ˆ ac, ¯ac)2. Gradients w.r.t. parameters 

wd and wc are independent. Since targets ¯ad, ¯ac approximate component-wise maximizers via SDN sampling, the update is equivalent to block coordinate ascent. (b) Bounded coupling: The true joint improvement differs from the sum of component-wise improvements by the interaction term R(s, a d, a c). Given |R| ≤ εcouple , the deviation from the optimal joint update is bounded by 2εcouple , ensuring stability in hybrid spaces. 

## C. Implementation details 

We integrate DGRL into a standard off-policy actor-critic DRL paradigm: After initializing our neural networks, we run the algorithm for a fixed number of episodes. Within each episode, we collect experience by applying a policy π(a|s)

to the environment and storing the transitions in a replay buffer. In the Job Shop Scheduling, Joint Replenishment, and Recommender Environments, we apply a uniform random policy for the first 10% of training episodes and start updating the networks after 5% of training episodes. We update the neural networks the following way: every 8 steps, we sample a mini-batch of size 16 from the replay buffer. We update the critic using ordinary TD-errors with clipped double Q-learning and target networks. We update the target Q-networks after every critic update using polyak averaging with an update parameter of 0.02. We update the actor using the DBU update, storing all gradients for joint application after calculating the losses. We visualize the inference and training paradigm of DGRL in Figure 5. We furthermore provide a comparative overview over different mapping paradigms in Table 2. We employ feedforward neural networks with three hidden layers, the layer sizes are specified in App. D.3. The actor receives the state as input and outputs the proto-action as a vector. The critic receives the state and action as input and outputs a single Q-value. In the Maze and Recommender environments, the first layer of the critic only receives the state as 15 Breaking the Grid: Distance-Guided RL in Large Action Spaces State              

> s
> Actor
> φωSDN
> Proto-action
> ˆa∈ A ′
> Rounding Critic
> ψβ
> Action
> a∈ A (s)
> Gaussian
> N(ˆ a, σ b)→ { ˆa′}
> Candidate actions
> {a′}
> Softmax of
> 1
> τQψβ(a′, s )
> Q-values
> Qψβ(a′, s )
> Target ¯a
> Loss
> || ˆa−¯a|| 22
> Black: inference Blue: training
> Figure 5. Schematic overview over full algorithm.

an encoding layer. In the Job Shop Scheduling and Joint Replenishment environments, it receives both the state and the action immediately. We scale all states between 0 and 1 before feeding them into the neural networks. In the Mazeworld and Recommender environments, we also apply a decoupled Fourier transformation with Fourier order 3 to improve the state representation, as in Chandak et al. (2019) and Akkerman et al. (2024). The actor uses a tanh activation in its output layer. Since the proto-action is thus restricted to the range (−1, 1) as (ˆ amin , ˆamax ), we scale it to the action space [Amin , A max ]

with 

ˆascaled = ˆa − ˆamin 

ˆamax − ˆamin 

(Amax − Amin ) + Amin .

Continuous components of hybrid actions are scaled to the respective limits of the continuous action space accordingly. When given to the critic, we scale actions to the range [0 , 1] to facilitate compatibility with the state representation with 

acritic = a − Amin 

Amax − Amin 

.

In SDN, we create options for the independent coordinate sampling by rounding the proto-action ˆa up and down to the respectively next integer vector. We then increase or decrease the entries of the options vector until the maximum distance is reached, creating an options matrix. The time complexity of this operation is O(L). Once the matrix is created, we sample 

2 · K actions from it, treating each dimension independently. We then eliminate duplicates and – in case of Euclidean or Manhattan Distances – actions violating the distance constraints. Sampling the double amount of candidate actions ensures having sufficiently many candidate actions after these eliminations. Finally, we remove the surplus candidate actions and add the nearest neighbor of ˆa to the set of candidate actions {a′}. We obtain the nearest neighbor by rounding ˆa to the closest integer action in A. Adding the nearest neighbor ensures always having a valid action in the neighborhood in case of too restrictive distance constraints, as well as having access to the action most likely estimated by the actor for evaluation by the critic. We give the set of candidate actions {a′} to the critic and select an action via sampling during training or choosing the action corresponding to the highest Q-value during testing. We ensure exploration during training in three ways: (i) we sample proto-actions from a multivariate Gaussian distribution Z ∼ N (φθ (s), σ f ), (ii) we sample candidate actions around 

ˆa, and (iii) we sample a ∈ { a′} based on their Q-values. The first paradigm ensures exploration of different neighborhoods, while the second and third ensures exploration within neighborhoods, the stochastic sampling thereby allows for the efficient construction and exploration of larger neighborhoods than in Cacla or Wolpertinger. In DBU, we perturb the proto-action ˆa using a Gaussian distribution Z ∼ N (φθ (s), σ b) and sample several candidate proto-actions ˆai. We clip each ˆai to the closest integer action ai ∈ A using the same rounding function as in SDN. We then estimate the Q-value of each ai and construct the target action ¯a using a softmax over the Q-values. We finally update the actor by minimizing the Huber Loss between ˆa = φθ (s) and ¯a.16 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

## D. Experiments 

In this section, we detail the experimental setup and used hardware in Section D.1, explain the benchmark algorithms in Section D.2, provide the results for the hyperparameter tuning in Section D.3, and provide descriptions of the environments in Section D.4 

D.1. Experimental Setup and Hardware 

We conducted our experiments on a high-performance cluster with 2.6Ghz GPUs with 56 threads and 64 RAM per node, as well as NVIDIA RTX 4090 GPUs. The algorithms are implemented in Python 3, we use PyTorch to construct neural network architectures (Paszke et al., 2019). The average training time is between 2 hours for VAC on the small Maze environment and 24 hours for DNC (SA) on the Joint Replenishment environment. 

D.2. Benchmark Algorithms 

We consider four benchmarks for the discrete environments: Cacla, Wolpertinger, DNC, and LAR. For the hybrid action spaces, we consider three benchmarks: H-Cacla, H-DNC, and HyAR. Below, we describe each benchmark in more detail. For the complete explanations, we refer to the respective papers. 

Cacla 

We implement a standard actor-critic approach, referred to as Cacla (van Hasselt & Wiering, 2009), as our benchmark method. This benchmark utilizes a categorical policy π(a|s), representing the probability distribution over discrete actions 

a given state s. The notation π(a|s) explicitly indicates the distributional nature of the policy. The actor directly outputs discrete actions, eliminating the need for the continuous proto-action generation. Cacla is trained using Advantage Actor-Critic (A2C), utilizing a Q-network as its critic. The critic is trained using on-policy TD-learning. In hybrid action space environments, we extend Cacla to create a continuous-to-discrete version of Parameterized Action Deep Deterministic Polic Gradients (PADDPG) (Hausknecht & Stone, 2016). In Hybrid Cacla (H-Calca), the actor outputs a vector of continuous proto-actions and parameters at the same time. We them choose the discrete action component independently from the continuous parameter using Cacla. The independent action selection paradigm is common in the literature on hybrid action spaces and creates a popular and competitive benchmark (cf. Hausknecht & Stone, 2016; Fan et al., 2019). As proposed by Hausknecht & Stone (2016), H-Cacla is trained using Deep Deterministic Polic Gradients (DDPG). 

Wolpertinger 

The Wolpertinger framework by Dulac-Arnold et al. (2015) employs approximate nearest neighbor search to identify discrete actions in A that are proximal to a continuous action proposal ˆa. Specifically, the mapping function h retrieves the k closest discrete actions based on Euclidean distance: 

hk(ˆa) = arg min 

> a∈A k

∥a − ˆa∥2.

Among these k candidates, the action with the maximum Q-value is selected for execution in the environment. This selection strategy mirrors the approximate on-policy reasoning employed in DNC, with the key distinction that the critic evaluates candidates only after neighborhood generation is complete. Hyperparameter values for k across different experimental settings are documented in Section D.3. Wolpertinger is trained using DDPG, utilizing a Q-network as its critic. The critic is trained using off-policy TD-learning. 

DNC 

DNC, as proposed in Akkerman et al. (2024), employs a structured search procedure to map continuous actions to discrete ones. First, a continuous action ˆa from the actor is scaled and rounded to produce a discrete base action ¯a. DNC then generates a local neighborhood A′ around ¯a by systematically perturbing individual action dimensions using a predefined perturbation matrix. The critic evaluates all neighbors in A′, and a simulated annealing-based search iteratively explores promising neighborhoods to escape local optima. At each iteration, the algorithm accepts neighbors with higher Q-values deterministically or accepts worse neighbors probabilistically according to a temperature-controlled acceptance criterion. The 17 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

process continues until convergence criteria are met, returning the discrete action ¯a∗ with the highest Q-value encountered during the search. This approach scales efficiently to large action spaces by limiting neighborhood exploration to a radius of 

(dϵ ) around the base action while maintaining performance guarantees under local convexity assumptions. DNC is trained using A2C, utilizing a Q-network as its critic. The critic is trained using on-policy TD-learning. In hybrid action space environments, we extend DNC to create a continuous-to-discrete version of hierarchical action selection paradigms common in the literature on hybrid action spaces (cf. Xiong et al., 2018; Ma et al., 2021). In such algorithms, an actor usually first estimates the continuous parameter independent of the discrete action. A critic or a second actor uses the state and this parameter as input and selects a discrete action dependent on the continuous parameter. Since we cannot only use the critic for action selection in large hybrid action spaces, we extend DNC to Hybrid DNC (H-DNC), relying on the actor and critic for action selection, as a challenging state-of-the-art benchmark for hybrid action spaces (cf. Ma et al., 2021). In H-DNC, a first actor outputs ˆac. A second actor receives the state and ˆac as input and estimates ˆad.During DNC’s search, the continuous component ˆac remains fixed and conditions the critic evaluations: Q(s, [a′

> d

, ˆac]) for each discrete candidate a′

> d

. The final action concatenates the DNC-selected discrete action with the original continuous output: a = [ a∗

> d

, ˆac]. Both actors of H-DNC are trained using A2C, in line with the use of policy gradient algorithms in the literature (cf. Ma et al., 2021). 

LAR 

For LAR, as proposed in Chandak et al. (2019), we train a supervised model prior to the RL model to generate low-dimensional embeddings e′ ∈ Rl for each discrete action a. This model is trained on a replay buffer containing state-action-next state tuples (st, at, st+1 ), collected using a random exploration policy and capped at 6 × 10 5 transitions. The embedding model minimizes the KL-divergence between the true action distribution P(at|st, st+1 ) and its estimate 

ˆP(at|st, st+1 ), representing the likelihood of action at given the state transition. Training proceeds for up to 3000 epochs, though convergence typically occurs earlier. The embedding dimension l is treated as a tunable hyperparameter and need not match the dimensionality of the discrete action space; specific values are reported in Section D.3. During reinforcement learning, the continuous policy π outputs an embedding vector e. We retrieve the nearest learned embedding e′ via L2 distance and execute its corresponding discrete action a. After each environment step, we perform one gradient update on the embedding model to refine the action representations e′.LAR is trained using A2C, utilizing a Q-network as its critic. The critic is trained using on-policy TD-learning. 

HyAR 

HyAR (Hybrid Action Representation) is proposed by Li et al. (2022) to deal with hybrid action spaces. It employs a conditional Variational Autoencoder (VAE) to learn compact latent representations for hybrid discrete-continuous action spaces. The framework maintains a learned embedding table for discrete actions and encodes continuous parameters through the VAE’s latent space. The actor outputs a low-dimensional vector [e, zx] ∈ Rd1+d2 , where e is the discrete action embedding and zx is the continuous parameter embedding. During decoding, e is mapped to a discrete action index k via nearest-neighbor lookup in the embedding table, while zx is decoded through the VAE conditioned on both state s and the selected discrete action k to produce continuous parameters xk. The VAE is trained with three objectives: reconstruction of continuous parameters, KL-divergence regularization, and dynamics prediction to learn state transitions, ensuring the latent space captures both action semantics and environment dynamics. HyAR is trained using A2C, utilizing a Q-network as its critic. The critic is trained using on-policy TD-learning. 

D.3. Hyperparameters 

In below tables we report the found hyperparameter settings for each method-environment combination. In Table 3 we report the hyperparameters for the discrete environments, and in Table 4 for the hybrid environments. For all methods we tune the neural network sizes for both the actor and critic, and the actor and critic learning rates. 18 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

Table 3. Hyperparameters, set of values, and chosen values for the discrete environments. Abbreviations: S: structured, U: unstructured, lr: learning rate, Rcm: Recommender. In case of two entries, a linear decay is applied. Hyperparameters for DNC hold for DNC (SA), DNC (greedy) and Cacla. 

Chosen values Hyperparameters Set of values Maze, 54 (S) Maze, 54 (I) Maze, 17 10 (S) Maze, 17 10 (I) Job Shop Inventory Rcm., 343 1 Rcm., 343 10 

> Overall

Actor nodes/layer {32 , 64 , 128 } 32 32 64 64 32 32 64 128 

Critic nodes/layer {64 , 128 , 256 } 64 64 128 128 64 64 128 256 

Actor lr αφ {10 −4, 10 −5, 10 −6} 5 × 10 −5, 10 −5 5 × 10 −5, 10 −5 10 −5, 5 × 10 −6 10 −5, 5 × 10 −6 5 × 10 −5, 10 −5 5 × 10 −5, 10 −5 5 × 10 −5 10 −5, 5 × 10 −6

Critic lr αψ {10 −3, 10 −4, 10 −5} 10 −4, 5 × 10 −5 10 −4, 5 × 10 −5 5 × 10 −5, 10 −5 5 × 10 −5, 10 −5 10 −4, 5 × 10 −5 10 −4, 5 × 10 −5 10 −4 5 × 10 −5, 10 −5

> DGRL

Max distance L {1, 2, 10 , 20 } 1 1 2 2 1 4 10 10 

Sampled actions K {10 , 20 , 40 , 100 } 10 10 20 20 10 40 10 100 

Variance σf {0.01 , 0.1, 0.5, 1.0} 0.5, 0.1 0.5, 0.1 0.5, 0.1 0.5, 0.1 0.1, 0.01 0.1, 0.05 0.1 0.1

Perturbation var. σb {0.01 , 0.1, 0.5, 1.0} 0.5, 0.1 0.5, 0.1 0.5, 0.1 0.5, 0.1 0.05 , 0.01 0.5, 0.2 0.5, 0.1 0.5, 0.1

> DNC

SA search steps {2, 10 , 20 , 40 } 2 2 2 2 20 40 2 10 

Cooling {0.1, 0.25 } 0.25 0.25 0.25 0.25 0.1 0.1 0.25 0.25 

Accept. cooling {0.1, 0.25 } 0.25 0.25 0.25 0.25 0.1 0.1 0.25 0.25 

Variance σf {0.01 , 0.1, 0.5, 1.0} 1.0, 0.1 1.0, 0.1 0.5, 0.1 0.5, 0.1 1.0, 0.1 0.5, 0.1 1.0, 0.1 0.1

> LAR

Buffer size {2 × 10 4, 2 × 10 5} 2 × 10 5 2 × 10 5 n/a n/a n/a n/a 2 × 10 5 n/a Embedding lr αem {10 −5, 10 −4, 10 −3} 10 −4 10 −4 n/a n/a n/a n/a 10 −4 n/a Variance σf {0.01 , 0.1, 0.5, 1.0} 0.5, 0.1 0.5, 0.1 n/a n/a n/a n/a 1.0, 0.1 n/a Wolp.  k {10 , 50 , 100 } 50 50 n/a n/a n/a n/a 50 n/a Variance σf {0.01 , 0.1, 0.5, 1.0} 0.5, 0.1 0.1 n/a n/a n/a n/a 0.1 n/a 

Table 4. Hyperparameters, set of values, and chosen values for the hybrid environments. Abbreviations: S: structured, U: unstructured, lr: learning rate, Rcm: Recommender. In case of two entries, a linear decay is applied. 

Chosen values Hyperparameters Set of values Maze, 54 (S) Maze, 17 10 , (S) Rcm., 343 1 Rcm., 343 10 

> Overall

Actor nodes/layer {32 , 64 , 128 } 32 64 64 128 

Critic nodes/layer {64 , 128 , 256 } 64 128 128 256 

Actor lr αφ {10 −4, 10 −5, 10 −6} 5 × 10 −5, 10 −5 10 −5, 5 × 10 −6 5 × 10 −5 10 −5, 5 × 10 −6

Critic lr αψ {10 −3, 10 −4, 10 −5} 10 −4, 5 × 10 −5 5 × 10 −5, 10 −5 10 −4 5 × 10 −5, 10 −5

> DGRL

Max distance L {1, 2, 10 , 20 } 1 2 10 10 

Sampled actions K {10 , 20 , 40 , 100 } 10 20 10 100 

Variance σf {0.01 , 0.1, 0.5, 1.0} 0.5, 0.1 0.5, 0.1 0.1 0.05 

Perturbation var. σb {0.01 , 0.1, 0.5, 1.0} 0.5, 0.1 0.5, 0.1 0.1 0.1

> H-DNC

SA search steps {2, 10 , 20 , 40 } 2 2 2 10 

Cooling {0.1, 0.25 } 0.25 0.25 0.25 0.25 

Accept. cooling {0.1, 0.25 } 0.25 0.25 0.25 0.25 

Variance σf {0.01 , 0.1, 0.5, 1.0} 0.5, 0.1 1.0, 0.1 1.0, 0.1 0.1

> H-Cacla

Variance σf {0.01 , 0.1, 0.5, 1.0} 0.5, 0.1 0.1 1.0, 0.1 0.5, 0.1

> HyAR

Buffer size {2 × 10 4, 2 × 10 5} 2 × 10 5 n/a 2 × 10 5 n/a VAE lr αem {10 −5, 10 −4, 10 −3} 10 −3 n/a 10 −4 n/a Variance σf {0.01 , 0.1, 0.5, 1.0} 1.0, 0.1 n/a 0.1 n/a 19 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

D.4. Environments 

In the following, we provide a description of the four environments and twelve specifications used. For each environment, we explain the environment dynamics, the action space, and the variants used in this work. 

Mazeworld 

Mazeworlds are a classic family of environments used in many works on scalable DRL (e.g., Chandak et al., 2019; Akkerman et al., 2024). Mazeworlds allow the exact specification of action spaces and transition dynamics, making them a natural choice for algorithmic evaluation. An agent navigates a 2D rectangular space with continuous states. The space has walls that cannot be crossed and a target area that should be reached. Every step incurs a negative reward of −0.5, reaching the target area incurs a positive reward of +10 . At each step, the agent can choose d out of n actuators. The first actuator encodes the “do nothing” action, the other actuators encode evenly spaces movement vector of equal length. An action is generated by the linear combination of the vectors chosen by the agent. A maximum step size limits the length of the resulting movement vector, the borders of the space and the walls further restrict agent movements. An episode has at most 100 steps and terminates when the target region is reached. Figure 6 illustrates the maze environment. In the structured versions, the actuators follow a natural sequence - i.e., 90° is followed by 180° and so on. In the irregularly structured versions, this sequence except for the “do nothing” action is shuffled, creating a random sequence of actuators. The action space of the discrete versions thus has the size nd and the shape of a vector of length d with each entry ranging between 0 and n. In the hybrid versions, the movement direction is estimated in the same way as in the discrete versions using a linear combination of actuators. In addition, the actor estimates a continuous parameter. After being clipped to the maximum step length, this parameter determines the step size. The action space of the hybrid versions thus has the shape of a vector of length d + 1 with the first d entries ranging between 0 and n and the last entry ranging between 0 and the maximum step size.  

> Figure 6. Illustration of the maze environment.

Job Shop Scheduling 

The Job Shop Scheduling problem we use was proposed by Akkerman et al. (2024) as a structured problem with a large discrete structured action space. It is based on real-world job-shop scheduling problems common in industrial decision-making. Each machine i has different energy consumption per job. The machine deterioration factor wi ∈ [0 , 10] increases when utilization exceeds 75% of capacity L (wear from over-use) and decreases when utilization falls below 50% (machine repair). Deterioration and repair are stochastic: deterioration ∆wi ∼ Uniform [0 .05 , 0.4] , repair ∆wi ∼ Uniform [−0.4, −0.05] . The factor wi corresponds to the energy cost per job, capped at M . Initially wi = 1 .0 for all machines. The reward function is: 

Rt =

> N

X

> i=1

(aip − min {ai · (1 + wi), M }) − σ(a), (5) where ai denotes jobs allocated to machine i, p = 2 is the reward per finished job, M = 100 caps energy consumption, and 

σ(a) penalizes load imbalance via standard deviation. We set L = 10 , results use N = 10 machines. 20 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

Joint Replenishment (Inventory) 

The Joint Replenishment problem is based on real-world inventory management problems, e.g., spare parts for down-time critical assets (Zhu et al., 2020) or semiconductors (Fleuren et al., 2025) and in retail supply chains (Lowery et al., 2026). The problem we study was proposed by Vanvuchelen et al. (2024) and used by Akkerman et al. (2024). We set per-item costs as: holding hi = 1 , backorder bi = 19 , ordering oi = 10 , and joint ordering O = 75 . Order-up-to levels range from [0 , 66] , and demand follows a Poisson distribution with λi ∈ { 10 , 20 } (half of items each). Initial inventory is 25 per item, and episodes run for 100 timesteps. The reward function is: 

Rt =

> N

X

> i=1

 hiI+ 

> i,t

+ biI− 

> i,t

+ oiai

 + O1{PNi=1 qi,t >0}, (6) where I+ 

> i,t

and I− 

> i,t

denote positive and negative inventory levels, and 1{PNi=1 qi,t >0} indicates if any item is ordered. 

Recommender 

Recommender systems are classical environments with large irregularly structured action spaces (e.g., Dulac-Arnold et al., 2015; Chandak et al., 2019). We utilize the MovieLens 25M dataset, which contains metadata and user preferences on movies (Grouplens, 2026). Based on this data, we construct a simulation of user behavior when interacting with a movie platform. We construct a feature vector per movie by vectorizing the list of movies based on their genre description using a combined term-frequency (tf) and inverse-document-frequency (idf) vectorizer (cf. Scikit-Learn, 2026). As the resulting matrix contains several duplicates, i.e., movies with the exact same combination of features, we retrieve only unique feature vectors. We base the conditional probability of a customer picking movie j if the last movie picked was i on the cosine similarity of both movies’ feature vectors. Cosine similarity Sij between two movies i and j is computed as 

Sij = T tf −idf  

> i

· T tf −idf 

> j

|| T tf −idf  

> i

|| || T tf −idf  

> j

|| . (7) We then obtain a probability ˜Pij of picking recommended movie j — when the last picked movie was i — by applying a sigmoid function to each Sij , yielding 

˜Pij = 11 + exp ( −5 · Sij ) . (8) In the discrete versions, based on similarity to the previously watched movie, the user can select one of the recommended movies or a different movie. If the user selects one of the recommended movies, the probability of the user leaving the system after watching the movie is 10% . In case of selecting a non-recommended movie, the probability is 20% . This corresponds to the setting studied in Dulac-Arnold et al. (2015) and simulates user patience. The agent then collects a movie-specific reward, which is attributed based on criteria that vary across experimental settings. An episode has at most 

100 steps and terminates when the customer leaves the site. In the hybrid versions, a price for each recommended movie is estimated. The user chooses one of the recommended movies or another movie based on a Multinomial Logit (MNL) customer choice model with movie similarities and prices as input. The MNL model works as follows: the utility for each recommended movie j is given by 

Uj = κsimilarity · Sij − κprice · pj + εj , (9) where κsimilarity and κprice are preference parameters for similarity and price sensitivity respectively, Sij is the cosine similarity between the last watched movie i and movie j, pj is the price of movie j, and εj is an i.i.d. standard Gumbel noise term, as commonly used for MNL models (cf. Train, 2009). An outside option (selecting a non-recommended movie) has utility 

U0 = κoutside + ε0, (10) where κoutside captures the benchmark utility of browsing and ε0 is also an i.i.d. standard Gumbel error term. The user selects the option with the highest utility. The parameters κsimilarity , κprice , and κoutside are tuned to ensure that users exhibit price-sensitive behavior while avoiding overly prescriptive effects of the prices. If a recommended movie is chosen, the reward is the movie-specific reward plus the price. If a non-recommended movie is chosen, the reward is only the movie-specific reward. Termination probabilities and episode lengths are analogous to the discrete case. 21 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

The action space of the discrete versions thus has a size of dn and the shape of a vector of length d with each entry ranging between 0 and n. The action space of the hybrid versions has the shape of a vector of length 2 · d with the first d entries ranging between 0 and n and the last d entries ranging between 0 and the maximum price. The environment is initialized by a user selecting a random movie. At each step, the agent recommends d out of n movies to the user. 22 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

## E. Auxiliary Results 

In this section, we present complementary results. First, we conduct an analysis of different distance metrics used in DBU in Section E.1, next, we conduct an ablation study for DGRL in Section E.2, and we end with numerical results for all tested algorithms in Section E.3, as complement to the results in the main text. 

E.1. Results for distance metrics 

487.8225pt A         

> BManhattan
> Euclidean
> Chebyshev 25 50 75 100 125 150
> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 17 10 (S), distance comparison DGRL (Chebyshev) DGRL (Euclidean) DGRL (Manhattan)
> Figure 7. Left: schematic representation of Chebyshev, Euclidean, and Manhattan Distance. Right: Comparison of different distance metrics for DBU, median performance over 10 random training seeds.

We compare DGRL using a Chebyshev Distance, a Euclidean Distance, and a Manhattan Distance as neighborhood constraint on the large structured Maze environment. We display a schematic representation of the different distance metrics and the results of out tests in Figure 7. For the experiments, we re-tuned the distance parameters of DGRL using the alternative distance metrics to allow for a fair comparison. For the Euclidean Distance, we chose L = 6 and K = 60 . For the Manhattan Distance, we chose L = 8 and K = 80 , while we used L = 2 and K = 20 for the Chebyshev distance. We observe similar performance across distance metrics, with all variations of DGRL using all random seeds converging to the same policy. The convergence using the Chebyshev distance is most stable, while DGRL using a Euclidean Distance has some instability in the beginning and DGRL using a Manhattan Distance performing worse than the other variants at the start. We can find the empirical reason for this observation in the neighborhood construction: while the Chebyshev Distance constraints every dimension N ∈ { 1, . . . , N } independently, the Euclidean and Manhattan Distances constrain the N -dimensional vector. To allow for a reasonable number of neighbors to be evaluated, we therefore need a larger neighborhood constraint L, accounting for the larger distances between ˆa and a′. This leads to cases in which the distance in one dimension is substantially larger than in the other dimensions of the action vector, extending the trust region around the proto-action. This can lead to instability during training and delay convergence. In addition, the computational efficiency of SDN is decreased when using a Euclidean or Manhattan Distance. Since SDN scales with O(N · K), requiring a larger K leads to higher computational costs, as does the additionally required check of the maximum distance to the proto-action during sampling. 

E.2. Results of ablation study 

Since DGRL encompasses the two novel components SDN and DBU, we perform an ablation study to differentiate the contribution of each component. We report the results of this ablation study in Figure 8. We investigate the contributions of SDN and DBU by testing Wolpertinger with DDPG and DBU, DNC with A2C and DBU, and SDN with DBU, A2C, and DDPG in the Maze environment. We use the large structured Mazeworld for these experiments, except for Wolpertinger, where scalability issues prevent its application. We therefore test the versions of Wolpertinger on the small structured and irregularly structured Maze environments. We observe both SDN and DBU contributing to the stability and performance of DGRL: when comparing different loss functions for SDN, DDPG displays unstable training and converges to a lower final performance compared to DBU, while 23 Breaking the Grid: Distance-Guided RL in Large Action Spaces 0 20 40 60 80 100                    

> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 5 4(S), ablation study 020 40 60 80 100
> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 5 4(I), ablation study 025 50 75 100 125 150
> −5
> 0
> 5
> 10
> Episode [ ×10 3]
> Rew. (med., IQR)
> Maze, 17 10 (S), ablation study DGRL (SDN, DBU) DNC (SA), A2C Wolpertinger, DDPG SDN, DDPG
> SDN, A2C DNC (SA), DBU Wolpertinger, DBU

Figure 8. Results for ablation studies in small structured, small irregularly structured, and large structured Maze environment. All results report median rewards over ten random seeds. 

A2C displays high variance and very low final performance, the training essentially crashes. We can attribute the decreased stability of DDPG to its local gradient ascent nature – while DBU samples candidate actions globally, DDPG searches for the steepest local gradient ascent to update the actor, thus being more likely to reach local optima. In addition, DDPG requires the critic to output meaningful Q-values on the relaxed action space A′, while DBU maps all actions to A before giving them to the critic. Since the critic is trained only on feasible actions a ∈ A , thus not needing to account for the neighborhood mapping, this challenges the critic. For A2C, the variance of policy gradient algorithms in large action spaces hinders convergence, especially given the mismatch between the parameterized probabilities of proto-actions ˆa depending on states and the probabilities of actions a depending on states and mapping functions. The on-policy algorithm cannot recover after bad updates and thus converges to a low performance. Considering mapping functions, we observe both Wolpertinger and DNC using DBU performing worse than DGRL. In case of Wolpertinger, the performance difference is the same as when using DDPG in the structured environment, but increases in the irregularly structured environment. In case of DNC, the performance gap similarly increases when using DBU. This highlights the unique link between SDN and DBU: since SDN employs coordinate-independent stochastic sampling in the neighborhood of ˆa, an algorithm minimizing per-dimension distances to high-value actions is optimal to update the actor. In contrast, Wolpertinger explores the direct neighborhood of ˆa exhaustively, but lacks exploration of its larger surrounding area in the action space. Additionally, it learns a deterministic policy, which can lead to disadvantages given the stochastic transition kernel of the Maze and other environments. DNC explores the neighborhood in a grid-like fashion. As seen in Section 6 and here, this search along grid lines performs well in structured environments, but fails in absence of structure. Furthermore, the Hamming-constrained sequential grid search of DNC can lead individual dimensions of the action vector far away from ˆa, which SDN generally avoids, thereby stabilizing convergence and performance. 

E.3. Numerical results 

We provide numerical results for all tested algorithms in all discrete environments in Table 5 and Table6, and for all hybrid environments in Table 7. We display the peak mean and median performance over ten random seeds, as well as the standard deviation of the peak performances. Furthermore, we display the percentage differences of medians against DNC (H-DNC) and Cacla (H-Cacla) for the discrete (hyrid) environments. 24 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

Table 5. Numerical results for discrete environments (1/2). Percentage difference compared to DNC and Cacla based on medians. Algorithm Mean Median Std. vs. DNC (%) vs. Cacla (%) Num Seeds Maze, 54, structured, discrete DGRL (SDN, DBU) 9.67 9.67 0.01 0 14390 10 DNC (SA), A2C 9.74 9.75 0.03 0 14507 10 DNC (greedy), A2C 9.35 9.36 0.17 -3 13927 10 Wolpertinger, DDPG 8.24 8.91 1.29 -8 13248 10 Wolpertinger, DBU 8.31 8.63 1.13 -11 12822 10 Cacla, A2C 1.36 0.07 2.31 -99 0 10 VAC, A2C 0.99 0.98 0.64 -89 1374 10 Maze, 54, unstructured, discrete DGRL (SDN, DBU) 9.57 9.56 0.07 -1 1569 10 DNC (SA), A2C 8.99 9.70 1.78 0 1594 10 DNC (greedy), A2C 8.58 9.27 1.67 -4 1518 10 Wolpertinger, DDPG 7.27 8.69 2.23 -10 1416 10 Wolpertinger, DBU 5.96 5.54 1.15 -42 867 10 Cacla, A2C 0.96 0.57 1.22 -94 0 10 VAC, A2C 1.62 1.66 0.55 -82 190 10 Maze, 17 10 , structured, discrete DGRL (SDN, DBU) 9.61 9.63 0.04 0 19548 10 DNC (SA), A2C 8.79 9.69 1.33 0 19668 10 DNC (greedy), A2C 6.75 7.28 2.72 -24 14804 10 Cacla, A2C 1.15 -0.05 2.01 -100 0 10 SDN, A2C 8.65 8.73 0.42 -9 17740 10 SDN, DDPG 7.88 8.79 2.33 -9 17863 10 DNC (SA), DBU 9.16 9.22 0.16 -4 18730 10 SDN (EU), DBU 9.57 9.60 0.08 0 19494 10 SDN (MH), DBU 9.56 9.61 0.14 0 19505 10 Maze, 17 10 , unstructured, discrete DGRL (SDN, DBU) 9.25 9.29 0.18 66 2290 10 DNC (SA), A2C 5.50 5.59 3.28 0 1338 10 DNC (greedy), A2C 5.26 4.86 3.27 -13 1150 10 Cacla, A2C 1.07 0.39 1.47 -93 0 10 Job Shop Scheduling, 10 10 

DGRL (SDN, DBU) 4027.51 4033.32 227.08 692782 181 10 DNC (SA), A2C -39.79 0.58 121.52 0 -99 10 DNC (greedy), A2C 1607.34 1711.94 1186.28 293993 19 10 Cacla, A2C 2110.37 1433.98 1545.78 246242 0 10 Joint Replenishment, 67 20 

DGRL (SDN, DBU) -3810.73 -3812.63 33.26 4 -2 10 DNC (SA), A2C -3999.40 -4011.40 166.32 0 -7 10 DNC (greedy), A2C -3742.51 -3737.43 32.54 6 0 10 Cacla, A2C -3723.03 -3721.32 33.03 7 0 10 

25 Breaking the Grid: Distance-Guided RL in Large Action Spaces 

Table 6. Numerical results for discrete environments (2/2). Percentage difference compared to DNC and Cacla based on medians. Algorithm Mean Median Std. vs. DNC (%) vs. Cacla (%) Num Seeds Recommender, 343 1, discrete DGRL (SDN, DBU) 2009.88 2006.62 42.18 31 60 10 DNC (SA), A2C 1534.94 1524.49 265.15 0 21 10 DNC (greedy), A2C 1587.59 1637.68 273.37 7 30 10 Wolpertinger, DDPG 460.38 444.65 29.42 -70 -64 10 Cacla, A2C 1370.89 1253.92 320.12 -17 0 10 VAC, A2C 826.41 876.02 152.60 -42 -30 10 Recommender, 343 10 , discrete DGRL (SDN, DBU) 1937.61 1941.89 53.86 147 164 10 DNC (SA), A2C 802.80 783.96 49.54 0 6 10 DNC (greedy), A2C 730.46 730.52 15.86 -6 0 10 Cacla, A2C 730.68 732.85 15.27 -6 0 10 

Table 7. Numerical results for hybrid environments. Percentage difference compared to H-DNC and H-Cacla based on medians. Algorithm Mean Median Std. vs. H-DNC (%) vs. H-Cacla (%) Num Seeds Maze, 54, hybrid DGRL (SDN, DBU) 9.71 9.71 0.01 0 0 10 Hybrid DNC, H-A2C 9.78 9.79 0.02 0 1 10 Hybrid Cacla, DDPG 9.57 9.66 0.17 -1 0 10 Maze, 17 10 , hybrid DGRL (SDN, DBU) 9.72 9.72 0.01 0 106 10 Hybrid DNC, H-A2C 9.59 9.75 0.27 0 107 10 Hybrid Cacla, DDPG 4.39 4.71 3.77 -51 0 10 Recommender, 343 1, hybrid DGRL (SDN, DBU) 1579.64 1577.34 23.34 11 17 10 Hybrid DNC, H-A2C 1362.25 1412.97 144.42 0 5 10 Hybrid Cacla, DDPG 1335.42 1342.34 34.54 -4 0 10 Recommender, 343 10 , hybrid DGRL (SDN, DBU) 2262.56 2258.83 60.81 129 29 10 Hybrid DNC, H-A2C 1011.78 982.57 112.17 0 -43 10 Hybrid Cacla, DDPG 1740.56 1748.69 43.23 77 0 10 

26