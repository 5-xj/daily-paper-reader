Title: Enhancing Genetic Algorithms with Graph Neural Networks: A Timetabling Case Study

URL Source: https://arxiv.org/pdf/2602.08619v1

Published Time: Tue, 10 Feb 2026 03:09:41 GMT

Number of Pages: 16

Markdown Content:
# Enhancing Genetic Algorithms with Graph Neural Networks: A Timetabling Case Study 

Laura-Maria Cornei ⋆1[0000 −0003 −1032 −0924] and Mihaela-Elena Breabăn 1[0000 −0003 −4468 −3889] 

Alexandru Ioan Cuza University, Faculty of Computer Science Iaşi, Romania 

{laura.cornei,mihaela.breaban}@info.uaic.ro 

Abstract. This paper investigates the impact of hybridizing a multi-modal Genetic Algorithm with a Graph Neural Network for timetabling optimization. The Graph Neural Network is designed to encapsulate gen-eral domain knowledge to improve schedule quality, while the Genetic Al-gorithm explores different regions of the search space and integrates the deep learning model as an enhancement operator to guide the solution search towards optimality. Initially, both components of the hybrid tech-nique were designed, developed, and optimized independently to solve the tackled task. Multiple experiments were conducted on Staff Rostering, a well-known timetabling problem, to compare the proposed hybridiza-tion with the standalone optimized versions of the Genetic Algorithm and Graph Neural Network. The experimental results demonstrate that the proposed hybridization brings statistically significant improvements in both the time efficiency and solution quality metrics, compared to the standalone methods. To the best of our knowledge, this work pro-poses the first hybridization of a Genetic Algorithm with a Graph Neural Network for solving timetabling problems. 

Keywords: Staff Scheduling · Nurse Rostering · Genetic Algorithms ·

Graph Neural Networks 

# 1 Introduction 

Genetic Algorithms (GAs), along with other meta-heuristics, have been tradi-tionally applied for solving timetabling problems, as they are able to provide a set of high-quality and potentially diverse schedules [2,9,29]. Compared to other meta-heuristics, GAs are well suited for tackling timetabling tasks, as according to Holland’s schema theorem [11], the small, high-quality and recurrent sched-ule patterns can be effectively preserved and combined over generations using crossover. In the case of real-world timetabling tasks, even small changes in the values of the instances’ parameters may lead to completely different solutions. As a result, the process of reapplying GAs, as well as other meta-heuristics, to new instances is usually time consuming.  

> ⋆

Corresponding author  

> arXiv:2602.08619v1 [cs.NE] 9 Feb 2026 2Laura-Maria Cornei, Mihaela-Elena Breabăn

To avoid this issue, Graph Neural Networks (GNNs) could be used [32], as they are capable of extracting useful patterns from the training data to quickly provide solutions to new test instances, due to their generalization ability. How-ever, for problems with a huge search space, the optimization process of the GNNs may be prone to getting trapped in local optima [3]. Consequently, to combine the strengths of both techniques and avoid the aforementioned drawbacks, we propose a hybrid technique in which a pretrained GNN is integrated within a multi-modal GA as a chromosome improvement op-erator. The pretrained Graph Neural Network would encapsulate general knowl-edge needed for improving the quality of schedules with possibly different sizes. The Genetic Algorithm would ensure efficient exploration of the search space, performing the concrete steps towards reaching diverse optimal solutions for given test instances. The current study brings the following main contributions: 

– developing and optimizing a Graph Neural Network for solving a well-known timetabling problem, namely Staff Rostering. 

– designing and optimizing a multi-modal Genetic Algorithm for solving Staff Rostering. 

– integrating the Genetic Algorithm with the Graph Neural Network and eval-uating this hybrid approach in comparison to the standalone GNN and GA. The comparative analysis was conducted under multiple settings and was based on several evaluation criteria, including computational time, quality and diversity of the obtained schedules. The experimental results indicate that the proposed hybridization brings statistically significant improvements compared to the standalone optimized versions of the GA and GNN. To the best of our knowledge, our paper is the first to introduce and investi-gate the impact of integrating a Graph Neural Network with a Genetic Algorithm for solving timetabling problems. Tests performed on the Staff Rostering prob-lem demonstrate that the developed hybridization leverages the complementary strengths of the GAs and GNNs. The rest of the paper is organized as follows: section 2 discusses related work, while section 3 introduces the Staff Rostering problem and the used mathemat-ical model. Sections 4 and 5 describe the process of creating and optimizing the Graph Neural Network and the Genetic Algorithm. Section 6 presents the exper-imental settings and summarizes the results. Finally, section 7 proposes future work directions and states the conclusions. 

# 2 Related work 

Over the years, hard timetabling problems have been solved using meta-heuristics [2, 8, 9, 24, 29, 30], mathematical optimization methods [25] or a combination of both (math-heuristics) [13,19,31,34]. More recently, GNNs have become increas-ingly popular for solving Combinatorial Optimization Problems (COPs), includ-ing timetabling tasks, due to their generalization ability and special properties (permutation invariance and equivariance) [3]. Enhancing GAs with GNNs 3

Novel techniques for solving general COPs using Graph Neural Networks usually model the problem as a (Mixed) Integer Linear Program ((M)ILP) and then encode this information into a bipartite graph associated to the GNN, such that one partition represents the variables of the (M)ILP, and the other the con-straints [10, 15, 16]. In order to be able to attain better results, the GNN is usu-ally integrated with other methods, such as Reinforcement Learning (RL) tech-niques [15], mathematical optimization methods [10], trajectory meta-heuristics (Simulated Annealing [16]) or population-based meta-heuristics (Large Neigh-borhood Search [20]). Although applicable to any COP, this kind of approach comes with the disadvantage that increasing the size or complexity of the in-stances leads to a major increase in the number of vertices and edges in the GNN graph. This further conducts to poorer generalization results, as the depth of the GNN cannot be scaled with graph size [6] due to potential over-smoothing issues [36]. Considering the No Free Lunch Theorem [1], devising GNN-based approaches specific to the employed task represents a promising research line. In case of timetabling tasks, GNNs have been applied in hybridizations with: 

– Reinforcement Learning algorithms, such that the GNN was used as a policy in the RL method. [26, 35] 

– the Large Neighborhood Search meta-heuristic, where the GNN was used as a destroy operator [23] (idea also presented in [20]). 

– Mathematical Optimization methods [17, 22] and classical heuristics [12], which aim to locally improve the solution outputted by the GNN. To the best of our knowledge, there are currently no papers integrating a Graph Neural Networks with a Genetic Algorithm for solving timetabling prob-lems. Also, the use of a GNN as an improvement operator within a meta-heuristic is a novel idea. Lastly, there are no current studies evaluating the impact of hy-bridizing GAs with GNNs for timetabling tasks. 

# 3 Staff Rostering - an Integer Programming model 

Staff Rostering [21], also known as Personnel Timetabling, Staff Scheduling, and Nurse Rostering in the literature, is an NP-hard combinatorial optimization problem involving the assignment of shifts to employees, while satisfying a series of hard and soft constraints. Starting from a well-known mathematical model for this problem [4], we selected and adjusted several restrictions to fit a real-world scheduling scenario and ensure compliance with our country’s work legislation. Let E be the number of employees, D the number of days in the planning horizon, and S the set of shifts, such that there are three 8-hour shifts in total: morning, afternoon and night. Let xeds be an indicator variable, taking a value of 1 if employee e is working shift s on day d, and 0 otherwise. The final model includes the following hard constraints: 4 Laura-Maria Cornei, Mihaela-Elena Breabăn 

– C1. Each employee works at most one shift per day: 

X

> s∈S

xeds ≤ 1, ∀e ∈ { 1, ..., E }, d ∈ { 1, ..., D } (1) 

– C2. Morning shifts cannot be preceded by night shifts: 

xeds + xe(d+1) r ≤ 1, ∀e ∈ { 1, ..., E },d ∈ { 1, ..., D − 1}, s = night, r = morning (2) 

– C3. Each employee has to work at least bmin hours and at most bmax hours: 

bmin ≤ X

> d∈{ 1,...,D }

X

> s∈S

8 · xeds ≤ bmax , ∀e ∈ { 1, ..., E } (3) 

– C4. Each employee has to work at most cmax consecutive shifts: 

> t+cmax

X

> d=t

X

> s∈S

xeds ≤ cmax , ∀e ∈ { 1, ..., E }, t ∈ { 1, ..., D − cmax } (4) 

– C5. When taking days off, each employee has to rest for at least omin days: 

(1 − X

> s∈S

xeds ) + 

> d+t

X

> j=d+1

X

> s∈S

xejs + (1 − X

> s∈S

xe(d+t+1) s) > 0,

∀e ∈ { 1, ..., E }, t ∈ { 1, ..., o min − 1}, d ∈ { 1, ..., D − (t + 1) }

(5) The objective function minimizes the sum of penalties associated to the un-derstaffing ( vmin ) and overstaffing ( vmax ) situations respectively, as well as the penalties corresponding to the work preferences of the employees ( ped ). Specifi-cally, ped is an indicator variable taking a value of 1 when employee e prefers not to work on day d, and 0 otherwise. P represents the total number of penalties of type ped .

min 

 X

> d∈{ 1,...,D }

X

> s∈S

yds vmin + X

> d∈{ 1,...,D }

X

> s∈S

zds vmax + X

> e∈{ 1,...,E }

X

> d∈{ 1,...,D }

X

> s∈S

ped xeds 



where: 

yds = max(0 , u ds − X

> e∈{ 1,...,E }

xeds ), zds = max(0 , X

> e∈{ 1,...,E }

xeds − uds ) (6) 

# 4 Graph Neural Network 

4.1 GNN learning procedure 

Figure 1 illustrates, on the left side, an overview of the GNN learning procedure and, on the right side, the structure of the graph and the embeddings associated to the employed GNN. Enhancing GAs with GNNs 5

Fig. 1: Overview of the GNN learning procedure (left side) and structure of the graph and the embeddings of the used GNN (right side) The used Graph Neural Network acts as a solution improvement operator, such that given as input a batch of schedules, it predicts timetables with a considerably higher quality with respect to the objective function. Although the GNN was trained on pairs of (input, target) schedules corresponding to multiple instances, each pair of timetables was associated to the same instance. As loss function, the Cross Entropy computed between the probability distributions of the predicted and target shifts was used. To avoid overfitting, we employed an early stopping procedure such that the training was halted when a number of patience epochs elapsed without a significant improvement in the validation loss. The validation loss measured the quality of the generated schedules by minimizing the difference between the number of unfeasible and optimal predicted schedules, divided by the total number of timetables. In case this value remained constant, the loss aimed to minimize the Mean Squared Error between the objective function values of the feasible suboptimal and optimal schedules. At test time, the use of the GNN within the Genetic Algorithm involved extracting the GNN embeddings from the schedules associated to each chromo-some, feeding them (only once) to the network, and then formatting the output to obtain the resulting chromosomes. 

4.2 Datasets generation 

To train the GNN we needed to collect pairs of schedules (s, s ′), such that s′ had a better (lower) objective function value than s. To construct the datasets, we first generated 250 problem instances parameterized by 100 persons and 7 days, and computed an optimal solution for each of them using the Gurobi solver [7]. For all instances, the following problem parameters were set to the indicated values: bmin = 32 , b max = 48 , c max = 5 , o min = 2 , v min = 100 , v max = 1 . The preferred coverage for each day d and shifts s (uds ) was set to the floor division between the number of employees E and the number of shifts |S|. The shift off 6 Laura-Maria Cornei, Mihaela-Elena Breabăn 

request penalties for each employee e and day d (ped ) randomly took values from the set {0, 1}. Next, we altered via random shift changes the optimal solutions to obtain for each of them 250 unique feasible schedules, resulting in a total of 62500 feasible timetables. Then, we applied random shift mutations to the feasible schedules to obtain one unfeasible timetable for each feasible one. The final dataset included pairs of unfeasible-feasible and feasible-optimal schedules. We additionally created other types of datasets, containing for ex-ample pairs of unfeasible-optimal and feasible-optimal timetables; however, the initially proposed version yielded the best results, so we adopted it. On this dataset, we applied an 80/10/10 split for training, validation and testing. 

4.3 GNN structure, model and embeddings 

As illustrated on the right side of Figure 1, the GNN graph contains three types of vertices corresponding to employees, shifts and days. A shift node was created for every (employee, day) pair and was connected to the related employee and day vertices via bidirectional edges. Shift nodes linked to the same employee vertex but to consecutive day vertices were connected as well, to encourage the flow of temporal information. Using the PyTorch Geometric library [5], we created a Graph Neural Network architecture consisting of nb _layers _conv TransformerConv GNN layers [28], followed by a multilayer perceptron (MLP) with nb _layers _M LP layers. To generate an output starting from the input embeddings, the network first applied the GNN convolutional layers to produce new node representations for the shift, employee and day nodes. The shift representations were further concatenated with the corresponding employee and day ones and the resulting embeddings were passed through the MLP to obtain for each (employee e, day d) pair the probabilities of e being assigned each type of shift (including the rest one) on day d. Given this generated output, the reconstruction of the resulting schedule was straightforward, by selecting, for each (employee e, day d) pair, the shift value with the highest probability. The used GNN was heterogeneous, such that different types of relations (employee-shift, shift-day, shift-shift) had associated separate Message Passing functions [27]. The features associated with each node in the GNN graph contained, in the case of the hard constraints, flags signaling their infringement and, in the case of both hard and soft restrictions, normalized values signifying the degree to which they were or were not satisfied. The final embeddings were formed, as highlighted in Figure 1, by concatenating 2 digits representing the type of the GNN node with three kinds of features (associated with employees, shifts, or days), such that zero values were placed in case the feature type did not match the the node type. The employee features included information regarding the C3 constraint, the day features retained the normalized understaffing and overstaffing soft penalties, while the shift features integrated the one hot encoding of the shifts, as well as information concerning all the other remaining restrictions. Enhancing GAs with GNNs 7

4.4 GNN hyper-parameter tuning 

To optimize the designed Graph Neural Network, we performed a random search hyper-parameter tuning by varying the following hyper-parameters: 

– batch _size ∈ { 64 , 128 , 256 } (training batch size) 

– lr ∈ { 0.01 , 0.001 } (learning rate) 

– opt _weight _decay ∈ { 0.001 , 0.0001 } (optimizer weight decay) 

– dropout _conv ∈ { 0, 0.1, 0.2} (dropout for GNN convolutional layers) 

– dropout _M LP ∈ { 0, 0.1, 0.2} (dropout for MLP layers) 

– nb _layers _conv ∈ { 3, 4} (number of GNN convolutional layers) 

– nb _layers _M LP ∈ { 3, 4, 5} (number of MLP layers) 

– nb _heads ∈ { 4, 8} (number of GNN attention heads) 

– patience ∈ { 20 , 30 , 40 } (early stopping patience) We used the AdamW optimizer due to its generalization ability [37]. The best obtained configuration out of 40 trials was: batch _size = 64 , opt _weight _decay =0.0001 , lr = 0 .001 , dropout _conv = 0 , dropout _M LP = 0 , nb _layers _conv =4, nb _layers _M LP = 4 , nb _heads = 8 , patience = 30 . To select the best con-figuration, we trained the GNN 30 times for each of them and compared the averaged results on the test dataset. 

# 5 Genetic Algorithm 

5.1 Overview of the algorithm 

The proposed Genetic Algorithm was used to find optimal and near-optimal diverse solutions for a given problem instance. To provide statistically significant results, we ran the algorithm 10 times on each of the 50 randomly generated instances. These instances had the same size (100 persons and 7 days) and were generated following the same procedure as the one employed for training the GNN, presented in section 4.2. However, we ensured that the generated instances were different from those used in the GNN learning and testing process. The pseudocode for the employed Genetic Algorithm, presented in Listing 1.1, integrates the classical GA operators (selection, crossover, mutation) and, optionally, the GNN-based improvement operator. Next, we discuss each com-ponent of the Genetic Algorithm. Listing 1.1: Genetic Algorithm Pseudocode 

def genetic_algorithm(pop_size, stop_cond_version, nb_max_epochs, max_patience, probab_crossover, probab_mutation, min_prob_greedy, use_GNN): pop = get_init_population(pop_size) epoch = 0 patience = 0 prob_greedy = min_prob_greedy fitness_and_pen_info = get_fitness_and_pen_info(pop) 8 Laura-Maria Cornei, Mihaela-Elena Breabăn 

while not stop_alg(stop_cond_version, epoch, nb_max_epochs, patience, max_patience, fitness_and_pen_info): offspring = crossover_all(pop, probab_crossover) offspring = mutation_all(offspring, probab_mutation) 

if use_GNN: offspring = GNN_improvement(offspring) distances = calc_crowding_distances(pop, offspring) matches = find_matchings(distances) prob_greedy = update_prob_greedy(prob_greedy, min_prob_greedy, epoch) pop = selection(pop, offspring, matches, prob_greedy) fitness_and_pen_info = get_fitness_and_pen_info(pop) patience = update_patience(patience, fitness_and_pen_info) epoch += 1 

Solution representation, initial population and fitness function A chro-mosome in the population encodes a schedule as a matrix, where each row corre-sponds to a person, each column to a day, and each entry maps the shift assigned to a person in a day. To obtain the initial population, we randomly generated 

pop _size chromosomes using the get _init _population function, allowing both feasible and unfeasible schedules to be part of the initial population. The fitness value of a chromosome minimizes the violation of the hard and soft constraints, being computed as: 

f itness _value = M AX _F IT N ESS _V ALU E −(pen _hard +pen _sof t ) (7) The pen _hard and pen _sof t scores represent the sums of normalized soft and hard penalties associated with the current schedule; each violation of a hard constraint resulted in a penalty of 1, while the violation of a soft constraint produced a penalty value in the range (0 , 1) . In f itness _and _pen _inf o , we retained information concerning the fitness values, the hard penalties, and the normalized and unnormalized soft penalties associated with the current pop-ulation. M AX _F IT N ESS _V ALU E represents an upper bound estimation for the maximum fitness value and was computed as nb _persons ∗ nb _days ∗

(nb _shif ts +1)+ nb _shif ts ∗nb _days +nb _persons . Consequently, an optimal solution has an associated fitness value of 2921 .

Stopping conditions In the experiments section 6, we used two versions of the GA, differing based on the employed stopping conditions. In the first version of the Genetic Algorithm denoted as GA _v1 (having stop _cond _version set to 1), the algorithm was halted either upon completion of nb _max _epochs or upon reaching an optimal solution. We considered a schedule to be optimal if it had a hard penalty equal to 0 and a soft (unnormalized) penalty equal to the minimum soft penalty associated with the current instance. In the second version of the Genetic Algorithm, denoted as GA _v2 (having stop _cond _version set to 2), the algorithm was stopped either after exceeding a maximum number of epochs Enhancing GAs with GNNs 9

nb _max _epochs , or after exceeding a maximum patience max _patience , in-dicating the number of epochs passed with no improvement in the best fitness value. The information regarding the best fitness value per epoch was retained in 

f itness _and _pen _inf o , and the update _patience function updated the cur-rent patience value. 

Variation operators The crossover _all function creates at first pop _size ÷ 2

pairs of parents and then applies the crossover operator with a probability of 

probab _crossover on each pair to obtain two offspring chromosomes. We chose 

pop _size to be an even number to avoid the extra step of pairing up the last chromosome. We implemented and tested multiple ways of performing crossover between two parents p1 and p2:

– cx _one _line randomly chooses one line from p1 and one line from p2 and swaps them. 

– cx _one _line _partially randomly chooses a piece of line from p1 and a piece of line from p2 and swaps them. Both pieces have the same length, but possibly different starting points in the lines. Each chromosome ch obtained via crossover is further mutated with a proba-bility of probab _mutation in the mutation _all function. We also tested multiple mutation operators, namely: 

– mut _swap _shif ts and mut _change _shif t . The first swapped two random shifts from ch , while the second randomly changed the value of an aleatory chosen shift. 

– mut _change _shif t _penalty , which randomly modified the shift s whose value v induced the highest penalty. This penalty was computed as the sum of hard and normalized soft penalties associated to the hard and soft constraints violated when assigning v to s.If the use _GN N flag is set to T rue , the GNN-based operator is applied to the offspring obtained via mutation, as described in section 4.1. 

Crowding distances computation and selection To enhance and pre-serve diversity in the population, we employed a niching mechanism based on probabilistic crowding [18]. The calc _crowding _distances function is used to compute the crowding distances between the initial parent chromosomes and the offspring generated after applying mutation and crossover. The crowding distance between two chromosomes is defined as the number of positions at which the corresponding shift values differ in the associated schedules. Next, the 

f ind _matchings function, taking as input these distances, performs a one-to-one matching between parents and offspring based on the linear sum assignment algorithm [33]. Therefore, the offspring chromosomes tended to be assigned to resembling parents, and vice versa. Finally, the tournament selection function selects from each (parent p, off-spring c) pair the winning chromosome as follows: with a probability equal to 

prob _greedy , the winning chromosome is deterministically chosen to be the one with the highest fitness value; otherwise, a probabilistic rule is used such that the 10 Laura-Maria Cornei, Mihaela-Elena Breabăn 

offspring is selected as the winner with a probability equal to f itness (c)/(f itness (c)+ 

f itness (p)) [18]. The parameter prob _greedy was initially set to min _prob _greedy 

and its value was linearly increased up to 1.0, in the update _prob _greedy func-tion, as generations evolve. 

5.2 Optimizing the Genetic Algorithm 

The presented Genetic Algorithm was implemented in Python in order to inte-grate the GNN-based operator. To speed up the GA functions, we used, when-ever possible, the Numba just-in-time compiler [14]. We experimentally tested multiple combinations of parameters and operators for the Genetic Algorithm. The final configuration that provided the best results, out of all trials, was the following: pop _size = 200 , nb _max _epochs = 50000 , max _patience =3000 , probab _crossover = 0 .5, probab _mutation = 1 .0, min _prob _greedy =0.4. The crossover function was randomly chosen with equal probability be-tween cx _one _line and cx _one _line _partially . The mutation operator ap-plied mut _swap _shif ts , mut _change _shif t and mut _change _shif t _penalty 

with probabilities 0.2, 0.2 and 0.6 respectively. Although fixing the mutation probability to 1.0 may appear unconventional, the perturbations induced by the mutation operators are very small ( 2 shifts are changed at most out of 700 ) and the penalty-based mutation randomly changes a shift that is very likely to be modified in a chromosome having a high fitness. 

# 6 Experiments and results 

We evaluated the Graph Neural Network (GNN), the Genetic Algorithm (GA) and their hybridization (GA+GNN) on the same test dataset formed out of 50 

randomly chosen instances. For the GA and GA+GNN versions, we performed 10 

runs for each instance, as explained in section 5.1. All experiments were executed on a Desktop with the following specifications: Intel core ultra 9 285k, NVIDIA GeForce RTX 5080 16GB, 192GB DDR5, 4TB SSD. We first evaluated the performance of the GNN on the test data set and we obtained the following mean values (the associated standard deviations are reported in parentheses): 0.2665 ( ±0.0169) for the percent of optimal gener-ated schedules, 0.6486 ( ±0.0519) for the percent of feasible suboptimal obtained timetables, and 0.0849 ( ±0.0365) for the percent of unfeasible generated sched-ules. Therefore, although obtaining a large ratio of feasible suboptimal timeta-bles, the GNN wasn’t able to generate an optimal solution for many of the test instances. In contrast, as we will further see, the GA and GA+GNN versions were able to find optimal solutions for all instances. Next, we tested the performance of the GA and GA+GNN methods. Ac-cording to section 5.1, to provide better insights, we tested two versions of the Genetic Algorithm differing based on the stopping condition: GA _v1 and 

GA _v2. Method GA _v1 was halted when encountering an optimal solution, Enhancing GAs with GNNs 11 

while method GA _v2 was terminated based on a best fitness patience, fixed to a value ( 3000 ) high enough to encourage the convergence of the algorithm. Figure 2 includes several plots highlighting the comparative evaluation of the 

GA _v1 and GA _v1+ GN N techniques with respect to: the mean and maximum fitness values, the mean and minimum soft & hard penalties, and the average number of feasible schedules. The plots illustrate the mean per generation for each metric, as well as the confidence interval at a significance level of 0.05 . In some cases, this interval was extremely small and thus may not be clearly visible. When computing the statistics for the soft penalties, we only considered those scores associated with feasible schedules, to provide a more reliable indicator. For the fitness and hard penalty metrics we additionally provide log scaled plots for better visualization. As shown in Figure 2, the GA _v1+ GN N version reaches an optimal solution significantly faster (with respect to the number of generations) than GA _v1. As the search progressed towards an optimal solution, the variability in the number of feasible schedules increased. This behavior may be explained by the increased difficulty in maintaining feasibility in this region, combined with the need to explore infeasible schedules in order to reach the optimal solution. Although not included, plots comparing GA _v2 and GA _v2+ GN N provided similar insights. Table 1 reports the mean values and standard deviations (reported in paren-thesis) of multiple metrics, obtained when applying all four methods: GA _v1,

GA _v1 + GN N , GA _v2, GA _v2 + GN N . Besides the previously mentioned metrics, we included the average number of optimal solutions, the mean and maximum crowding distance, the average total time in minutes per run and the mean stopping generation number. A high value of the crowding distance indicates a diverse population, which is desirable. The means and standard de-viations were computed by aggregating each metric’s values at the generation corresponding to the best fitness value within the last 1000 generations. To compare GA _v1 and GA _v1 + GN N , as well as GA _v2 and GA _v2 + 

GN N , we applied the Welch’s t-test to each metric’s results and highlighted in bold the winning outcomes, whenever the difference was statistically significant at the 95% confidence level. On average, a run with GA _v1 + GN N was more than 10 times faster than a run with GA _v1 and this difference in terms of efficiency was also reflected in the average value of the stop generation ( ∼ 400 vs. ∼ 40480 ). For the other metrics, the average values of GA _v1+ GN N were usually lower or slightly lower than those of GA _v1. However, the GA _v1 + GN N version obtained results that were statistically indistinguishable from those of GA _v1 for three metrics indicating the quality of the best solutions: maximum fitness value, minimum soft penalty, minimum hard penalty. In conclusion, GA _v1 + GN N represents a significantly faster alternative to GA _v1, such that the quality of best schedules obtained by both methods was statistically comparable. The GA _v2+ GN N obtained considerably better results compared to GA _v2

for all metrics except for the minimum hard penalty, for which the results were comparable, and for the crowding distance, for which the results were worse. 12 Laura-Maria Cornei, Mihaela-Elena Breabăn 

Fig. 2: Comparative evaluation of the GA _v1 and GA _v1 + GN N methods for the fitness, hard & soft penalty scores, and number of feasible schedules metrics A notable aspect is the increase brought by GA _v2 + GN N in the average number of optimal schedules when compared to the classical GA ( ∼ 156 .8 vs 

∼ 20 .5). Also, almost all schedules associated to the chromosomes from the 

GA _v2 + GN N ’s population were feasible (on average 199 .8 out of 200 ). These improvements brought by GA _v2 + GN N came with a trade-off, as the total computation time per run for GA _v2 + GN N increased by almost a factor of four compared to GA _v2.Enhancing GAs with GNNs 13 

Table 1: Comparative evaluation of the GA _v1, GA _v1 + GN N , GA _v2 and 

GA _v2 + GN N methods for multiple metrics                   

> Metric / Method GA_v1 GA_v1 + GNN GA_v2 GA_v2 + GNN Fitness value mean 2920.2208 (0.3655)
> 2919.7809 (1.4256) 2920.5296 (0.1464)
> 2920.7144 (0.0047)
> max 2920.7177 (0.0006) 2920.7178 (0.0006) 2920.7165 (0.0017)
> 2920.7187 (0.0007)
> Soft penalty min 19380.6833 (7.7819) 19380.4833 (7.7841) 19388.8333 (13.7633)
> 19373.65 (7.9550)
> mean 19562.3814 (84.3713)
> 20045.8460 (513.1283) 19550.3685 (120.4645)
> 19380.5479 (7.9052)
> Hard penalty min 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) mean 0.493 (0.3649)
> 0.9263 (1.4214) 0.1834 (0.1447)
> 0.0034 (0.0047)
> Number of feasible schedules mean 145.5333 (31.5709)
> 132.8 (54.1553) 173.8857 (16.2386)
> 199.8 (0.4472)
> Nb. optimal schedules mean 1.0 (0.0) 1.0 (0.0) 20.5428 (35.7008)
> 156.8292 (25.7235)
> Crowding distance mean 471.8697 (8.9183)
> 453.3819 (9.4435)
> 471.7008 (17.4030)
> 439.6751 (6.0230) max 539.0 (8.7662)
> 514.2666 (10.0150)
> 542.8833 (22.9539)
> 498.9166 (7.4339) Total time 1 run (mins.) mean 23.1038 (11.0696)
> 2.2620 (3.2678) 17.9557 (6.1041)
> 69.9784 (20.7085) Stop generation mean 40479.75 (2987.77)
> 400.15 (579.39)
> 34032.25 (11635.94)
> 12956.58 (3852.54)

We continued the previous experiment by rerunning the GA _v2+ GN N ver-sion and adding a new stop criterion: each run of the algorithm was terminated when its execution time reached the total execution time of the correspond-ing matching run associated to GA _v2. We named this new hybrid version 

GA _v2 + GN N ∗. The comparison between GA _v2 and GA _v2 + GN N ∗ en-abled us to investigate whether the hybrid version could improve the values of the metrics for the same running times. Table 2 contains the results of the comparison between GA _v2 and GA _v2+ 

GN N ∗. As before, we used the Welch’s t-test and marked in bold the results that were statistically better, considering a 95% confidence level. The GA _v2 + GN N ∗ attained superior results for the following metrics: average and maximum fitness value, minimum and mean soft penalty, mean hard penalty, and average number of feasible schedules, when compared to GA _v2.On the other hand, GA _v2 + GN N ∗ obtained worse results for the average and maximum crowding distance and for the average number of optimal schedules. To conclude, the GA _v2 + GN N ∗ method generated less diverse timetables and fewer optimal solutions. However, in contrast to GA _v2, the quality of the 14 Laura-Maria Cornei, Mihaela-Elena Breabăn 

Table 2: Comparative evaluation of the GA _v2 and GA _v2 + GN N ∗ methods          

> Metric Method Fitness value Soft penalty Hard penalty Number of feasible schedules Number of optimal schedules Crowding distance mean max min mean min mean mean mean mean max
> GA _v2
> 2920.5296 (0.1464) 2920.7165 (0.0017) 19388.8333 (13.7633) 19550.3685 (120.4645) 0.0 (0.0) 0.1834 (0.1447) 173.8857 (16.2386)
> 20.5428 (35.7008) 471.7008 (17.4030) 542.8833 (22.9539)
> GA _v2 +  GN N  ∗
> 2920.6107 (0.0437) 2920.7178 (0.0007) 19380.5 (8.2616) 19421.6939 (16.4438)
> 0.0 (0.0)
> 0.1051 (0.0434) 183.3333 (6.1952)
> 4.8667 (8.0610) 463.7544 (5.1455) 523.1833 (6.8035)

provided schedules was statistically higher, both when comparing all obtained timetables or only the best ones. 

# 7 Conclusions and future work 

The current paper investigates the impact of combining two fundamentally dif-ferent optimization paradigms - a search heuristic based on populations in the form of a Genetic Algorithm and a data-driven Graph Neural Network - for solv-ing a timetabling problem of great practical importance, namely Staff Rostering. To the best of our knowledge, this study is the first to hybridize a Genetic Algo-rithm with a Graph Neural Network to address timetabling tasks. The GNN is capable of extracting and encapsulating general domain knowledge during train-ing to enhance the quality of suboptimal schedules, while the GA explores the multi-modal solution space, exploiting the GNN accumulated knowledge in the form of a variation operator. A thorough experimental analysis demonstrates that the hybrid method provides statistically significant improvements in both time efficiency and solution quality compared to the standalone methods. Regarding future directions, this study can be further extended to tackle other types of timetabling problems or more general scheduling tasks. Another promising direction focuses on devising a new technique to improve the gener-alization ability of the Graph Neural Network and investigating novel ways of hybridizing GNNs with meta-heuristics. 

# References 

1. Adam, S.P., Alexandropoulos, S.A.N., Pardalos, P.M., Vrahatis, M.N.: No free lunch theorem: A review. Approximation and optimization: Algorithms, complexity and applications pp. 57–82 (2019) 2. Borgonjon, T., Maenhout, B.: A genetic algorithm for the personnel task reschedul-ing problem with time preemption. Expert Systems with Applications 238 , 121868 (2024) Enhancing GAs with GNNs 15 3. Cappart, Q., Chételat, D., Khalil, E.B., Lodi, A., Morris, C., Veličković, P.: Com-binatorial optimization and reasoning with graph neural networks. Journal of Ma-chine Learning Research 24 (130), 1–61 (2023) 4. Curtois, T., Qu, R.: Computational results on new staff scheduling benchmark instances (2014) 5. Fey, M., Lenssen, J.E.: Fast graph representation learning with pytorch geometric (2019), https://pytorch-geometric.readthedocs.io/en/latest/ 6. Gamarnik, D.: Barriers for the performance of graph neural networks (gnn) in dis-crete random structures. Proceedings of the National Academy of Sciences 120 (46), e2314092120 (2023) 7. Gurobi Optimization, L.: Gurobi optimizer reference manual (2024), https://www.gurobi.com 8. Hafsa, M., Wattebled, P., Jacques, J., Jourdan, L.: A multi-objective evolutionary approach to professional course timetabling: A real-world case study. In: 2021 IEEE Congress on Evolutionary Computation (CEC). pp. 997–1004. IEEE (2021) 9. Hafsa, M., Wattebled, P., Jacques, J., Jourdan, L.: Solving a multiobjective profes-sional timetabling problem using evolutionary algorithms at mandarine academy. International Transactions in Operational Research 32 (1), 244–269 (2025) 10. Han, Q., Yang, L., Chen, Q., Zhou, X., Zhang, D., Wang, A., Sun, R., Luo, X.: A gnn-guided predict-and-search framework for mixed-integer linear programming (2023) 11. Holland, J.H.: Genetic algorithms and adaptation. In: Adaptive control of ill-defined systems, pp. 317–333. Springer (1984) 12. Huang, P., Peng, Z., Li, Z., Peng, Q.: Solving the railway timetable rescheduling problem with graph neural networks. Railway Engineering Science pp. 1–22 (2025) 13. Klyve, K.K., Senthooran, I., Wallace, M.: Nurse rostering with fatigue modelling: Incorporating a validated sleep model with biological variations in nurse rostering. Health care management science 26 (1), 21–45 (2023) 14. Lam, S.K., Pitrou, A., Seibert, S.: Numba: A llvm-based python jit compiler. In: Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC. pp. 1–6 (2015) 15. Lee, T.H., Kim, M.S.: Rl-milp solver: a reinforcement learning approach for solv-ing mixed-integer linear programs with graph neural networks. arXiv preprint arXiv:2411.19517 (2024) 16. Liu, Y., Zhang, P., Gao, Y., Zhou, C., Li, Z., Chen, H.: Combinatorial optimization with automated graph neural networks. arXiv preprint arXiv:2406.02872 (2024) 17. Liu, Y., Chen, X., Xu, Y., Xiang, D., Mo, L.: Accelerating model solving for in-tegrated optimization of timetabling and vehicle scheduling based on graph con-volutional network. In: 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC). pp. 880–886. IEEE (2023) 18. Mengshoel, O.J., Goldberg, D.E.: The crowding approach to niching in genetic algorithms. Evolutionary computation 16 (3), 315–354 (2008) 19. Mohd Razali, S.N.A.B., Tamilarasan, T.A.A., Basri, B.B., Bin Arbin, N., et al.: Maximizing shift preference for nurse rostering schedule using integer linear pro-gramming and genetic algorithm. International Journal of Advanced Computer Science & Applications 16 (5) (2025) 20. Nair, V., Alizadeh, M., et al.: Neural large neighborhood search. In: Learning Meets Combinatorial Algorithms at NeurIPS2020 (2020) 21. Ngoo, C.M., Goh, S.L., Sabar, N.R., Abdullah, S., Kendall, G., et al.: A survey of the nurse rostering solution methodologies: The state-of-the-art and emerging trends. IEEE Access 10 , 56504–56524 (2022) 16 Laura-Maria Cornei, Mihaela-Elena Breabăn 22. Nguyen, D.H., Truong, T.Q.A., Tran-Thanh, L.: Faster, larger, stronger: Optimally solving employee scheduling problems with graph neural networks. In: International Symposium on Information and Communication Technology. pp. 141–151. Springer (2024) 23. Oberweger, F.F., Raidl, G.R., Rönnberg, E., Huber, M.: A learning large neigh-borhood search for the staff rerostering problem. In: International Conference on Integration of Constraint Programming, Artificial Intelligence, and Operations Re-search. pp. 300–317. Springer (2022) 24. Otero-Caicedo, R., Casas, C.E.M., Jaimes, C.B., Garzón, C.F.G., Vergel, E.A.Y., Valdés, J.C.Z.: A preventive-reactive approach for nurse scheduling considering ab-senteeism and nurses’ preferences. Operations Research for Health Care 38 , 100389 (2023) 25. Perreault-Lafleur, C., Carvalho, M., Desaulniers, G.: A stochastic integer program-ming approach to reserve staff scheduling with preferences. International Transac-tions in Operational Research 32 (1), 289–313 (2025) 26. Platten, B., Macfarlane, M., Graus, D., Mesbah, S.: Automated personnel schedul-ing with reinforcement learning and graph neural networks. In: HR@ RecSys (2022) 27. Schlichtkrull, M., Kipf, T.N., Bloem, P., Van Den Berg, R., Titov, I., Welling, M.: Modeling relational data with graph convolutional networks. In: European semantic web conference. pp. 593–607. Springer (2018) 28. Shi, Y., Huang, Z., Feng, S., Zhong, H., Wang, W., Sun, Y.: Masked label predic-tion: Unified message passing model for semi-supervised classification pp. 1548– 1554 (2021) 29. Tayarani-N, M.H.: Novel operators for quantum evolutionary algorithm in solving timetabling problem. Evolutionary Intelligence 14 (4), 1869–1893 (2021) 30. Thepphakorn, T., Pongcharoen, P., Vitayasak, S.: Multi-objective hybrid optimiza-tions for designing course schedules based on operating costs and resource utiliza-tion. Annals of Operations Research pp. 1–68 (2024) 31. Turhan, A.M., Bilgen, B.: A mat-heuristic based solution approach for an extended nurse rostering problem with skills and units. Socio-Economic Planning Sciences 

82 , 101300 (2022) 32. Veličković, P.: Everything is connected: Graph neural networks. Current Opinion in Structural Biology 79 , 102538 (2023) 33. Virtanen, P., Gommers, R., Oliphant, T.E., Haberland, M., Reddy, T., Cour-napeau, e.a.S.: SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods 17 , 261–272 (2020). https://doi.org/10.1038/s41592-019-0686-2 34. Xu, Y., Wang, X.: A hybrid integer programming and artificial bee colony algo-rithm for staff scheduling in call centers. Computers & Industrial Engineering 171 ,108312 (2022) 35. Yue, P., Jin, Y., Dai, X., Feng, Z., Cui, D.: Reinforcement learning for scalable train timetable rescheduling with graph representation. IEEE Transactions on Intelligent Transportation Systems 25 (7), 6472–6485 (2024) 36. Zhang, X., Xu, Y., He, W., Guo, W., Cui, L.: A comprehensive review of the over-smoothing in graph neural networks. In: CCF Conference on Computer Supported Cooperative Work and Social Computing. pp. 451–465. Springer (2023) 37. Zhou, P., Xie, X., Lin, Z., Yan, S.: Towards understanding convergence and general-ization of adamw. IEEE transactions on pattern analysis and machine intelligence 

46 (9), 6486–6493 (2024)