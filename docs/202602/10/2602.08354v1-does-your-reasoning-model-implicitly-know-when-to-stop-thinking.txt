Title: Does Your Reasoning Model Implicitly Know When to Stop Thinking?

URL Source: https://arxiv.org/pdf/2602.08354v1

Published Time: Tue, 10 Feb 2026 02:50:27 GMT

Number of Pages: 25

Markdown Content:
# Does Your Reasoning Model Implicitly Know When to Stop Thinking? 

Zixuan Huang 1 2 Xin Xia 2 Yuxi Ren 2 Jianbin Zheng 2 Xuanda Wang 2 Zhixia Zhang 1 Hongyan Xie 1

Songshi Liang 3 Zehao Chen 1 Xuefeng Xiao 2 Fuzhen Zhuang 1 Jianxin Li 1 Yikun Ban 1 Deqing Wang 1

Abstract 

Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through long Chains of Thought (CoTs). However, this approach of-ten results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly un-cover and empirically verify that LRMs implic-itly know the appropriate time to stop thinking, while this capability is obscured by current sam-pling paradigms. Motivated by this, we introduce 

SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, in-tegrating SAGE as mixed sampling into group-based reinforcement learning ( SAGE-RL ) effec-tively incorporates SAGE-discovered efficient rea-soning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challeng-ing mathematical benchmarks. 

1. Introduction 

Reinforcement learning from verifiable rewards (RLVR) algorithms, such as GRPO (Shao et al., 2024; Yang et al., 2026) and GSPO (Zheng et al., 2025a), have played a pivotal role in enabling test-time scaling. This capability allows large reasoning models (LRMs) like o3 (OpenAI, 2025a) and DeepSeek-R1 (Guo et al., 2025) to ‚Äúthink longer‚Äù. Longer CoTs enable LRMs to explore intermediate steps in greater depth and reduce abrupt logical leaps, thereby achieving unprecedented performance on challenging rea-

> 1

Beihang University 2Bytedance China 3Renmin University of China. Correspondence to: Yikun Ban <yikunb@buaa.edu.cn >,Deqing Wang <dqwang@buaa.edu.cn >.

Preprint. February 10, 2026. Avg. +2.1% Better Accuracy Avg. 44.1% Fewer Tokens                  

> </think>
> </think>
> Pass@k demonstrates the
> efficient reasoning potential.
> Pass@1 obscures the
> efficient reasoning potential.
> Pass@k Pass@1
> ùë•
> SAGE -RL effectively enables LRMs to learn efficient reasoning patterns.
> SAGE
> Policy
> Model
> ùë•
> Reference
> Model
> Verifiable
> Rewards
> RLVR
> Baseline ùê¥
> ùë¶ !
> "ùë¶ #$"
> "ùë¶ !
> %
> ‚Ä¶ùë¶ "
> %
> ‚Ä¶
> ùëü !
> "
> ùëü #$"
> "
> ‚Ä¶
> r!
> %
> ùëü "
> %
> ‚Ä¶
> SAGE -RL
> SAGE unleashes the efficient
> reasoning potential.
> Greedy Sampling
> SAGE
> 3222 tokens
> ‚Ä¶
> 5722 tokens
> 6331 tokens
> ùë•
> 2784 tokens
> 4322 tokens
> ùë•
> ‚Ä¶
> 4322 tokens 6784 tokens
> 2784 tokens
> </think> </think>
> </think>
> </think>
> </think>
> </think>

Figure 1. SAGE unleashes the efficient reasoning potential of LRMs obscured by pass@1 and identifies the optimal completions within the model‚Äôs capability hidden in pass@k. By enabling LRMs to learn these efficient reasoning patterns, SAGE-RL-tuned models simultaneously enhance reasoning capacity and concise-ness on multiple challenging mathematical benchmarks. 

soning benchmarks such as AIME (Art of Problem Solving, 2024), OlympiadBench (Chaoqun et al., 2024) and IMO (Luong et al., 2025). While longer reasoning chains are expected for solving harder problems, prior work shows that length inflation can be uncorrelated with correctness, and that shorter chains may in fact yield better accuracy. For example, Balachan-dran et al. (2025) observe that on AIME 2025, DeepSeek-R1 produces responses nearly 5√ó longer than Claude 3.7 Sonnet while achieving comparable accuracy; Hassid et al. (2025) show that on AIME and HMMT, the shortest responses from QwQ-32B outperform randomly sampled ones by 2 percentage points using 31% fewer tokens. These findings collectively reveal that current CoT outputs often contain substantial redundancy and irrelevant tokens that do not 1

> arXiv:2602.08354v1 [cs.AI] 9 Feb 2026 Submission and Formatting Instructions for ICML 2026

contribute to the final solution. These unnecessary tokens dramatically reduce reasoning efficiency. This naturally raises a pertinent question: do LRMs know the appropriate time to terminate thinking? 

We find that, during the exploration of multiple reasoning chains, LRMs consistently assign high confidence to concise yet effective reasoning paths. However, current sampling-based inference strategies typically overlook or fail to select these short and effective chains. Moreover, this phenomenon exhibits clear convergence behavior and becomes increas-ingly pronounced as the exploration space expands. Taken together, these results strongly indicate that reasoning mod-els implicitly know the appropriate moment to terminate their reasoning process, but this capability is obscured by current pass@1 training and inference paradigms .Motivated by this insight, we introduce SAGE (Self-Aware 

Guided Efficient Reasoning), a simple yet effective de-coding strategy that leverages the reasoning model‚Äôs self-confidence to discover relatively precise reasoning chains. By incorporating SAGE as mixed sampling into group-based reinforcement learning ( SAGE-RL ), we enable the reasoning model to learn concise yet effective thinking pat-terns without altering its original reasoning paradigm. In summary, our contributions in this work are as follows: ‚Ä¢ We uncover and demonstrate that LRMs implicitly know the appropriate time to stop thinking , but this capability is obscured by current sampling paradigms. ‚Ä¢ We propose SAGE , a novel sampling paradigm that unleashes the efficient reasoning potential of LRMs, si-multaneously improving both accuracy and conciseness of reasoning chains. ‚Ä¢ We propose SAGE-RL , a simple modification to RLVR frameworks that integrates SAGE into the rollout pro-cess. As shown in Figure 1, SAGE-RL-tuned models achieve consistent gains across six challenging reason-ing benchmarks, including MATH-500, AIME 2024, AIME 2025, AMC23, OlympiaBench and Minerva. 

2. Dilemmas of Reasoning Models under Current Sampling Paradigms 

To investigate whether reasoning models possess the ability to recognize the appropriate moment to terminate thinking, we first need to re-examine the dilemmas faced by these models under current sampling paradigms. 

Pass@k: Scaling CoT length does not lead to correct answers. Assuming that LRMs using current sampling paradigms can reliably stop thinking at the appropriate mo-ment, longer CoTs should outperform shorter ones in lead-ing to correct solutions. However, extensive experiments involving multiple samplings of the same problem refute this assumption. Balachandran et al. (2025) observes that on AIME 2025, DeepSeek-R1 produces responses nearly 5√ó longer than Claude 3.7 Sonnet while achieving comparable accuracy; Hassid et al. (2025) also shows that on AIME and HMMT, the shortest responses from QwQ-32B outperform randomly sampled ones by 2 percentage points using 31% fewer tokens. Shrivastava et al. (2025) found that, on AIME 2025, in 72% of problems where both correct and incor-rect answers were generated, the longer response was more likely to be incorrect than the shorter one. These findings collectively reveal that: once the chain-of-thought length reaches a certain threshold, simply scaling the length further does not lead to a corresponding improve-ment in the model‚Äôs reasoning capability. Furthermore, the optimal response within the model‚Äôs capability is obscured by existing sampling paradigms and can currently only be retrieved post hoc through test-time scaling methods. 

Pass@1: Existing sampling strategies fail to enable timely termination of thinking. To gain a finer-grained understanding of these findings and precisely locate its root cause, we build upon this observation and take a step further. Reasoning tasks, particularly in mathematical reasoning and code generation, typically require step-by-step answers. Leveraging this observation, we introduce a simple metric to quantify the efficient reasoning capability of models: the 

Ratio of the First Correct Step (RFCS), defined as the step index at which the correct answer first appears divided by the total number of reasoning steps. Specifically, we utilize DeepSeek-distilled-Qwen-1.5B (DS-1.5B), (DeepSeek-AI, 2025), DeepScaleR (Luo et al., 2025b) and Qwen3-8B (Yang et al., 2025a) to generate answers for MATH-500 (Lightman et al., 2023) problems. For each response, we segment it into distinct reasoning steps by ‚Äú \n\n‚Äù (Chen et al., 2025a) and compute RFCS for each problem. As illustrated in Figure 2, the model cor-rectly derives the answer using only 500 tokens, yet under the current sampling strategy, it continues with an additional 452 redundant tokens before terminating the reasoning pro-cess. This clearly demonstrates that the LRM fails to end its thinking at the appropriate moment on this problem. Such cases are not isolated in our study. More statistical results are summarized in Figure 3, where RFCS( < 1) and 

RFCS(avg) respectively denote the number of correct re-sponses where RFCS is not equal to 1 and the average RFCS value across all correct responses. From the statistical re-sults, all models exhibit significant ineffective steps in over half of the samples. Moreover, compared to DS-1.5B, mod-els with higher post-training extent (DeepScaleR), or more advanced reasoning capabilities (Qwen3-8B) show no sub-stantial improvement on this metric. This indicates that, in general scenarios, existing reasoning models struggle to 2Submission and Formatting Instructions for ICML 2026 John buys twice as many red ties as blue ties . The red ties cost 50 % more than blue ties .                                                                                                                                                  

> He spent $200 on blue ties that cost $40 each .How much did he spend on ties?
> Okay, so Ineed to figure out how much John spent on ties .Let's start by parsing the
> information given step by step .
> First, John buys twice as many red ties as blue ties .Hmm, that means if he buys xblue ties,
> he buys 2xred ties .That makes sense .
> Next, the red ties cost 50 %more than the blue ties .Alright, so if the blue tie is priced at,
> say, ydollars, then red ties would cost yplus 50 %of y, which is 1.5y.Alternatively, it can
> be written as 1.5y.
> Now, let's figure out the cost per red tie .We know that the red ties cost 50 %more than the
> blue ties .The blue tie costs $40 ,so 50 %of 40 is 20 .Therefore, each red tie costs 40 +20
> =$60 .
> Total spent = Total cost of blue ties + Total cost of red ties
> Total spent = 200 + 600 = $800 .
> So, John spent a total of $800 on ties.
> Wait, let me double -check my calculations to be sure I didn't make any mistakes .
> First, blue ties: 200 / 40 is indeed 5 ties. Red ties are twice that, so 10 ties. Each blue tie is
> $40, red tie is 50% more, which is $60. 10 red ties at $60 each: 10 * 60 is 600. Blue ties
> total $200. So 600 + 200 is 800. Yeah, that checks out.
> Total expenditure: 200 + 600 = 800 . Yeah, definitely $800 .
> I think that even if I messed up in the calculation, this straightforward approach seems
> correct . So, he spent $800 on ties .

# ‚Ä¶

# ‚Ä¶

Question 

Reasoning 50 0 tokens    

> 45 2tokens
> Correct Answer: 80 0

Figure 2. Illustration of the step-by-step answering process. DS-1.5B DeepScaleR Qwen3-8B         

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 408/500 426/500 463/500
> 284/500 265/500 261/500 0.574 0.602 0.614
> Correct Num RFCS( < 1) RFCS(avg)

Figure 3. Statistics of RFCS on MATH500 across LRMs. 

terminate their thinking process at the appropriate moment under the current inference paradigm (i.e., pass@1). 

In summary, the surprising performance of relatively shorter responses in pass@k reveals the inherent poten-tial of the model for efficient reasoning. The pervasive redundancy of reasoning steps in pass@1 indicates that current sampling paradigms obscure this potential. 

Therefore, we attempt to adopt a sampling strategy with a larger exploration space built upon pass@1 to intentionally uncover the precise reasoning chains that are hidden within the broader pass@k distribution. 

3. Intentionally Exploring Shorter CoTs 

Notations. Given a query x and a prefix y<k =

(y1, y 2, ...y k‚àí1) previously generated by the language model œÄŒ∏ , We define Œ¶ as the average cumulative log-probability up to generation step k, where œï(yi; y<i) is the (next-token) log-probability of the i-th token in œÄŒ∏ :

Œ¶(y‚â§k) = 1

k

k

‚àë

i=1

œï(yi; y<i). (1) 

œï(yi; y<i) = log œÄŒ∏ (yi ‚à£ y<i, x). (2) 

Token-Wise Reasoning Path Exploration. We first pro-pose a token-wise reasoning path expansion algorithm until a maximum step budget Tmax is reached. With exploration width m (denoted as EW ), we maintain the top-m candidate sequences according to the scoring function Œ¶, and expands them in subsequent decoding steps (Meister et al., 2020). Formally, given a set of m candidate sequences Yi‚àí1 =

{y(1)

‚â§i‚àí1, y(2)

‚â§i‚àí1, . . . , y(m)

‚â§i‚àí1} at timestep i ‚àí 1, for y(j)

‚â§i‚àí1 ‚àà

Yi‚àí1, j ‚àà [1, m ], we select the top 2m most probable tokens 

T (j) = Top 2m ({ yi‚à£ yi ‚àà V}; œï(‚ãÖ; y(j)

‚â§i‚àí1)) (3) where Top m(‚ãÖ; œï) denotes an operator that ranks all can-didate elements in descending order according to their œï-scores and returns the subset consisting of the top 2m ele-ments with the highest scores. This yields a candidate group of size 2m √ó m = 2m2, formally written as 

ÀÜYi = {y(j,k )

‚â§i ‚à£ j ‚àà [m], k ‚àà [2m]} , (4) where each candidate sequence is constructed by appending the k-th best token to the j-th beam: 

y(j,k )

‚â§i = y(j)

‚â§i‚àí1 ‚äï y(j,k )

i , y(j,k )

i ‚àà T (j). (5) We retain the top-m highest-scoring candidate sequences for next iteration: 

Yi = Top m ({ y(j,k )

‚â§i ‚à£ j ‚àà [m], k ‚àà [2m]} ; Œ¶ ). (6) 

Exploration Termination. We denote the Tolerance ac-cept rank Ratio of </think > h/2m as TR , where h ‚àà

{1, 2, . . . , 2m} is a hyperparameter representing the toler-ance for the rank of </think > . Given the required number of CoTs r ‚àà {1, 2, . . . , m }, once we have reached a can-didate sequence y(j,k )

‚â§i , where y(j,k )

i is </think > and within the top-h probable tokens top h T (j), we add it as a com-pletion to the candidate sequence set O. Otherwise, we discard this candidate sequence, as the model‚Äôs confidence in terminating the thinking process is low at this point (Liu et al., 2025). When ‚à£O‚à£ ‚â• r, we terminate the en-tire process. If when i = Tmax and ‚à£O‚à£ < r, we also add 

top r‚àí‚à£O‚à£ ({ YTmax ‚äï</think >}; Œ¶ ) to O to ensure ‚à£O‚à£ = r.

Greedy Sampling of the Answers. Through the above process, we generate r reasoning chains ti ‚àà O for each question x. Next, we derive the answer œÄŒ∏ (ai‚à£x, ti) greedily based on the query and the internal reasoning chains. Ulti-mately, for each question x, our decoding strategy generates 

r completions {ti, ai}, i ‚àà [r].3Submission and Formatting Instructions for ICML 2026 

Notably, although our algorithm is built upon vanilla beam search, it exhibits significant differences. We provide a detailed comparative analysis in Appendix B. 

4. Your Reasoning Model Implicitly Knows When to Stop Thinking 

Built upon the observations of Section 2 and take a step further, we conduct analytical experiments involving the following related algorithms: 

TSearch(m, r) w/ Œ¶ denotes the algorithm from Section 3 with exploration width m and r returned completions. 

TSearch (m, r) w/ œï is a TSearch variant used to ablate the role of Œ¶, which greedily retains the top-m candidate sequences with the most probable new token at each step according to the following equation instead of Equation 6:            

> Yi={y(j,k )
> ‚â§i
> ¬ª¬ª¬ª¬ª¬ª¬ª (j, k )‚àà
> arg Top m({ y(j,k )
> i‚à£j‚àà[m], k ‚àà[2m]} ;œï(‚ãÖ;y(j)
> ‚â§i‚àí1))} .
> (7)

EW = 0 denotes greedy sampling, which essentially rep-resents a degeneration of TSearch with no exploration. Li et al. (2023) found that its performance is comparable to the average results obtained from random sampling. 

Random refers to standard random sampling with tempera-ture and top-p both set to 1.0. 

4.1. High-Confidence Paths Lead to Efficient Reasoning 

We use reasoning chains retained by Œ¶ to represent the high-confidence paths generated during TSearch. To assess the role of the Œ¶ during this process, we compare TSearch w/ Œ¶

with TSearch w/ œï across increasing exploration width m.Tsearch w/ Œ¶ TSearch w/ ùúô      

> Length Collapse
> Figure 4. Comparison of TSearch variants with increasing EW on DS-7B and a randomly selected subset of MATH-500 (size = 100) under a 10k token budget. To directly investigate the influence of
> Œ¶, we uniformly set TR = 1.

Enlarging the exploration width m influences TSearch in two contrasting ways. On the positive side, a broader candi-date token window T facilitates the discovery of more varied reasoning paths and improves the probability of identifying optimal solutions among pass@k samples (Shrivastava et al., 2025; Hassid et al., 2025). On the negative side, a larger T

is used for </think > detection. With TR = 1, termination occurs immediately upon </think > ‚Äôs appearance, probably leading to significant length collapse. The results in Figure 4 clearly demonstrate the pivotal role of Œ¶.

Observation 1 (Figure 4). In TSearch w/ Œ¶, increas-ing m leads to a consistent reduction in response length accompanied by a steady improvement in accuracy. By contrast, TSearch w/ œï suffers a rapid degradation in accuracy that closely tracks the sharp decline in response length. Furthermore, enlarging the exploration space represents an opportunity to enhance reasoning chain quality when Œ¶ is present, whereas its absence makes length collapse and performance deterioration an in-evitable consequence. These results indicate that the high-confidence branches preserved by Œ¶ are not only markedly shorter, but also substantially more effec-tive. 

4.2. High-Confidence Paths Lead to Confident Ends 

To further investigate the length collapse problem in TSearch w/ œï illustrated in Section 4.1, we apply TR to drop branches concluded with low confidence. The experi-mental results are shown in Table 1.                                                      

> Table 1. Comparison of TSearch (4,1) variants under different TR
> with the same settings of Figure 4. When TR <1, TSearch prunes candidate sequences where the rank ratio of </think >within T
> is lower than TR .ACC denotes the accuracy, LEN refers to the average response length, T-LEN represents the average number of think tokens. Method TR ACC T-LEN LEN
> Random -0.84 3126 3419 TSearch (4,1) w/ œï1.00 0.79 1712 2129 TSearch (4,1) w/ œï0.75 0.82 2022 2333 TSearch (4,1) w/ œï0.50 0.89 2176 2609 TSearch (4,1) w/ Œ¶1.00 0.92 2213 2609 TSearch (4,1) w/ Œ¶0.75 0.92 2221 2621 TSearch (4,1) w/ Œ¶0.50 0.91 2212 2632

As for TSearch w/ Œ¶, varying the TR has virtually no impact on performance. By comparison, it exerts a strong influence on TSearch w/ œï. This indicates a strong correlation be-tween the presence of Œ¶ and the ranking of </think > within 

T . To further study the correlation between them, we record the average rank ratio at which </think > appears during TSearch and illustrate them in in Figure 5. We observe that as EW increases, the </think > token iden-tified by TSearch w/ Œ¶ consistently ranks first within the candidate set T at the moment it appears when evaluated by Œ¶. This behavior indicates that the policy is highly con-fident in terminating the reasoning process once </think >

enters T . In contrast, for TSearch w/ œï, the rank ratio of 4Submission and Formatting Instructions for ICML 2026 

Figure 5. The average rank ratio of </think > in T upon appearance. 

the </think > token gradually increases as measured by œï,suggesting increasing uncertainty about whether the next token should be </think > . This discrepancy explains the significant differences in the role of TR between TSearch w/ Œ¶ and TSearch w/ œï, as reported in Table 1. 

Observation 2 (Figure 6). The policy implicitly ex-hibits high confidence in terminating a high-confidence reasoning chain, as supported by TS EARCH with the cumulative probability Œ¶. However, the final </think >

token may have a relatively low next-token probabil-ity, which is revealed by TS EARCH w/ œï. This discrep-ancy indicates that many short yet high-quality reasoning chains are likely to be overlooked by greedy or random sampling strategies. </think>       

> Step jAlternatively
> Wait
> Thus ‚Ä¶
> ‚Ä¶
> ‚Ä¶
> ‚Ä¶
> ‚Ä¶
> Step i
> ‚Ä¶
> Confident to terminate thinking !
> </think>
> Alternatively
> Wait
> Thus
> Unconfident to terminate thinking !
> Step 0
> Select Based on
> Model‚Äôs Confidence
> Select withou t
> Model‚Äôs Confidence

Figure 6. Illustration of Observation 2. When reasoning branches are retained according to the model‚Äôs confidence at each expansion step, the model is able to conclude them with strong confidence. 

4.3. Scaling Exploration Drives Capability Convergence 

In this section, we conduct further experiments to probe the upper boundary of the efficient reasoning capability illus-trated in Section 4.1. Specifically, under sufficient token budget Tmax = 32,768, we adopt TSearch (m, 1) w/ Œ¶ as the sampling strategy, and compare the pass@1 and response length of DS-1.5B and DeepScaleR on MATH-500 and AMC23 as the exploration width m increases. An increase in m corresponds to a larger exploration space during the generation of reasoning chains. The results are shown in Figure 15. For a clearer visualization of the model‚Äôs per-formance trends, we measure reasoning efficiency for each run in Figure 15 using token efficiency (pass@1 / response length), as illustrated in Figure 7. 

(1) At EW = 0, the model operates in a completely non-exploratory regime and exhibits limited reasoning efficiency. This indicates that standard non-exploratory greedy or ran-dom sampling constrains the model‚Äôs inherent ability, which is fully consistent with the observations in Section 2. 

(2) As shown in Figure 15, enlarging the exploration width leads to consistent improvements in pass@1 while simulta-neously reducing response length, with both metrics exhibit-ing a trend toward gradual convergence. This trend further verifies reasoning models‚Äô inherent efficient reasoning capa-bility. From Figure 7, we can clearly find that this capability is progressively unleashed as the exploration width grows. 

(3) LRMs gradually approach the boundary of their inherent efficient reasoning capability as the degree of exploration increases, and this phenomenon is not an isolated occurrence but a universal pattern observed across models and datasets. None -Exploration 

> Unleashing Capabilities
> through Exploration

Figure 7. Token efficiency comparison on each run in Figure 15. 

Observation 3 (Figure 7) . As the exploration space expands during reasoning, LRM is increasingly capa-ble of identifying precise and compact reasoning paths with high confidence. Furthermore, with the continued growth of the exploration space, this behavior demon-strates an obvious convergence trend. 

Furthermore, as a post-trained version of DS-1.5B, Deep-ScaleR exhibits steeper token efficiency improvement on both MATH-500 and AMC23. This suggests that greater post-training enhances the model‚Äôs ability to leverage in-creased exploration space for unleashing its intrinsic effi-cient reasoning potential. In summary, when provided with adequate exploration space, LRMs can identify precise and concise reasoning chains with high confidence and appropriately terminate the reasoning process, indicating that these models pos-sess an inherent sense of when to stop reasoning. By contrast, current purely sampling-based strategies im-plicitly limit this capability of LRMs by relying solely on the next-token probability distribution. 5Submission and Formatting Instructions for ICML 2026 MATH50 0 AMC 2023  

> Figure 8. Performance comparison with SAGE and Degrade-SAGE on MATH-500 and AMC23 under different generation step budgets.

5. Self-Aware Guided Efficient Reasoning 

5.1. Methodology 

While Section 4.3 demonstrates that TSearch w/ Œ¶ effec-tively unleashes the efficient reasoning potential of LRMs as the exploration space expands, the method remains in-herently greedy. Our goal, however, is to translate this insight into random sampling‚Äìbased inference paradigms. Fortunately, from prior analysis in Section 4.2, when Œ¶ is present, </think > consistently achieves the top rank upon appearance. This observation implies that TSearch w/ Œ¶ is effectively equivalent to directly identifying reasoning steps that terminate with </think > , rendering token-level reason-ing chain expansion unnecessary. Based on this observation and built upon TSearch w/ Œ¶, we introduce Self-Aware 

Guided Efficient Reasoning ( SAGE ), a simple yet effective sampling paradigm that performs step-wise reasoning chain expansion. SAGE differs from TSearch w/ Œ¶ in only the following two respects: 

Step-Wise Reasoning Chain Exploration. At step i, each candidate sequence is extended by one full reasoning step r

until the maximum reasoning step limit Tmax is reached: 

y(j,k ) 

> ‚â§i

= y(j) 

> ‚â§i‚àí1

‚äï r(j,k ) 

> i

, r(j,k ) 

> i

‚àà R(j), (8) where R(j) ‚âú {r(j, 1) 

> i

, r(j, 2) 

> i

, . . . , r(j, 2m) 

> i

} denotes the set of 

2m reasoning steps independently sampled from the policy 

œÄŒ∏ conditioned on the query x and prefix y(j) 

> ‚â§i‚àí1

using vanilla random sampling. This process replaces the token-level expansion in Equation 5. 

Exploration Termination. Based on the conclusions in Section 4.2, we no longer need to manually set the tolerance rank ratio TR as the high-confidence reasoning branches consistently lead to confident ends. Our termination con-dition can be simply defined as : If we have reached a candidate sequence y(j,k ) 

> ‚â§i

, where r(j,k ) 

> k

ends with </think > ,we add it as a completion to the candidate sequence set O.

5.2. SAGE Inference Scaling Trends with Step Budget 

We introduce a step-wise alternative to random sampling namely Degrade SAGE to ablate the exploration space of SAGE. Degrade SAGE directly samples one reasoning step at each iteration until </think > appears or Tmax is reached. To balance computational efficiency and performance (dis-cussed in Appendix D.4), we adopt SAGE (2,1) as the rep-resentative of our algorithm. We scale the maximum reason-ing step budget gradually and compare the pass@1 and re-sponse length of SAGE and Degraded SAGE on MATH-500 (mean@4) and AMC23 (mean@16) respectively. We mark 

Random results for DeepScaleR and DS-1.5B at 32,768 token budget with red and blue dashed lines, respectively. 

(1) The inference scaling trends of SAGE demonstrate the model‚Äôs capability to terminate thinking at appro-priate timings. Under constrained step budgets, SAGE out-performs Degraded SAGE in pass@1 with similar sequence lengths. This advantage stems from SAGE stopping think-ing earlier, leading to more complete CoTs. When step bud-gets are ample, a relatively stable performance gap emerges between SAGE and Degraded SAGE. Here, with token count no longer a bottleneck, the difference stem solely from reasoning chain exploration. These results clearly show that SAGE effectively identifies reasoning chains superior to those of Degraded SAGE, as they are both shorter and more likely to lead to correct answers. 

(2) SAGE prioritizes performance for strong models and hard datasets, and efficiency for weaker models and sim-ple datasets. On stronger DeepScale and harder AMC23, we observe greater pass@1 gains. In contrast, on weaker DS-1.5B and simpler MATH-500, we note larger response length reductions. From the model‚Äôs perspective , stronger models have a higher capability ceiling, enabling SAGE to deliver larger accuracy gains with more necessary tokens. In contrast, weaker models suffer from more severe overthink-ing, creating more chances for token redundancy reduction. 

From the dataset‚Äôs perspective , LRMs can solve most problems on easier datasets, making response length the key optimization goal. By exploiting the model‚Äôs inherent sense of when to stop thinking, SAGE identifies shorter reasoning chains to reduce response length significantly. Conversely, harder datasets contain more challenging problems requiring more tokens to solve, and SAGE boosts accuracy notably on them, confirming its efficacy on uncovering correct rea-soning chains with minimal necessary tokens. 6Submission and Formatting Instructions for ICML 2026 

Table 2. Pass@1, response length (LEN) and token efficiency (TE) results on four complex mathematical benchmarks. TE is calculated as Pass@1 / LEN. Bold and underlined denote the best and second-best results. 

Method MATH-500 AIME 2024 AIME 2025 OlympiadBench Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3) Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3) Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3) Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3)

DS-1.5B 83.2 4882 17.0 25.1 12300 2.04 20.9 11669 1.79 33.4 8954 3.73 + LC-R1 80.4 (‚Üì2.8) 2973 (‚Üì1909) 27.0 (‚Üë58.8%) 23.3 (‚Üì1.8) 7098 (‚Üì5202) 3.28 (‚Üë60.8%) 20.9 (‚Üë0.0) 6942 (‚Üì4727) 3.01 (‚Üë68.2%) 32.0 (‚Üì1.4) 4632 (‚Üì4322) 6.91 (‚Üë85.3%) 

+ ThinkPrune-2k 81.7 (‚Üì1.5) 2826 (‚Üì2056) 28.9 (‚Üë70.0%) 23.7 (‚Üì1.4) 7085 (‚Üì5215) 3.35 (‚Üë64.2%) 19.7 (‚Üì1.2) 6918 (‚Üì4751) 2.85 (‚Üë59.2%) 32.9 (‚Üì0.5) 4752 (‚Üì4202) 6.92 (‚Üë85.5%) 

+ AdaptThink 80.4 (‚Üì2.8) 2563 (‚Üì2319) 31.4 (‚Üë84.1%) 25.7 (‚Üë0.6) 8055 (‚Üì4245) 3.19 (‚Üë56.4%) 21.8 (‚Üë0.9) 8155 (‚Üì3514) 2.67 (‚Üë49.2%) 32.6 (‚Üì0.8) 4563 (‚Üì4391) 7.14 (‚Üë91.4%) 

+ Efficient Reasoning 82.0 (‚Üì1.2) 2821 (‚Üì2061) 29.1 (‚Üë70.6%) 26.2 (‚Üë1.1) 9189 (‚Üì3111) 2.85 (‚Üë39.7%) 22.9 (‚Üë2.0) 8590 (‚Üì3079) 2.67 (‚Üë49.2%) 33.8 (‚Üë0.4) 5755 (‚Üì3199) 5.87 (‚Üë57.4%) 

+ GRPO 83.6 (‚Üë0.4) 3907 (‚Üì975) 21.4 (‚Üë25.6%) 28.3 (‚Üë3.2) 8767 (‚Üì3533) 3.23 (‚Üë58.3%) 24.1 (‚Üë3.2) 8263 (‚Üì3406) 2.92 (‚Üë63.1%) 34.2 (‚Üë0.8) 6323 (‚Üì2631) 5.41 (‚Üë45.0%) 

+ SAGE-GRPO 84.8 (‚Üë1.6) 2915 (‚Üì1967) 29.1 (‚Üë70.7%) 28.8 (‚Üë3.7) 7243 (‚Üì5057) 3.98 (‚Üë95.1%) 26.5 (‚Üë5.6) 7479 (‚Üì4190) 3.54 (‚Üë97.8%) 36.9 (‚Üë3.5) 5050 (‚Üì3904) 7.31 (‚Üë96.0%) 

+ GSPO 83.4 (‚Üë0.2) 3898 (‚Üì984) 25.3 (‚Üë21.4%) 28.3 (‚Üë3.2) 8604 (‚Üì3696) 3.29 (‚Üë61.3%) 25.1 (‚Üë4.2) 8227 (‚Üì3442) 3.05 (‚Üë70.4%) 34.6 (‚Üë1.2) 6410 (‚Üì2544) 5.40 (‚Üë44.8%) 

+ SAGE-GSPO 85.2 (‚Üë2.0) 2921 (‚Üì1961) 29.2 (‚Üë71.6%) 28.5 (‚Üë3.4) 6889 (‚Üì5411) 4.14 (‚Üë102.9%) 27.1 (‚Üë6.2) 7167 (‚Üì4502) 3.78 (‚Üë111.1%) 37.3 (‚Üë3.9) 5172 (‚Üì3782) 7.21 (‚Üë93.3%) 

DeepScaleR 86.0 3805 22.6 31.4 9370 3.35 25.4 9310 2.73 35.9 5972 6.01 + ThinkPrune-2k 82.5 (‚Üì3.5) 2946 (‚Üì859) 28.0 (‚Üë23.9%) 33.5 (‚Üë2.1) 8108 (‚Üì1262) 4.13 (‚Üë23.3%) 26.0 (‚Üë0.6) 7486 (‚Üì1824) 3.47 (‚Üë27.1%) 35.1 (‚Üì0.8) 4723 (‚Üì1249) 7.43 (‚Üë23.6%) 

+ GRPO 87.6 (‚Üë1.6) 3482 (‚Üì323) 25.2 (‚Üë11.3%) 35.6 (‚Üë4.2) 8592 (‚Üì778) 4.14 (‚Üë23.6%) 27.4 (‚Üë2.0) 8185 (‚Üì1125) 3.35 (‚Üë22.7%) 36.2 (‚Üë0.3) 5443 (‚Üì529) 6.65 (‚Üë10.6%) 

+ SAGE-GRPO 88.8 (‚Üë2.8) 3117 (‚Üì688) 28.4 (‚Üë25.7%) 36.1 (‚Üë4.7) 8094 (‚Üì1276) 4.46 (‚Üë33.1%) 27.2 (‚Üë1.8) 7704 (‚Üì1606) 3.53 (‚Üë29.3%) 36.5 (‚Üë0.6) 4890 (‚Üì1082) 7.46 (‚Üë24.1%) 

DS-7B 91.6 3871 23.7 51.9 11305 4.59 37.1 12540 2.96 39.8 7839 5.08 + LC-R1 87.3 (‚Üì4.3) 2076 (‚Üì1795) 42.1 (‚Üë77.7%) 51.7 (‚Üì0.2) 6820 (‚Üì4485) 7.58 (‚Üë65.1%) 35.7 (‚Üì1.4) 7458 (‚Üì5082) 4.79 (‚Üë61.8%) 41.4 (‚Üë1.6) 4193 (‚Üì3646) 9.87 (‚Üë94.3%) 

+ AdaptThink 88.9 (‚Üì2.7) 2199 (‚Üì1672) 40.4 (‚Üë70.9%) 52.1 (‚Üë0.2) 6679 (‚Üì4626) 7.80 (‚Üë69.9%) 35.0 (‚Üì2.1) 7807 (‚Üì4733) 4.48 (‚Üë72.3%) 38.9 (‚Üì0.9) 4915 (‚Üì2924) 7.91 (‚Üë55.7%) 

+ Efficient Reasoning 89.8 (‚Üì1.8) 2408 (‚Üì1463) 37.3 (‚Üë57.6%) 51.9 (‚Üë0.0) 6667 (‚Üì4638) 7.78 (‚Üë69.5%) 36.2 (‚Üì0.9) 7501 (‚Üì5039) 4.82 (‚Üë62.8%) 40.1 (‚Üë0.3) 4599 (‚Üì3240) 8.72 (‚Üë71.7%) 

+ GRPO-LEAD 89.5 (‚Üì2.1) 2752 (‚Üì1119) 32.5 (‚Üë37.1%) 53.1 (‚Üë1.2) 7023 (‚Üì4282) 7.56 (‚Üë64.7%) 36.1 (‚Üì1.0) 7842 (‚Üì4698) 4.60 (‚Üë55.4%) 40.6 (‚Üë0.8) 4972 (‚Üì2867) 8.17 (‚Üë60.8%) 

+ GRPO 92.0 (‚Üë0.4) 3219 (‚Üì652) 28.5 (‚Üë20.2%) 52.5 (‚Üë0.6) 8424 (‚Üì2881) 6.23 (‚Üë35.7%) 38.4 (‚Üë1.3) 10123 (‚Üì2417) 3.79 (‚Üë28.0%) 41.2 (‚Üë1.4) 5498 (‚Üì2341) 7.50 (‚Üë47.6%) 

+ SAGE-GRPO 93.0 (‚Üë1.4) 2141 (‚Üì1730) 43.4 (‚Üë83.1%) 55.3 (‚Üë3.4) 6422 (‚Üì4883) 8.61 (‚Üë87.6%) 38.0 (‚Üë0.9) 6583 (‚Üì5957) 5.77 (‚Üë94.9%) 41.8 (‚Üë2.0) 4435 (‚Üì3404) 9.42 (‚Üë85.4%) 

Qwen3-8B 94.4 5640 16.7 73.2 15920 4.60 67.3 18342 3.67 46.6 11707 4.00 + GRPO 93.6 (‚Üì0.8) 4470 (‚Üì1170) 20.9 (‚Üë25.1%) 72.8 (‚Üì0.4) 10573 (‚Üì5347) 6.89 (‚Üë49.8%) 66.6 (‚Üì0.7) 13981 (‚Üì4361) 4.76 (‚Üë29.7%) 45.1 (‚Üì1.5) 7512 (‚Üì4195) 6.00 (‚Üë50.0%) 

+ SAGE-GRPO 95.0 (‚Üë0.6) 3015 (‚Üì2625) 31.5 (‚Üë88.2%) 73.5 (‚Üë0.3) 8975 (‚Üì6945) 8.19 (‚Üë78.0%) 66.6 (‚Üì0.7) 10052 (‚Üì8290) 6.58 (‚Üë79.3%) 45.4 (‚Üì1.2) 5972 (‚Üì5735) 7.60 (‚Üë90.0%) 

+ GSPO 94.6 (‚Üë0.2) 4342 (‚Üì1298) 22.2 (‚Üë32.9%) 73.0 (‚Üì0.2) 10544 (‚Üì5376) 6.92 (‚Üë50.4%) 66.2 (‚Üì1.1) 14082 (‚Üì4260) 4.70 (‚Üë30.2%) 46.6 (‚Üë0.0) 7964 (‚Üì3743) 5.85 (‚Üë46.2%) 

+ SAGE-GSPO 94.4 (‚Üë0.0) 2753 (‚Üì2887) 34.3 (‚Üë105.3%) 73.7 (‚Üë0.5) 8547 (‚Üì7373) 8.62 (‚Üë87.4%) 66.0 (‚Üì1.3) 9183 (‚Üì9159) 7.19 (‚Üë95.9%) 46.7 (‚Üë0.1) 5436 (‚Üì6271) 8.59 (‚Üë114.7%) 

6. SAGE-RL: Integrating Efficient Reasoning Patterns into Current Inference Paradigms 

As shown in Section 5, SAGE effectively unleashes reason-ing models‚Äô implicit capacity for efficient reasoning. An appealing extension is to incorporate the efficient reasoning pattern uncovered by SAGE into standard pass@1 inference. Thus, we introduce SAGE-RL , a simple modification to RLVR, to achieve this goal. Given a question q, RLVR typically samples a group of responses G = {o1, . . . , o G} from the current policy. The sole difference between SAGE-RL and RLVR lies in the rollout phase, where SAGE-RL employs a hybrid sampling strategy. SAGE-RL employs SAGE (m,r) to generate r

responses {oS

1 , o S

2 , . . . , o Sr } and uses standard random sam-pling for the remaining G‚àír responses {oR

1 , o R

2 , . . . , o RG‚àír }.Ultimately, the rollout phase in SAGE-RL yields the set of responses G = {oS

1 , . . . , o Sr , o R

1 , . . . , o RG‚àír } for each q.

7. Experiments 

We apply both RLVR (GRPO (Shao et al., 2024), GSPO (Zheng et al., 2025a) ) and corresponding SAGE-RL method (SAGE-GRPO, SAGE-GSPO) to tune four widely adopted LRMs with a group size of G = 8. The training objectives of these algorithms can be found in Appendix C.1. Within each group, SAGE-RL employs SAGE (2,2) to search for two completions with precise reasoning chains, while the remaining six completions are obtained through default random sampling in verl . We also compare with exist-ing open-source methods, including LC-R1 (Cheng et al., 2025), ThinkPrune (Hou et al., 2025), AdaptThink (Zhang et al., 2025), Efficient-Reasoning (Arora et al., 2025), and GRPO-LEAD (Zhang et al., 2025). Additional implemen-tation details are provided in Appendix C.2 due to space constraints. 

7.1. Main Results 

Table 2 presents a performance comparison among SAGE-RL and baselines. Due to space constraints, we present results from only four out of the six evaluated datasets. The complete experimental results and additional analysis are provided in Appendix D. 

(1) SAGE-RL achieves comprehensive improvements in both reasoning capability and token efficiency. As shown in Table 2, most baselines achieve token compression at the cost of reduced reasoning capability. For instance, on MATH-500, AdaptThink compresses the token count of DS-1.5B from 4,882 to 2,563, but at the expense of a 2.8% drop in pass@1. Similar performance degradation is also widely observed across AIME 2024, AIME 2025 and OlympiaBench. RLVR was initially proposed to improve reasoning performance through extended reasoning lengths (DeepSeek-AI, 2025), yet existing baselines compromise this capability to different extents. In contrast, SAGE-RL consistently achieve the best or second-best token efficiency across all benchmarks, while effectively improving the base models‚Äô capabilities on these complex reasoning tasks. This is because SAGE-RL achieves efficient reasoning by enabling LRMs to learn more precise reasoning chains, simultaneously shortening 7Submission and Formatting Instructions for ICML 2026 

Figure 9. Training Dynamics comparison between RLVR and SAGE-RL. The left two figures present results evaluated every 10 steps on MATH-500 under an 8,192 token budget. The right two figures illustrate the entropy and KL divergence of the policy for every step. 

the inference trajectories while enhancing reasoning capabil-ity. As illustrated in Figure 8, the reasoning chains sampled by SAGE are shorter than those from standard sampling and more effectively guide the model toward correct so-lutions. In group-based comparison processes similar to GRPO, this advantage is amplified by the baseline‚Äôs regu-larization. Since SAGE more frequently yields high-reward outcomes, the policy model naturally shifts its reasoning patterns toward the efficient modes discovered by SAGE. 

(2) SAGE-RL effectively enables LRMs to learn efficient reasoning patterns. As shown in Table 2, although vanilla GRPO and GSPO moderately improve the reasoning ca-pability of LRMs compared to other baselines, the infer-ence trajectories learned by LRMs from standard random sampling still contain substantial token redundancy. Conse-quently, the overall token efficiency remains significantly lower than that of efficient reasoning baselines. In con-trast, SAGE-RL achieves substantial improvements in both reasoning capability and token efficiency. Since the only dif-ference lies in the sampling strategy for 2 out of 8 samples per group, the results demonstrate that SAGE-RL effectively enables the policy model to learn shorter yet more accurate reasoning patterns. Figure 9 clearly illustrates this process. As training pro-gresses, deploying SAGE-RL on both GRPO and GSPO leads to more pronounced improvements in pass@1 and greater reductions in response length. In contrast to stan-dard RLVR, SAGE-RL shows a more significant entropy reduction, suggesting that the policy model gradually ac-quires the precise reasoning chains identified by SAGE, resulting in greater confidence during inference as training progresses. In terms of KL divergence, SAGE-RL also ex-hibits a more pronounced increasing trend. This indicates that the policy model deviates more significantly from the original probability distribution as training progresses. Such behavior suggests that the reasoning chains generated by SAGE, compared to those from random sampling, induce larger updates in the model. This is primarily because un-leashing the model‚Äôs efficient reasoning capability requires more substantial updates to learn reasoning patterns that SAGE-RL-DS-1.5B SAGE-RL-DeepScaleR SAGE-RL-Qwen3-8B         

> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 425/500 438/500 477/500
> 184/500 163/500 151/500
> 0.704 0.712 0.832
> Correct Num RFCS( < 1) RFCS(avg)

Figure 10. Statistics of RFCS on MATH-500 across different SAGE-RL-tuned models. 

differ markedly from the original ones. As SAGE-RL‚Äôs improvement solely stems from the rollout phase, the direct comparison with RLVR in this section serves as an effective ablation study of our approach. 

7.2. Analysis on Reasoning Behavior 

We computed the RFCS metric on MATH-500 for SAGE-GRPO-tuned models, with results shown in Figure 10. Across all models, the proportion of samples with RFCS( <1) decreases substantially compared to Figure 3, indicating a significant reduction in redundant reasoning steps. Simulta-neously, the RFCS(avg) increases markedly, suggesting that the reasoning models more frequently terminate thinking immediately after producing the correct answer. As shown in Figure 16 and Figure 17, SAGE-GRPO-tuned models effectively avoid generating a large number of ineffective reasoning steps. These findings strongly confirm that SAGE-RL effectively teaches LRMs precise reasoning patterns. 

8. Conclusion 

In this work, we uncover and demonstrate that LRMs implic-itly know the appropriate time to stop thinking, but this po-tential is obscured by current sampling paradigms. Built on this observation, We propose SAGE, a sampling paradigm that unleash this capability to uncover precise reasoning chains, yielding significantly CoT length reduction and ac-curacy improvement. By simply integrating SAGE into the rollout process of RLVR, SAGE-RL achieves lasting gains in inference-time reasoning efficiency. 8Submission and Formatting Instructions for ICML 2026 

Impact statement 

This paper uncovers and demonstrates the inherent efficient reasoning potential of LRMs, contributing to the broader field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. 

References 

Aggarwal, P. and Welleck, S. L1: Controlling how long a reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. Aggarwal, P., Madaan, A., Yang, Y., et al. Let‚Äôs sample step by step: Adaptive-consistency for efficient reasoning and coding with llms. arXiv preprint arXiv:2305.11860, 2023. Arora, D. and Zanette, A. Training language models to rea-son efficiently. arXiv preprint arXiv:2502.04463, 2025. Arora et al. Training language models to reason efficiently, 2025. URL https://arxiv.org/abs/2502.04463 .Art of Problem Solving. American invita-tional mathematics examination. https: //artofproblemsolving.com/wiki/index.php/ American Invitational Mathematics Examination ,2024. Accessed: 2025-03-28. Aytes, S. A., Baek, J., and Hwang, S. J. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025. Balachandran, V., Chen, J., Chen, L., Garg, S., Joshi, N., Lara, Y., Langford, J., Nushi, B., Vineet, V., Wu, Y., et al. Inference-time scaling for complex tasks: Where we stand and what lies ahead. arXiv preprint arXiv:2504.00294, 2025. Chaoqun, H., Renjie, L., Yuzhuo, B., Shengding, H., Zhen, T., Junhao, S., Jinyi, H., Xu, H., Yujie, H., Yuxiang, Z., Jie, L., Lei, Q., Zhiyuan, L., and Maosong‚Äù, S. OlympiadBench: A challenging benchmark for promot-ing AGI with olympiad-level bilingual multimodal scien-tific problems. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 3828‚Äì3850, Bangkok, Thailand, August 2024. Association for Computational Linguis-tics. doi: 10.18653/v1/2024.acl-long.211. URL https: //aclanthology.org/2024.acl-long.211/ .Chen, Q., Qin, L., Wang, J., Zhou, J., and Che, W. Un-locking the capabilities of thought: A reasoning bound-ary framework to quantify and optimize chain-of-thought. Advances in Neural Information Processing Systems , 37: 54872‚Äì54904, 2024. Chen, R., Zhang, Z., Hong, J., Kundu, S., and Wang, Z. Seal: Steerable reasoning calibration of large language models for free. arXiv preprint arXiv:2504.07986, 2025a. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., Wang, R., Tu, Z., Mi, H., and Yu, D. Do NOT think that much for 2+3=? on the overthinking of long reasoning models. In Forty-second International Conference on Machine Learning , 2025b. URL https://openreview.net/ forum?id=MSbU3L7V00 .Chen, Z., Ai, T., Li, Y., Li, G., Wei, Y., Zhou, W., Li, G., Yu, B., Chen, Z., Sun, H., Zhuang, F., Li, J., Wang, D., and Ban, Y. Llmboost: Make large language models stronger with boosting, 2025c. URL https://arxiv.org/abs/ 2512.22309 .Cheng, Z., Chen, D., Fu, M., and Zhou, T. Optimizing length compression in large reasoning models, 2025. URL https://arxiv.org/abs/2506.14755 .Chuang, Y.-N., Zhou, H., Sarma, P., Gopalan, P., Boccio, J., Bolouki, S., and Hu, X. Learning to route llms with confidence tokens. arXiv preprint arXiv, 2410, 2024. Cui, Y., He, P., Zeng, J., Liu, H., Tang, X., Dai, Z., Han, Y., Luo, C., Huang, J., Li, Z., et al. Step-wise perplexity-guided refinement for efficient chain-of-thought reasoning in large language models. arXiv preprint arXiv:2502.13260, 2025. Dai, M., Yang, C., and Si, Q. S-grpo: Early exit via rein-forcement learning in reasoning models. arXiv preprint arXiv:2505.07686, 2025. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning ca-pability in llms via reinforcement learning, 2025. URL 

https://arxiv.org/abs/2501.12948 .Fan, C., Zhang, Y., Jia, J., Hero, A., and Liu, S. Cycli-creflex: Improving large reasoning models via cycli-cal reflection token scheduling, 2025. URL https: //arxiv.org/abs/2506.11077 .Gao, J., Xu, S., Ye, W., Liu, W., He, C., Fu, W., Mei, Z., Wang, G., and Wu, Y. On designing effective rl reward at training time for llm reasoning, 2024. URL 

https://arxiv.org/abs/2410.15115 .Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-centivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 9Submission and Formatting Instructions for ICML 2026 

Han, T., Wang, Z., Fang, C., Zhao, S., Ma, S., and Chen, Z. Token-budget-aware llm reasoning, 2024. Han, T., Wang, Z., Fang, C., Zhao, S., Ma, S., and Chen, Z. Token-budget-aware llm reasoning, 2025. URL https: //arxiv.org/abs/2412.18547 .Hassid, M., Synnaeve, G., Adi, Y., and Schwartz, R. Don‚Äôt overthink it. preferring shorter thinking chains for im-proved llm reasoning. arXiv preprint arXiv:2505.17813, 2025. He, X., Ban, Y., Zou, J., Wei, T., Cook, C., and He, J. Llm-forest: Ensemble learning of llms with graph-augmented prompts for data imputation. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 6921‚Äì6936, 2025. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-ematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hou, B., Zhang, Y., Ji, J., Liu, Y., Qian, K., Andreas, J., and Chang, S. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025. URL https: //arxiv.org/abs/2504.01296 .Huang, S., Wang, H., Zhong, W., Su, Z., Feng, J., Cao, B., and Fung, Y. R. Adactrl: Towards adaptive and con-trollable reasoning via difficulty-aware budgeting, 2025a. URL https://arxiv.org/abs/2505.18822 .Huang, X., Vangani, T. K., Liu, Z., Zou, B., and Aw, A. T. Adacot: Rethinking cross-lingual factual reason-ing through adaptive chain-of-thought, 2025b. URL 

https://arxiv.org/abs/2501.16154 .Huang, Z., Ban, Y., Fu, L., Li, X., Dai, Z., Li, J., and Wang, D. Adaptive sample scheduling for direct preference optimization. arXiv preprint arXiv:2506.17252, 2025c. Huang, Z., Xia, X., Ren, Y., Zheng, J., Xiao, X., Xie, H., Li, H., Liang, S., Dai, Z., Zhuang, F., Li, J., Ban, Y., and Wang, D. Real-time aligned reward model beyond se-mantics. 2026. URL https://api.semanticscholar. org/CorpusID:285240754 .Kang, Y., Sun, X., Chen, L., and Zou, W. C3ot: Gen-erating shorter chain-of-thought without compromising effectiveness. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 39, pp. 24312‚Äì24320, 2025. Kimi Team. Kimi k2: Open agentic intelligence. https: //moonshotai.github.io/Kimi-K2/ , 2025a. Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025b. URL https://arxiv.org/abs/2501. 12599 .Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Ef-ficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124 .Lee, A., Che, E., and Peng, T. How well do llms compress their own chain-of-thought? a token complexity approach. arXiv preprint arXiv:2503.01141, 2025. Lewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems , 2022. URL https:// openreview.net/forum?id=IFXTZERXdM7 .Li, Y., Yuan, P., Feng, S., Pan, B., Wang, X., Sun, B., Wang, H., and Li, K. Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning. arXiv preprint arXiv:2401.10480, 2024. Li, Z., Xu, T., Zhang, Y., Lin, Z., Yu, Y., Sun, R., and Luo, Z.-Q. Remax: A simple, effective, and efficient reinforce-ment learning method for aligning large language models. arXiv preprint arXiv:2310.10505, 2023. Liao, B., Xu, Y., Dong, H., Li, J., Monz, C., Savarese, S., Sahoo, D., and Xiong, C. Reward-guided specula-tive decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324, 2025. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let‚Äôs verify step by step, 2023. URL https: //arxiv.org/abs/2305.20050 .Liu, T., Guo, Q., Hu, X., Jiayang, C., Zhang, Y., Qiu, X., and Zhang, Z. Can language models learn to skip steps? arXiv preprint arXiv:2411.01855, 2024. 10 Submission and Formatting Instructions for ICML 2026 

Liu, Y., Zheng, J., Sun, Z., Peng, Z., Dong, W., Sha, Z., Cui, S., Wang, W., and He, X. Thought manipulation: External thought can be efficient for large reasoning models. arXiv preprint arXiv:2504.13626, 2025. Luo, H., Shen, L., He, H., Wang, Y., Liu, S., Li, W., Tan, N., Cao, X., and Tao, D. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025a. Luo, M., Tan, S., Wong, J., Shi, X., Tang, W. Y., Roongta, M., Cai, C., Luo, J., Li, L. E., Popa, R. A., and Stoica, I. DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL, 2025b. Luong, M.-T., Hwang, D., Nguyen, H. H., Ghiasi, G., Chervonyi, Y., Seo, I., Kim, J., Bingham, G., Lee, J., Mishra, S., et al. Towards robust mathematical reason-ing. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing , pp. 35406‚Äì 35430, 2025. Ma, W., He, J., Snell, C., Griggs, T., Min, S., and Zaharia, M. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025a. Ma, X., Wan, G., Yu, R., Fang, G., and Wang, X. Cot-valve: Length-compressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025b. Manvi, R., Singh, A., and Ermon, S. Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024. Mathematical Association of America. Amc con-tests. https://maa.org/student-programs/amc/ ,2023. Accessed: 2025-03-28. Meister, C., Vieira, T., and Cotterell, R. Best-first beam search. TACL, 2020. Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with a reference-free reward, 2024. URL 

https://arxiv.org/abs/2405.14734 .Munkhbat, T., Ho, N., Kim, S. H., Yang, Y., Kim, Y., and Yun, S.-Y. Self-training elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122, 2025. Ong, I., Almahairi, A., Wu, V., Chiang, W.-L., Wu, T., Gonzalez, J. E., Kadous, M. W., and Stoica, I. Routellm: Learning to route llms with preference data, 2024. URL https://arxiv. org/abs/2406.18665, 2024. OpenAI. Learning to reason with llms. https://openai. com/research/learning-to-reason-with-llms ,2025. Accessed: 15 March 2025. OpenAI. Introducing openai o3 and o4-mini. https:// openai.com/index/introducing-o3-and-o4-mini/ ,April 2025a. OpenAI. OpenAI o3: Most advanced reason-ing model. https://openai.com/index/ introducing-o3-and-o4-mini/ , April 2025b. Qi, P., Liu, Z., Pang, T., Du, C., Lee, W. S., and Lin, M. Optimizing anytime reasoning via budget relative pol-icy optimization, 2025. URL https://arxiv.org/abs/ 2505.13438 .Qiao, Z., Deng, Y., Zeng, J., Wang, D., Wei, L., Meng, F., Zhou, J., Ren, J., and Zhang, Y. Concise: Confidence-guided compression in step-by-step efficient reasoning, 2025. URL https://arxiv.org/abs/2505.04881 .Qu, Y., Yang, M. Y., Setlur, A., Tunstall, L., Beeching, E. E., Salakhutdinov, R., and Kumar, A. Optimizing test-time compute via meta reinforcement fine-tuning. arXiv preprint arXiv:2503.07572, 2025. Qwen Team. Qwq-32b: Embracing the power of re-inforcement learning. https://qwenlm.github.io/ blog/qwq-32b/ , 2025. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly a reward model, 2024. URL 

https://arxiv.org/abs/2305.18290 .Renze, M. and Guven, E. The benefits of a concise chain of thought on problem-solving in large language models. In 2024 2nd International Conference on Foundation and Large Language Models (FLLM) , pp. 476‚Äì483. IEEE, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347 .Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Push-ing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shen, Y., Zhang, J., Huang, J., Shi, S., Zhang, W., Yan, J., Wang, N., Wang, K., Liu, Z., and Lian, S. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472, 2025a. Shen, Y., Zhang, J., Huang, J., Shi, S., Zhang, W., Yan, J., Wang, N., Wang, K., Liu, Z., and Lian, S. Dast: Difficulty-adaptive slow-thinking for large reasoning models, 2025b. URL https://arxiv.org/abs/2503.04472 .11 Submission and Formatting Instructions for ICML 2026 

Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flex-ible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Shrivastava, V., Awadallah, A., Balachandran, V., Garg, S., Behl, H., and Papailiopoulos, D. Sample more to think less: Group filtered policy optimization for concise reasoning. arXiv preprint arXiv:2508.09726, 2025. Song, M., Zheng, M., Li, Z., Yang, W., Luo, X., Pan, Y., and Zhang, F. Fastcurl: Curriculum reinforcement learning with progressive context extension for efficient training r1-like reasoning models, 2025. URL https://arxiv. org/abs/2503.17287 .Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Wen, L., Cai, Y., Xiao, F., He, X., An, Q., Duan, Z., Du, Y., Liu, J., Tang, L., Lv, X., Zou, H., Deng, Y., Jia, S., and Zhang, X. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025a. URL https: //arxiv.org/abs/2503.10460 .Wen, X., Liu, Z., Zheng, S., Ye, S., Wu, Z., Wang, Y., Xu, Z., Liang, X., Li, J., Miao, Z., et al. Reinforcement learn-ing with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245, 2025b. Wu, S., Xie, J., Zhang, Y., Chen, A., Zhang, K., Su, Y., and Xiao, Y. Arm: Adaptive reasoning model, 2025. URL 

https://arxiv.org/abs/2505.20258 .Xia, H., Leong, C. T., Wang, W., Li, Y., and Li, W. Token-skip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. Xie, Y., Kawaguchi, K., Zhao, Y., Zhao, J. X., Kan, M.-Y., He, J., and Xie, M. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36:41618‚Äì41650, 2023. Xu, S., Xie, W., Zhao, L., and He, P. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025a. Xu, S., Xie, W., Zhao, L., and He, P. Chain of draft: Thinking faster by writing less, 2025b. URL https: //arxiv.org/abs/2502.18600 .Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yang, C., Si, Q., Duan, Y., Zhu, Z., Zhu, C., Li, Q., Lin, Z., Cao, L., and Wang, W. Dynamic early exit in reasoning models, 2025b. URL https://arxiv.org/abs/2504. 15895 .Yang, F., Chen, Z., Wang, X., Lu, X., Chai, J., Yin, G., Lin, W., Ma, S., Zhuang, F., Wang, D., Yang, Y., Li, J., and Ban, Y. Your group-relative advantage is biased, 2026. URL https://arxiv.org/abs/2601.08521 .Yeo, E., Tong, Y., Niu, X., Neubig, G., and Yue, X. De-mystifying long chain-of-thought reasoning in LLMs. In ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models , 2025. URL 

https://openreview.net/forum?id=AgtQlhMQ0V .Yi et al. Shorterbetter: Guiding reasoning models to find optimal inference length for efficient reasoning, 2025. URL https://arxiv.org/abs/2504.21370 .Yu, P., Xu, J., Weston, J., and Kulikov, I. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yue, C., Dong, C., Gao, Y., He, H., Chai, J., Yin, G., and Lin, W. Promoting efficient reasoning with verifiable stepwise reward. arXiv preprint arXiv:2508.10293, 2025. Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892 .Zhang, J., Lin, N., Hou, L., Feng, L., and Li, J. Adaptthink: Reasoning models can learn when to think, 2025. URL 

https://arxiv.org/abs/2505.13417 .Zhang et al. Grpo-lead: A difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models, 2025. URL https://arxiv.org/ abs/2504.09696 .Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025a. Zheng, H., Zhou, Y., Bartoldson, B. R., Kailkhura, B., Lai, F., Zhao, J., and Chen, B. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts. arXiv preprint arXiv:2506.02177, 2025b. Zou, J., Ban, Y., Li, Z., Qi, Y., Qiu, R., Yang, L., and He, J. Transformer copilot: Learning from the mistake 12 Submission and Formatting Instructions for ICML 2026 

log in LLM fine-tuning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems ,2025. URL https://openreview.net/forum?id= MRvxlTlkNQ .13 Submission and Formatting Instructions for ICML 2026 

A. Related Work 

A.1. Stimulating Reasoning Capabilities through Reinforcement Learning 

The introduction of OpenAI o1 (OpenAI, 2025) marks a major advance in reasoning performance and the beginning of the LRM era, inspiring efforts to replicate such strong reasoning abilities(Zou et al., 2025; Chen et al., 2025c; He et al., 2025). DeepSeek-R1, for example, achieves comparable results using a simple rule-based reward with the group relative policy optimization (GRPO)(Shao et al., 2024) algorithm, and its open-source release has established RLVR (DeepSeek-AI, 2025; Kimi Team, 2025b; Gao et al., 2024; Lambert et al., 2025; Zeng et al., 2025; Wen et al., 2025a; Song et al., 2025) as an effective paradigm for improving LLM reasoning, and Yang et al. (2026) provides a principled theoretical analysis of its advantage estimation. This paradigm simplifies reward design by employing binary 0/1 rewards determined through rule-based correctness evaluation, eliminating the need for separate reward models as required in original GRPO (Shao et al., 2024; DeepSeek-AI, 2025; Schulman et al., 2017; Huang et al., 2026) implementations, thereby substantially reducing memory and computational overhead during RL training. Subsequent models, including the Kimi K series (Kimi Team, 2025b;a), QwQ (Qwen Team, 2025), and O3 (OpenAI, 2025b), further advance these capabilities. RLVR assigns scores to trajectories based on pre-designed rules, rewarding desirable behaviors and penalizing undesirable ones. This encourages models to generate long CoTs to maximize correctness, fostering advanced reasoning behaviors such as search and backtracking. However, this also engenders a bias toward redundancy over the risk of error, which results in overthinking‚Äîwasting computational resources, impairing model performance, and ultimately limiting the practical applicability of LRMs. 

A.2. Explorations in Efficient Reasoning 

Overthinking issue is first identified and analyzed by Chen et al. (2025b), who observe that LRMs generate lengthy outputs that neither improve accuracy nor introduce new solution strategies especially for easy prompt. To address this, various works explore efficient reasoning from different angles. 

Training-Free Methods typically improve reasoning efficiency through prompting engineering (Han et al., 2024; Xu et al., 2025a; Lee et al., 2025; Renze & Guven, 2024; Chen et al., 2024; Aytes et al., 2025; Chuang et al., 2024; Ong et al., 2024; Xu et al., 2025b; Huang et al., 2025b; Han et al., 2025), Best-of-N sampling pruning (Xie et al., 2023; Liao et al., 2025) and optimizations (Li et al., 2024; Manvi et al., 2024; Aggarwal et al., 2023) , and early-exit (Ma et al., 2025a; Yang et al., 2025b; Fan et al., 2025) mechanisms during reasoning. These approaches cannot fundamentally resolve the issue of redundant reasoning in models, and their effectiveness is often heavily contingent upon the model‚Äôs instruction-following capability. In practice, the observed improvements in experiments are typically modest or insignificant. While SAGE itself is also a training-free algorithm, it essentially serves to unleash the model‚Äôs inherent potential for efficient reasoning. This allows the LRMs to select the currently optimal candidate sequence based on its self-aware at each inference iteration step. 

Offline Training Methods primarily supervised fine-tuning models with variable-length CoT data (Yu et al., 2024; Kang et al., 2025; Xia et al., 2025; Ma et al., 2025b; Munkhbat et al., 2025; Liu et al., 2024; Han et al., 2024). Recently, ConCISE (Qiao et al., 2025) constructs concise CoT data by inserting prompt tokens and employing early-exit during inference, then enhances the model‚Äôs reasoning conciseness through SFT/SimPO (Rafailov et al., 2024; Meng et al., 2024). The primary challenge of this line of work lies in the difficulty of obtaining high-quality short chains of thought, and the offline training paradigm tends to limit the model‚Äôs exploration ability on difficult problems. For similar reasons, we do not choose offline distillation to learn trajectories sampled by SAGE in this work. Since distillation depends on a strong teacher model, we are concerned that self-distillation will limit the upper boundary of the model‚Äôs reasoning capability. 

Online Training Methods mainly adopt reinforcement learning for better generalization. (Kimi Team, 2025b; Shen et al., 2025b; Yeo et al., 2025; Cheng et al., 2025; Team et al., 2025; Luo et al., 2025a; Aggarwal & Welleck, 2025; Arora & Zanette, 2025; Yeo et al., 2025; Shen et al., 2025a; Qu et al., 2025; Cui et al., 2025) introduce length penalties in the reward function to suppress overly long reasoning traces. Yi et al. (2025), Hou et al. (2025), and Qi et al. (2025) optimize performance under a fixed token budget to balance efficiency and effectiveness. GFPO (Shrivastava et al., 2025) attains 14 Submission and Formatting Instructions for ICML 2026 

sampling outputs aligned with the optimization objective via oversampling. S-GRPO (Dai et al., 2025) and VSRM (Yue et al., 2025) truncate reasoning steps and perform repeated rollouts to evaluate the rewards of reasoning subchains, which are then leveraged for RL training. Zhang et al. (2025), Huang et al. (2025a), and Wu et al. (2025) assign predefined thinking patterns based on task difficulty, which essentially reflects a length budget. All the aforementioned methods are heavily rely on sophisticated reward design, which can easily lead to training instability or even reward hacking during the RL training process. Moreover, explicit or implicit integration of length compression into the optimization objective may impair the model‚Äôs reasoning capabilities. In this work, instead of modifying the optimization objective, we optimize the sampling process to enable the policy model to directly learn the efficient reasoning chains uncovered by SAGE via the advantage estimation of RLVR. This design yields the following two key advantages: (1) Low Computational Cost : We eliminate the need for extra oversampling as in GFPO (Shrivastava et al., 2025), where a single parallel sampling step suffices to generate high-quality reasoning chains. Additionally, we do not require repeated rollouts for reward value estimation, a step essential to methods such as S-GRPO (Dai et al., 2025) and VSRM (Yue et al., 2025). (2) Stable Training Dynamics : By preserving all components of RLVR except for the rollout procedure, SAGE-RL exhibits no significant difference in training stability compared with vanilla RLVR. 

B. Significant Differences from Beam Search 

In this section, we highlight the significant distinctions between TSearch w/ Œ¶ and Beam Search from two perspectives: experimental results and underlying principles.                         

> Table 3. Performance Comparison of different sampling strategies on different models (Max Tokens=10,086). Due to the inherent characteristic of Beam Search that it returns multiple responses by default, when calculating the ACC of Beam Search and TSearch, we consider a result correct if it contains at least one correct answer. Model Sampling Strategy ACC LEN
> DS-1.5B Greedy 0.81 4216 Random 0.81 4142 Beam Search (4, 4) 0.82 4472 TSearch w/ Œ¶(4, 4) 0.84 2972
> Qwen3-8B Greedy 0.82 4505 Random 0.82 4526 Beam Search (4, 4) 0.84 4655 TSearch w/ Œ¶(4, 4) 0.89 2946

We compared the performance of vanilla beam search with TSearch w/ Œ¶ on a randomly selected subset of MATH-500 (size=100). For the fairness of comparison, we uniformly set the exploration width to m = 4. Since Beam Search directly returns the final set of candidate sequences, i.e., the number of returned sequences r = m, we therefore uniformly set r = 4.As shown in Table 3, Even though Beam Search generates four responses for each question, its final ACC is only comparable to those of random sampling and greedy sampling. Conversely, our algorithm achieves markedly higher accuracy while significantly reducing average response length. We analyze and illustrate the root causes of these differences in Figure 11. In Case A , although </think > appears within the log-probability window, the corresponding candidate sequence is discarded because its overall confidence score Œ¶ does not rank first. In Case B , a candidate sequence containing </think > is initially retained but is subsequently pruned during further expansion. In contrast, our algorithm directly accepts the sequence upon detecting </think > . These results indicate that our algorithm prevents the premature discarding of precise reasoning branches in later steps and significantly enhancing reasoning efficiency. 15 Submission and Formatting Instructions for ICML 2026 </think> 

Alternatively 

Wait 

Thus 

## ùëí !

0.67 

0.56 

0.78 

0.44 

</think> 

Alternatively 

Wait 

Thus 

0.97 

0.56 

0.78 

0.44 

## ‚Ä¶

## ‚Ä¶

Case A

Case B

## ‚Ä¶    

> Shared Path
> TSearch w/ Œ¶
> Beam Search

## ‚Ä¶

## ‚Ä¶

## ‚Ä¶

## ‚Ä¶

Direct Discard in Beam 

Search Caused by Low Œ¶

## ùëí !

Discarding of Previously Included 

Branches in Beam Search 

Figure 11. Two distinctions between TSearch w/ Œ¶ and vanilla beam search. 

C. Experimental Details 

C.1. Objectives and Training Hyperparameters 

The objectives of GRPO and SAGE-GRPO are as follows: 

JGRPO (Œ∏) = Ex‚àºD, {yi}Gi=1‚àºœÄŒ∏old (‚ãÖ‚à£x) [ 1

G

> G

‚àë

> i=1

1

‚à£yi‚à£

> ‚à£yi‚à£

‚àë

> t=1

min (wi,t (Œ∏)ÃÇAi,t , clip (wi,t (Œ∏), 1 ‚àí Œµ, 1 + Œµ)ÃÇ Ai,t )] , (9) 

JSAGE-GRPO (Œ∏) = Ex‚àºD, {yi}Gi=1‚àºœÄŒ∏old (‚ãÖ‚à£x)[ 1

G (

> r

‚àë

> i=1

1

‚à£yi‚à£

> ‚à£yi‚à£

‚àë

> t=1

min (wi,t (Œ∏)ÃÇAi,t , clip (wi,t (Œ∏), 1 ‚àí Œµ, 1 + Œµ)ÃÇAi,t )√ç√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√ë√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√è 

> SAGE (m, r)

+

> G

‚àë

> i=r+1

1

‚à£yi‚à£

> ‚à£yi‚à£

‚àë

> t=1

min (wi,t (Œ∏)ÃÇ Ai,t , clip (wi,t (Œ∏), 1 ‚àí Œµ, 1 + Œµ)ÃÇAi,t )√ç√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√ë√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√è 

> Random Sampling

)] 

(10) where G is the number of generated responses to each query x (i.e., the group size), and the importance ratio wi,t (Œ∏) and advantage ÃÇ Ai,t of token yi,t are: 

wi,t (Œ∏) = œÄŒ∏ (yi,t ‚à£x, y i, <t)

œÄŒ∏old (yi,t ‚à£x, y i, <t) ,ÃÇ Ai,t =ÃÇ Ai = r(x, y i) ‚àí mean ({ r(x, y i)} Gi=1)

std ({ r(x, y i)} Gi=1) , (11) The objectives of GSPO and SAGE-GSPO are as follows: 

JGSPO (Œ∏) = Ex‚àºD, {yi}Gi=1‚àºœÄŒ∏old (‚ãÖ‚à£x) [ 1

G

> G

‚àë

> i=1

min (si(Œ∏)ÃÇAi, clip (si(Œ∏), 1 ‚àí Œµ, 1 + Œµ)ÃÇ Ai)] , (12) 16 Submission and Formatting Instructions for ICML 2026 

JSAGE-GSPO (Œ∏) = Ex‚àºD, {yi}Gi=1‚àºœÄŒ∏old (‚ãÖ‚à£x)[ 1

G (

> r

‚àë

> i=1

min (si(Œ∏)ÃÇAi, clip (si(Œ∏), 1 ‚àí Œµ, 1 + Œµ)ÃÇAi)√ç√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√ë√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√è 

> SAGE (m, r)

+

> G

‚àë

> i=r+1

min (si(Œ∏)ÃÇAi, clip (si(Œ∏), 1 ‚àí Œµ, 1 + Œµ)ÃÇAi)√ç√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√ë√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√í√è 

> Random Sampling

)] 

(13) 

where we adopt the group-based advantage estimation: ÃÇ

Ai = r(x, y i) ‚àí mean ({ r(x, y i)} Gi=1)

std ({ r(x, y i)} Gi=1) , (14) and define the importance ratio si(Œ∏) based on sequence likelihood: 

si(Œ∏) = ( œÄŒ∏ (yi‚à£x)

œÄŒ∏old (yi‚à£x) )

> 1
> ‚à£yi ‚à£

= exp ( 1

‚à£yi‚à£

> ‚à£yi‚à£

‚àë

> t=1

log œÄŒ∏ (yi,t ‚à£x, y i, <t)

œÄŒ∏old (yi,t ‚à£x, y i, <t) ) . (15) 

C.2. Experimental Setup 

To thoroughly evaluate the effectiveness of SAGE-RL, we conduct experiments using several widely adopted LRMs as base models, including DeepSeek-R1-Distill-Qwen-1.5B (DS-1.5B), DeepSeek-R1-Distill-Qwen-7B (DS-7B) (DeepSeek-AI, 2025), DeepScaleR (Luo et al., 2025b), and Qwen3-8B (Yang et al., 2025a). 

Training Data Considering the importance of training data quality (Huang et al., 2025c), we use the English subset of DAPO (Yu et al., 2025) as well as MATH (Hendrycks et al., 2021) problems with difficulty from level 3 to level 5 (Zheng et al., 2025b). This collection consists of approximately 20,000 carefully curated problems covering a wide range of difficulty levels. 

Training Configuration We use the verl (Sheng et al., 2024) framework for SAGE-RL training using the rule based reward function. To ensure a completely fair comparison that highlights the role of SAGE in the rollout phase, we adopt identical hyperparameter settings for the same base model across SAGE-RL and all its baselines and variants. We tune the base models with a global batch size of 32 across 8 GPUs for 600 steps with the Adam optimizer with learning rate of 1e-6, cosine warmup for the first 50 steps, and sampling temperature T = 1.0. We apply KL regularization with Œ≤ = 0.001 and an entropy coefficient of Œ≥ = 0.001 . Our models are trained with 9,216 maximum context length, with 1,024 tokens reserved for the prompt. 

Sampling Strategy We tune all models with a group size of G = 8. Within each group, SAGE-RL employs SAGE (2,2) to search for two completions with precise reasoning chains, while the remaining six completions are obtained through the default random sampling in verl .

Evaluation We follow previous work (Yue et al., 2025; Liu et al., 2025; Dai et al., 2025) and select a comprehensive set of benchmarks, AIME24, AIME25(Art of Problem Solving, 2024),OlympiadBench(Chaoqun et al., 2024), MATH-500, Minerva(Lewkowycz et al., 2022), and AMC23(Mathematical Association of America, 2023), providing broader coverage than previous studies. During evaluation, we set the maximum generation length at 32768 tokens, consistent with Hou et al. (2025)‚Äôs work and DeepSeek-R1. The temperature and top-p are set to 1.0 and 0.95, respectively. For all benchmarks, we report the average pass@1, response length(LEN) and token efficiency(TE) over N runs. Specifically, for OlympiadBench, Minerva and MATH-500 where the benchmark sizes are relatively large, we set N to 8; for the other benchmarks, we set N to 32 to reduce randomness. 

D. Additional Experimental Results 

D.1. Comparison with Extended Datasets and Additional Analysis 

In this section, we present the complete evaluation results on six mathematical datasets. We divide the benchmarks into two groups of equal size. The three datasets in the upper part of Table 4 are more challenging than those in the lower part. 17 Submission and Formatting Instructions for ICML 2026 

Table 4. Pass@1, response length(LEN) and TE results on six benchmarks and four base models before and after LC-R1, ThinkPrune-2k, AdaptThink, Efficient Reasoning, GRPO-LEAD, GRPO, GSPO, SAGE-GRPO and SAGE-GSPO. TE is calculated as Pass@1/LEN. Bold and underlined numbers denote the best and second-best results. The percentage in parentheses after TE indicates the improvement compared with the base model. 

Method AIME 2024 AIME 2025 OlympiadBench Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3) Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3) Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3)

DS-1.5B 25.1 12300 2.04 20.9 11669 1.79 33.4 8954 3.73 + LC-R1 23.3 (‚Üì1.8) 7098 (‚Üì5202) 3.28 (‚Üë60.8%) 20.9 (‚Üë0.0) 6942 (‚Üì4727) 3.01 (‚Üë68.2%) 32.0 (‚Üì1.4) 4632 (‚Üì4322) 6.91 (‚Üë85.3%) 

+ ThinkPrune-2k 23.7 (‚Üì1.4) 7085 (‚Üì5215) 3.35 (‚Üë64.2%) 19.7 (‚Üì1.2) 6918 (‚Üì4751) 2.85 (‚Üë59.2%) 32.9 (‚Üì0.5) 4752 (‚Üì4202) 6.92 (‚Üë85.5%) 

+ AdaptThink 25.7 (‚Üë0.6) 8055 (‚Üì4245) 3.19 (‚Üë56.4%) 21.8 (‚Üë0.9) 8155 (‚Üì3514) 2.67 (‚Üë49.2%) 32.6 (‚Üì0.8) 4563 (‚Üì4391) 7.14 (‚Üë91.4%) 

+ Efficient Reasoning 26.2 (‚Üë1.1) 9189 (‚Üì3111) 2.85 (‚Üë39.7%) 22.9 (‚Üë2.0) 8590 (‚Üì3079) 2.67 (‚Üë49.2%) 33.8 (‚Üë0.4) 5755 (‚Üì3199) 5.87 (‚Üë57.4%) 

+ GRPO 28.3 (‚Üë3.2) 8767 (‚Üì3533) 3.23 (‚Üë58.3%) 24.1 (‚Üë3.2) 8263 (‚Üì3406) 2.92 (‚Üë63.1%) 34.2 (‚Üë0.8) 6323 (‚Üì2631) 5.41 (‚Üë45.0%) 

+ SAGE-GRPO 28.8 (‚Üë3.7) 7243 (‚Üì5057) 3.98 (‚Üë95.1%) 26.5 (‚Üë5.6) 7479 (‚Üì4190) 3.54 (‚Üë97.8%) 36.9 (‚Üë3.5) 5050 (‚Üì3904) 7.31 (‚Üë96.0%) 

+ GSPO 28.3 (‚Üë3.2) 8604 (‚Üì3696) 3.29 (‚Üë61.3%) 25.1 (‚Üë4.2) 8227 (‚Üì3442) 3.05 (‚Üë70.4%) 34.6 (‚Üë1.2) 6410 (‚Üì2544) 5.40 (‚Üë44.8%) 

+ SAGE-GSPO 28.5 (‚Üë3.4) 6889 (‚Üì5411) 4.14 (‚Üë102.9%) 27.1 (‚Üë6.2) 7167 (‚Üì4502) 3.78 (‚Üë111.1%) 37.3 (‚Üë3.9) 5172 (‚Üì3782) 7.21 (‚Üë93.3%) 

DeepScaleR 31.4 9370 3.35 25.4 9310 2.73 35.9 5972 6.01 + ThinkPrune-2k 33.5 (‚Üë2.1) 8108 (‚Üì1262) 4.13 (‚Üë23.3%) 26.0 (‚Üë0.6) 7486 (‚Üì1824) 3.47 (‚Üë27.1%) 35.1 (‚Üì0.8) 4723 (‚Üì1249) 7.43 (‚Üë23.6%) 

+ GRPO 35.6 (‚Üë4.2) 8592 (‚Üì778) 4.14 (‚Üë23.6%) 27.4 (‚Üë2.0) 8185 (‚Üì1125) 3.35 (‚Üë22.7%) 36.2 (‚Üë0.3) 5443 (‚Üì529) 6.65 (‚Üë10.6%) 

+ SAGE-GRPO 36.1 (‚Üë4.7) 8094 (‚Üì1276) 4.46 (‚Üë33.1%) 27.2 (‚Üë1.8) 7704 (‚Üì1606) 3.53 (‚Üë29.3%) 36.5 (‚Üë0.6) 4890 (‚Üì1082) 7.46 (‚Üë24.1%) 

DS-7B 51.9 11305 4.59 37.1 12540 2.96 39.8 7839 5.08 + LC-R1 51.7 (‚Üì0.2) 6820 (‚Üì4485) 7.58 (‚Üë65.1%) 35.7 (‚Üì1.4) 7458 (‚Üì5082) 4.79 (‚Üë61.8%) 41.4 (‚Üë1.6) 4193 (‚Üì3646) 9.87 (‚Üë94.3%) 

+ AdaptThink 52.1 (‚Üë0.2) 6679 (‚Üì4626) 7.80 (‚Üë69.9%) 35.0 (‚Üì2.1) 7807 (‚Üì4733) 4.48 (‚Üë72.3%) 38.9 (‚Üì0.9) 4915 (‚Üì2924) 7.91 (‚Üë55.7%) 

+ Efficient Reasoning 51.9 (‚Üë0.0) 6667 (‚Üì4638) 7.78 (‚Üë69.5%) 36.2 (‚Üì0.9) 7501 (‚Üì5039) 4.82 (‚Üë62.8%) 40.1 (‚Üë0.3) 4599 (‚Üì3240) 8.72 (‚Üë71.7%) 

+ GRPO-LEAD 53.1 (‚Üë1.2) 7023 (‚Üì4282) 7.56 (‚Üë64.7%) 36.1 (‚Üì1.0) 7842 (‚Üì4698) 4.60 (‚Üë55.4%) 40.6 (‚Üë0.8) 4972 (‚Üì2867) 8.17 (‚Üë60.8%) 

+ GRPO 52.5 (‚Üë0.6) 8424 (‚Üì2881) 6.23 (‚Üë35.7%) 38.4 (‚Üë1.3) 10123 (‚Üì2417) 3.79 (‚Üë28.0%) 41.2 (‚Üë1.4) 5498 (‚Üì2341) 7.50 (‚Üë47.6%) 

+ SAGE-GRPO 55.3 (‚Üë3.4) 6422 (‚Üì4883) 8.61 (‚Üë87.6%) 38.0 (‚Üë0.9) 6583 (‚Üì5957) 5.77 (‚Üë94.9%) 41.8 (‚Üë2.0) 4435 (‚Üì3404) 9.42 (‚Üë85.4%) 

Qwen3-8B 73.2 15920 4.60 67.3 18342 3.67 46.6 11707 4.00 + GRPO 72.8 (‚Üì0.4) 10573 (‚Üì5347) 6.89 (‚Üë49.8%) 66.6 (‚Üì0.7) 13981 (‚Üì4361) 4.76 (‚Üë29.7%) 45.1 (‚Üì1.5) 7512 (‚Üì4195) 6.00 (‚Üë50.0%) 

+ SAGE-GRPO 73.5 (‚Üë0.3) 8975 (‚Üì6945) 8.19 (‚Üë78.0%) 66.6 (‚Üì0.7) 10052 (‚Üì8290) 6.58 (‚Üë79.3%) 45.4 (‚Üì1.2) 5972 (‚Üì5735) 7.60 (‚Üë90.0%) 

+ GSPO 73.0 (‚Üì0.2) 10544 (‚Üì5376) 6.92 (‚Üë50.4%) 66.2 (‚Üì1.1) 14082 (‚Üì4260) 4.70 (‚Üë30.2%) 46.6 (‚Üë0.0) 7964 (‚Üì3743) 5.85 (‚Üë46.2%) 

+ SAGE-GSPO 73.7 (‚Üë0.5) 8547 (‚Üì7373) 8.62 (‚Üë87.4%) 66.0 (‚Üì1.3) 9183 (‚Üì9159) 7.19 (‚Üë95.9%) 46.7 (‚Üë0.1) 5436 (‚Üì6271) 8.59 (‚Üë114.7%) 

Method MATH-500 Minerva AMC23 Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3) Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3) Pass@1 ‚Üë(%) LEN ‚Üì TE ‚Üë(√ó10 ‚àí3)

DS-1.5B 83.2 4882 17.0 30.1 6210 4.85 60.1 8250 7.28 + LC-R1 80.4 (‚Üì2.8) 2973 (‚Üì1909) 27.0 (‚Üë58.8%) 31.8 (‚Üë1.7) 3512 (‚Üì2698) 9.06 (‚Üë86.8%) 61.8 (‚Üë1.7) 4889 (‚Üì3361) 12.6 (‚Üë73.6%) 

+ ThinkPrune-2k 81.7 (‚Üì1.5) 2826 (‚Üì2056) 28.9 (‚Üë70.0%) 32.9 (‚Üë2.8) 3667 (‚Üì2543) 8.97 (‚Üë85.0%) 60.8 (‚Üë0.7) 5224 (‚Üì3026) 11.6 (‚Üë59.9%) 

+ AdaptThink 80.4 (‚Üì2.8) 2563 (‚Üì2319) 31.4 (‚Üë84.1%) 32.3 (‚Üë2.2) 2912 (‚Üì3298) 11.1 (‚Üë128.7%) 62.3 (‚Üë2.2) 4969 (‚Üì3281) 12.5 (‚Üë71.7%) 

+ Efficient Reasoning 82.0 (‚Üì1.2) 2821 (‚Üì2061) 29.1 (‚Üë70.6%) 31.4 (‚Üë1.3) 3530 (‚Üì2680) 8.90 (‚Üë83.5%) 64.7 (‚Üë4.6) 5202 (‚Üì3048) 12.4 (‚Üë70.9%) 

+ GRPO 83.6 (‚Üë0.4) 3907 (‚Üì975) 21.4 (‚Üë25.6%) 32.0 (‚Üë1.9) 4806 (‚Üì1404) 6.66 (‚Üë37.3 %) 65.4 (‚Üë5.3) 5771 (‚Üì2479) 11.3 (‚Üë55.6%) 

+ SAGE-GRPO 84.8 (‚Üë1.6) 2915 (‚Üì1967) 29.1 (‚Üë70.7%) 33.8 (‚Üë3.7) 3735 (‚Üì2475) 9.05 (‚Üë86.6%) 66.3 (‚Üë6.2) 5091 (‚Üì3159) 13.0 (‚Üë78.9%) 

+ GSPO 83.4 (‚Üë0.2) 3898 (‚Üì984) 25.3 (‚Üë21.4%) 32.0 (‚Üë1.9) 4454 (‚Üì1756) 7.18 (‚Üë48.0%) 66.1 (‚Üë6.0) 6095 (‚Üì2191) 10.9 (‚Üë63.1%) 

+ SAGE-GSPO 85.2 (‚Üë2.0) 2921 (‚Üì1961) 29.2 (‚Üë71.6%) 33.6 (‚Üë3.5) 3647 (‚Üì2563) 9.21 (‚Üë89.9%) 68.3 (‚Üë8.2) 5278 (‚Üì2972) 12.9 (‚Üë77.7%) 

DeepScaleR 86.0 3805 22.6 38.6 5184 7.45 64.2 6683 9.61 + ThinkPrune-2k 82.5 (‚Üì3.5) 2946 (‚Üì859) 28.0 (‚Üë23.9%) 37.9 (‚Üì0.7) 3188 (‚Üì1996) 11.9 (‚Üë59.6%) 65.8 (‚Üë1.6) 5046 (‚Üì1637) 13.0 (‚Üë35.6%) 

+ GRPO 87.6 (‚Üë1.6) 3482 (‚Üì323) 25.2 (‚Üë11.3%) 40.4 (‚Üë1.8) 4386 (‚Üì798) 9.21 (‚Üë23.6%) 69.3 (‚Üë5.1) 5872 (‚Üì811) 11.8 (‚Üë22.8%) 

+ SAGE-GRPO 88.8 (‚Üë2.8) 3117 (‚Üì688) 28.4 (‚Üë25.7%) 41.4 (‚Üë2.8) 3817 (‚Üì1367) 10.9 (‚Üë45.6%) 70.9 (‚Üë6.7) 5438 (‚Üì1245) 13.0 (‚Üë35.7%) 

DS-7B 91.6 3871 23.7 43.0 5490 7.83 81.9 7170 11.4 + LC-R1 87.3 (‚Üì4.3) 2076 (‚Üì1795) 42.1 (‚Üë77.7%) 44.4 (‚Üë1.4) 2834 (‚Üì2656) 15.7 (‚Üë100.0%) 79.1 (‚Üì2.8) 3686 (‚Üì3484) 21.5 (‚Üë87.9%) 

+ AdaptThink 88.9 (‚Üì2.7) 2199 (‚Üì1672) 40.4 (‚Üë70.9%) 45.2 (‚Üë2.2) 2869 (‚Üì2621) 15.8 (‚Üë101.2%) 80.7 (‚Üì1.2) 5130 (‚Üì2040) 15.7 (‚Üë37.7%) 

+ Efficient Reasoning 89.8 (‚Üì1.8) 2408 (‚Üì1463) 37.3 (‚Üë57.6%) 45.7 (‚Üë2.7) 2903 (‚Üì2587) 15.7 (‚Üë101.0%) 80.7 (‚Üì1.2) 4933 (‚Üì2237) 16.4 (‚Üë43.3%) 

+ GRPO-LEAD 89.5 (‚Üì2.1) 2752 (‚Üì1119) 32.5 (‚Üë37.1%) 46.3 (‚Üë3.3) 2990 (‚Üì2500) 16.0 (‚Üë104.3%) 82.7 (‚Üë0.8) 4384 (‚Üë2786) 18.9 (‚Üë65.8%) 

+ GRPO 92.0 (‚Üë0.4) 3219 (‚Üì652) 28.5 (‚Üë20.2%) 46.0 (‚Üë3.0) 3510 (‚Üì1980) 13.1 (‚Üë67.4%) 83.0 (‚Üë1.1) 4880 (‚Üì2290) 17.0 (‚Üë49.0%) 

+ SAGE-GRPO 93.0 (‚Üë1.4) 2141 (‚Üì1730) 43.4 (‚Üë83.1%) 45.2 (‚Üë2.2) 2692 (‚Üì2798) 16.8 (‚Üë114.4%) 84.9 (‚Üë3.0) 3953 (‚Üì3217) 21.5 (‚Üë88.1%) 

Qwen3-8B 94.4 5640 16.7 51.8 7358 7.04 90.5 10852 8.34 + GRPO 93.6 (‚Üì0.8) 4470 (‚Üì1170) 20.9 (‚Üë25.1%) 52.6 (‚Üë0.8) 4964 (‚Üì2394) 10.6 (‚Üë50.6%) 88.6 (‚Üì1.9) 7079 (‚Üì3773) 12.5 (‚Üë50.1%) 

+ SAGE-GRPO 95.0 (‚Üë0.6) 3015 (‚Üì2625) 31.5 (‚Üë88.2%) 53.5 (‚Üë1.7) 3390 (‚Üì3968) 15.8 (‚Üë124.2%) 90.7 (‚Üë0.2) 5563 (‚Üì5289) 16.3 (‚Üë95.4%) 

+ GSPO 94.6 (‚Üë0.2) 4342 (‚Üì1298) 22.2 (‚Üë32.9%) 49.6 (‚Üì2.2) 3962 (‚Üì3396) 12.5 (‚Üë77.8%) 87.7 (‚Üì2.8) 6464 (‚Üì4388) 13.6 (‚Üë63.1%) 

+ SAGE-GSPO 94.4 (‚Üë0.0) 2753 (‚Üì2887) 34.3 (‚Üë105.3%) 53.7 (‚Üë1.9) 3363 (‚Üì3995) 16.0 (‚Üë126.8%) 90.9 (‚Üë0.4) 5041 (‚Üì5811) 18.0 (‚Üë106.7%) 

18 Submission and Formatting Instructions for ICML 2026 

Performance on DS-1.5B and DS-7B For experiments with DS-1.5B as the base model, SAGE-RL consistently achieve the best or second-best performance across all benchmarks, while effectively improving the original model‚Äôs capabilities across all six mathematical reasoning benchmarks. Notably, SAGE-GSPO yields significant pass@1 gains of 6.2 % on AIME 2025 and 8.2 % on AMC23. AdaptThink stands out as a powerful baseline, attaining the highest token efficiency on MATH-500 and Minerva, while exhibiting the most pronounced reasoning simplification on OlympiadBench, MATH-500, and Minerva, but this high level of conciseness restrict the model‚Äôs ability to explore different solution strategies. As a result, AdaptThink struggle in terms of overall performance and consistently lag behind our method. As for the other baselines, both their performance and efficiency are generally less competitive compared to SAGE-RL. A similar trend is observed when using DS-7B as the base model. Both GRPO-LEAD and Efficient-Reasoning adopt a strategy of sacrificing less compression in exchange for improved performance; however, the reasoning capability gains they achieve remain substantially smaller than those of SAGE-RL. For instance, on AIME 2024, SAGE-GRPO not only outperforms GRPO-LEAD by 2.2 % in pass@1, but also produces noticeably shorter responses. These two sets of experiments together indicate that, on distilled models, SAGE-RL not only effectively alleviates the overthinking problem but also substantially enhances the model‚Äôs reasoning capability on complex mathematical problems. Since our method simultaneously improves the base model‚Äôs reasoning capability and the precision of its thinking process, it achieves more consistent and substantially larger gains in token efficiency compared to other approaches. 

Performance on DeepScaleR DeepScaleR has undergone systematic and comprehensive reinforcement learning (Luo et al., 2025b). Consequently, additional fine-tuning on this model typically yields only marginal performance gains, which accounts for the scarcity of related works that adopt DeepScaleR as a base model. Nevertheless, SAGE-RL still achieves relatively significant token efficiency improvements on DeepScaleR. Particularly on OlympiaBench, Math-500 and Minerva, SAGE-RL delivers roughly twice the token efficiency gains of GRPO, demonstrating that SAGE-RL yields relatively substantial benefits even for models with extensive post-training. 

Performance on Qwen3-8B As one of the strongest reasoning models under the same parameter scale, Qwen3-8B achieves excellent performance across various mathematical reasoning tasks. Even on the highly challenging AIME 2025, it attains an impressive pass@1 of 67.3%. However, as illustrated in Figure 3, the overthinking problem remains largely unresolved in this model. For instance, on MATH-500, despite comparable pass@1 performance, the average response length is more than 2.5 times that of SAGE-GRPO-tuned DS-7B. Notably, vanilla RLVR is capable of moderately reducing the response length of the base model. This effect stems from the training procedure, where sequences must be padded to a fixed batch length, causing the token budget in evaluation to be significantly smaller than that used in inference. As a result, the model tends to receive positive rewards more readily for short answers, encouraging shorter generations. Nevertheless, this mechanism limits the model‚Äôs ability to improve or may even cause declines on reasoning tasks of varying difficulty, particularly on datasets such as AIME 2024 and AMC 23. In contrast, SAGE-RL still achieves moderate improvements in the reasoning capability of Qwen3-8B under limited training token budgets, while effectively reducing the redundancy in the thinking process. For example, SAGE-GSPO attains a 1.9% increase in pass@1 on Minerva, while compressing the average response length to only 45.7% of the original. These results strongly demonstrate that SAGE-RL remains highly effective even on state-of-the-art reasoning models. 

Comparison of SAGE-GRPO and SAGE-GSPO As shown in Figure 9, across both GRPO and GSPO, the key variations in pass@1, response length, and KL loss are driven by the incorporation of SAGE-RL rather than fundamental RLVR algorithms. This underscores the robust positive impact of our approach across different RLVR implementations. In terms of entropy, GSPO-tuned models show elevated values, largely due to sequence-level importance sampling disregarding fine-grained token-level variations, thereby resulting in higher inference uncertainty. From the experimental results shown in Table 4, SAGE-GSPO exhibits particularly strong performance in reducing response length and slightly outperforms SAGE-GRPO in overall metrics. We hypothesize that this advantage stems from the greater stability of GSPO‚Äôs sequence-level importance sampling compared to GRPO‚Äôs token-level importance sampling, which is especially beneficial in more unstable scenarios such as MoE models (Zheng et al., 2025a). A similar issue arises in SAGE-RL due to the hybrid sampling used in the rollout phase. In SAGE-GRPO, some of the rollouts are generated by selecting sequences at every reasoning step based on the full-sequence confidence score Œ¶, rather than greedly choosing the highest log-probability token as in Equation 7. Consequently, as indicated in Equation 11, the 19 Submission and Formatting Instructions for ICML 2026 

probability œÄŒ∏old (yi,t ‚à£ x, y i, <t) under the old policy may be lower than that of random sampling, increasing the likelihood of clipping during importance sampling. In contrast, GSPO treats the entire sequence as the basic unit for importance sampling, thereby avoiding this issue entirely. Overall, Table 4 demonstrates that SAGE-RL achieves substantially superior performance compared to all baseline methods across six challenging mathematical reasoning tasks. Meanwhile, Figure 9 reveals higher pass@1 scores and increased KL divergence, accompanied by reduced response entropy and shorter response lengths. These results indicate that SAGE successfully unleashes the model‚Äôs implicit capacity for timely thinking termination. Consequently, the model learns efficient reasoning with increased confidence, confirming the viability of leveraging RLVR to instill effective reasoning patterns. This is consistent with the results of Wen et al. (2025b), which demonstrate that RLVR effectively promotes correct reasoning chains in base LLMs. 

D.2. Hyperparameters Sensitivity Analysis 

This section examines the influence of the two primary hyperparameters influencing SAGE-RL: the SAGE exploration width 

m and the total number of rollouts r produced by SAGE per group. We evaluate SAGE-RL under various combinations of these parameters and denote each setting as SAGE (m, r )-RL. Figure 12 illustrates the training dynamics of DS-1.5B with SAGE-GRPO under different hyperparameter combinations and GRPO. The corresponding evaluation results on four mathematical datasets are reported in Table 5. 

Table 5. A comparison of experimental results for DS-1.5B under different SAGE-GRPO parameter settings. Here, SAGE (m, r) denotes an exploration width of m, with the final retention of r different trajectories.                                                                                                                              

> Method MATH-500 AIME 2024 AIME 2025 OlympiadBench Pass@1 ‚Üë(%) LEN ‚ÜìTE ‚Üë(√ó10 ‚àí3)Pass@1 ‚Üë(%) LEN ‚ÜìTE ‚Üë(√ó10 ‚àí3)Pass@1 ‚Üë(%) LEN ‚ÜìTE ‚Üë(√ó10 ‚àí3)Pass@1 ‚Üë(%) LEN ‚ÜìTE ‚Üë(√ó10 ‚àí3)
> DS-1.5B 83.2 4882 17.0 25.1 12300 2.04 20.9 11669 1.79 33.4 8954 3.73 + GRPO 83.6 (‚Üë0.4) 3907 (‚Üì975) 21.4 (‚Üë25.6%) 28.3 (‚Üë3.2) 8767 (‚Üì3533) 3.23 (‚Üë58.3%) 24.1 (‚Üë3.2) 8263 (‚Üì3406) 2.92 (‚Üë63.1%) 34.2 (‚Üë0.8) 6323 (‚Üì2631) 5.41 (‚Üë45.0%)
> +SAGE (1,1)-GRPO 84.0 (‚Üë0.8) 3416 (‚Üì1466) 24.6 (‚Üë44.7%) 28.3 (‚Üë3.2) 7979 (‚Üì4321) 3.55 (‚Üë74.0%) 24.8 (‚Üë3.9) 7730 (‚Üì3939) 3.21 (‚Üë79.3%) 34.5 (‚Üë1.1) 5857 (‚Üì3097) 5.89 (‚Üë57.9%)
> +SAGE (2,1)-GRPO 84.2 (‚Üë1.0) 2952 (‚Üì1930) 28.5 (‚Üë67.8%) 28.5 (‚Üë3.4) 7308 (‚Üì4992) 3.90 (‚Üë91.2%) 25.7 (‚Üë4.8) 7603 (‚Üì4066) 3.38 (‚Üë88.8%) 35.2 (‚Üë1.8) 5267 (‚Üì3687) 6.68 (‚Üë79.1%)
> +SAGE (2,2)-GRPO 84.8 (‚Üë1.6) 2915 (‚Üì1967) 29.1 (‚Üë70.7%) 28.8 (‚Üë3.7) 7243 (‚Üì5057) 3.98 (‚Üë95.1%) 26.5 (‚Üë5.6) 7479 (‚Üì4190) 3.54 (‚Üë97.8%) 36.9 (‚Üë3.5) 5050 (‚Üì3904) 7.31 (‚Üë96.0%)

Figure 12. Training dynamics comparison for SAGE-GRPO with distinct hyperparameter combination: average response length when tested on MATH500, average SAGE-produced trajectory length during training, entropy, and KL divergence. 

The Impact of SAGE Rollout Quantity As shown in Table 5, the transition of r from 1 to 2 has limited effect on the results. From a policy optimization perspective, larger r allows the policy model to learn from more efficient reasoning samples; however, the advantage estimate per sample becomes less sharp compared to r = 1, leading to similar overall updates. Figure 12 shows that SAGE (2,1)-GRPO and SAGE (2,2)-GRPO display very similar trends in entropy and KL divergence, markedly different from those of SAGE (1,1)-GRPO and vanilla GRPO. This indicates that enlarging r has little impact on policy updates, as rollouts with similar reasoning trajectories offer minimal additional information .

The Impact of Exploration Width On the other hand, enlarging m from 1 to 2 yields substantial performance gains. According to the results shown in Table 5, while SAGE (1,1)-GRPO yields moderate improvements over the vanilla GRPO baseline, its performance is markedly inferior to that of SAGE (2,1)-GRPO. 20 Submission and Formatting Instructions for ICML 2026 

This indicates that exploration width significantly influences the activation of the model‚Äôs efficient reasoning capability, consistent with the findings in Figure 11. As illustrated in Figure 12, SAGE (1,1)-GRPO exhibits significantly milder entropy reduction and KL divergence increase relative to SAGE (2,1)-GRPO, and its training dynamics remain much closer to the vanilla GRPO. More directly, both the average length of SAGE-produced rollouts in SAGE(2,1)-GRPO and the average response length of the model at test time are significantly shorter than those observed in SAGE(1,1)-GRPO. These results indicate that a limited exploration width m causes SAGE-RL to largely collapse to the standard GRPO optimization behavior .

D.3. SAGE-RL shows Promising Potential in Difficult Reasoning Tasks 

To more clearly elucidate the operational mechanism behind SAGE-RL, we compare the training dynamics of SAGE-GRPO-DS-1.5B (Ours) and GRPO-DS-1.5B (GRPO) on MATH-500 across five difficulty levels as training steps scales. The level 1-5 ranges from low to high, reflecting increasing levels of difficulty. 0 150 300 450    

> Training Steps
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> Pass@1
> 0150 300 450
> Training Steps
> 1000
> 2000
> 3000
> 4000
> 5000
> 6000
> Response Length
> Level 1 (Ours)
> Level 2 (Ours)
> Level 3 (Ours)
> Level 4 (Ours)
> Level 5 (Ours)
> Level 1 (GRPO)
> Level 2 (GRPO)
> Level 3 (GRPO)
> Level 4 (GRPO)
> Level 5 (GRPO)

Figure 13. The training dynamics of SAGE-GRPO- DS-1.5B (Ours) and GRPO-DS-1.5B (GRPO) on MATH-500 across level 1-5. 

As illustrated in Figure 13, both GRPO and SAGE-GRPO show steady performance gains across all difficulty levels of MATH500 as training progresses. SAGE-GRPO converges markedly faster than GRPO at every level and eventually attains performance comparable to GRPO on level 1-3 problems. A clear divergence appears on level 4-5 problems, where SAGE-GRPO achieves substantially superior pass@1 and lower response length. Remarkably, the downward trend in response length for SAGE-GRPO continues even after GRPO has converged. These observations suggest that SAGE-RL primarily improves overall performance by dramatically increasing reasoning efficiency on difficult problems. This is consistent with the results in Table 4, which reveal significantly larger gains from SAGE-RL fine-tuning on more challenging benchmarks such as AIME 2024, AIME 2025, OlympiadBench, and Minerva than on relatively easier ones such as MATH-500 and AMC23. Collectively, these findings highlight the considerable potential of SAGE-RL in overcoming the reasoning performance bottlenecks faced by current LRMs on highly difficult tasks .

D.4. Time Complexity Analysis Time Complexity Analysis of SAGE SAGE generates 2m reasoning steps in parallel with a fixed exploration width m at each expansion step. Therefore, it theoretically achieves the same time complexity as Degrade SAGE, meanwhile, its space complexity is approximately 2m times higher. However, as we adopt vLLM (Kwon et al., 2023) as the inference engine, whose core design philosophy centers on a space-for-time tradeoff: it maximizes GPU memory utilization to minimize inference latency. Nevertheless, our implementation is constrained to the use of only 8 GPUs. Under this memory-limited setting, SAGE incurs higher inference-time cost compared to Degrade SAGE. We report the average per-sample runtime of SAGE (m, 1) under different EW in this constrained hardware environment. Here, m denotes the exploration width EW. When m = 0, SAGE degenerates to Degrade SAGE. As shown in Figure 14(a), the inference time of DS-1.5B remains consistently higher than that of DeepScaleR. Moreover, the average inference time 21 Submission and Formatting Instructions for ICML 2026 Slow Growth Slow Growth Rapid Growth Rapid Growth Avg. 28.7% Faster    

> (a) Time Complexity of SAGE (a) Time Complexity of SAGE -GRPO Tuned Models
> Degrade -SAGE Degrade -SAGE

Figure 14. (a) Average inference time of SAGE on each question; (b) Comparison of Normalized inference time between the base models and the SAGE-GRPO tuned models on each question, approximated and normalized by the average response length. 

per response increases significantly with larger exploration widths. This primarily arises from the trade-off adopted by vLLM : elevated space complexity is exchanged for reduced time complexity in the context of limited computational resources. In particular, once the exploration width exceeds 2, the growth rate of inference time accelerates further. Therefore, we primarily set exploration width m = 2, which represents the transition point between the slow-growth and fast-growth regions, to achieve a balanced trade-off between efficiency and performance. Time Complexity Analysis of SAGE-RL Tuned Models In the standard pass@1 inference setting, the KV cache is prefilled during the initial prompt processing phase, which ensures that the generation latency per subsequent token remains approximately constant. Consequently, for short queries, the total inference time of each completion scales nearly linearly with the number of generated tokens. However, vLLM aggressively optimizes inference speed through techniques such as KV cache reuse and continuous batching, which compromises the fairness of direct wall-clock time comparisons. Given the approximately linear relationship between inference time and the number of generated tokens, we adopt the proxy metric 0.0001 √ó (average response length ) to reflect the average inference latency. We compare this normalized metric between the base models and our SAGE-GRPO-tuned models. As shown in Figure 14(b), although SAGE incurs increasing inference-time cost with larger exploration widths under constrained hardware, SAGE-RL-tuned models can significantly reduce the average inference time in the standard pass@1 inference paradigm. Specifically, even on the relatively easier MATH-500 and AMC23 subsets among the six datasets we evaluated, our approach still achieves a 28.7% reduction in inference latency. When approximating average inference time using the average response length, Table 4 clearly shows that, compared to the baseline, SAGE-RL-tuned models reduce inference latency by more than 40% across the majority of models and benchmarks. 

22 Submission and Formatting Instructions for ICML 2026 MATH500 AMC 2023  

> DeepScaleR DS -1.5B
> Exploration Width
> Exploration Width Exploration Width Exploration Width
> Exploration Width Exploration Width Exploration Width
> Exploration Width

Figure 15. Performance on DeepSacleR and DS-1.5B with different exploration width on MATH500 and AMC23. Under all settings, both pass@1 and response length gradually converge. 

23 Submission and Formatting Instructions for ICML 2026 ** Final Answer** John spent a total of \boxed {$800 } on ties. 

John buys twice as many red ties as blue ties . The red ties cost 50 % more than blue ties .

He spent $200 on blue ties that cost $40 each . How much did he spend on ties? 

Okay, so I need to figure out how much John spent on ties . Let's start by parsing the 

information given step by step .

First, John buys twice as many red ties as blue ties . Hmm, that means if he buys x blue ties, 

he buys 2x red ties . That makes sense .

Next, the red ties cost 50 % more than the blue ties . Alright, so if the blue tie is priced at, 

say, y dollars, then red ties would cost y plus 50 % of y, which is 1.5y. Alternatively, it can 

be written as 1.5y.

Now, let's figure out the cost per red tie . We know that the red ties cost 50 % more than the 

blue ties . The blue tie costs $40 , so 50 % of 40 is 20 . Therefore, each red tie costs 40 + 20 

= $60 .

Total spent = Total cost of blue ties + Total cost of red ties 

Total spent = 200 + 600 = $800 .

So, John spent a total of $800 on ties. 

Wait, let me double -check my calculations to be sure I didn't make any mistakes .

First, blue ties: 200 / 40 is indeed 5 ties. Red ties are twice that, so 10 ties. Each blue tie is 

$40, red tie is 50% more, which is $60. 10 red ties at $60 each: 10 * 60 is 600. Blue ties 

total $200. So 600 + 200 is 800. Yeah, that checks out. 

Total expenditure: 200 + 600 = 800 . Yeah, definitely $800 .

I think that even if I messed up in the calculation, this straightforward approach seems 

correct . So, he spent $800 on ties .

# ‚Ä¶

# ‚Ä¶

Question 

Reasoning 957 tokens 

Correct Answer: 80 0

John buys twice as many red ties as blue ties . The red ties cost 50 % more than blue ties .

He spent $200 on blue ties that cost $40 each . How much did he spend on ties? 

Okay, so I need to figure out how much John spent on ties . Let's start by parsing the 

information given step by step .

First, John buys twice as many red ties as blue ties, if he buys x blue ties, he buys 2x red 

ties .

Red ties are 50% more expensive than blue ties. For a blue tie priced at y dollars, the red 

tie price is 1.5y. 

Now, let‚Äòs figure out the cost per red tie. Since red ties are 50% more expensive than blue 

ties ($40), each red tie costs 40 + 20 = $60. 

Total spent = Total cost of blue ties + Total cost of red ties 

Total spent = 200 + 600 = $800 .

So, John spent a total of $800 on ties. 

# ‚Ä¶

Question 

Reasoning 467 tokens 

Correct Answer: 80 0

** Final Answer** John spent a total of \boxed {$800 } on ties. 

SAGE -GRPO -DS -1.5B DS -1.5B 

Figure 16. Case Study 1 

24 Submission and Formatting Instructions for ICML 2026 **Final Answer** The polar coordinates are : \boxed{( 3, \frac{ \pi}{ 2})} 

Convert the point $(0,3)$ in rectangular coordinates to polar coordinates. 

Okay, so I need to convert the rectangular coordinate (0, 3) to polar coordinates . Hmm, I

remember that polar coordinates are represented as (r, Œ∏)‚Ä¶

First, let me recall the formulas ‚Ä¶ I think the formula for r ‚Ä¶ But wait, I also remember 

that sometimes arctan(y/x) doesn't give the correct angle because it depends on the 

quadrant where the point is located . So, I need to be careful about that .

Alright, let's apply this to the point (0, 3) ‚Ä¶ Okay, so r is 3. That makes sense because the 

point is 3 units above the origin on the y-axis, so the distance from the origin is 3.

Now, let's figure out Œ∏. Since x is 0, we have Œ∏ = arctan( 3/0). Wait, division by zero is 

undefined . Hmm, that's a problem . I need to think about what angle Œ∏ would make the 

point (0, 3) in polar coordinates .

I know that when x is 0, the point lies on the y-axis . If y is positive, it's on the positive y-

axis, which is at 90 degrees or œÄ/ 2 radians from the positive x-axis . If y were negative, it 

would be at 270 degrees or 3œÄ/ 2 radians . Since our y is positive, Œ∏ should be œÄ/ 2.

Wait, but sometimes when x is 0, the angle is either œÄ/ 2 or 3œÄ/ 2, but since our y is positive, 

it's œÄ/ 2. So, Œ∏ is œÄ/ 2.

So, putting it all together, the polar coordinates are (3, œÄ/ 2). Let me just double -check . ‚Ä¶

Cos (œÄ/ 2) is 0, so x is 0. Y is r sin Œ∏ = 3 sin( œÄ/ 2). Sin( œÄ/ 2) is 1, so y is 3. That gives us (0, 3), 

which is exactly the point we started with . So, that seems correct .

So, putting it all together, the polar coordinates are (3, œÄ/2). 

Let me just double -check ‚Ä¶ That gives us (0, 3), 

which is exactly the point we started with. So, that seems correct. 

# ‚Ä¶

# ‚Ä¶

Question 

Reasoning 1381 tokens 

Correct Answer: (3, \frac{ \pi}{ 2}) 

Convert the point $(0,3)$ in rectangular coordinates to polar coordinates. 

Okay, so I need to convert the rectangular coordinate (0, 3) to polar coordinates . Hmm, I

remember that polar coordinates are represented as (r, Œ∏) ‚Ä¶

First , let me recall the formulas for converting rectangular coordinates (x, y) to polar 

coordinates (r, Œ∏) . I think the formula for r is the square root of (x squared plus y squared) .

So, r = ‚àö(x¬≤ + y¬≤) . And for Œ∏, I believe it's the arctangent of y over x, so Œ∏ = arctan(y/x) .

Alright, let's apply this to the point (0, 3)‚Ä¶ Okay , so r is 3. That makes sense because the 

point is 3 units above the origin on the y-axis, so the distance from the origin is 3.

I remember that angles in polar coordinates are measured from the positive x-axis, going 

counterclockwise ‚Ä¶ Let me confirm that . Yes, œÄ/ 2 radians is 90 degrees, which points 

straight up along the y-axis . So, Œ∏ should be œÄ/ 2.

So, putting it all together, the polar coordinates should be (3, œÄ/ 2).

Let me just write that down : (3, œÄ/ 2).

# ‚Ä¶

Question 

Reasoning 71 2 tokens 

** Final Answer** The polar coordinates are : \boxed{( 3, \frac{ \pi}{ 2})} 

SAGE -GRPO -DS -1.5B DS -1.5B 

Correct Answer: (3, \frac{ \pi}{ 2}) 

Figure 17. Case Study 2 

25