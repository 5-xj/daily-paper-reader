Title: An objective-function-free algorithm for general smooth constrained optimization

URL Source: https://arxiv.org/pdf/2602.11770v1

Published Time: Fri, 13 Feb 2026 01:47:26 GMT

Number of Pages: 21

Markdown Content:
# An objective-function-free algorithm for general smooth constrained optimization 

Stefania Bellavia âˆ—

, Serge Gratton â€ 

, Benedetta Morini â€¡

, Philippe L. Toint Â§

February 13, 2026 

Abstract 

A new algorithm for smooth constrained optimization is proposed that never computes the value of the problemâ€™s objective function and that handles both equality and inequality constraints. The algorithm uses an adaptive switching strategy between a normal step aiming at reducing constraintâ€™s infeasibility and a tangential step improving dual optimality, the latter being inspired by the AdaGrad-norm method. Its worst-case iteration complexity is analyzed, showing that the norm of the gradients generated converges to zero like O(1 /âˆšk + 1) for problems with full-rank Jacobians. Numerical experiments show that the algorithmâ€™s performance is remarkably insensitive to noise in the objective functionâ€™s gradient. 

Keywords: Objective-function-free optimization (OFFO), general constraints, nonconvex problems, reliability in the presence of noise, complexity. 

# 1 Introduction 

The design and analysis of deterministic algorithms for solving constrained continuous optimiza-tion problems have a long history and have produced well-assessed techniques such as penalty methods, SQP methods, interior-point methods or filter methods (see [17, 6, 10, 24] for example). These techniques all require the computation of both the function and derivative evaluation of the objective and the constraints. By contrast, this paper addresses the solution of the problem min  

> xâˆˆF

f (x) where F = {x âˆˆ IRn, c(x) = 0 x â‰¥ 0}, (1) using a first-oder objective function-free (OFFO) method. Here f is a smooth (possibly nonconvex) function from an open set containing the feasible region F âŠ† IRn into I R, c(x) : I Rn â†’ IRm with 

m â‰¤ n and equalities and inequalities are meant componentwise. Problem (1) includes general constrained optimization since all problems in this class can be cast into this form by using slack variables. We assume that, given x, we can compute both the gradient g(x) = âˆ‡xf (x) of f and the value of the constraints c(x) as well as their Jacobian J(x) = âˆ‡xc(x) âˆˆ IRmÃ—n, which we will assume (for the purpose of our analysis) is full rank for any x â‰¥ 0. 

> âˆ—

Dipartimento di Ingegneria Industriale, Universit` a degli Studi di Firenze, Firenze, Italia. Member of the INdAM Research Group GNCS. Email: stefania.bellavia@unifi.it. Work partially supported by Progetti di Ricerca INDAM-GNCS. 

> â€ 

UniversitÂ´ e de Toulouse, INP, IRIT, Toulouse, France. Work partially supported by 3IA Artificial and Natu-ral Intelligence Toulouse Institute (ANITI), French â€œInvesting for the Future - PIA3â€ program under the Grant agreement ANR-19-PI3A-0004. Email: serge.gratton@toulouse-inp.fr. 

> â€¡

Dipartimento di Ingegneria Industriale, Universit` a degli Studi di Firenze, Firenze, Italia. Member of the INdAM Research Group GNCS. Email: benedetta.morini@unifi.it. Work partially supported by Progetti di Ricerca INDAM-GNCS. 

> Â§

Namur Center for Complex Systems (naXys), University of Namur, Namur, Belgium.Work partially sup-ported by 3IA Artificial and Natural Intelligence Toulouse Institute (ANITI) and DIEF (Florence). Email: philippe.toint@unamur.be. 

1

> arXiv:2602.11770v1 [math.OC] 12 Feb 2026

Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 2First-order OFFO procedures do not employ the value of the objective function but rely on gradients. They are known to be suitable for the solution of problems in which the function is approximated or subject to noise, and have exhibited remarkable robustness in the presence of noisy gradients [2, 7, 13, 15, 23]. Our approach is inspired by the â€trust-funnelâ€ approach [12, 20], which, as [25], has roots in the much older Himmelblauâ€™s â€flexible tolerance â€ method [16]. As in these references, the new method uses an adaptive switching strategy to select a normal or a tangential steps, the first aiming at reducing the violation of the constraints and the second at improving the objective-function value. The stepsize in the latter is reminiscent of the stepsize formula used in Adagrad-norm method [8, 22] for unconstrained optimization. Our method can therefore be seen as a AdaGrad-like method for solving equality and inequality constrained problems. Other OFFO methods for constrained optimization using AdaGrad stepsizes have been proposed by the authors in the papers [1, 15]. In [1] bound constrained optimization problems are considered; stochastic estimators of the gradient are allowed and second-order information are used when available. The paper [15] considers nonlinear equality constrained problems with full-rank Jacobians and proposes a first-order algorithm that adaptively selects steps in the plane tangent to the constraints or steps that reduce infeasibility. The evaluation complexity is analyzed, in both cases yielding a global convergence rate in O(1 /âˆšk + 1), identical in order to that of steepest-descent and Newtonâ€™s methods for unconstrained problems [5]. Our present proposal builds on these contributions and extends [15] to handle inequality con-straints and thus to cover general smooth constrained optimization. To accommodate such con-straints, we revisited the procedure from [15] by introducing suitable primal and dual criticality measures and redefining both tangential and normal steps, while avoiding a technical assumption on the first iteration. Three different techniques are provided for the computation of the tangen-tial step. We analyze the worst-case iteration complexity of our procedures and show that the norm of the gradients generated converges to zero like O(1 /âˆšk + 1). Numerical experiments show that, in line with what happens on simpler problems, the algorithmâ€™s performance is remarkably insensitive to noise in the objective functionâ€™s gradient. The authors are aware of four other papers on OFFO procedures [2, 7, 9, 21] for constrained problems. The paper [2] presents objective function-free Sequential Quadratic Programming (SQP) algorithms to solve smooth optimization problems with stochastic objective and deter-ministic nonlinear equality constraints. It employs a stepsize selection scheme based on Lipschitz constants (or adaptively estimated Lipschitz constants) in place of the linesearch. This approach has been extended in [7] to handle deterministic inequality constraints. A convergence analysis in expectation is carried out, but the worst-case complexity has not been analyzed. The method introduced in [9] is designed to solve nonlinear optimization problems with stochastic objectives and deterministic equality constraints. It again employs normal and tangential steps, the latter being computed using a standard trust-region technique; an explicit penalty parameter is used and dynamically updated throughout the process, without requiring the objective functionâ€™s com-putation. Global almost-sure convergence is proved. [21] proposes a variant of the SQP approach of [2] for equality-constrained problems with full-rank Jacobian using first-order methods with momentum and analyzes its rate of convergence. Our paper is organized as follows. The ADIC (ADagrad with Inequality Constraints) class of algorithms is introduced in Section 2 with its algorithmic options. Section 3 analyzes its worst-case complexity. Results obtained from the numerical validation of the algorithms are described in Section4. Section 5 finally summarizes our contributions and discusses perspectives for further research. 

Notations: In what follows, âˆ¥ Â· âˆ¥ denotes the Euclidean norm unless otherwise specified, and 

Ïƒmin [A] denotes the smallest singular value of the matrix A.Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 3

# 2 The ADIC class of algorithms 

In the new class of iterative methods that we are going to describe, a new iterate is formed using either a tangential step (i.e. a step in the plane tangent to the manifold of equality constraints) or a normal step (mostly orthogonal to that manifold), the choice between the two being based on a comparison of the primal and dual criticality measures. One of the interesting features of this algorithmic framework is that it allows the use of fairly general non-negative bounded dual and primal criticality measures, denoted Ï‰T (x) and Ï‰N (x) respectively. In the algorithmâ€™s description on the following page, the successive iterates are denoted by 

xk and we let gk = g(xk), ck = c(xk), Jk = J(xk), Ï‰T (xk) = Ï‰T,k , Ï‰N (x) = Ï‰N,k . These criticality measures are computed in Step 1, together with a tangential stepsize whose form is, as we will detail later, directly inspired by the AdaGrad [8, 22] algorithm. Whether the step taken is tangential or normal is decided by comparing their sizes, each of these steps being designed to provide a first-order improvement (of a carefully chosen Lyapunov function) comparable to the relevant criticality measure while being of a size ensuring that first-order effects dominate (as we will prove below). In our subsequent analysis, we need to distinguish between iterates using tangential or normal steps. We denote by {kÏ„ } âŠ† { k} the index subsequence of iterations such that a tangential step 

sT,k was computed (implying that (3) holds), while {kÎ½ } is the index subsequence of iterations where a normal step sN,k was computed. Note that {kÏ„ } and {kÎ½ } need not be disjoint, but that 

{k} = {kÏ„ } âˆª { kÎ½ }. By convention, we will define sT,k = 0 for kÌ¸ âˆˆ { kÏ„ } and sN,k = 0 for kÌ¸ âˆˆ { kÎ½ }.We will also consider the Lyapounov function (whose value is hopefully decreased as the iter-ations progress) given by 

Ïˆ(x, Î» ) def 

= L(x, Î» ) + Ïâˆ¥c(x)âˆ¥, (11) where Ï is a fixed constant (to be determined below) and L(x, Î» ) is the standard Lagrangian 

L(x, Î» ) = f (x) + Î»T c(x), (12) for some multiplier Î» âˆˆ IRm. The function Ïˆ(x, Î» ) is sometimes called the â€sharp augmented Lagrangianâ€ (see [3, 4, 19] for instance). Of particular interest in our argument is the least-squares Lagrange multiplier bÎ»(x) defined by 

 J(x)J(x)T  bÎ»(x) = âˆ’ J(x) g(x) (13) when the Jacobian J(x) has full rank. It is important to note to this point that, because all norms are equivalent in I Rn, our theo-retically convenient choice of expressing (10) and (5) in Euclidean norm is by no means crucial. Should other norms be used, as we will see below, the relevant equivalence constants may be absorbed in Î¸T and Î¸N . It is also useful to notice (2) implies that 

Î±T,k â‰¤ Î·

âˆšÏ‚ and Î±T,k Ï‰T,k < Î·. (14) Clearly, much else remains to be specified in our algorithmic outline: details of which criticality measures are considered together with which norm and methods to compute the tangential step 

sT,k itself as well as the normal step sN,k must be clarified. In order to simplify exposition, we focus in our theory on a single technique for computing the normal step sN,k , and propose to define it by one (or more) step(s) of a trust-region algorithm applied on the constrained violation 12 âˆ¥c(x)âˆ¥2

using a linear model. Lemma 3.2 below will show that such a step satisfies our requirements of Step 2 with 

Ï‰N,k = Ï‡N,k = |cTk JkdN,k |. (15) Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 4

Algorithm 2.1: ADIC (x0)Step 0: Initialization: The constants Î², Î· > 0, Î¸T , Î¸ N â‰¥ 1, 0 < Î· min â‰¤ Î·max and Îºt, Îº n, Ï‚ âˆˆ

(0 , 12 ] are given. Project x0 onto the positive orthant. Set k = 0 and Î“ 0 = 0. 

Step 1: Evaluations: Evaluate ck = c(xk), Jk = J(xk), gk = âˆ‡f (xk). Then compute the dual measure Ï‰T,k , the primal measure Ï‰N,k and the stepsize 

Î±T,k = Î·

q

Î“k + Ï‰2 

> T,k

+ Ï‚. (2) 

Step 2: Normal step: Except possibly if 

Ï‰N,k â‰¤ Î² Î± T,k Ï‰T,k , (3) compute sN,k such that 

xk + sN,k â‰¥ 0, (4) 

âˆ¥sN,k âˆ¥ â‰¤ Î¸N Ï‰N,k , (5) and there exists a contant Îºn âˆˆ (0 , 12 ) independent of k such that  

> 12

âˆ¥c(xk + sN,k )âˆ¥2 â‰¤ 12 âˆ¥ckâˆ¥2 âˆ’ Îºn Ï‰2

> N,k

. (6) Then set x+ 

> k

= xk + sN,k . If (3) holds and sN,k was not computed, set x+ 

> k

= xk.

Step 3: Tangential step: If (3) holds, compute a step sT,k such that 

xk + sT,k â‰¥ 0 (7) 

JksT,k = 0, (8) 

gTk sT,k â‰¤ âˆ’Îºt Î±T,k Ï‰2

> T,k

, (9) 

âˆ¥sT,k âˆ¥ â‰¤ Î¸T Î±T,k Ï‰T,k , (10) and set xk+1 = x+ 

> k

+ sT,k and Î“ k+1 = Î“ k + Ï‰2

> T,k

.Otherwise (i.e. if (3) fails), set xk+1 = x+ 

> k

and Î“ k+1 = Î“ k.

Step 4: Loop: Increment k by one and go to Step 1. Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 5where dN,k solves the problem min d cTk Jkdxk + d â‰¥ 0

âˆ¥dâˆ¥âˆ â‰¤ 1.

(16) By contrast, we will exploit the freedom in our model to introduce a few variants for the compu-tation of the tangential step. 

2.1 ADIC-LP : two variants based on linear optimization 

We start by describing a variant based on the dual criticality measure given by 

Ï‰T,k = Ï‡T,k = |gTk dT,k |, (17) where dT,k is the solution of the linear optimization 1 problem min d gTk dJkd = 0 

xk + d â‰¥ 0

âˆ¥dâˆ¥âˆ â‰¤ 1.

(18) Also observe that 

Ï‡T,k â‰¤ âˆ¥ gkâˆ¥ âˆ¥ dT,k âˆ¥ â‰¤ âˆšnâˆ¥gkâˆ¥. (19) The tangential step sT,k can then be computed in two ways. The first is to define sT,k as the solution of the linear programming problem min s gTk sJks = 0 

xk + s â‰¥ 0

âˆ¥sâˆ¥âˆ â‰¤ Î±T,k Ï‰T,k .

(20) (Note that (20) only differs from (18) in the definition of its bounds, and that we have used the liberty in the choice of norms to express the bound on the step in âˆ¥ Â· âˆ¥ âˆ). A second, simpler, possibility is to choose a multiple of dT,k and simply set 

sT,k = Î±T,k Ï‰T,k 

âˆ¥dT,k âˆ¥âˆ

dT,k . (21) Defined in either of these ways, sT,k clearly satisfies (7), (8) and (10) (with Î¸T â‰¥ âˆšn), and it is not difficult to verify that it also satisfies (9) with Îºt = 1 / max[ Î·, 1]. 

Lemma 2.1 Suppose that, at tangential iteration kÏ„ , sT,k Ï„ is defined by either (20) or (21). Then we have that 

|gTkÏ„ sT,k Ï„ | â‰¥ Î±T,k Ï„ Ï‡2 

> T,k Ï„

= Î±T,k Ï„

max[ Î·, 1] Ï‰2 

> T,k Ï„

. (22) 

Proof. Suppose first that âˆ¥sT,k Ï„ âˆ¥âˆ â‰¥ âˆ¥ dT,k Ï„ âˆ¥âˆ. Then dT,k Ï„ is feasible for problem (20) and thus 

|gTkÏ„ sT,k Ï„ | â‰¥ | gTkÏ„ dT,k Ï„ | = Ï‰T,k Ï„ â‰¥ Î±T,k Ï„

Î· Ï‰2 

> T,k Ï„

,

where we used (14) to deduce the last inequality. Suppose now that âˆ¥sT,k Ï„ âˆ¥âˆ < âˆ¥dT,k Ï„ âˆ¥âˆ.Then we must have that âˆ¥sT,k Ï„ âˆ¥âˆ = Î±T,k Ï„ Ï‰T,k Ï„ . The vector y = ( âˆ¥sT,k Ï„ âˆ¥âˆ/âˆ¥dT,k Ï„ âˆ¥âˆ)dT,k Ï„

is therefore feasible for problem (18) and thus 

|gTkÏ„ y| = âˆ¥sT,k Ï„ âˆ¥âˆ

âˆ¥dT,k Ï„ âˆ¥âˆ

|gTkÏ„ dT,k Ï„ | = Î±T,k Ï„ Ï‰T,k Ï„

âˆ¥dT,k Ï„ âˆ¥âˆ

Ï‰T,k Ï„ â‰¥ Î±T,k Ï„ Ï‰2 

> T,k Ï„

(23) 

> 1Formerly known as â€linear programmingâ€.

Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 6If sT,k is defined by (21), then sT,k = y and (22) follows. Otherwise, we obtain from (20) that 

|gTkÏ„ sT,k Ï„ | â‰¥ | gTkÏ„ y| and (22) also follows from (23). 2

When the measure (17) is used, it is also useful to note that, for x such that c(x) = 0, 

Ï‡T k = Ï‡T (xk) = min 

> âˆ¥dâˆ¥âˆâ‰¤1

{âˆ‡ xL(xk, bÎ»(xk)) T d | J(xk)d = 0 and xk + d â‰¥ 0} (24) where the Lagrangian L(x, Î» ) is defined in (12) and bÎ»(x) is given by (13). 

2.2 ADIC-P1 : a projection-based variant 

We next consider a variant based on the dual criticality measure given by 

Ï‰T,k = Ï€T (xk) with Ï€T (x) = âˆ¥Î F (x)



x âˆ’ g(x)



âˆ’ xâˆ¥ def 

= âˆ¥p1(x)âˆ¥. (25) where Î  F (x) is the orthogonal projection onto F(x) def 

= {x + y âˆˆ IRn | J(x)y = 0 and x + y â‰¥ 0}

(see [6, Section 12.1.4], for instance). In this setting, one still defines Î±T,k by (2) and one simply chooses 

sT,k = min[ Î±T,k , 1] p1(xk). (26) Again, we note that 

Ï€T (xk) â‰¤ âˆ¥ gkâˆ¥. (27) The minimum in (26) ensures that, by construction, xk + sT,k âˆˆ F (xk) and thus that (7) holds. The definition (26) also implies that (8) holds, while (10) with Î¸T = 1 directly results from (25). The nature of the orthogonal projection also ensures the following result. 

Lemma 2.2 Suppose that, at a tangential iteration kÏ„ , sT,k Ï„ is defined by (26). Then 

|gTkÏ„ sT,k Ï„ | â‰¥ âˆšÏ‚

max[ Î·, 1] Î±T,k Ï„ Ï€T (xkÏ„ )2 =

âˆšÏ‚

max[ Î·, 1] Î±T,k Ï„ Ï‰2 

> T,k Ï„

. (28) 

Proof. The optimal nature of the projection implies that 



[xkÏ„ âˆ’ gkÏ„ ] âˆ’ [xkÏ„ + p1(xkÏ„ )] 

T 

[xkÏ„ + p1(xkÏ„ )] âˆ’ xkÏ„



â‰¥ 0and thus 

gTkÏ„ p1(xkÏ„ ) â‰¤ âˆ’âˆ¥ p1(xkÏ„ )âˆ¥2 = âˆ’Ï€T (xkÏ„ )2.

Suppose first that sT,k = Î±T,k p1(xk). Then, 

|gTkÏ„ sT,k | â‰¥ Î±T,k Ï€T (xkÏ„ )2. (29) Alternatively, if sT,k = p1(xk), this implies that Î±T,k â‰¥ 1. Now, (2) gives that âˆšÏ‚Î± T,k â‰¤

max[ Î·, 1] and hence 

|gTkÏ„ sT,k | â‰¥ Ï€T (xkÏ„ )2 â‰¥âˆšÏ‚

max[ Î·, 1] Î±T,k Ï€T (xkÏ„ )2. (30) Combining (29) and (30) yields (28). 2

Thus the step (26) also satisfies (9) with Îºt = âˆšÏ‚/ max[ Î·, 1]. Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 7

2.3 Comments 

Some observations are in order at this stage. 1. Three types of iterations may occur in the course of the execution of the algorithm. 

â€¢ The first is when the constraint violation is large, in which case condition (3) typically fails. A normal step sN,k is then computed but a tangential step is not, which is probably reasonable because the meaning of a move in the tangent plane far away from the constraint is debatable, as it could result in very large steps which take forever to recover from. 

â€¢ The second is when the constraint violation is moderate and (3) holds. Both normal and tangential step may then be computed. 

â€¢ The third is when the constraint violation is small. Condition (3) holds so that a tangential step is computed, but a normal step is not. What actually happens in a run depends on the choice of the constant Î² in (3) and the userâ€™s decision to avoid or force a normal step when possible. 2. The tangential stepsize formula (2) is of course reminiscent of the stepsize formula used in AdaGrad for unconstrained problems. Note that the running sum of squares of dual measures (Î“ k) is only updated at tangential iterations. 3. We have chosen to use the âˆ¥.âˆ¥âˆ norm in (16), (18) and (20) so that these problems are standard linear programs, but, as we noted above, this is not necessary. In particular, variants using the (isotropic) Euclidean norm or preconditioned version of these norms may also be considered. One reason to consider Euclidean or other ellipsoidal norms is that n

inequality constraints created by the box constraints in the linear programs are replaced by a single constraint. 4. In our statement of the ADIC framework, we have assumed that subproblems ((18), (20) or the projection problem in (25)) are solved exactly. This is not necessary and it is sufficient that approximate solution are accurate enough to produce a decrease in the Lagrangian at least a fraction of the optimal one (as suggested by the introduction of the constant Îºt). 

# 3 Worst-case complexity analysis 

This section is devoted to the theoretical study of the ADIC method(s). Its main result is that, under suitable conditions, the average value of ( Ï‰T,k + âˆ¥ckâˆ¥) tends to zero like 1 /âˆšk + 1. We need the following assumptions to derive it. 

AS.0: f and c are continuously differentiable on on open set containing the positive orthant of IR n.

AS.1: For all x â‰¥ 0, f (x) â‰¥ flow .

AS.2: For all x â‰¥ 0, âˆ¥g(x)âˆ¥ â‰¤ Îºg where Îºg â‰¥ Î·Î² .

AS.3: For all x â‰¥ 0, âˆ¥c(x)âˆ¥ â‰¤ Îºc, where Îºc > 1. 

AS.4: For all x â‰¥ 0, âˆ¥J(x)âˆ¥ â‰¤ ÎºJ

AS.5: For all x â‰¥ 0, Ïƒmin [J(x)] â‰¥ Ïƒ0 âˆˆ (0 , 1], 

AS.6: The gradient g(x) is globally Lipschitz continuous on the positive orthant (with constant 

Lg ). Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 8

AS.7: The Jacobian J(x) is globally Lipschitz continuous on the positive orthant (with constant 

LJ ). 

AS.8: There exists a constant Î¾ âˆˆ (0 , 1] such that, for all k â‰¥ 0, Ï‰N,k â‰¥ Î¾âˆ¥ckâˆ¥.Assumptions AS.1â€“AS.4 hold if the iterates remain, as is often the case, in a closed bounded set. Using the fact that the product of bounded and Lipschitz functions is Lipschitz, we deduce the following properties, whose detailed proofs can be found in appendix. 

Lemma 3.1 Suppose that AS.0 and AS.2â€“AS.7 hold. Then we have that 1. c(x) is Lipschitz continuous on the positive orthant (with constant Lc), 2. âˆ‡x( 12 âˆ¥c(x)âˆ¥2) = J(x)T c(x) is Lipschitz continuous on the positive orthant (with con-stant LJT c â‰¥ 1), 3. bÎ»(x) is well-defined on the positive orthant, 4. bÎ»(x) is bounded (by the constant ÎºÎ») and Lipschitz continuous (with constant LÎ») on the positive orthant, 5. âˆ‡xL(x, Î» ) is Lipschitz continuous on the positive orthant (with constant LL). We also observe that (16), (15), AS.3 and AS.4 ensure that 

Ï‡N,k â‰¤ âˆšn âˆ¥JTk ckâˆ¥ â‰¤ âˆšn Îº J Îºc, (31) Finally, AS.8 assumes that there exists a â€œsufficient-descentâ€ direction for the problem (16). Specif-ically, the normal step is designed to reduce Ï‡N,k but it does not guarantee that {âˆ¥ ckÎ½ âˆ¥} also converges to zero. In fact, without further assumption, the minimization of 12 âˆ¥c(x)âˆ¥2 may end up at a local minimizer xloc of this function which is infeasible for the original problem because 

c(xloc )Ì¸ = 0. The existence of such local minimizers may be caused by a singular Jacobian J(xloc )(in which case J(xloc )T c(xloc ) = 0 does not imply c(xloc ) = 0), or by the presence of bounds since 

âˆ’J(xloc )T c(xloc ) may then belong to the normal cone of the bound constraints at xloc . Unfortu-nately, convergence to such an xloc cannot be avoided without either applying a global optimization method to minimize 12 âˆ¥c(x)âˆ¥2, or restricting the class of problems under consideration. Here we follow the second approach and first note that AS.5 already ensures that J(xloc )T c(xloc ) = 0 im-plies c(xloc ) = 0. Making AS.8 is motivated by the observation that descent along any direction for problem (16) not hitting the non-negativity constraints must be limited by the bound âˆ¥dâˆ¥âˆ â‰¤ 1. Thus at least one component of dN,k , say components i âˆˆ I âŠ† { 1, . . . , n }, must be equal to one in absolute value, which implies that Ï‡N,k = |cTk JkdN,k | â‰¥ âˆ¥ [JTk ck]I âˆ¥1. AS.8 then guarantees that 

âˆ¥[JTk ck]I âˆ¥1 is not negligible with respect to âˆ¥JTk ckâˆ¥ â‰¥ Ïƒ0âˆ¥ckâˆ¥.To maintain generality, we finally assume that the considered criticality measures are bounded. 

AS.9: There exists a constant ÎºÏ‰ > 0 such that, for all x âˆˆ IRn, Ï‰T (x) â‰¤ ÎºÏ‰ and Ï‰N (x) â‰¤ ÎºÏ‰ .For the special cases discussed in Sections 2.1 and 2.2, AS.9 automatically results from AS.2â€“AS.4, as can be seen from (19), (27) and (31). The next result shows our requirements on the normal step in Step 3 of the ADIC are not excessive. This is achieved by exhibiting one particular computational scheme (a trust-region method) which satisfies the conditions (4)â€“(6). Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 9

Lemma 3.2 The normal step sN,k (in Step 2) can be computed using a trust-region algo-rithm applied to minimizing 12 âˆ¥c(x)âˆ¥2 subject to (4) and (5) with âˆ¥.âˆ¥ = âˆ¥Â·âˆ¥ âˆ and Ï‰N,k = Ï‡N,k 

defined by (15), starting with the radius Î¸N Ï‡N,k .

Proof. For any âˆ† â‰¤ min[1 , Î¸ N Ï‡N,k ], let sT R (âˆ†) be the solution of the problem of minimizing 

cTk Jks over the constraints x+sT R (âˆ†) â‰¥ 0 and âˆ¥sT R (âˆ†) âˆ¥âˆ â‰¤ âˆ†. Using the Lipschitz continuity of J(x)T c(x) (with constant LJT c ), we obtain that 

> 12



âˆ¥c(xk + sT R (âˆ†)) âˆ¥2 âˆ’ âˆ¥ ckâˆ¥2

â‰¤ cTk JksT R (âˆ†) + LJT c 

2 âˆ†2 â‰¤ cTk JksT R (âˆ†) + max[ ÎºÏ‰ , L JT c ]2 âˆ†2,

(32) where ÎºÏ‰ is defined in AS.9. Now, since âˆ† â‰¤ min[1 , Î¸ N Ï‡N,k ], given the vector dN,k solution to (16), the vector âˆ† dN,k is feasible for (4)-(5), and thus, from (15), 

cTk JksT R (âˆ†) â‰¤ cTk Jk(âˆ† dN,k ) = âˆ’Ï‡N,k âˆ†.

Hence 

cTk JksT R (âˆ†) + max[ ÎºÏ‰ , L JT c ]2 âˆ†2 â‰¤ âˆ’ Ï‡N,k âˆ† + max[ ÎºÏ‰ , L JT c ]2 âˆ†2. (33) It is then easy to verify that, if âˆ† â‰¤ Ï‡N,k / max[ ÎºÏ‰ , L JT c ] then (33) gives that 

cTk JksT R (âˆ†) + max[ ÎºÏ‰ , L JT c ]2 âˆ†2 â‰¤ âˆ’ 12 Ï‡N,k âˆ†. (34) Remembering (31), we may then define 

sN,k = sT R (âˆ† âˆ—) with âˆ†âˆ— = min 



1, Ï‡N,k 

max[ ÎºÏ‰ , L JT c ]



= Ï‡N,k 

max[ ÎºÏ‰ , L JT c ] â‰¤ Î¸N Ï‡N,k 

where the last inequality, which shows that (5) holds, is derived using the bounds LT c â‰¥ 1 and 

Î¸N > 1. Substituting this value in (34) and using (32) then yields that 

> 12



âˆ¥c(xk + sN,k )âˆ¥2 âˆ’ âˆ¥ ckâˆ¥2

â‰¤ âˆ’ Ï‡2

> N,k

2 max[ ÎºÏ‰ , L JT c ]which proves the desired conclusion (with Îºn = 1 /(2 max[ ÎºÏ‰ , L JT c ]) âˆˆ (0 , 12 )), because the radius âˆ† can then be reduced (if necessary) starting from Î¸N Ï‡N,k until (6) holds. 2

The previous result implies that the normal step can be computed using a trust-region al-gorithm for minimizing 12 âˆ¥c(x)âˆ¥2 subject to (4) and (5), âˆ¥.âˆ¥ = âˆ¥ Â· âˆ¥ âˆ and imposing that the trust-region solution sT R (âˆ†) satisfies  

> 12

âˆ¥c(xk + sT R (âˆ†)) âˆ¥2 â‰¤ 12 âˆ¥ckâˆ¥2 âˆ’ 12 Ï‡N,k (âˆ†) .

Our analysis now proceeds by studying the behaviour of the Lyapunov function (11) for iter-ations using normal and tangential steps (indexed by kÎ½ and kÏ„ , respectively), before combining the results and deriving global rates of convergence of ( Ï‰T,k + âˆ¥ckâˆ¥) to zero along the sequences 

{kÏ„ }, {kÎ½ } and, finally, {k}. For brevity, we define the abbreviated notations 

Ïˆ(x) def 

= Ïˆ x, bÎ»(x) and bÎ»k = bÎ»(xk). (35) We also observe that (12) and (13) ensure that, for Î»â€  = bÎ»(x), 

âˆ‡xL(x, Î» â€ ) = g(x) + J(x)T bÎ»(x) = gT (x), (36) where gT (x) is the orthogonal projection of g(x) onto the nullspace of J(x), and consequently, using AS.2, 

âˆ¥âˆ‡ xL(x, Î» â€ )âˆ¥ â‰¤ âˆ¥ g(x)âˆ¥ â‰¤ Îºg . (37) Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 10 

3.1 Descent at normal steps 

We first consider the effect of normal steps on the value of the Lyapunov function Ïˆ. We start by a very simple observation. 

Lemma 3.3 Suppose that AS.5 and AS.8 hold and that a normal step is used at iteration 

kÎ½ . Then c+ 

> kÎ½

= c(xkÎ½ + sN,k Î½ ) satisfies 

âˆ¥c+ 

> kÎ½

âˆ¥ âˆ’ âˆ¥ ckÎ½ âˆ¥ â‰¤ âˆ’ ÎºnÎ¾ Ï‰ N,k Î½ . (38) 

Proof. We have from (6) that âˆ¥c+ 

> kÎ½

âˆ¥ < âˆ¥ckÎ½ âˆ¥. Then, 2âˆ¥ckÎ½ âˆ¥(âˆ¥ckÎ½ âˆ¥ âˆ’ âˆ¥ c+ 

> kÎ½

âˆ¥) â‰¥ (âˆ¥ckÎ½ âˆ¥ + âˆ¥c+ 

> kÎ½

âˆ¥)( âˆ¥ckÎ½ âˆ¥ âˆ’ âˆ¥ c+ 

> kÎ½

âˆ¥) = âˆ¥ckÎ½ âˆ¥2 âˆ’ âˆ¥ c+ 

> kÎ½

âˆ¥2,

and therefore, using (6) and AS.8, that 

âˆ¥c+ 

> kÎ½

âˆ¥ âˆ’ âˆ¥ ckÎ½ âˆ¥ â‰¤ âˆ’ ÎºnÏ‰2

> N,k Î½

âˆ¥ckÎ½ âˆ¥ â‰¤ âˆ’ ÎºnÎ¾ Ï‰ N,k Î½

2

We then use this observation to deduce the following result. 

Lemma 3.4 Suppose that AS.3â€“AS.9 hold and that a normal step is used at iteration kÎ½ .Define 

Ï = 1

ÎºnÎ¾ ,



(Îºg + Îºc LÎ»)Î¸N +

 LL

2 + LÎ»Lc



Î¸2 

> N

ÎºÏ‰ + Î·



(39) Then x+ 

> kÎ½

= xkÎ½ + sN,k Î½ satisfies 

Ïˆ(x+ 

> kÎ½

) âˆ’ Ïˆ(xkÎ½ ) â‰¤ âˆ’ Î· Ï‰ N,k Î½ . (40) 

Proof. We have that 

Ïˆ(x+ 

> kÎ½

) âˆ’ Ïˆ(xkÎ½ ) = Ïˆ(x+ 

> kÎ½

, bÎ»kÎ½ ) âˆ’ Ïˆ(xkÎ½ , bÎ»kÎ½ )

| {z }

> âˆ†x

+ Ïˆ(x+ 

> kÎ½

, bÎ»+ 

> kÎ½

) âˆ’ Ïˆ(x+ 

> kÎ½

, bÎ»kÎ½ )

| {z }

> âˆ†Î»

. (41) Now consider âˆ† x and âˆ† Î» separately. Using the Lipschitz continuity of âˆ‡xÏˆ(x, bÎ») ( Ï is fixed in (39)) and (38), we obtain that âˆ†x = Ïˆ(x+ 

> kÎ½

, bÎ»kÎ½ ) âˆ’ Ïˆ(xkÎ½ , bÎ»kÎ½ )= L(x+ 

> kÎ½

, bÎ»kÎ½ ) âˆ’ L(xkÎ½ , bÎ»kÎ½ ) + Ï âˆ¥c+ 

> kÎ½

âˆ¥ âˆ’ âˆ¥ ckÎ½ âˆ¥

â‰¤ (âˆ‡xL(xkÎ½ , bÎ»kÎ½ )T sN,k Î½ + r3 âˆ’ ÏÎº nÎ¾ Ï‰ N,k Î½

(42) with 

|r3| â‰¤ LL

2 âˆ¥sN,k Î½ âˆ¥2.Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 11 We now invoke the Cauchy-Schwartz inequality, (37) and (5) to deduce that âˆ†x â‰¤ âˆ¥âˆ‡ xL(xkÎ½ , bÎ»kÎ½ )âˆ¥ âˆ¥ sN,k Î½ âˆ¥ âˆ’ Ï ÎºnÎ¾

2 Ï‰N,k Î½ + LL

2 âˆ¥sN,k Î½ âˆ¥2

â‰¤ Îºg âˆ¥sN,k Î½ âˆ¥ âˆ’ ÏÎº nÎ¾ Ï‰ N,k Î½ + LL

2 âˆ¥sN,k Î½ âˆ¥2

â‰¤ Îºg Î¸N Ï‰N,k Î½ âˆ’ ÏÎº nÎ¾ Ï‰ N,k Î½ + LL

2 Î¸2 

> N

Ï‰2 

> N,k Î½

.

(43) Using now the definition of âˆ† Î» in (41), AS.6, the Lipschitz continuity of bÎ» and c and AS.3 then yields that âˆ†Î» = Ïˆ(x+ 

> kÎ½

, bÎ»(x+ 

> kÎ½

)) âˆ’ Ïˆ(x+ 

> kÎ½

, bÎ»(xkÎ½ )) 

â‰¤ (âˆ¥ckÎ½ âˆ¥ + âˆ¥c+ 

> kÎ½

âˆ’ ckÎ½ âˆ¥) âˆ¥bÎ»+ 

> kÎ½

âˆ’ bÎ»kÎ½ âˆ¥â‰¤ LÎ» âˆ¥sN,k Î½ âˆ¥ âˆ¥ ckÎ½ âˆ¥ + LÎ»Lcâˆ¥sN,k Î½ âˆ¥2

â‰¤ LÎ» Î¸N ÎºcÏ‰N,k Î½ + LÎ»LcÎ¸2 

> N

Ï‰2

> N,k Î½

(44) and thus, summing (43) and (44), that 

Ïˆ(x+ 

> kÎ½

) âˆ’ Ïˆ(xkÎ½ )

â‰¤ âˆ’ ÏÎº nÎ¾ Ï‰ N,k Î½ + Îºg Î¸N Ï‰N,k Î½ + LÎ» Îºc Î¸N Ï‰N,k Î½ +

 Î¸2 

> N

LL

2 + Î¸2 

> N

LÎ»Lc



Ï‰2

> N,k Î½

â‰¤ âˆ’ ÏÎº nÎ¾Ï‰ N,k Î½ + ( Îºg Î¸N + LÎ» Îºc Î¸N ) Ï‰N,k Î½ +

 Î¸2 

> N

LL

2 + Î¸2 

> N

LÎ»Lc



ÎºÏ‰ Ï‰N,k Î½ ,

where we have used AS.9 to deduce the second inequality. The bound (40) then follows from (39). 

2

Note that, should sN,k belong to the range space of Jk, the first term in the last right-hand side of (42) vanishes and Îºg disappears from (43) and, consequently, from (39). 

3.2 Descent at tangential steps 

We now turn to considering the effect of tangential steps. 

Lemma 3.5 Suppose that AS.4â€“AS.8 hold. Then 

Ïˆ(xkÏ„ +1 ) âˆ’ Ïˆ(x+ 

> kÏ„

) â‰¤ âˆ’ ÎºtÎ±T,k Ï„ Ï‰2 

> T,k Ï„

+ Îºtan Î±2 

> T,k Ï„

Ï‰2 

> T,k Ï„

. (45) where 

Îºtan =

 Î¸2

> T

2



LL + ÏL c



+ Î²Î¸ N Î¸T



LL + ÎºJ LÎ» + ÏL J



+ Î²Î¸ T LÎ»

Î¾ + Î¸2 

> T

LcLÎ». (46) 

Proof. As in (41), we now have that 

Ïˆ(xkÏ„ +1 ) âˆ’ Ïˆ(x+ 

> kÏ„

) = Ïˆ(xkÏ„ +1 , bÎ»+ 

> kÏ„

) âˆ’ Ïˆ(x+ 

> kÏ„

, bÎ»+ 

> kÏ„

)

| {z }

> âˆ†x

+ Ïˆ(xkÏ„ +1 , bÎ»kÏ„ +1 ) âˆ’ Ïˆ(xkÏ„ +1 , bÎ»+ 

> kÏ„

)

| {z }

> âˆ†Î»

. (47) The Lipschitz continuity of âˆ‡xÏˆ(x, bÎ»), (11) and (35) give that âˆ†x = âˆ‡xL(x+ 

> kÏ„

, bÎ»+ 

> kÏ„

)T sT,k Ï„ + r0 + Ï(âˆ¥ckÏ„ +1 âˆ¥ âˆ’ âˆ¥ c+ 

> kÏ„

âˆ¥) with |r0| â‰¤ LL

2 âˆ¥sT,k Ï„ âˆ¥2. (48) Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 12 Equation (8) gives 

âˆ¥c(xkÏ„ +1 )âˆ¥ = âˆ¥c(x+ 

> kÏ„

) âˆ’ JkÏ„ sT,k Ï„ + ( J+ 

> kÏ„

âˆ’ JkÏ„ )sT,k Ï„ + r1âˆ¥â‰¤ âˆ¥ c+ 

> kÏ„

âˆ¥ + âˆ¥r1âˆ¥ + âˆ¥J+ 

> kÏ„

âˆ’ JkÏ„ âˆ¥ âˆ¥ sT,k Ï„ âˆ¥â‰¤ âˆ¥ c+ 

> kÏ„

âˆ¥ + âˆ¥r1âˆ¥ + LJ âˆ¥sN,k Ï„ âˆ¥ âˆ¥ sT,k Ï„ âˆ¥

with âˆ¥r1âˆ¥ â‰¤ Lc 

> 2

âˆ¥sT,k Ï„ âˆ¥2. Now âˆ¥sN,k Ï„ âˆ¥ is either zero (if kÏ„Ì¸ âˆˆ { kÎ½ }) or, using (3) for kÏ„ ,

Ï‰N,k Ï„ â‰¤ Î²Î± T,k Ï„ Ï‰T,k Ï„

and thus 

âˆ¥sN,k Ï„ âˆ¥ â‰¤ Î¸N Ï‰N,k Ï„ â‰¤ Î²Î¸ N Î±T,k Ï„ Ï‰T,k Ï„ , (49) so that, whether kÏ„ âˆˆ { kÎ½ } or not, using (10), 

âˆ¥c(xkÏ„ +1 )âˆ¥ âˆ’ âˆ¥ c(x+ 

> kÏ„

)âˆ¥ â‰¤ 

 Î¸2 

> T

Lc

2 + Î²Î¸ N Î¸T LJ



Î±2 

> T,k Ï„

Ï‰2 

> T,k Ï„

. (50) Now differentiating L with respect to its first argument and using the Lipschitz continuity of 

âˆ‡xL with respect to this first argument, AS.4, (8) and the Lipschitz continuity of bÎ» gives that 

âˆ‡xL(x+ 

> kÏ„

, bÎ»+ 

> kÏ„

)T sT,k Ï„ =



âˆ‡xL(x+ 

> kÏ„

, bÎ»+ 

> kÏ„

)T sT,k Ï„ âˆ’ âˆ‡ xL(xkÏ„ , bÎ»+ 

> kÏ„

)T sT,k Ï„



+



âˆ‡xL(xkÏ„ , bÎ»+ 

> kÏ„

)T sT,k Ï„ âˆ’ âˆ‡ xL(xkÏ„ , bÎ»kÏ„ )T sT,k Ï„



+ gTkÏ„ sT,k Ï„ + bÎ»TkÏ„ JkÏ„ sT,k Ï„

=



âˆ‡xL(x+ 

> kÏ„

, bÎ»+ 

> kÏ„

)T sT,k Ï„ âˆ’ âˆ‡ xL(xkÏ„ , bÎ»+ 

> kÏ„

)T sT,k Ï„



+



(bÎ»+ 

> kÏ„

)T JkÏ„ âˆ’ bÎ»TkÏ„ JkÏ„

T

sT,k Ï„ + gTkÏ„ sT,k Ï„ + bÎ»TkÏ„ JkÏ„ sT,k Ï„

â‰¤



LL + ÎºJ LÎ»



âˆ¥sN,k Ï„ âˆ¥ âˆ¥ sT,k Ï„ âˆ¥ + gTkÏ„ sT,k Ï„ + bÎ»TkÏ„ JkÏ„ sT,k Ï„

â‰¤ gTkÏ„ sT,k Ï„ + Î²



LL + ÎºJ LÎ»



Î¸N Î¸T Î±2 

> T,k Ï„

Ï‰2

> T,k Ï„

where the last inequality results from (8) and (49). Hence we obtain from (48), (9), (10) and (50) that âˆ†x â‰¤ gTkÏ„ sT,k Ï„ + r0 + Ï

 Î¸2 

> T

Lc

2 + Î²Î¸ N Î¸T LJ



Î±2 

> T,k Ï„

Ï‰2 

> T,k Ï„

+ Î²Î¸ N Î¸T



LL + ÎºJ LÎ»



Î±2 

> T,k Ï„

Ï‰2

> T,k Ï„

â‰¤ gTkÏ„ sT,k Ï„ +

 Î¸2

> T

2 (LL + ÏL c) + Î²Î¸ N Î¸T



LL + ÎºJ LÎ» + ÏL J



Î±2 

> T,k Ï„

Ï‰2

> T,k Ï„

â‰¤ âˆ’ ÎºtÎ±T,k Ï„ Ï‰2 

> T,k Ï„

+

 Î¸2

> T

2 (LL + ÏL c) + Î²Î¸ N Î¸T



LL + ÎºJ LÎ» + ÏL J



Î±2 

> T,k Ï„

Ï‰2 

> T,k Ï„

.

(51) Now, we may use the Lipschitz continuity of bÎ» and c, inequality (6), the Cauchy-Schwartz inequality, and AS.8 to deduce that âˆ†Î» = cTkÏ„ +1 

 bÎ»kÏ„ +1 âˆ’ bÎ»+

> kÏ„



= ( ckÏ„ +1 âˆ’ c+ 

> kÏ„

)T  bÎ»kÏ„ +1 âˆ’ bÎ»+

> kÏ„

 + ( c+ 

> kÏ„

)T  bÎ»kÏ„ +1 âˆ’ bÎ»+

> kÏ„



â‰¤ âˆ¥ c+ 

> kÏ„

âˆ¥ âˆ¥ bÎ»kÏ„ +1 âˆ’ bÎ»+ 

> kÏ„

âˆ¥ + âˆ¥ckÏ„ +1 âˆ’ c+ 

> kÏ„

âˆ¥ âˆ¥ bÎ»kÏ„ +1 âˆ’ bÎ»+ 

> kÏ„

âˆ¥â‰¤ âˆ¥ ckÏ„ âˆ¥ âˆ¥ bÎ»kÏ„ +1 âˆ’ bÎ»+ 

> kÏ„

âˆ¥ + âˆ¥ckÏ„ +1 âˆ’ c+ 

> kÏ„

âˆ¥ âˆ¥ bÎ»kÏ„ +1 âˆ’ bÎ»+ 

> kÏ„

âˆ¥â‰¤ LÎ»

Î¾ Ï‰N,k Ï„ âˆ¥sT,k Ï„ âˆ¥ + LcLÎ»âˆ¥sT,k Ï„ âˆ¥2.

(52) Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 13 Again using (3) for k âˆˆ { kÏ„ }, (10) and (49), we obtain that âˆ†Î» â‰¤ Î²Î¸ T LÎ»

Î¾ Î±2 

> T,k Ï„

Ï‰2 

> T,k Ï„

+ Î¸2 

> T

LcLÎ»Î±2 

> T,k Ï„

Ï‰2 

> T,k Ï„

.

Thus, summing âˆ† x and âˆ† Î», we deduce that 

Ïˆ(xkÏ„ +1 ) âˆ’ Ïˆ(x+ 

> kÏ„

) â‰¤ âˆ’ ÎºtÎ±T,k Ï„ Ï‰2 

> T,k Ï„

+

 Î¸2

> T

2



LL + ÏL c



+ Î²Î¸ N Î¸T



LL + ÎºJ LÎ» + ÏL J



Î±2 

> T,k Ï„

Ï‰2

> T,k Ï„

+

 Î²Î¸ T LÎ»

Î¾ + Î¸2 

> T

LcLÎ»



Î±2 

> T,k Ï„

Ï‰2

> T,k Ï„

and (45) follows. 2

Observe that the second term in the bracket of (46) only appears when kÏ„ âˆˆ { kÎ½ }. The bound (45) quantifies the effect of tangential steps on the Lyapunov function, and its right-hand side involves a first-order (descent) term and a second-order perturbation term. We now derive crucial bounds on these terms, using the fact that Î“ k is not updated at normal iterations. 

Lemma 3.6 Suppose that AS.2 and AS.5 hold. If we denote Î“kÏ„0 = 0 , Î“kÏ„ +1 = Î“ kÏ„ + Ï‰2 

> T,k Ï„

, Î±T,k Ï„ = Î·

pÏ‚ + Î“ kÏ„ +1 

,

then, for all Ï„0 â‰¤ Ï„1,

> Ï„1

X 

> Ï„=Ï„0

Î±T,k Ï„ Ï‰2 

> T,k Ï„

> Î· âˆšÏ‚

s

1 + Î“kÏ„1+1 

Ï‚ âˆ’ Î·âˆšÏ‚ (53) 

> Ï„1

X 

> Ï„=Ï„0

Î±2 

> T,k Ï„

Ï‰2 

> T,k Ï„

â‰¤ Î·2 log 



1 + Î“kÏ„1+1 

Ï‚



. (54) 

Proof. Let wkÏ„ +1 = pÎ“kÏ„ +1 + Ï‚. The definition of Î±T,k Ï„ in (2) implies that 

> Ï„1

X 

> Ï„=Ï„0

Î±T,k Ï„ Ï‰2 

> T,k Ï„

= Î·

> Ï„1

X 

> Ï„=Ï„0

Ï‰2

> T,k Ï„

pÏ‚ + Î“ kÏ„ +1 

> Î· 

> Ï„1

X 

> Ï„=Ï„0

Ï‰2

> T,k Ï„

wkÏ„ +1 + wkÏ„

= Î·

> Ï„1

X 

> Ï„=Ï„0

w2  

> kÏ„+1

âˆ’ w2

> kÏ„

wkÏ„ +1 + wkÏ„

= Î·

> Ï„1

X 

> Ï„=Ï„0

(wkÏ„ +1 âˆ’ wkÏ„ )= Î· wkÏ„1+1 âˆ’ wkÏ„0

.

Now observe that, using Î“ kÏ„0 = 0, 

wkÏ„1+1 âˆ’ wkÏ„0 =

q

Ï‚ + Î“ kÏ„1+1 âˆ’

q

Ï‚ + Î“ kÏ„0 =

q

Ï‚ + Î“ kÏ„1+1 âˆ’ âˆšÏ‚, Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 14 which then gives (53). Using the concavity and the increasing nature of the logarithm, we also have from (2) that 

Î±2 

> T,k Ï„

Ï‰2 

> T,k Ï„

= Î·2 Ï‰2

> T,k Ï„

Ï‚ + Î“ kÏ„ +1 

= Î·2 Î“kÏ„ +1 âˆ’ Î“kÏ„

Ï‚ + Î“ kÏ„ +1 

â‰¤ Î·2 log( Ï‚ + Î“ kÏ„ +1 ) âˆ’ log( Ï‚ + Î“ kÏ„ ) .

Summing for Ï„ âˆˆ { Ï„0, . . . , Ï„ 1} then yields that 

> Ï„1

X 

> Ï„=Ï„0

Î±2 

> T,k Ï„

Ï‰2 

> T,k Ï„

â‰¤ Î·2 log( Ï‚ + Î“ kÏ„1+1 ) âˆ’ log( Ï‚ + Î“ kÏ„0 ) ,

and (54) follows, again using Î“ kÏ„0 = 0. 2

3.3 Telescoping sum 

Having considered the impacts of tangential and normal steps separately, we now combine them to derive a crucial inequality. 

Lemma 3.7 Suppose that AS.0â€“AS.8 hold. Then, for any Ï„1 > 0 and any Î½1 â‰¥ 0, 

s

1 + Î“kÏ„1+1 

Ï‚ +

> Î½1

X

> Î½=Î½0

Ï‰N,k Î½ â‰¤ Îºgap + Îºtan 

Îºt

âˆšÏ‚ log 



1 + Î“kÏ„1+1 

Ï‚



, (55) where 

Îºgap = 1

Î·Îº t

âˆšÏ‚ (1 + Ïˆ(x0) + ÎºcÎºÎ» + ÏÎº c âˆ’ flow ) .

Proof. Consider k â‰¥ 0. Then, defining min[ kÎ½0 , k Ï„0 ] = 0 and max[ kÎ½1 , k Ï„1 ] = k, we have that Î“ kÏ„0 = 0 and we may apply Lemma 3.6. Combining (45), (53) and (54), we obtain that 

> Ï„1

X 

> Ï„=Ï„0

 Ïˆ(xkÏ„ +1 ) âˆ’ Ïˆ(x+ 

> kÏ„

) â‰¤ Î·Îº t

âˆšÏ‚ âˆ’ Î·Îº t

âˆšÏ‚

s

1 + Î“kÏ„1+1 

Ï‚ + Î·2Îºtan log 



1 + Î“kÏ„1+1 

Ï‚



. (56) Also considering (40) and observing that x+ 

> k

= xk+1 when k âˆˆ { kÎ½ } \ { kÏ„ } and x+ 

> k

= xk when 

k âˆˆ { kÏ„ } \ { kÎ½ } therefore yields that 

Ïˆ(xk+1 ) âˆ’ Ïˆ(x0) = 

> Ï„1

X 

> Ï„=Ï„0

 Ïˆ(xkÏ„ +1 ) âˆ’ Ïˆ(x+ 

> kÏ„

) +

> Î½1

X

> Î½=Î½0

 Ïˆ(x+ 

> kÎ½

) âˆ’ Ïˆ(xkÎ½ )

â‰¤ Î·Îº t

âˆšÏ‚ âˆ’ Î·Îº t

âˆšÏ‚

s

1 + Î“kÏ„1+1 

Ï‚ âˆ’ Î·

> Î½1

X

> Î½=Î½0

Ï‰N,k Î½ + Î·2Îºtan log 



1 + Î“kÏ„1+1 

Ï‚



â‰¤ Î·Îº t

âˆšÏ‚ âˆ’ Î·Îº t

âˆšÏ‚

s

1 + Î“kÏ„1+1 

Ï‚ âˆ’ Î·Îº t

âˆšÏ‚

> Î½1

X

> Î½=Î½0

Ï‰N,k Î½ + Î·Îº tan log 



1 + Î“kÏ„1+1 

Ï‚



,

(57) where we used the facts that Îºt

âˆšÏ‚ < 1 and Î· â‰¤ 1. Using now (11), (35), the Cauchy-Schwartz inequality, the boundedness of bÎ»(x), AS.1 and AS.3, we have that 

Ïˆ(xk+1 ) âˆ’ Ïˆ(x0) âˆ’ Î·Îº t

âˆšÏ‚ â‰¥ flow âˆ’ ÎºcÎºÎ» âˆ’ ÏÎº c âˆ’ Ïˆ(x0) âˆ’ Î·Îº t

âˆšÏ‚ def 

= âˆ’Î·Îº t

âˆšÏ‚ Îº gap ,

so that (57) implies (55). 2Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 15 

3.4 Tangential complexity 

Lemma 3.7 implies upper bounds for both Î“ kÏ„ and Ï‰N,k Î½ . We now exploit the first of these to derive the rate of convergence for tangential steps proper, after establishing a useful technical result. 

Lemma 3.8 Suppose that at â‰¤ b + c log( t) for t â‰¥ 1 and a, c > 0. Then, 

t â‰¤ 2ba + 2ca



log 

 2ca



âˆ’ 1



.

Proof. See [1, Lemma 3.2]. 2

Lemma 3.9 Suppose that AS.0â€“AS.9 hold. Then, for any Ï„1 > 0, 

q

Ï‚ + Î“ kÏ„1+1 â‰¤ ÎºT

> def

= 2 Îºgap 

âˆšÏ‚ + 4Îºtan 

Îºt



log 

 4Îºtan 

Îºt

âˆšÏ‚



âˆ’ 1



(58) and 

Î¾

> Ï„1

X 

> Ï„=Ï„0



Ï‰T,k Ï„ + âˆ¥ckÏ„ âˆ¥



â‰¤

> Ï„1

X 

> Ï„=Ï„0



Ï‰T,k Ï„ + Ï‰N,k Ï„



â‰¤ ÎºT

âˆšÏ„1 + 1 



1 + Î²Î· 

âˆšÏ‚



. (59) 

Proof. The bound (55) implies that 

s

1 + Î“kÏ„1+1 

Ï‚ â‰¤ Îºgap + Îºtan 

Îºt

âˆšÏ‚ log 



1 + Î“kÏ„1+1 

Ï‚



= Îºgap + 2Îºtan 

Îºt

âˆšÏ‚ log 

ï£«ï£­s

1 + Î“kÏ„1+1 

Ï‚

ï£¶ï£¸ .

Using Lemma 3.8 with 

t =

s

1 + Î“kÏ„1+1 

Ï‚ , a = 1 , b = Îºgap and c = 2Îºtan 

Îºt

âˆšÏ‚ ,

we then obtain that 

q

Ï‚ + Î“ kÏ„1+1 = âˆšÏ‚

s

1 + Î“kÏ„1+1 

Ï‚ â‰¤ âˆšÏ‚



2Îºgap + 4Îºtan 

Îºt

âˆšÏ‚



log 

 4Îºtan 

Îºt

âˆšÏ‚



âˆ’ 1

 

.

This is (58). We may now invoke the inequality 

> k

X

> j=0

aj â‰¤ âˆšk + 1 

vuut kX

> j=0

a2

> j

for nonnegative {aj }kj=0 to deduce from the definition of Î“ kÏ„ and (58) that 

> Ï„1

X 

> Ï„=Ï„0

Ï‰T,k Ï„ â‰¤ âˆšÏ„1 + 1 

vuut

> Ï„1

X 

> Ï„=Ï„0

Ï‰2 

> T,k Ï„

= âˆšÏ„1 + 1 

q

Î“kÏ„1 +1 < âˆšÏ„1 + 1 

q

Ï‚ + Î“ kÏ„1 +1 â‰¤ âˆšÏ„1 + 1 ÎºT .

(60) Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 16 Using the switching condition (3) and the first part of (14), we then deduce that, whether kÏ„

belongs to {kÎ½ } or not, 

> Ï„1

X 

> Ï„=Ï„0

Ï‰N,k Ï„ â‰¤

> Ï„1

X 

> Ï„=Ï„0

Î²Î± T,k Ï„ Ï‰T,k Ï„ â‰¤ Î²Î· 

âˆšÏ‚

> Ï„1

X 

> Ï„=Ï„0

Ï‰T,k Ï„ < Î²Î·Îº T

âˆšÏ„1 + 1 

âˆšÏ‚ .

Summing this bound with (60) then gives the second inequality of (59). The first results from AS.8. 2

3.5 Normal complexity 

We now exploit the bound on Ï‰N,k Î½ stated in Lemma 3.7 to analyze the complexity of the subse-quence of normal iterations. We first show that the sum of the norms of constraint violations is bounded. 

Lemma 3.10 Suppose that AS.0â€“AS.9 hold. Then, for any Î½1 > 0, 

Î¾

> Î½1

X

> Î½=Î½0

âˆ¥ckÎ½ âˆ¥ â‰¤ 

> Î½1

X

> Î½=Î½0

Ï‰N,k Î½ < Îº N , (61) where 

ÎºN = Îºgap + Îºtan log 



1 + Îº2

> T

Ï‚



. (62) 

Proof. The bound (55) ensures that 

> Î½1

X

> Î½=Î½0

Ï‰N,k Î½ â‰¤ Îºgap + Îºtan log 



1 + Î“kÏ„1 +1 

Ï‚



, (63) where kÏ„1 is the index of the last tangential iteration before kÎ½1 . Substituting the bound (58) in this inequality then gives the second inequality of (61), the first resulting again from AS.8. 

2

This allows us to derive boundedness of a combined primal and dual criticality measure. 

Lemma 3.11 Suppose that AS.0â€“AS.9 hold. Then, for any Î½1 â‰¥ 0, 

Î¾

> Î½1

X  

> Î½=Î½0,k Î½Ì¸âˆˆ{ kÏ„}



Ï‰T,k Î½ + âˆ¥ckÎ½ âˆ¥



â‰¤

> Î½1

X  

> Î½=Î½0,k Î½Ì¸âˆˆ{ kÏ„}



Ï‰T,k Î½ + Ï‰N,k Î½



< Îº N



1 + ÎºT

Î²Î· 



. (64) 

Proof. Using the switching condition (3) for kÎ½Ì¸ âˆˆ { kÏ„ }, we obtain that, for such kÎ½ with 

Î½ âˆˆ { Î½0, . . . , Î½ 1},

Ï‰N,k Î½ > Î² Î± T,k Î½ Ï‰T,k Î½ . (65) As in the previous lemma, let kÏ„1 be the index of the last tangential iteration before kÎ½1 . Thus using (58), 

Î±T,k Î½ = Î·

pÏ‚ + Î“ kÎ½

â‰¥ Î·ÎºT

.Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 17 Substituting this bound in (65), we find that, for Î½ âˆˆ { Î½0, . . . , Î½ 1},

Ï‰N,k Î½ â‰¥ Î²Î· ÎºT

Ï‰T,k Î½ . (66) With (61), this implies that 

> Î½1

X

> Î½=Î½0

Ï‰T,k Î½ â‰¤ ÎºT

Î²Î· 

> Î½1

X

> Î½=Î½0

Ï‰N,k Î½ â‰¤ ÎºN ÎºT

Î²Î· .

Summing this bound with (61) and using AS.8 gives (64). 2

3.6 Combined complexity 

We finally assemble the pieces of the puzzle to derive our main result on the global rate of con-vergence of the ADIC algorithm. 

Theorem 3.12 Suppose that AS.0-AS.9 hold. Then, for any k â‰¥ 0, 1

k + 1 

> k

X

> j=0



Ï‰T,j + âˆ¥cj âˆ¥



â‰¤ ÎºADIC ,1 

âˆšk + 1 + ÎºADIC ,2 

k + 1 = O

 1

âˆšk + 1 



, (67) where 

ÎºADIC , 1 = ÎºT

Î¾



1 + Î²Î· 

âˆšÏ‚



and ÎºADIC , 2 = ÎºN

Î¾



1 + ÎºT

Î²Î· 



.

Proof. Consider iterations of both types (tangential and normal) from 0 to k by defining min[ kÎ½0 , k Ï„0 ] = 0 and max[ kÎ½1 , k Ï„1 ] = k (as in Lemma 3.7). We then obtain, by combining (59) and (64), that 

> k

X

> j=0

 Ï‰T,j + âˆ¥cj âˆ¥



=

> Ï„1

X 

> Ï„=Ï„0



Ï‰T,k Ï„ + âˆ¥ckÏ„ âˆ¥



+

> Î½1

X  

> Î½=Î½0,k Î½Ì¸âˆˆ{ kÏ„}



Ï‰T,k Î½ + âˆ¥ckÎ½ âˆ¥



â‰¤ ÎºT

Î¾

âˆšk + 1 



1 + Î²Î· 

âˆšÏ‚



+ ÎºN

Î¾



1 + ÎºT

Î²Î· 



,

where we used the inequalities Ï„1 â‰¤ kÏ„1 â‰¤ k and kÎ½1 â‰¤ k. The bound (67) is finally obtained by dividing both sides by k + 1. 2

Remarkably, Theorem 3.12 implies that obtaining an Ïµ-approximate first-order critical point, that is an iterate xk such that Ï‰T,j +âˆ¥cj âˆ¥ â‰¤ Ïµ, requires at most O(Ïµâˆ’2) iterations of the ADIC algorithm, a complexity which is, in order, the same as that of steepest-descent and Newtonâ€™s methods on unconstrained problems [5, Theorems 2.2.2 and 3.1.1]. 

# 4 Numerical illustration 

We now illustrate the behaviour of three variants of the ADIC algorithm on problems from the 

CUTEst [11] collection as provided in Matlab by S2MPJ [14]. All nonlinear optimization problems in the collection involving general constraints and at most 200 variables were considered, leading to a test set of 312 problems. The algorithmic variants are defined as follows. Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 18 

â€¢ The first variant (ADIC-LP) follows Section 2.1 and computes the dual criticality measure and tangential step using the linear optimization subproblems (18) and (20), respectively. 

â€¢ The second variant (ADIC-BK) again follows Section 2.1 and computes the dual criticality measure using (18), but then uses the simple formula (21) to define the tangential step. 

â€¢ The third variant (ADIC-PR) uses the projection approach of Section 2.2, in which the dual criticality is given by (25) and the tangential step is defined by (26). All three variants have been (trivially) extended to handle general lower and upper bounds on the variables (instead of mere non-negativity constraints), thereby making them applicable to general constrained problems (after transformation of inequality constraints into equalities and the introduction of slack variables, if needed). Because the variants use different criticality measures, a uniform (external) termination cri-terion was implemented in order to enforce consistency in the comparison. For all variants, a problem was considered solved as soon as 

Ï‡T,k â‰¤ 10 âˆ’4 and Ï‡N,k â‰¤ 10 âˆ’5,

where Ï‡T,k and Ï‡N,k are defined in (17) and (15), respectively. Note that this accomodates the (unfortunate but unavoidable) case where an infeasible minimizer of the equality constraintâ€™s violation is found (the bound constraints are satisfied throughout the algorithms). This situation is excluded from our theoretical analysis by AS.8 but does occur in practice. A maximum number of 50000 iterations and a 3600 seconds time limit were imposed. Finally, the algorithmic parameters were chosen as 

Ï‚ = 10 âˆ’5, Î· = 2 , Î¸T = 1 , Î¸N = 5 , Îºn = 10 âˆ’2 and Î² = 10 3.

We first report on a set of experiments in which the gradients used by the three variants were exact. To summarize the results, we computed three performance statistics: efficiency in terms of iterations, efficiency in terms of CPU time needed and reliability. The latter, which we denote by â€Relâ€ in what follows, is simply computed as the percentage of successfully solved problems. For the two first efficiency statistics, we follow the approach of [18] and compute, for each variant, the area below the relevant curve in a performance profile comparing the three variants, truncated at a â€œratio to best performanceâ€ equal to 10. The iteration-based statistic is denoted by â€œItersâ€ and the CPU-based one by â€œTimeâ€. Values of these statistics should be as close to one as possible. Results are presented in Table 1. The corresponding iteration and CPU performance profiles are shown in Figure 1. Variant Iters Time Rel ADIC-LP 0.54 0.57 68.27 ADIC-BK 0.43 0.48 61.54 ADIC-PR 0.61 0.59 71.15 Table 1: Efficiency and reliability statistics for three variants of ADIC on 312 constrained CUTEst 

problems (noiseless gradients) Table 1 and Figure 1 indicate that the projection-based ADIC-PR clearly outperforms both ADIC-LP and ADIC-BK, on all 3 statistics. The dominance of ADIC-PR over ADIC-LP in CPU time is however marginal, despite the fact that two linear optimization subproblems must be solved at each iteration of ADIC-LP, against a single projection subproblem for ADIC-PR. The ADIC-BK variant, which only requires the solution of a single linear optimization problem per iteration, remains slower mostly because it typically needs more iterations than other variants per successfully solved problem. We finally show that our claim that OFFO methods are reliable in the presence of noise is vindicated in practice. To analyze this, we considered the subset of our problems by keeping those Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 19 1 2 3 4 5 6 7 8 9 10          

> 0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1
> Iterations (312 problems)
> ADIC-LP
> ADIC-BK
> ADIC-PR 12345678910
> 0
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1
> CPU (312 problems)
> ADIC-LP
> ADIC-BK
> ADIC-PR

Figure 1: Iteration (left) and CPU time (right) performance profiles for three variants of ADIC on 312 constrained CUTEst problems (noiseless gradients) Variant 0% 5% 15% 25% 50% ADIC-LP 81.78 74.55 72.85 73.14 72.73 ADIC-BK 74.09 69.11 67.11 65.77 59.37 ADIC-PR 89.88 77.79 74.82 72.57 67.19 Table 2: Reliability statistics for three variants of ADIC on 247 constrained CUTEst problems for relative random Gaussian noise levels of 0%, 5%, 15%, 25% and 50% on the objective functionâ€™s gradient that were solved in the absence of noise by at least one variant, giving a set of 247 test problems. We then added relative random Gaussian 2 noise of increasing magnitude (5%, 15%, 25% and 50%) to the gradients of the objective function and ran each problem 20 times independently, with Ï‡T,k â‰¤ 10 âˆ’3 and Ï‡N,k â‰¤ 10 âˆ’3. We then computed the total reliability of our three variants on the resulting 4940 runs for each of the 5 noise levels. The results are presented in Table 2. They show an impressive stability for increasing noise levels, and indicate that, in our view remarkably, 

ADIC is capable of handling very substantial perturbations of the gradient of the objective function (50% relative noise results in barely one significant digit in the gradient) for a reasonable accuracy requirement. It is also interesting to note that ADIC-LP becomes marginally more reliable than ADIC-PR for large noise levels. 

# 5 Conclusions and perspectives 

We have proposed a new OFFO algorithm for the solution of smooth optimization problems, with excellent stability in the presence of noise on the objective functionâ€™s gradient. This â€trust-funnelâ€ algorithm uses adaptive switching between a normal step (reducing constraint violation), and tangential steps (improving dual optimality), the latter being inspired by the AdaGrad-norm algorithm [8, 22] for unconstrained problems. We have also provided a full analysis of the methodâ€™s worst-case iteration complexity, showing that its global rate of convergence is, for problems with full-rank Jacobians, identical in order to the (optimal) rate of steepest-descent and Newtonâ€™s method on unconstrained problems. This also provides an evaluation complexity for evaluations of the objective functionâ€™s gradient, because each iteration requires a single gradient computation. Evaluation complexity for the constraint function and Jacobian is not direct and depends on the algorithm used in the normal step. We have finally conducted illustrative numerical experiments 

> 2

With zero mean and unit variance. Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 20 suggesting that the algorithmâ€™s performance and reliability are satisfactory (although admittedly not state-of-the-art) on noiseless problems, but that its reliability in the presence of significant noise on the objective functionâ€™s gradient is very remarkable. Many questions remain for further investigation, including the incorporation of second-order information, should it be available, a component-wise version of the algorithm (closer to AdaGrad as opposed to AdaGrad-norm) and a full stochastic complexity analysis. These topics are the subject of ongoing research. Exploiting the independent structure of normal and tangential steps to allow for specific preconditioning of the normal step and relaxing the full-rank assumption on the Jacobians are also of interest. 

Acknowledgement 

Philippe Toint is grateful for the continued and friendly support of the APO team at Toulouse IRIT (F) and of DIEF at the University of Florence (I). 

# References 

[1] S. Bellavia, S. Gratton, B. Morini, and Ph. L. Toint. Fast stochastic second-order Adagrad for nonconvex bound-constrained optimization. arXiv:2505.06374, 2025. [2] A. S. Berahas, F. E. Curtis, D. Robinson, and B. Zhou. Sequential quadratic optimization for nonlinear equality constrained stochastic optimization. SIAM Journal on Optimization, 31(2):1352â€“1379, 2021. [3] R. S. Burachik, R. N. Gasimov, N. A. Ismayilova, and C. Y. Kaya. On a modified subgradient algorithm for dual problems via sharp augmented Lagrangian. Journal of Global Optimization, 34:55â€“78, 2006. [4] R. S. Burachik, A. N. Iusem, and J. G. Melo. A primal dual modified subgradient algorithm with sharp Lagrangian. Journal of Global Optimization, 46:347â€“361, 2010. [5] C. Cartis, N. I. M. Gould, and Ph. L. Toint. Evaluation complexity of algorithms for nonconvex optimization. Number 30 in MOS-SIAM Series on Optimization. SIAM, Philadelphia, USA, June 2022. [6] A. R. Conn, N. I. M. Gould, and Ph. L. Toint. Trust-Region Methods. Number 1 in MOS-SIAM Optimization Series. SIAM, Philadelphia, USA, 2000. [7] F. Curtis, D. Robinson, and B. Zhou. Sequential quadratic optimization for stochastic optimization with deterministic nonlinear inequality and equality constraints. SIAM Journal on Optimization, 34:3592â€“3622, 2024. [8] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimiza-tion. Journal of Machine Learning Research, 12(7):2121â€“2159, 2011. [9] Y. Fang, S. Na, M. Mahoney, and M. Kolar. Fully stochastic trust-region sequential quadratic programming for equality constrained optimization problems. SIAM Journal on Optimization, 34:2007â€“2037, 2024. [10] N. I. M. Gould, D. Orban, and Ph. L. Toint. Numerical methods for large-scale nonlinear optimization. Acta Numerica, 14:299â€“361, 2005. [11] N. I. M. Gould, D. Orban, and Ph. L. Toint. CUTEst : a constrained and unconstrained testing environment with safe threads for mathematical optimization. Computational Optimization and Applications, 60:545â€“557, 2015. [12] N. I. M. Gould and Ph. L. Toint. Nonlinear programming without a penalty function or a filter. Mathematical Programming A, 122(1):155â€“196, 2010. See also [ ?]. [13] S. Gratton, A. KopaniË‡ cÂ´ akovÂ´ a, and Ph. L. Toint. Multilevel objective-function-free optimization with an application to neural networks training. SIAM Journal on Optimization, 33(4):2772â€“2800, 2023. [14] S. Gratton and Ph. L. Toint. S2MPJ and cutest optimization problems for Matlab, Python and Julia. Optimization Methods and Software, 40(4):871â€“903, 2025. [15] S. Gratton and Ph. L. Toint. A simple first-order algorithm for full-rank equality constrained optimization. arXiv:2510.16390, 2025. [16] D. M. Himmelblau. Applied Nonlinear Programming. McGraw-Hill, New-York, 1972. [17] J. Nocedal and S. J. Wright. Numerical Optimization. Series in Operations Research. Spinger Verlag, Heidel-berg, 1999. [18] M. Porcelli and Ph. L. Toint. BFO, a trainable derivative-free brute force optimizer for nonlinear bound-constrained optimization and equilibrium computations with continuous and discrete variables. Transactions of the ACM on Mathematical Software, 44:2007â€“2037, 2017. [19] J. L. Romero, D. Fernandez, and G.A. Torres. Enhancing sharp augmented Lagrangian methods with smooth-ing techniques for nonlinear programming. Journal of Optimization Theory and Applications, 208(23), 2026. Bellavia, Gratton, Morini, Toint: BGMT8 - draft of 3 II 2026 - not for circulation 21 

[20] Ph. R. Sampaio and Ph. L. Toint. A derivative-free trust-funnel method for equality-constrained nonlinear optimization. Computational Optimization and Applications, 61(1):25â€“49, 2015. [21] Q. Wang, Ch. Peirmarini, Y. Zhu, and F. E. Curtis. Projected stochastic momentum methods for nonlinear equality-constrained optimization for machine learning. arXiv preprint arXiv:2601.11795, 2026. [22] R. Ward, X. Wu, and L. Bottou. Adagrad stepsizes: sharp convergence over nonconvex landscapes. Journal of Machine Learning Research, 21(1):9047â€“9076, 2020. [23] X. Wu, R. Ward, and L. Bottou. WNGRAD: Learn the learning rate in gradient descent. arXiv:1803.02865, 2018. [24] Y. Yuan. Recent advances in trust region algorithms. Mathematical Programming A, 151(1):249â€“281, 2015. [25] C. Zoppke-Donaldson. A Tolerance-Tube Approach to Sequential Quadratic Programming with Applications. PhD thesis, Department of Mathematics and Computer Science, University of Dundee, Dundee, Scotland, UK, 1995. 

# Proof of Lemma 3.1 

We prove the five statements Lemma 3.1 for arbitrary x, y â‰¥ 0. 1. That c(x) is Lipschitz continuous with constant Lc = ÎºJ directly follows from AS.4. 2. We have, from AS.3, AS.4 and AS.7 that 

âˆ¥J(x)T c(x) âˆ’ J(y)T c(y)âˆ¥ =



J(x) âˆ’ J(y)

T

c(x) + J(y)T 

c(x) âˆ’ c(y)



â‰¤ (ÎºcLJ + ÎºJ Lc)âˆ¥x âˆ’ yâˆ¥

= ( ÎºcLJ + Îº2 

> J

)âˆ¥x âˆ’ yâˆ¥,

yielding LJT c = max[1 , Îº cLJ + Îº2 

> J

]. 3. Define A(x) = J(x)J(x)T . AS.5 then implies that A(x) is (symmetric) positive-definite with smallest eigen-value bounded below by Ïƒ20 . As a consequence, bÎ»(x) is well defined by (13). 4. Moreover, AS.2 and AS.4 then imply that 

âˆ¥bÎ»(x)âˆ¥ â‰¤ Îºg ÎºJ

Ïƒ20

,

yielding ÎºÎ» = Îºg ÎºJ /Ïƒ 20 . We have also, using AS.4 and AS.7, that 

âˆ¥A(x) âˆ’ A(y)âˆ¥ â‰¤  J(x) âˆ’ J(y)



J(x)T + J(y)



J(x) âˆ’ J(y)

T

â‰¤ 2ÎºJ LJ âˆ¥x âˆ’ yâˆ¥.

from which we deduce that 

âˆ¥A(x)âˆ’1 âˆ’ A(y)âˆ’1âˆ¥ = A(x)âˆ’1

A(x) âˆ’ A(y)



A(y)âˆ’1 â‰¤ 2ÎºJ LJ

Ïƒ40

âˆ¥x âˆ’ yâˆ¥.

We also have that 

âˆ¥J(x)g(x) âˆ’ J(y)g(y)âˆ¥ =



J(x) âˆ’ J(y)



g(x) + J(y)



g(x) âˆ’ g(y)



â‰¤ (Îºg LJ + ÎºJ Lg )âˆ¥x âˆ’ yâˆ¥.

where we used AS.2, AS.4, AS.6 and AS.7. Therefore, using (13), 

âˆ¥bÎ»(x) âˆ’ bÎ»(y)âˆ¥ =



A(x)âˆ’1 âˆ’ A(y)âˆ’1

J(x)g(x) + A(y)âˆ’1

J(x)g(x) âˆ’ J(y)g(y)



â‰¤ 1

Ïƒ20

2Îºg Îº2 

> J

LJ

Ïƒ20

+ Îºg LJ + ÎºJ Lg

!

âˆ¥x âˆ’ yâˆ¥,

yielding LÎ» =



(2 Îºg Îº2 

> J

LJ )/Ïƒ 20 + Îºg LJ + ÎºJ Lg



/Ïƒ 20 .5. Finally, we obtain that, for any Î» such that âˆ¥Î»âˆ¥ â‰¤ ÎºÎ»,

âˆ¥âˆ‡ xL(x, Î» ) âˆ’ âˆ‡ y L(y, Î» )âˆ¥ = g(x) âˆ’ g(y) + 



J(x) âˆ’ J(y)T Î» â‰¤ (Lg + ÎºÎ»LJ ) âˆ¥x âˆ’ yâˆ¥,

yielding LL = Lg + ÎºÎ»LJ = Lg + Îºg ÎºJ LJ /Ïƒ 20 .