Title: Fast Tuning the Index Construction Parameters of Proximity Graphs in Vector Databases

URL Source: https://arxiv.org/pdf/2602.11573v1

Published Time: Fri, 13 Feb 2026 01:32:22 GMT

Number of Pages: 13

Markdown Content:
# Fast Tuning the Index Construction Parameters of Proximity Graphs in Vector Databases 

Wenyang Zhou, Jiadong Xie, Yingfan Liu, Zhihao Yin, Jeffrey Xu Yu, Hui Li, Zhangqian Mu, Xiaotian Qiao, and Jiangtao Cui 

Abstract â€”k-approximate nearest neighbor search ( k-ANNS) in high-dimensional vector spaces is a fundamental problem across many fields. With the advent of vector databases and retrieval-augmented generation, k-ANNS has garnered increasing attention. Among existing methods, proximity graphs (PG) based approaches are the state-of-the-art (SOTA) methods. However, the construction parameters of PGs significantly impact their search performance. Before constructing a PG for a given dataset, it is essential to tune these parameters, which first recommends a set of promising parameters and then estimates the quality of each parameter by building the corresponding PG and then testing its k-ANNS performance. Given that the construction complexity of PGs is superlinear, building and evaluating graph indexes accounts for the primary cost of parameter tuning. Unfortunately, there is currently no method considered and optimized this process. In this paper, we introduce FastPGT, an efficient framework for tuning the PG construction parameters. FastPGT accelerates parameter estimation by building multiple PGs simultaneously, thereby reducing repeated computations. Moreover, we modify the SOTA tuning model to recommend mul-tiple parameters at once, which can be efficiently estimated using our method of building multiple PGs simultaneously. Through extensive experiments on real-world datasets, we demonstrate that FastPGT achieves up to 2.37x speedup over the SOTA method VDTuner, without compromising tuning quality. 

Index Terms â€”Index Construction Parameter Tuning, Approx-imate Nearest Neighbor Search, Proximity Graphs 

I. I NTRODUCTION 

With the breakthrough in learning-based embedding mod-els, objects such as text chunks [1] and images [2] can be embedded into high-dimensional vectors, capturing their semantics. This advancement has shifted the task of find-ing similar objects to k-approximate nearest neighbor ( k-ANN) search ( k-ANNS) in high-dimensional spaces, which has been a fundamental problem in fields such as information retrieval [3], [4], recommendation systems [5], and retrieval-augmented generation [6]â€“[10]. Specifically, given a dataset 

D containing vectors in Rd space and a query vector q âˆˆ Rd,

k-ANNS aim at returning k vectors that are sufficiently close to the query vector q among vectors in D.

This work was supported by projects funded by National Natural Science Foundation of China (NSFC) under Grants 62002274. (Corresponding authors: Yingfan Liu) Wenyang Zhou, Yingfan Liu, Zhihao Yin, Hui Li, Zhangqian Mu, Xi-aotian Qiao,and Jiangtao Cui are with the School of Computer Science and Technology, Xidian University, Xiâ€™an 710126, China (e-mail: wenyang-chou@outlook.com; liuyingfan@xidian.edu.cn) Jiadong Xie and Jeffrey Xu Yu are with the Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong,Hong Kong ,China (e-mail: jdxie@se.cuhk.edu.hk). 

During the past decades, a bulk of k-ANNS approaches have been proposed in the literature, including tree based indexes [11]â€“[14], hash based indexes [15]â€“[19], invert index based indexes [20], [21], and proximity graph (PG) based indexes (PG) [22]â€“[27]. According to recent works [28]â€“[30], PG based methods significantly outperform other methods in 

k-ANNS performance. Therefore, in this paper, we focus on the PG-based approaches for k-ANNS. Existing PG-based methods could be divided into three categories [25]: (1) k-nearest neighbor graph (KNNG) [31], [32] that simply builds directed edges from each vector in D to its k-ANN; (2) relative neighborhood graph (RNG) [23], [24], [26], [33], [34] assumes the full knowledge of D and builds undirected edges between each vector and its pruned set of close neighbors via various pruning strategies; and (3) navigable small world graph (NSWG) [22], [35] that assumes no prior knowledge of D and builds the graph by inserting each vector into the graph one by one. In general, KNNG has the worst k-ANNS performance compared to RNG and NSWG [28], [29]. Hence, in this paper, we primarily study the issues with NSWG and RNG. For all PG-based methods, their k-ANNS performance is notably sensitive to their index construction parameters. Take, for instance, HNSW, a representative NSWG method shown in Figure 1a, which has two construction parameters: 

M for the out-degree limit in its graph index and ef c for the search pool size. Similarly, Vamana, a representative RNG method depicted in Figure 1b, involves three construction parameters: M , serving as the out-degree limit in its graph index, L for the search pool size, and Î± for its pruning strategy. The results show both are significantly affected by their construction parameters, underscoring the importance of tuning the index construction parameters of PGs for optimal search performance in practice. Existing tuning works for PG-based methods contain two steps: (1) parameter recommendation that recommends promising candidates of construction parameters, and (2) pa-rameter estimation that evaluates the quality of each candidate by testing the k-ANNS performance of the PG built accord-ingly. Existing works all focus on parameter recommenda-tion and can be divided into two categories: heuristic-based methods and learning-based methods. The former employs heuristics to fast generate a set of parameter candidates, while the latter employs machine learning techniques to produce high-quality candidates. Specifically, as the classic heuristic-based method, Grid Search [36] samples a sufficient number of candidates from the parameter space in a grid manner.         

> arXiv:2602.11573v1 [cs.DB] 12 Feb 2026 0.90 0.92 0.94 0.96 0.98 1.00 0246810 12 QPS Recall@10  M=8efc=30  M=8efc=120  M=16 efc=200 (deafult)  M=32 efc=400 M=50 efc=800 Ã—10 3(a) HNSW on Sift 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 012345QPS Recall@10                            
>                 
>                 
>                  
>                    
> Ã—10
> 3(b) Vamana on Sift

Fig. 1: The effects of construction parameters on k-ANNS performance of PGs. QPS indicates the queries processed per second, while Recall @k is the accuracy of k-ANN obtained. Nevertheless, because the construction cost of a PG increases superlinearly with the number of vectors in D, Grid Search suffers from tuning inefficiencies due to the vast number of sampled candidates, which necessitate the creation of nu-merous PGs. To reduce the number of candidates, Random Search [37] randomly samples a specific number of candidates from the parameter space, improving tuning efficiency at the expense of tuning quality. The learning-based methods like VDTuner [38], OtterTune [39], and PGTuner [40] all consider expediting parameter tuning by leveraging distinct learning techniques to decrease the number of parameters recommended through a polling-and-abandonment strategy. However, as previously discussed, existing works tend to overlook the expenses associated with parameter estimation, which constitutes the majority of the tuning cost. For example, as illustrated later in Table I, in VDTuner, 98.67% of the time cost arises from parameter estimation, i.e., the multiple PG constructions, on Sift. Consequently, they all suffer from low tuning efficiency in practice. To address this issue, we focus on accelerating param-eter estimation by introducing a novel model-agnostic tun-ing framework. First, we explore a common manner in the parameter recommendations across various tuning methods: they generate similar sets of parameter candidates, leading to shared distance computations on similar graph structures during construction. However, storing all distance values be-comes impractical due to potentially extensive memory re-quirements, reaching up to O(n2) for multiple PGs. To resolve this limitation, we propose a strategy of grouping PGs for parameter estimation to leverage the structural overlap among them, thereby enhancing parameter estimation efficiency. Ad-ditionally, we introduce a deterministic random strategy to further maximize the overlap among multiple PG indexes. However, it is still non-trivial to design such a scheduling mechanism, due to two factors: (1) the SOTA recommendation models such as VDTuner [38] generate each candidate in a greedy manner, whose quality is further used to refine the model, and (2) PGs have various construction methods, making it challenging to create a method appliable for all PGs. For the first factor, we propose extending the SOTA method VDTuner to recommend multiple parameters per iteration, thereby building multiple PGs accordingly. For the second factor, we first analyze the construction process of existing PGs, and figure out two operations contributing to the major building cost, i.e, (1) Search that conducts k-ANNS for each vector u âˆˆ D on an initially built KNNG, and (2) 

Prune that prunes redundant neighbors in the k-ANN of each u. Next, by leveraging shared computations in Search 

and Prune phases, we design efficient multiple k-ANNS methods and multiple pruning methods, and then seamlessly integrate them into the construction of various PGs. Finally, we conducted extensive experiments on real-world datasets to demonstrate the advantages of our proposed framework, FastPGT. According to our experiments, FastPGT achieves up to 2.37x speedup over the SOTA methods in tuning efficiency while producing candidate parameters of comparable or even better quality. Our principal contributions are as follows.  

> â€¢

We identify the limitations of existing solutions in pa-rameter tuning and the major challenges that need to be addressed in parameter tuning for PG index construction.  

> â€¢

We propose FastPGT, a novel PG construction tuning framework that can recommend multiple high-quality parameters and efficiently estimate their quality by si-multaneously building multiple PGs.  

> â€¢

We conduct extensive experiments to validate the effec-tiveness of FastPGT. According to our experimental re-sults, compared to SOTA methods, FastPGT significantly accelerates the parameter tuning for various PGs, while not compromising the tuning quality. II. P RELIMINARIES 

Given a dataset D âŠ‚ Rd and a query q âˆˆ Rd, k-approximate nearest neighbor ( k-ANN) search ( k-ANNS) returns k vectors in D that are sufficiently close to q. In this work, we use the Euclidean distance to measure the distance between two vectors u and v in D, which is denoted as Î´(u, v ).According to recent studies [28]â€“[30], [41], [42], proxim-ity graph (PG) based methods are considered as the SOTA methods for k-ANNS. A PG G = ( V, E ) of dataset D is a directed graph, where V denotes its vertex set and E is its edge set. Each u âˆˆ V uniquely represents a vector in D, and an edge (u, v ) âˆˆ E indicates that v is a close neighbor of u

in D. NG(u) represents the set {v âˆˆ V | (u, v ) âˆˆ E}, i.e., the out-neighbors of u in G. During the past decade, a bulk of PGs [22]â€“[24], [26], [28], [31], [33], [34], [43] have been proposed, which share the same vertex set V but have distinct edge sets E due to their different edge selection strategies. Although various PGs have different graph structures, they share the same k-ANNS algorithm, as shown in Algorithm 1. It starts from an entry point ep (line 2), which is specified in advance or randomly selected, and greedily approaches the query q. A sorted array pool of size ef â‰¥ k, which contains the currently found closest neighbors, is maintained and takes 

ep as its first member (line 2). The main loop (lines 3-8) Algorithm 1: KANNS( G, q, k, ep, ef )

Input : A PG G, query q, k for k-ANN, the entering point ep , and a parameter ef for pool size 

Output: k-ANN of q 

> 1

i â† 0 ; 

> 2

pool [0] â† (ep, Î´ (q, ep )) ; 

> 3

while i < ef do  

> 4

u â† pool [i]; 

> 5

for each v âˆˆ NG(u) do  

> 6

insert (v, Î´ (q, v )) into pool ; 

> 7

sort pool and keep the ef closest neighbors;  

> 8

i â† index of the first unexpanded point in pool ; 

> 9

return pool [0 , . . . , k âˆ’ 1] ;Initialization  G0 Search  Prune +     

> Connect RNG End Terminate?
> YES
> NO
> (a) RNG construction process Search NSWG End
> Add one point
> No more point Prune +
> Connect
> (b) NSWG construction process

Fig. 2: The illustration of the PG construction methods. iteratively extracts the closest unexpanded neighbor u from 

pool (line 4Ëš a) and then expands it (lines 5-7), where each 

v âˆˆ NG(u) is treated as a k-ANN candidate and verified by an expensive distance computation. Once all the ef nodes in 

pool have been expanded, the search terminates (line 3) and then returns the first k neighbors in pool as the k-ANN results of q (line 9). 

A. PG Construction Methods 

In this part, we briefly review the PG construction al-gorithms. First, PGs could be roughly divided into three categories: k-nearest neighbor graph (KNNG), relative neigh-borhood graph (RNG), and navigable small world graph (NSWG) [25]. KNNG builds directed edges from each u âˆˆ D

to its k-ANN, but suffers from relatively poor search perfor-mance [28], [29]. Hence, we focus on RNG and NSWG in the rest of this work, which are SOTA methods among PGs. RNG methods such as NSG [23], DPG [28], NSSG [33] and Vamana [24] assume that they have the full knowledge D in advance. As illustrated in Figure 2a, their construction could be divided into four operations, i.e., Initialization ,

Search , Prune and Connect . Initialization builds an initial PG G0, which could be a KNNG with moderate accuracy as in NSG [23], [28], [33], [43] or a randomly generated KNNG as in Vamana [24]. Then, Search conducts 

k-ANNS (Algorithm 1) for each u âˆˆ D with G0 in order to obtain refined k-ANN results, denoted as C(u). In Prune ,each C(u) is pruned to generate NG(u). The most widely used pruning method is the RNG pruning, where (u, v ) âˆˆ E

Algorithm 2: Prune( u, C (u), M , Î±)                                                      

> Input :a vertex u, its candidate neighbor set C(u),out-degree limit Mand a parameter Î±
> Output : a pruned neighbor set of u
> 1P N â† âˆ… ;
> 2for each vâˆˆC(u)in the ascending order of Î´(u, v )do
> 3F lag â†false;
> 4for each wâˆˆP N do
> 5if Î±Â·Î´(v, w )< Î´ (u, v )then
> 6F lag â†true;
> 7if F lag =false then
> 8P N â†P N âˆª { v};
> 9if |P N | â‰¥ Mthen break ;
> 10 return P N ;

iff there exists no edge (u, w ) âˆˆ E such that Î´(u, w ) < Î´ (u, v )

and Î´(v, w ) < Î± Â· Î´(u, v ). Î± is set as 1 in NSG and HNSW, but is a configurable parameter in Vamana. To be specific, we present the RNG pruning in Algorithm 2. The pruned neighbor set P N for C(u) that is sorted in the ascending order of the distance to u, is initialized as âˆ… (line 1). Then, each neighbor 

v âˆˆ C(u) is tried to insert into P N (lines 2-9). F lag is initialized as false (line 3), which indicates whether or not a neighbor in P N dominates v. If Î´(v, w ) < Î± Â· Î´(u, v ) (line 5), we say that w âˆˆ P N dominates v, and set F lag as true (line 6). v will be inserted into P N if all neighbors in P N do not dominate v (lines 7-8). Once P N contains M members, the pruning operation terminates (line 9). As to Connect , it aims to enhance the connectivity of the built PG by adding reversing edges or additional edges between connected components in the current PG. Unlike RNG, NSWG methods such as NSW [35] and HNSW [22] build the PG from scratch and insert each u âˆˆ D

into the current incomplete NSWG index one by one without the full knowledge of D. As depicted in Figure 2b, the insertion of each u first conducts k-ANNS on the currently incomplete NSWG in Search . Next, they conduct pruning of the k-ANNS results to obtain the final neighbor set by the RNG pruning in HNSW [22] (Algorithm 2) or directly removing some neighbors with larger distances in NSW [35]. Regarding Connect , NSWG adds reverse edges to enhance graph connectivity. 

B. Problem Statement 

In this paper, we focus on efficiently and effectively tuning the PG construction parameters of both NSWG and RNG methods for their optimal search performance. For the sake of discussion, we take HNSW [22] as the representative NSWG method, and Vamana [24], NSG [23] as the representative RNG methods. Notably, HNSW and NSG are two SOTA in-memory solutions, while Vamana is the key component of DiskANN [24], which is the SOTA I/O-efficient PG method. Let N (q) denote the set of k-ANNS results, and N âˆ—(q)

the ground truth, Recall @k = |N (q) âˆ© N âˆ—(q)|/k . Formally, we give the problem statement as follows. Given an integer k of returned neighbors and a target Recall @k value t, our objective is to tune the construction parameters of PG and the k-ANNS parameter ef (as in Algorithm 1) to achieve the 

k-ANNS performance of Recall @k = t. As aforementioned, we focus on three PGs in this work, i.e., HNSW, Vamana and NSG. (i) HNSW has two construction parameters, i.e., (1) 

ef c indicates the parameter ef of k-ANNS for each u âˆˆ D in 

Search , and (2) M indicates the out-degree limit in Prune .

(ii) Vamana requires four construction parameters, i.e., (1) L

indicates ef in k-ANNS during Search , (2) R is the number of returned neighbors in Search , (3) M is the out-degree limit in Prune , and (4) Î± is a parameter in Prune , i.e., Î± of Algorithm 2. (iii) NSG needs four construction parameters, i.e., (1) K indicates the out-degree of the initial graph in 

Initialization , (2-4) L, R, M has the same meaning as in Vamana. Notably, our method can be easily extended to other PGs, given their inherent similarity in construction [25]. III. E XISTING METHODS AND THEIR ISSUES 

In this section, we first review the existing methods and then point out their issues. In the literature, there exists a bulk of methods that can be used to tune the parameters for PG index construction. In general, existing methods contain two essential operations: (1) parameter recommendation (denoted as Recom.) and (2) parameter estimation (denoted as Est.). The former selects and recommends promising parameter candidates, while the latter estimates the quality of each recommended candidate. Existing works all focus on efficiently recommending pa-rameter candidates via various methods, which could be divided into two categories: heuristic-based methods [36], [37] and learning-based methods [38]â€“[40]. As the widely-used heuristic-based method, Grid Search [36] exhaustively enumerates discrete parameter combinations, at the expense of high computational costs with numerous candidates. To enhance efficiency, Random Search [37] samples parameters randomly in the parameter space at the risk of missing the parameters of high quality. Learning-based methods construct predictive models to understand the relationships between parameters and performance metrics, and then use them to guide parameter recommendations. OtterTune [39] employs Gaussian Process Regression to identify optimal parameters. PGTuner [40] leverages a pre-trained query performance model and a deep reinforcement learning based parameter recommendation model to produce high-quality candidates. VDTuner [38], as a state-of-the-art method, leverages multi-objective Bayesian optimization and employs Expected Hy-pervolume Improvement (EHVI) as the metric to find optimal configurations. Notably, VDTuner and PGTuner are specially designed for the PG construction tuning, while others are general-purpose tuning methods. Moreover, VDTuner assumes no prior knowledge of the tuning task, whereas PGTuner relies on a pre-trained model to accelerate it. In this paper, we follow the same assumption as VDTuner that no prior knowledge is required and tune the PG from scratch. TABLE I: The Cost Decomposition of Existing Methods on Sift.                   

> Method Recom. Est. Total
> RandomSearch [37] 2s 40,570s (100.00%) 40,572s VDTuner [38] 438s 32,568s (98.67%) 33,006s OtterTune [39] 161s 39,730s (99.59%) 39,891s FastPGT (ours) 617s 14,428s (95.89%) 15,045s

Issues of Existing Methods. All existing works focus on recommending high-quality candidates but overlook the pa-rameter estimation, which is the main contributor to the total tuning cost. To estimate the quality of each recommended parameter, existing methods directly build the corresponding PG and then test its k-ANNS performance. Notably, the cost of building a PG is superlinear to the number of vectors in D,which is pretty costly [25]. As shown in Table I, at least 98.5% of the total cost comes from parameter estimation. Hence, it is vital to accelerate the parameter estimation in order to improve the overall tuning efficiency. IV. O UR SOLUTION 

In this section, we introduce our FastPGT method, fo-cusing on expediting the parameter estimation operation, a key contributor to overall cost but is overlooked by existing methods. To begin, in Section IV-A, we analyze the parameter 

R utilized in RNG construction and propose its removal from the parameter space to streamline parameter enumeration and enhance acceleration. Next, to leverage computing resources effectively, we advocate recommending multiple parameters and concurrently constructing multiple PGs with respect to various parameter candidates in Section IV-B. Furthermore, we examine the time breakdown of current PG-based algo-rithms, emphasizing the time-intensive aspects of Search 

and Prune to expedite parameter estimation, as detailed in Section IV-C. We then propose an efficient algorithm for batch parameter estimation that facilitates the construction of multiple PGs simultaneously in Sections IV-D, IV-E, and IV-F. 

Overview : We depict the pipeline of FastPGT in Figure 3, where iterative parameter tuning takes place. Initially, we utilize a parameter recommendation model to recommend multiple promising construction parameters. To expedite the construction of multiple PGs in line with the recommended parameters, we exploit the common graph structures and the shared distance computations during index construction. After testing the constructed PGs, the tuning model will be further refined using the parameters and their corresponding performance metrics, and new parameters will be proposed for evaluation. It is worth noting that our FastPGT is model-agnostic and can accelerate parameter estimation across various existing parameter recommendation methods. In this paper, we use VDTuner as the tuning model for parameter recommendations. Given that VDTuner lacks inherent support for recommending multiple parameters concurrently, we extend it to enable this capability. Dataset                     

> ð‘· ðŸ
> ð‘· ðŸ
> ð‘· ð’Ž
> Optimized
> parameters P*
> Test the Quality of
> PGs and Feedback to
> the Tuning Model
> Tuning Model recommends
> Multiple Parameter Candidates
> Recall@k &
> QPS
> Recall@k &
> QPS
> Recall@k &
> QPS
> Test
> Efficient Construction
> of Multiple PGs
> Test
> Test
> ð† ðŸ
> ð† ðŸ
> ð† ð’Ž
> (ð‘· ðŸ ,ð’“ð’†ð’„ð’‚ð’ ð’ ðŸ ,ð‘¸ð‘· ð‘º ðŸ ),(ð‘· ðŸ ,ð’“ð’†ð’„ð’‚ð’ ð’ ðŸ ,ð‘¸ð‘· ð‘º ðŸ ),â€¦,(ð‘· ð’Ž ,ð’“ð’†ð’„ð’‚ð’ ð’ ð’Ž ,ð‘¸ð‘· ð‘º ð’Ž )

...  

> Parameter
> Tuning Model
> Dataset &
> PG Type

Fig. 3: Illustrating the framework of our method FastPGT. 

A. Parameter R in RNG Construction 

In this part, we analyze the parameter R within RNG construction, as utilized in both NSG and Vamana. This parameter controls the number of neighbors returned in the 

Search phase for subsequent Prune . To elaborate, consider 

pool L = {v1, v 2, Â· Â· Â· , v L} as the data point set obtained from 

k-ANNS (Algorithm 1) when ef = L, with q is a data point 

u âˆˆ D. Assuming Î´(v1, u ) â‰¤ Î´(v2, u ) â‰¤ Â· Â· Â· â‰¤ Î´(vL, u ),

R is employed to establish C(u) = {v1, v 2, Â· Â· Â· , v R}, where 

R â‰¤ L. Let P N (R) denote the set P N derived from the 

Prune process (Algorithm 2) when |C(u)| = R for a node u

and C(u) is conducted with the same value of L and same PG. The subsequent theorem demonstrates that the set P N (R) is a subset of P N (Râ€²) when R â‰¤ Râ€².

Theorem 1. Given a data point u âˆˆ D and a set 

{v1, v 2, Â· Â· Â· , v L} with Î´(v1, u ) â‰¤ Î´(v2, u ) â‰¤ Â· Â· Â· â‰¤ Î´(vL, u ),let P N (R) be the results produced by Algorithm 2 with 

C(u) = {v1, Â· Â· Â· , v R}. For the same M and Î± values, it follows that P N (R) âŠ† P N (Râ€²) when R â‰¤ Râ€² â‰¤ L.Proof. If |P N (R)| â‰¥ M , the algorithm returns P N upon termination at line 9. Hence, elements v âˆˆ C(u) with greater distances to u are not enumerated. This results in P N (R) = 

P N (Râ€²) for Râ€² â‰¥ R. Otherwise, if we let P N (R) =

vp1 , v p2 , Â· Â· Â· , v pc , where c < M and p1 < p 2 < Â· Â· Â· < p c, for 

i < p c such that viÌ¸ âˆˆ P N (R), it follows that viÌ¸ âˆˆ P N (Râ€²)

as well. This is because both P N (R) and P N (Râ€²) operate on the same elements in the same order for pruning nodes, if a node vj âˆˆ P N (R) satisfies Î± Â· Î´(vi, v j ) < Î´ (vi, u ), it will also exclude vi from P N (Râ€²).Hence, from the above theorem, it is evident that increasing the value of R initially results in a sufficient neighbor list, i.e., the |P N | increases and finally reaches M , and then will not affect P N . Additionally, since the efficiency of Algorithm 2 remains unaffected by the value of R, courtesy of the early break at line 9, the traversal of more candidate nodes in C(u)

is avoided when the neighbor size is M . Therefore, setting the value of R as L during index construction is a direct approach, rendering fine-tuning unnecessary, as opting for R = L proves to be the optimal choice. 

B. Multiple Parameter Recommendations 

To maximize the utilization of computing resources, we aim for the recommendation model to generate multiple promising parameters. This approach enables the simultaneous gener-ation of multiple PGs, facilitating the assessment of their performance for subsequent refinement of the recommendation model and parameter recommendations. In this part, we consider extending the SOTA method VDTuner [38] to yield multiple parameters in each itera-tion. VDTuner utilizes Multi-Objective Bayesian Optimization (MOBO), which is designed to balance the tradeoff between two key metrics: QPS and Recall @k in k-ANNS performance. VDTuner consists of two primary components: a surrogate model and an acquisition function. The surrogate model maps each construction parameter to the k-ANNS performance of the PG built accordingly, while the acquisition function utilizes the surrogate modelâ€™s predictions to balance â€œexplo-rationâ€ and â€œexploitationâ€ in the next recommendation step. Specifically, VDTuner employs a Gaussian process (GP) as the surrogate model to capture the intricate relationships between the PG parameters, i.e., M, ef c, ef of NSWG and 

L, M, Î±, ef of RNG, and the performance metrics, i.e., QPS and Recall @k. To stabilize the GP training when performance values vary significantly, the model is trained on normalized performance indicators. For a parameter Pi with performance metrics (yqps  

> i

, y recall  

> i

), where yqps  

> i

and yrecall  

> i

represents its QPS and Recall @k respectively, its normalized performance 

(Ë† yqps  

> i

, Ë†yrecall  

> i

) is defined as 

(Ë† yqps  

> i

, Ë†yrecall  

> i

) = ( yqps 

> i

yqps , yrecall 

> i

yrecall ), (1) NSG HNSW 20 40 60 80 100 Building Time (s)         

> 33.59% 43.59% 82.03% 77.90% 20.43% 17.26
> Initialization Search Prune Connect
> 2.39% 0.71% 16.54% 5.56% Vamana (a) Sift NSG HNSW 100 200 300 400 500 Building Time (s)
> 43.25% 49.03% 86.73% 86.82% 7.16% 13.15
> Initialization Search Prune Connect
> 0.56% 0.12% 5.45% 7.73% Vamana (b) Gist

Fig. 4: The cost decomposition of PG construction. where (yqps , y recall ) is calculated over the set of all currently found non-dominated parameters representing the most bal-anced solution found so far, i.e., 

arg max 

> (yqps ,y recall )âˆˆY

1

|yqps /y qps  

> max

âˆ’ yrecall /y recall  

> max

| ,

where yqps  

> max

and yrecall  

> max

are the maximum QPS and Recall @k

found within the non-dominated set Y.VDTuner employs Expected Hypervolume (HV) Improve-ment (EHVI) as its acquisition function [44]. HV measures the volume dominated by a set of non-dominated solutions, i.e., the Pareto front, with a lower bound defined by a preset refer-ence point r. The EHVI acquisition function directs the search by identifying the single candidate point that maximizes the EHVI. However, the standard EHVI is primarily designed for 

sequential optimization, recommending a single best candidate per iteration [44]. Moreover, it is non-trivial to extend it for batch recommendations, since no analytical formula exists for computing the joint expected EHVI of multiple candidates. To address this issue, we propose a heuristic method, called 

mEHVI, that enables batch recommendations, where m is the number of parameters recommended in each batch. The 

mEHVI computes the exact joint hypervolume improvement (HVIs) from m candidates to model their collective effect. Given a set Y of already evaluated non-dominated points and a reference point r, the mEHVI for a candidate set 

P = {P1, P 2, . . . , P m} is defined as: 

Î±mEHVI ({Pi}mi=1 ) = E [HVI ({f (Pi)}mi=1 , Y, r )] = E [HV (r, Y âˆª { f (Pi)}mi=1 ) âˆ’ HV (r, Y)] , (2) where f (Pi) denotes the normalized performance of the new candidate Pi predicted by the surrogate model. The HV func-tion is utilized to calculate the HV of the observed parameter-performance pairs. Thus, Î±mEHVI quantifies the joined EHVI after adding the m candidates. 

C. Parameter Estimation Analysis 

In this part, we first analyze the construction cost of the three representative PGs: HNSW [45], NSG [23], and Vamana [24]. As shown in Figure 4, we can find that Search 

contributes the most to the overall construction cost in all three PGs. In particular, Search occupies 86.7%, 86.8%, and 49.0% of HNSW, Vamana, and NSG on Gist, respectively. Moreover, Prune constitutes a significant portion of the total cost. Therefore, it is crucial to improve parameter estimation efficiency, accelerating Search and Prune .Next, we analyze the graph structures of PGs under various parameters and demonstrate significant overlaps among graph structures, i.e., their neighbor lists. We first consider the pa-rameter M used in all three representative PGs. The following theorem demonstrates that the neighbor list for a smaller value of M is a subset of that for a larger value of M .

Theorem 2. Given a data point u âˆˆ D and a set C(u), let 

P N (M ) be the results produced by Algorithm 2 with input 

M . For the same Î± value, it follows that P N (M ) âŠ† P N (M â€²)

when M â‰¤ M â€² â‰¤ | C(u)|.Proof. If |P N (M )| < M , all elements v âˆˆ C(u) have been traversed. Thus, the value of P N is independent of M , leading to P N (M ) = P N (M â€²). If |P N (M )| â‰¥ M , the algorithm returns P N at line 9 upon termination. Hence, elements v âˆˆ

C(u) with greater distances to u are not enumerated for M ,but they may be included with a larger value of M and could be contained in P N (M â€²). This implies P N (M ) âŠ† P N (M â€²)

for M â€² â‰¥ M .Next, we analyze the size |C(u)| of the candidate neighbor set for Prune , i.e., ef c in HNSW and L in NSG and Vamana, as well as Î±. We analyze these parameters in an experimental study to demonstrate their effects on the overlaps between the graph structures. Here, we take Vamana as an example. For two PGs G1 and G2 with different parameters, the overlap of two neighbor lists on u, i.e., NG1 (u) and NG2 (u), is defined as N LO (NG1 (u), N G2 (u)) = |NG1 (u) âˆ© NG2 (u)|/|NG1 (u)|.Hence, the overlap between G1 and G2 is defined as 

N LO (G1, G 2) = 1 /|D| Â· P 

> uâˆˆD

N LO (NG1 (u), N G2 (u)) . We show the results in Figure 5, where we fix M = 50 . In Figure 5a, we fix Î± = 1 .2 and present the N LO value between two graphs with different L values. Obviously, the closer L

values are, the larger the N LO is, i.e., the more similar their graph structures are. In Figure 5b, we fix L = 100 and vary Î±. Similarly, the closer Î± values lead to similar graph structures built accordingly. Besides, similar phenomena could be observed in other PGs and other datasets, which are omitted due to the space limit. As discussed above, we observe significant overlaps in the graph structures of the PGs built from similar parameters. Hence, we explore the efficient simultaneous construction of multiple PGs to leverage these overlaps. This approach enables us to maximize the utilization of the common graph structures and similar computations during construction. To further enhance the overlap among multiple PGs, we employ a deterministic random strategy for HNSW layer determination and KNNG initialization in NSG and Vamana. 

Deterministic Random Strategy : The key concept behind deterministic randomness is to leverage pseudo-random num-ber generators to deterministically produce the same number sequence with a consistent seed value. In detail, for each node in HNSW, we ascertain its layer deterministically prior to 100s150200250300400100s1502002503004000.70.80.90.91.0LL(a) Overlap for L1.0s1.11.21.31.41.51.0s1.11.21.31.41.50.40.60.70.91.0

 (b) Overlap for Î±

Fig. 5: The effects of L and Î± on the neighbor list overlap between the Vamana graphs built accordingly on Sift. subsequent index construction. This ensures uniformity across multiple PGs in terms of node layers. In NSG and Vamana, to build the initial KNNG G0, we deterministically establish the initial neighbor list for each node. These neighbor lists are then utilized to initialize KNNG across all concurrently constructed PGs during parameter estimation. To reduce memory usage, we opt not to store neighbor lists and layer information explicitly. Instead, we generate them using a pseudo-random number generator with a consistent seed value set across all instances. 

D. Efficient Search Operations for Multiple PGs 

In this part, we aim to accelerate the Search opera-tions when building multiple PGs simultaneously. Notably, the Search operation independently conducts k-ANNS on the current PG (e.g., the current HNSW, the initially built KNNG, or the current Vamana) for each u âˆˆ D. Hence, we consider conducting the k-ANNS for u âˆˆ D on the multiple PGs simultaneously, which does not affect the results of the 

Search operations. As follows, we first analyze the shared computations among a node u âˆˆ D in multiple PGs. 

Repeated Computations in Search Operations: As shared graph structures and other overlaps lead to repeated dis-tance computations, which constitute a significant cost in 

k-ANNS, as shown in prior studies [25], our focus here is on the distance computations across multiple PGs. There exist numerous distance computations when building multiple PGs, especially in the Search operations. Intuitively, this is because the distances of u âˆˆ D to its close points will be computed more than once when conducting k-ANNS for u on multiple PGs. Moreover, the more similar their construction parameters are, the more repeated distance computations. The more Search operations conducted simultaneously, the more repeated distance computations are. To illustrate this phenomenon, we take HNSW as an ex-ample and conduct experiments in the real-world datasets, whose results are shown in Table II. Here, we build three HNSW graphs with three construction parameters and count the repeated distance computations in the construction of those HNSW graphs. Let dist A, dist B , and dist C be the set of vector pairs for distance computations under the parameter TABLE II: Illustrating repeated computations on Sift and Glove under different parameter settings for HNSW.                                

> Datasets (ef c, M )#dist (Ã—10 9)ratio rp ratio srp ratio prp
> Sift A:(300,18) 5B:(300,20) 5.3 54% 60% 33% C:(300,22) 5.5 A:(400,28) 7.7 B:(400,30) 7.9 56% 61% 36% C:(400,32) 8.1 Glove A:(300,18) 7.5 B:(300,20) 7.9 55% 60% 35% C:(300,22) 8.3 A:(400,28) 12.1 B:(400,30) 12.5 58% 63% 38% C:(400,32) 12.9

Algorithm 3: mKANNS( G, q, k, ef, ep, V Î´ )

Input : A PG G, query q, k for k-ANN, the entering point ep , a parameter ef for pool size, and 

VÎ´ for record distance computations 

Output: k-ANN of query u 

> 1

i â† 0; 

> 2

pool [0] â† (ep, dist (u, ep )) ; 

> 3

while i < ef do  

> 4

u â† pool [i]; 

> 5

for each v âˆˆ NG(u) do  

> 6

if VÎ´ [v]! = âˆ’1 then  

> 7

fetch Î´(u, v ) from VÎ´ [v]; 

> 8

else  

> 9

compute Î´(u, v ) and store it in VÎ´ [v]; 

> 10

insert (v, Î´ (q, v )) into pool ; 

> 11

sort pool and keep the ef closest neighbors;  

> 12

i â† index of the first unexpanded point in pool ; 

> 13

return pool [0 , . . . , k âˆ’ 1] 

settings A, B, and C, respectively. We use #dist to denote the total distance computations, and ratio rp to denote the ratio of common distance computations among all three settings, i.e., ratio rp = |dist A âˆ© dist B âˆ© dist C |/(|dist A| + |dist B | +

|dist C |). Let dist sA, dist sB , and dist sC be the set of vector pairs for distance computations caused by Search operations. 

ratio srp = |dist sA âˆ© dist sB âˆ© dist sC |/(|dist sA| + |dist sB | +

|dist sC |). The results show that more than half of the distance computations are repeated under the different parameter set-tings. Regarding Search operations, they share a substantial portion (i.e., at least 60%) of redundant distance computations, excluding those that could be prevented by utilizing a bitmap 

visited in Algorithm 1, where each element in visit indicates whether or not it has been verified in this k-ANNS operation. 

Fast Multiple Search Operations: Suppose that we build 

m PGs {G1, G 2, . . . , G m} simultaneously w.r.t. a set of pa-rameters P = {P1, P 2, . . . , P m}. For each node u âˆˆ D and each parameter Pi (1 â‰¤ i â‰¤ m), we need to conduct a 

k-ANNS on a specific graph index. To avoid the repeated distance computations during those m k -ANNS operations for a b d e g h           

> abcdef
> adh
> acd
> Prune
> Prune
> C!(ð‘¢ )ð¶ !"(ð‘¢ )
> C#(ð‘¢ )ð¶ #"(ð‘¢ )

Fig. 6: Illustrating the repeated computations among two 

Prune operations on the same node u under different param-eters. Here, Î´(a, d ) is computed in both Prune operations. 

u, we cache all the distances computed so far. Let VÎ´ with n

elements be the set of cached distances, where each element is initialized as -1. Once Î´(u, v ) is computed, VÎ´ [v] = Î´(u, v ).We show the k-ANNS operation with VÎ´ in Algorithm 3, which is similar to Algorithm 1 but differs in the distance com-putations. Before each distance Î´(u, v ), Algorithm 3 checks 

VÎ´ [v] first to determine whether it has been cached (line 6). If Î´(u, v ) has been computed, just fetch it from VÎ´ (line 7). Otherwise, directly compute it and update VÎ´ [v]. In this way, our method avoids repeated distance computations across multiple k-ANNS operations on the same u.

E. Efficient Prune Operations for Multiple PGs 

In this part, let us consider m k -ANNS for the same node 

u âˆˆ D on m distinct PGs, as discussed in the last part. Let 

C1(u), C 2(u), . . . , C m(u) be the corresponding m sets of k-ANNS results. Then, we need to prune each Ci(u) (1 â‰¤ i â‰¤

m) in each Pune operation to get the pruned neighbor set 

Câ€² 

> i

(u). Notably, as in Figure 4, Prune operations make a significant contribution to the overall construction cost. Hence, accelerating Prune operations enhances overall construction efficiency. Like multiple Search operations on the same u,we observe the repeated computations among those m Prune 

operations. 

Repeated Computations in Multiple Prune Operations. 

Consider Ci(u), C j (u), where 1 â‰¤ iÌ¸ = j â‰¤ m. Since they both contain the close neighbors of u, it is easy to derive that Ci(u) âˆ© Cj (u)Ì¸ = âˆ…. Moreover, the more similar the corresponding construction parameters Pi and Pj are, the more similar Ci(u) and Cj (u) are. As illustrated in Figure 6, 

Ci(u) = {a, b, d, e, g, h } and Cj (u) = {a, b, c, d, e, f }. For simplicity, all neighbors are sorted in the increasing order of their distance to u, e.g., Î´(u, a ) â‰¤ Î´(u, b ) â‰¤ Â· Â· Â· â‰¤ Î´(u, h )

in Ci(u). After a joins both Ci(u) and Cj (u), Î´(a, d ) will be computed by both Prune operations to check is d is dominated by a. Here, Î´(a, d ) is computed repeatedly. A naive method to address this issue is to use a hash table, where the vector ID pair (id 1, id 2) of the distance serves as the key, and the distance value is set as the actual distance for its first verification. Then, before each distance computation in each pruning, we first check whether or not it has been in the hash table. However, it is practically inefficient because the size of such a hash table is up to O(| âˆª mi=1 Ci(u)|2),

Algorithm 4: mPrune( u, C i(u), M, Î± )                                                                    

> Input :a vertex u, its candidate neighbor set Ci(u),out-degree limit Mand a parameter Î±
> Output : a pruned neighbor set Câ€²
> i(u)
> 1add the first neighbor uâˆ—of Ci(u)to Câ€²
> i(u);
> 2for each vâˆˆCi(u)\ { uâˆ—}do
> 3F lag â†false;
> 4for each wâˆˆCâ€²
> i(u)do
> 5if v, w âˆˆCâ€²
> iâˆ’1(u)then
> 6continue ;
> 7if Î±Â·Î´(v, w )< Î´ (u, v )then
> 8F lag â†true;
> 9if F lag =false then
> 10 Câ€²
> i(u)â†Câ€²
> i(u)âˆª { v};
> 11 if |Câ€²
> i(u)| â‰¥ Mthen break ;
> 12 return Câ€²
> i(u);

where |Ci(u)| could be several hundreds. As a result, the maintenance and queries of such a hash table incur signifi-cantly additional cost, including memory allocation for newly inserted elements and the computation of hash keys. 

Fast Pruning for Multiple Graphs. In this work, we reduce the repeated distance computations between two consecutive pruning operations via only slightly extra storage. We sub-sequently prune C1(u), C 2(u), . . . , C m(u). As in Figure 6, consider that we just obtain Câ€² 

> i

(u) by pruning Ci(u) and then prune Cj (u), where j = i+1 . During the pruning, we compute 

Î´(a, d ), to check whether or not d is dominated by a. However, with a âˆˆ Ci(u)âˆ©Ci+1 (u) and d âˆˆ Câ€² 

> i

(u), we have that Î´(a, d )

has been computed before and a does not dominate d. In this way, we can avoid computing Î´(a, d ) again. Note that the more similar Ci(u) and Ci+1 (u) are, the more repeated distance computations can be saved. We show the details in Algorithm 4. Note that pruning on 

C1(u) is unchanged as in Algorithm 2. However, starting from pruning on C2(u), we employ our method in Algorithm 4. Since Ci(u) is sorted in the ascending order of the distance 

u, the first neighbor, i.e., the closest one to u, is added into 

Câ€² 

> i

(u) (line 1). Then, we check whether or not each neighbor 

v âˆˆ Ci(u) is dominated by existing ones in Câ€² 

> i

(u) (lines 2-11). Different from the original pruning method, we avoid computing Î´(v, w ) if v, w âˆˆ Câ€²

> iâˆ’1

(u). This indicates that v, w 

have been verified in the last pruning operation, and w is not dominated by v. Thus, repeated distance computations are saved and avoided. 

F. Efficient Construction of Multiple PGs 

This part delves into the efficient construction of multiple PGs with respect to a parameter set P = {P1, P 2, . . . , P m}

through our efficient multiple Search and multiple Prune 

methods. As previously mentioned, we consider three promi-nent PGs: HNSW [22], NSG [23], and Vamana [24]. 

Efficient Construction of Multiple HNSW Graphs. We present the details in Algorithm 5. Notably, ef and M are two Algorithm 5: BuildMultiHNSW( D, P )

Input : D âŠ‚ Rd and the candidate parameter set P

Output: G = {G1, G 2, . . . , G m} 

> 1

for i â† 1, . . . m do  

> 2

initialize G0 

> i

with a random point v âˆˆ D; 

> 3

mL â† 0; 

> 4

for each u âˆˆ D \ { v} do  

> 5

randomly determine the highest layer l of u; 

> 6

if (l > m L) then mL â† l and ep â† u; 

> 7

initialize VÎ´ as âˆ’1 for each vector;  

> 8

for i â† 1, . . . m do  

> 9

(ef c i, M i) â† Pi and c â† ep ; 

> 10

for each j â† mL downto l + 1 do  

> 11

c â† mKANNS (Gji , u, 1, 1, c, V Î´ ); 

> 12

Cl+1 â† { c}; 

> 13

for j â† l downto 0 do  

> 14

Cj â†

mKANNS (Gji , u, ef c i, ef c i, C j+1 [0] , V Î´ ); 

> 15

NGji

(u) â† mPrune (u, C j , M i, 1) ; 

> 16

for each v âˆˆ NGji

(u) do  

> 17

NGji

(v) â† NGji

(v) âˆª { u}; 

> 18

if |NGji

(v)| > M i then  

> 19

NGji

(v) â†

P rune (v, N Gji

(v), M i, 1) ; 

> 20

return G = {G1, . . . , G m};key construction parameters of HNSW. First, we initialize each HNSW graph (lines 1-3), where mL indicates the highest layer of the current HNSW graphs. Then, we insert each u âˆˆ D

into the graphs iteratively (lines 4-19). In the loop, we first determine the highest layer l for u (line 5), which is used for all m graphs, and then update mL if necessary (line 6). We initialize VÎ´ (line 7) and then insert u into the m graphs one by one (lines 8-19). For the layers from mL to l + 1 , we find the 1-ANN of u in each layer, and use it as the entry point of the next layer (line 11). Here, Gji represents the j-th layer of the i-th graph Gi. For the remaining layers from l to 0, we find their ef c j -ANNs using our efficient mKANNS method (line 14), followed by our mPrune method (line 15). Then, we add reverse edges from the pruned neighbors to u (lines 17) and ensure that the nodeâ€™s out-degree limit is met (lines 18-19). Note that this method could be easily extended to other NSWGs such as NSW [35], where we fix mL as 1, employ the closeness-first pruning method, and remove the out-degree limit. Note that VÎ´ will be deleted after inserting u into all m

graphs, since storing n V Î´ arrays requires O(n2) space. 

Efficient Construction of Multiple Vamana Graphs. We present the details in Algorithm 6. First, we build m random graphs, G01, G 02, . . . , G 0

> m

, with different out-degrees according to each construction parameter (lines 1-2). Let c be the centroid 

Algorithm 6: BuildMultiVamana( D, P )

Input : D âŠ‚ Rd and the candidate parameter set P

Output: G = {G1, G 2, . . . , G m} 

> 1

for 1 â‰¤ i â‰¤ m do  

> 2

initialize a random KNNG Gi according to Pi; 

> 3

let c denote the centroid of dataset D; 

> 4

for each u âˆˆ D do  

> 5

initialize VÎ´ as âˆ’1 for each point;  

> 6

for 1 â‰¤ i â‰¤ m do  

> 7

(Li, M i, Î± i) â† Pi; 

> 8

Ci â† mKANNS (Gi, u, L i, L i, c, V Î´ ); 

> 9

NGi (u) â† mPrune (u, C i, M i, Î± i) ; 

> 10

for each v in NGi (u) do  

> 11

if |NGi (v) âˆª { u}| > M i then  

> 12

Prune (v, N Gi (v) âˆª { u}, M i, Î± i); 

> 13

return G = {G1, . . . , G m};TABLE III: Statistics of Datasets.                     

> Dataset Dim. #vectors #queries Type
> Sift 128 1,000,000 1,000 Image Gist 960 1,000,000 1,000 Image Glove 100 1,183,514 1,000 Text Msong 420 992,272 200 Audio

of D, which is used as the entry point of k-ANNS (line 3). Then, we insert each u âˆˆ D into the m graphs (lines 4-12), starting by initializing VÎ´ (line 5). To insert u into each graph 

Gi (lines 6-12), we first obtain Ci(u) (line 8) by our mKANNS method (line 8), followed by our mPrune method (line 9). We guarantee the out-degree limit for each node when inserting the edges from neighbors in NGi (u) to u (lines 10-12). 

Efficient Construction of Multiple NSG Graphs. We can extend Algorithm 6 for other RNGs by only modifying the construction of the initial graph and the pruning strategy. Take NSG as an example, we first build a KNNG G0 

> i

by KGraph [31] in line 2 instead of a random one. Next, we employ the KNNG G0 

> i

to search for line 8, and fix Î±i = 1 for each parameter, since NSG employs Î±i = 1 in any case. V. E XPERIMENTS 

In this section, we present our experimental results. We show the experimental settings and results in order to demon-strate the superiority of our method over its competitors. 

A. Experimental Settings 

Datasets. We conduct 4 public datasets, Sift, Gist, Glove and Msong, which are widely used to evaluate the performance of k-ANNS methods. Their data statistics are shown in Table III. Here, we use dim. to represent the dimensionality of the vectors and #vectors as the number of vectors in D. Let Q

be the set of queries and #queries the number of queries in TABLE IV: Comparing FastPGT with Baseline Methods in Parameter Tuning Efficiency.                                                                                                                                 

> Datasets Methods #Parameters HNSW NSG Vamana
> #dist (Ã—10 9)cost (sec) #dist (Ã—10 9)cost (sec) #dist (Ã—10 9)cost (sec) Gist RandomSearch 100 452 40,572 521 34,867 833 45,670 OtterTune 100 413 39,891 490 32,234 772 43,129 VDTuner 100 382 33,006 453 30,154 721 42,882 FastPGT 100 189 15,045 143 12,697 211 18,278 Sift RandomSearch 100 567 4,594 401 4,108 263 4,854 OtterTune 100 564 4,529 390 4,010 252 4,773 VDTuner 100 407 3,282 349 3,424 211 4,045 FastPGT 100 184 2,071 163 2,010 133 2,336 Glove RandomSearch 100 732 18,987 2,304 18,018 1,320 17,437 OtterTune 100 729 18,321 2,297 17,717 1,301 16,749 VDTuner 100 653 16,130 1,898 15,015 1,104 16,366 FastPGT 100 383 10,711 389 10,485 332 8,421 Msong RandomSearch 100 641 12,315 310 9,421 931 13,540 OtterTune 100 727 10,898 432 13,717 1,201 15,246 VDTuner 100 654 12,390 333 9,698 976 14,349 FastPGT 100 150 6,164 67 4,615 299 6,673

Q. To be specific, Sift 1 contains 1,000,000 128-dimensional SIFT vectors. Gist 1 consists of 1,000,000 960-dimensional GIST vectors. Glove 2 comprises 1,183,514 100-dimensional word feature vectors extracted from Tweets. Msong 3 con-tains 994,185 420-dimensional Temporal Rhythm Histograms extracted from the same number of contemporary popular music tracks. For each dataset, we randomly select a specified number of points, as presented in Table III, to form the query set Q, which is used to evaluate the k-ANNS performance of PGs generated according to the construction parameters. 

Performance Indicators. We are concerned with two perfor-mance aspects of each tuning method, i.e., tuning efficiency and tuning quality. The former is estimated using two perfor-mance indicators: the time cost incurred and the number of distance computations during the tuning process. The tuning quality can be evaluated by the k-ANNS performance of the PG generated according to the finally returned parameter. As aforementioned, the k-ANNS performance could be estimated by two performance indicators, i.e., Queries Per Second (QPS) and Recall @k. The reported Recall @k is averaged over the entire query set Q. By default, we set k = 10 unless specified. All reported results are averaged over 3 independent runs. 

Environments. We implement all methods in C++ and com-pile them with g++ 11 . We conduct all the experiments on a server equipped with two Intel(R) Xeon(R) Gold 6240 CPUs, each of which has 18 cores and 36 hyper-threads, and 380 GB of memory. Its operating system is CentOS 7.6 .

Baselines. We compare our method FastPGT with several SOTA a parameter tuning methods, i.e., RandomSearch [37], VDTuner [38] and OtterTune [39]. To make a fair compar-ison, we set the tuning budget for each method to explore 100 parameter candidates. We set the batch size to 10 for 

> 1http://corpus-texmex.irisa.fr .
> 2http://nlp.stanford.edu/projects/glove/ .
> 3http://www.ifs.tuwien.ac.at/mir/msd/download.html .

FastPGT, which means that it recommends 10 parameter candidates in each iteration. Here, we omit GridSearch due to its prohibitively high computational cost, which grows exponentially with the number of construction parameters, making it impractical for our tuning task. 

B. Main Results 

In this part, we show the main experimental results, where we tune the construction parameters for three representative PGs, i.e., HNSW, NSG, and Vamana. 

Tuning Efficiency. Given the same number of parameters recommended and estimated, we compare the tuning efficiency between our method FastPGT and its competitors in tuning efficiency in Table IV. We can see that FastPGT obviously achieves less tuning cost than its competitors. In particular, FastPGT obtains 2.2x, 2.37x, 2.35x speedup over the SOTA method VDTuner in time cost, when tuning HNSW, NSG, and Vamana on Gist, respectively. Moreover, similar phenom-ena could be found on other datasets. This is because our method effectively reduces the repeated distance computations in parameter estimation. To be specific, FastPGT only com-putes 50%, 31.1%, and 29.2% of distances of HNSW, NSG, and Vamana on Gist, respectively, compared with VDTuner. Moreover, similar phenomena can also be observed in other datasets and baselines. As a result, our method delivers clear speedups during the tuning process. 

Tuning Quality. Given the time budget for tuning, we com-pare the performance of k-ANNS between FastPGT and its competitors. We show the experimental results on HNSW, NSG, and Vamana in Figure 7, 8 and 9, respectively. Here, we consider three Recall @k values â€” 0.9, 0.95, and 0.99 â€”and use the corresponding QPS values to measure the k-ANNS performance. With the same budget, FastPGT presents better tuning quality than its competitors in almost all settings. In particular, take NSG on Gist as an example in Figure 8. VDTuner FastPGT Random Search OtterTune 

(a) Recall>0.9 (Sift) (b) Recall>0.95 (Sift) (c) Recall>0.99 (Sift) (d) Recall>0.9 (Gist) (e) Recall>0.95 (Gist) (f) Recall>0.99 (Gist) 

(g) Recall>0.9 (Glove) (h) Recall>0.95 (Glove) (i) Recall>0.99 (Glove) (j) Recall>0.9 (Msong) (k) Recall>0.95 (Msong) (l) Recall>0.99 (Msong)                                                      

> 01234
> 0
> 4
> 8
> 12
> Ã—10 4
> QPS
> Time(s) Ã—10 301234
> 0
> 4
> 8
> 12
> Ã—10 4
> QPS
> Time(s) Ã—10 301234
> 0
> 2
> 4
> 6
> 8Ã—10 4
> QPS
> Time(s) Ã—10 301234
> 0
> 2
> 4
> 6
> 8
> 10
> Time(s)
> Ã—10 3
> QPS
> Ã—10 401234
> 0
> 2
> 4
> 6
> Time(s)
> Ã—10 3
> QPS
> Ã—10 401234
> 0
> 1
> 2
> 3
> Time(s)
> Ã—10 3
> QPS
> Ã—10 4
> 0510 15
> 0
> 1
> 2
> QPS
> Time(s)
> Ã—10 3
> Ã—10 30510 15
> 0
> 2
> 4
> QPS
> Time(s)
> Ã—10 3
> Ã—10 30510 15
> 0
> 2
> 4
> 6
> 8
> QPS
> Time(s)
> Ã—10 3
> Ã—10 30510 15
> 0
> 1
> 2
> QPS
> Time(s)
> Ã—10 4
> Ã—10 30510 15
> 0
> 1
> 2
> QPS
> Time(s)
> Ã—10 4
> Ã—10 30510 15
> 0
> 1
> 2
> QPS
> Time(s)
> Ã—10 4
> Ã—10 3

Fig. 7: Comparison of the performance of our method and the baseline in tuning HNSW. 0 1 2 3 4                                          

> 0
> 2
> 4
> 6
> 8
> Ã—10 4
> QPS
> Time(s) Ã—10 301234
> 0
> 2
> 4
> 6
> Ã—10 4
> QPS
> Time(s) Ã—10 301234
> 0
> 1
> 2Ã—10 4
> QPS
> Time(s) Ã—10 30123
> 0
> 2
> 4
> 6
> Ã—10 3
> QPS
> Time(s) Ã—10 40123
> 0
> 2
> 4
> 6Ã—10 3
> QPS
> Time(s) Ã—10 40123
> 0
> 1
> 2
> 3
> 4Ã—10 3
> QPS
> Time(s) Ã—10 4
> 0510 15
> 0
> 4
> 8
> 12
> QPS
> Time(s)
> Ã—10 3
> Ã—10 30510 15
> 0
> 4
> 8
> 12
> QPS
> Time(s)
> Ã—10 3
> Ã—10 30510 15
> 0
> 4
> 8
> QPS
> Time(s)
> Ã—10 3
> Ã—10 3

VDTuner FastPGT Random Search OtterTune         

> 0510
> 0
> 2
> 4
> QPS
> Time(s)
> Ã—10 3
> Ã—10 30510
> 0
> 2
> 4
> QPS
> Time(s)
> Ã—10 3
> Ã—10 30510
> 0
> 1
> 2
> QPS
> Time(s)
> Ã—10 3
> Ã—10 3

(a) Recall>0.9 (Sift) (b) Recall>0.95 (Sift) (c) Recall>0.99 (Sift) (d) Recall>0.9 (Gist) (e) Recall>0.95 (Gist) (f) Recall>0.99 (Gist) 

(g) Recall>0.9 (Glove) (h) Recall>0.95 (Glove) (i) Recall>0.99 (Glove) (j) Recall>0.9 (Msong) (k) Recall>0.95 (Msong) (l) Recall>0.99 (Msong) 

Fig. 8: Comparison of the performance of our method and the baseline in tuning NSG. FastPGT spends only 37.8% of the tuning cost of VDtuner to obtain comparable tuning quality across various Recall @k

values, and consumes only 9.81%, 19.82%, and 23.22% of the tuning cost of OtterTuner to obtain comparable or even better tuning quality with the three Recall @k values, respectively. Hence, it demonstrates that FastPGT incurs much lower tuning costs to achieve comparable or even better tuning quality than its competitors. 

C. Ablation Study 

The tuning efficiency of FastPGT is mainly attributed to our efficient multiple Search operations (ESO) and efficient multiple Prune method (EPO). In this part, we study their contributions to the overall cost. We design three configu-rations of FastPGT, as summarized in Table V, Config (I) disables both ESO and EPO, Config (II) activates ESO only, and Config (III) enables both ESO and EPO. Note that all of them have the same tuning quality but different tuning efficiency. Hence, we compare them in tuning efficiency with two performance metrics: (1) relative tuning cost (RTC) to Config (I) and (2) relative number of distance computations (RDC) to Config (I). Both RTC and RDC of Config (I) are 1. We present the results on Msong in Table V. 

Effectiveness of the ESO. By comparing Config (I) and (II), the introduction of ESO yields substantial performance im-provements. Specifically, Config (II) with ESO only computes 39%, 44% and 57% distances of Config (I) on NSG, HNSW, and Vamana, respectively, and thus requires 54%, 52% and 54% tuning cost of Config (I,) respectively. 

Effectiveness of the EPO. Config (III) with EPO obviously computes fewer distances and thus less tuning cost than Config (II) without EPO, since EPO reduces the repeated distance computations in Prune operations. To be specific, let us consider NSG. Config (III) 67 billion distances, which is only 52% of Config (II). Further, the tuning cost of Config (III) is 0.87 of Config (II). 0 2 4                                                                         

> 0
> 5
> 10
> 15
> Ã—10 4
> QPS
> Time(s) Ã—10 3024
> 0
> 5
> 10
> Ã—10 4
> QPS
> Time(s) Ã—10 3024
> 0
> 2
> 4
> 6
> Ã—10 4
> QPS
> Time(s) Ã—10 301234
> 0
> 2
> 4
> 6
> 8Ã—10 3
> QPS
> Time(s) Ã—10 301234
> 0
> 2
> 4
> 6Ã—10 3
> QPS
> Time(s) Ã—10 401234
> 00000
> 1
> 2
> 0
> 1
> 2Ã—10 3
> QPS
> Time(s) Ã—10 4
> 0510 15
> 0
> 5
> 10
> 15
> Ã—10 3
> QPS
> Time(s) Ã—10 30510 15
> 0
> 5
> 10 Ã—10 3
> QPS
> Time(s) Ã—10 30510 15
> 0
> 1
> 2
> 3
> 4Ã—10 3
> QPS
> Time(s) Ã—10 3
> VDTuner FastPGT Random Search OtterTune
> 0510 15 20
> 0
> 2
> 4
> QPS
> Time(s)
> Ã—10 3
> Ã—10 30510 15 20
> 0
> 2
> 4
> QPS
> Time(s)
> Ã—10 3
> Ã—10 30510 15 20
> 0
> 1
> 2
> QPS
> Time(s)
> Ã—10 3
> Ã—10 3
> (a) Recall>0.9 (Sift) (b) Recall>0.95 (Sift) (c) Recall>0.99 (Sift) (d) Recall>0.9 (Gist) (e) Recall>0.95 (Gist) (f) Recall>0.99 (Gist)
> (g) Recall>0.9 (Glove) (h) Recall>0.95 (Glove) (i) Recall>0.99 (Glove) (j) Recall>0.9 (Msong) (k) Recall>0.95 (Msong) (l) Recall>0.99 (Msong)

Fig. 9: Comparison of the performance of our method and the baseline in tuning Vamana. TABLE V: The effects of ESO and EPO. 

Index Config ESO EPO cost (sec) #dist (Ã—10 9) RTC RDC 

(I) Ã— Ã— 9,698 331 1 1NSG (II) âœ“ Ã— 5,209 129 0.54 0.39 (III) âœ“ âœ“ 4,615 67 0.47 0.18 (I) Ã— Ã— 12,390 531 1 1HNSW (II) âœ“ Ã— 6,532 229 0.52 0.44 (III) âœ“ âœ“ 6,164 150 0.49 0.29 (I) Ã— Ã— 14,349 982 1 1Vamana (II) âœ“ Ã— 7,797 558 0.54 0.57 (III) âœ“ âœ“ 6,673 299 0.47 0.31 

TABLE VI: Extending our method to RandomSearch (RS). RS + denotes the RS enhanced with both ESO and EPO. 

Datasets Methods cost (sec) #dist (Ã—10 9) RTC RDC 

Sift RS 4,594 567 1 1RS + 1,975 97 0.43 0.17 Gist RS 40,572 452 1 1RS + 13,794 68 0.34 0.15 Glove RS 18,987 732 1 1RS + 9,873 154 0.52 0.21 Msong RS 12,315 641 1 1RS + 5,049 122 0.41 0.19 

D. Extensions to Other Recommendation Models 

Notably, our method for accelerating the construction of multiple PGs is model-agnostic, independent of the recom-mendation model, and can be seamlessly integrated into models that enable batch recommendations. In this part, we combine ESO and EPO with RandomSearch, denoted as RandomSearch + (RS +). RandomSearch first generates its batch of candidate parameters, and our methods are then employed to build the corresponding PGs simultaneously. As shown in Table VI, RandomSearch + obviously speeds up RandomSearch. To be specific, RandomSearch + only consumes 34%-52% of the tuning cost of RandomSearch, since it reduces the distance computations. RandomSearch +

only computes 15%-21% of the distance computations of RandomSearch. VI. C ONCLUSION 

In this paper, we study the tuning of the construction parameters of proximity graphs (PG), which are the SOTA methods for k-ANNS. We aim to efficiently recommend high-quality construction parameters for a given dataset and a specific type of PG. To address this problem, we propose a novel tuning framework, FastPGT, comprising two steps: pa-rameter recommendation and parameter estimation. We design a new parameter recommendation model that recommends a batch of parameters per iteration based on the SOTA method, VDTuner. Further, we build multiple PGs simultaneously in each parameter estimation, which is equipped with our effi-cient multiple Search and Prune operations. Notably, our efficient parameter estimation method is both model-agnostic and PG-agnostic. We conduct extensive experiments on real datasets to demonstrate the superiority of our method FastPGT over its competitors. According to the results, FastPGT con-sumes much lower tuning cost than its competitors while achieving comparable or even better tuning quality on three representative PGs, i.e., HNSW, Vamana, and NSG. REFERENCES [1] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi-tionality,â€ NeurIPS , vol. 26, 2013. [2] N. M. Nasrabadi and R. A. King, â€œImage coding using vector quantiza-tion: A review,â€ IEEE Transactions on communications , vol. 36, no. 8, pp. 957â€“971, 1988. [3] J. Huang, A. Sharma, S. Sun, L. Xia, D. Zhang, P. Pronin, J. Padmanab-han, G. Ottaviano, and L. Yang, â€œEmbedding-based retrieval in facebook search,â€ in KDD , pp. 2553â€“2561. [4] S. Li, F. Lv, T. Jin, G. Lin, K. Yang, X. Zeng, X.-M. Wu, and Q. Ma, â€œEmbedding-based product retrieval in taobao search,â€ in SIGKDD ,2021, pp. 3181â€“3189. [5] S. Okura, Y. Tagami, S. Ono, and A. Tajima, â€œEmbedding-based news recommendation for millions of users,â€ in SIGKDD . ACM, 2017, pp. 1933â€“1942. [6] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, â€œRetrieval augmented language model pre-training,â€ in ICML , 2020, pp. 3929â€“ 3938. [7] A. Asai, S. Min, Z. Zhong, and D. Chen, â€œRetrieval-based language models and applications,â€ ACL Tutorial , 2023. [8] D. Liu, M. Chen, B. Lu, H. Jiang, Z. Han, Q. Zhang, Q. Chen, C. Zhang, B. Ding, K. Zhang et al. , â€œRetrievalattention: Accelerating long-context llm inference via vector retrieval,â€ arXiv preprint arXiv:2409.10516 ,2024. [9] Y. Deng, Z. You, L. Xiang, Q. Li, P. Yuan, Z. Hong, Y. Zheng, W. Li, R. Li, H. Liu et al. , â€œAlayaDB: The data foundation for efficient and effective long-context llm inference,â€ in Companion of SIGMOD , 2025, pp. 364â€“377. [10] H. Zhang, X. Ji, Y. Chen, F. Fu, X. Miao, X. Nie, W. Chen, and B. Cui, â€œPqcache: Product quantization-based kvcache for long context llm inference,â€ Proceedings of the ACM on Management of Data , vol. 3, no. 3, pp. 1â€“30, 2025. [11] J. L. Bentley, â€œMultidimensional binary search trees used for associative searching,â€ Communications of the ACM , vol. 18, no. 9, pp. 509â€“517, 1975. [12] A. Guttman, â€œR-trees: a dynamic index structure for spatial searching,â€ in SIGMOD , 1984, pp. 47â€“57. [13] S. Berchtold, D. A. Keim, and H.-P. Kriegel, â€œThe X-tree: An index structure for high-dimensional data,â€ in VLDB , 1996, pp. 28â€“39. [14] N. Katayama and S. Satoh, â€œThe sr-tree: an index structure for high-dimensional nearest neighbor queries,â€ in SIGMOD , 1997, pp. 369 â€“ 380. [15] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni, â€œLocality-sensitive hashing scheme based on p-stable distributions,â€ in SoCG , 2004, pp. 253â€“262. [16] Q. Lv, W. Josephson, Z. Wang, M. Charikar, and K. Li, â€œMulti-probe LSH: efficient indexing for high-dimensional similarity search,â€ in VLDB , 2007, pp. 950â€“961. [17] Y. Tao, K. Yi, C. Sheng, and P. Kalnis, â€œQuality and efficiency in high dimensional nearest neighbor search,â€ in SIGMOD , 2009, pp. 563â€“576. [18] J. Gan, J. Feng, Q. Fang, and W. Ng, â€œLocality sensitive hashing scheme based on dyanmic collision counting,â€ in SIGMOD , 2012, pp. 541â€“552. [19] Y. Liu, J. Cui, Z. Huang, H. Li, and H. shen, â€œSK-LSH : an efficient index structure for approximate nearest neighbor search,â€ PVLDB , vol. 7, no. 9, pp. 745â€“756, 2014. [20] A. Babenko and V. Lempitsky, â€œThe inverted multi-index,â€ IEEE TPAMI ,vol. 37, no. 6, pp. 1247â€“1260, 2014. [21] H. Jegou, M. Douze, and C. Schmid, â€œProduct quantization for nearest neighbor search,â€ IEEE TPAMI , vol. 33(1), pp. 117â€“128, 2011. [22] Y. Malkov and D. Yashunin, â€œEfficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs,â€ IEEE TPAMI , vol. 42, no. 4, pp. 824â€“836, 2018. [23] C. Fu, C. Xiang, C. Wang, and D. Cai, â€œFast approximate nearest neighbor search with the navigating spreading-out graph,â€ PVLDB ,vol. 12, no. 5, pp. 461â€“474, 2019. [24] S. J. Subramanya, Devvrit, R. Kadekodi, R. Krishnaswamy, and H. Simhadri, â€œDiskann: Fast accurate billion-point nearest neighbor search on a single node,â€ in NeurIPS , 2019. [25] S. Yang, J. Xie, Y. Liu, J. X. Yu, X. Gao, Q. Wang, Y. Peng, and J. Cui, â€œRevisiting the index construction of proximity graph-based approximate nearest neighbor search,â€ PVLDB , vol. 18, no. 6, pp. 1825â€“1838, 2025. [26] J. Xie, J. X. Yu, and Y. Liu, â€œGraph based k-nearest neighbor search revisited,â€ ACM TODS , May 2025. [27] Y. Liu, Y. Zhang, J. Xie, H. Li, J. X. Yu, and J. Cui, â€œPrivacy-preserving approximate nearest neighbor search on high-dimensional data,â€ in ICDE . IEEE, 2025, pp. 3017â€“3029. [28] W. Li, Y. Zhang, Y. Sun, W. Wang, M. Li, W. Zhang, and X. Lin, â€œApproximate nearest neighbor search on high dimensional data â€“ experiments, analyses, and improvement,â€ IEEE TKDE , vol. 32, no. 8, pp. 1475â€“1488, 2019. [29] M. Wang, X. Xu, Q. Yue, and Y. Wang, â€œA comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search,â€ PVLDB , vol. 14, no. 11, p. 1964â€“1978, 2021. [30] I. Azizi, K. Echihabi, and T. Palpanas, â€œGraph-based vector search: An experimental evaluation of the state-of-the-art,â€ Proceedings of the ACM on Management of Data , vol. 3, no. 1, pp. 43:1â€“43:31, 2025. [31] W. Dong, C. Moses, and K. Li, â€œEfficient k-nearest neighbor graph construction for generic similarity measures,â€ in WWW , 2011, pp. 577â€“ 586. [32] Y. Liu, H. Cheng, and J. Cui, â€œRevisiting k-nearest neighbor graph construction on high-dimensional data : Experiments and analyses,â€ in 

arXiv preprint arXiv:2112.02234 , 2021. [33] C. Fu, C. Wang, and D. Cai, â€œHigh dimensional similarity search with satellite system graph: Efficiency, scalability, and unindexed query compatibility,â€ IEEE TPAMI , vol. 44, no. 8, p. 4139â€“4150, 2022. [34] Y. Peng, B. Choi, T. N. Chan, J. Yang, and J. Xu, â€œEfficient approximate nearest neighbor search in multi-dimensional databases,â€ Proceedings of the ACM on Management of Data , vol. 1, no. 1, pp. 54:1â€“54:27, 2023. [35] Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov, â€œApproximate nearest neighbor algorithm based on navigable small world graphs,â€ 

Information Systems , vol. 45, pp. 61â€“68, 2014. [36] P. Liashchynskyi and P. Liashchynskyi, â€œGrid search, random search, genetic algorithm: a big comparison for nas,â€ arXiv preprint arXiv:1912.06059 , 2019. [37] J. Bergstra and Y. Bengio, â€œRandom search for hyper-parameter opti-mization,â€ Journal of Machine Learning Research , Mar 2012. [38] T. Yang, W. Hu, W. Peng, Y. Li, J. Li, G. Wang, and X. Liu, â€œVdtuner: Automated performance tuning for vector data management systems,â€ in ICDE . IEEE, 2024, pp. 4357â€“4369. [39] D. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang, â€œAutomatic database management system tuning through large-scale machine learn-ing,â€ in SIGMOD , 2017, pp. 1009â€“1024. [40] H. Duan, Y. Song, B. Yao, and A. Liang, â€œPgtuner: An efficient frame-work for automatic and transferable configuration tuning of proximity graphs,â€ CoRR , vol. abs/2508.17886, 2025. [41] M. AumÂ¨ uller, E. Bernhardsson, and A. J. Faithfull, â€œAnn-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms,â€ 

Information Systems , vol. 87, 2020. [42] C. Li, M. Zhang, D. G. Andersen, and Y. He, â€œImproving approximate nearest neighbor search through learned adaptive early termination,â€ in 

SIGMOD . ACM, 2020, pp. 2539â€“2554. [43] H. Ootomo, A. Naruse, C. Nolet, R. Wang, T. Feher, and Y. Wang, â€œCagra: Highly parallel graph construction and approximate nearest neighbor search for gpus,â€ in ICDE . IEEE, 2024, pp. 4236â€“4247. [44] K. Yang, M. Emmerich, A. Deutz, and T. BÂ¨ ack, â€œMulti-objective bayesian global optimization using expected hypervolume improvement gradient,â€ Swarm and Evolutionary Computation , p. 945â€“956, Feb 2019. [Online]. Available: http://dx.doi.org/10.1016/j.swevo.2018.10.007 [45] â€œHeader-only c++/python library for fast approximate nearest neigh-bors,â€ https://github.com/nmslib/hnswlib, 2018, accessed: 2025-05-01.