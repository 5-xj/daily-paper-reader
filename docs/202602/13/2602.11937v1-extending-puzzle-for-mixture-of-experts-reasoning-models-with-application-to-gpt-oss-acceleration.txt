Title: Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration

URL Source: https://arxiv.org/pdf/2602.11937v1

Published Time: Fri, 13 Feb 2026 02:00:49 GMT

Number of Pages: 20

Markdown Content:
# EXTENDING PUZZLE FOR MIXTURE -OF -E XPERTS REASONING 

# MODELS WITH APPLICATION TO GPT-OSS A CCELERATION 

## TECHNICAL REPORT 

Akhiad Bercovich, Nir Ailon, Vladimir Anisimov, Tomer Asida, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Roi Koren, Itay Levy, Zach Moshe, Pavlo Molchanov, Najeeb Nabwani, Mostofa Patwari, Omri Puny, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, and Ran El-Yaniv NVIDIA, {abercovich, relyaniv }@nvidia.com 

## ABSTRACT 

Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B , a deployment-optimized derivative. Our approach combines heteroge-neous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8 ×H100 node we achieve 1.63 × and 1.22 × throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82 × on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2× throughput gain is erased if traces grow 2 ×. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that nor-malize throughput by tokens generated and trace an accuracy–speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29 × higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality. 

## 1 Introduction 

Recent advances in large language models (LLMs) have been accompanied by a shift toward models that explicitly spend more computation at inference time [Blakeman et al., 2025a, Guo et al., 2025, Bercovich et al., 2025c]. Reasoning-oriented models often operate over long contexts and generate long reasoning traces to improve answer quality, but the additional tokens dramatically increase serving cost. In autoregressive inference, self-attention incurs computation that grows with sequence length, while the key-value (KV) cache introduces a memory footprint and bandwidth demand that scale linearly with the number of tokens. At context lengths of 128K and beyond, KV-cache capacity and memory access can dominate end-to-end latency and throughput, limiting feasible batch sizes and lowering hardware utilization. Agentic workflows can further amplify these costs by chaining many long-context calls and accumulating increasingly long histories. These pressures motivate post-training inference optimization: tailoring a trained model to                     

> arXiv:2602.11937v1 [cs.LG] 12 Feb 2026 246810 12 14 16
> Relative request rate
> 40
> 45
> 50
> 55
> 60
> Average accuracy (%)
> Speed
> Accuracy
> gpt-oss-120B (BF16 KV)
> gps-oss-puzzle-88B (BF16 KV)
> gpt-oss-120B (FP8 KV)
> gps-oss-puzzle-88B (FP8 KV)
> HyperNova-60B (BF16 KV)
> Low reasoning
> Med reasoning
> High reasoning

(a) 8 ×H100 node 0 5 10 15 20 25 30 35               

> Relative request rate
> 40
> 45
> 50
> 55
> 60
> Average accuracy (%)
> Speed
> Accuracy
> gpt-oss-120B (BF16 KV)
> gps-oss-puzzle-88B (BF16 KV)
> gpt-oss-120B (FP8 KV)
> gps-oss-puzzle-88B (FP8 KV)
> HyperNova-60B (BF16 KV)
> Low reasoning
> Med reasoning
> High reasoning

(b) Single H100 GPU 

Figure 1: Accuracy–speed frontier that accounts for both per-token throughput and tokens generated: (a) an 8 ×H100 node and (b) a single H100 GPU. The x-axis shows relative request rate (higher is faster), computed as max token throughput (best configuration per model) in a 64K/64K scenario divided by the average number of tokens generated per request across our benchmark suite, and normalized to gpt-oss-120B (KV BF16, high reasoning effort) in the corresponding hardware setting. The y-axis is the suite’s average accuracy. Colors denote models (blue: gpt-oss-120B; green: gpt-oss-puzzle-88B; purple: HyperNova-60B [Multiverse Computing, 2026]), line style denotes KV precision (KV BF16 dashed; KV FP8 solid), and markers denote reasoning effort (High/Medium/Low). HyperNova-60B is a third-party compressed derivative of gpt-oss-120B. specific hardware and inference scenarios under explicit deployment constraints (e.g., memory footprint, latency, and throughput). Reasoning introduces a second axis of efficiency—the number of tokens generated per request—so per-token through-put/latency can misstate end-to-end speedups (e.g., a 2 × throughput gain is erased if traces grow 2 ×). Conversely, throughput gains can be reinvested into longer traces to improve accuracy. Puzzle [Bercovich et al., 2025d] is a decomposed neural architecture search (NAS) framework for improving LLM inference efficiency via neural architectural optimization. Starting from a trained parent model, Puzzle constructs a block library , a discrete set of per-layer alternatives (“puzzle pieces”), and associates each candidate block with measured resource costs in the target deployment scenario. To make search tractable at LLM scale, Puzzle uses a decomposed 

replace-1-block scoring scheme: each candidate block is evaluated in isolation by swapping it into the parent model at a single location, and the quality of a full architecture is estimated as a sum of its per-layer replace-1-block scores. Given per-block costs and scores, Puzzle solves a mixed-integer program (MIP) to select one block per layer that maximizes estimated quality under deployment constraints, thus producing a heterogeneous architecture tailored to the target hardware and desired inference scenarios. While the replacement process can be somewhat destructive (depending on compression levels) the reassembled model can be “healed” and refined with a short end-to-end knowledge distillation training phase to improve blocks’ functionality and inter-block compatibility. Puzzle enabled effective compression of Llama 3 Instruct models [Grattafiori et al., 2024] in the Llama-Nemotron series, achieving 1.7×–2.1× speedup gains while maintaining competitive performance across benchmarks [Bercovich et al., 2025a,c]. Applying Puzzle to recent LLMs that incorporate new architectural components and are expected to perform reasoning over longer generation sequences introduces new challenges. First, mixture-of-experts (MoE) [Shazeer et al., 2017, Fedus et al., 2022] feed-forward networks (FFNs) are now widespread [DeepSeek-AI, 2024, Yang et al., 2025, OpenAI, 2025], and practical MoE modifications must respect routing behavior and expert-parallel deployment constraints, making “how much to prune” a layer-dependent decision. Second, long-context reasoning makes attention a dominant bottleneck and strongly incentivizes KV-cache-reducing mechanisms such as window/streaming attention [Beltagy et al., 2020, Xiao et al., 2024]; yet switching the wrong layers to window attention can harm long-range dependencies. Finally, token-local replace-1-block scores (e.g., KL- or activation-MSE-based) may not reliably predict long-context degradations when switching to window attention, since they do not directly probe long-range interactions. In this technical report, we cover the extension and application of Puzzle to the recent gpt-oss-120B model [OpenAI, 2025], a strong open-weights reasoning model, and derive gpt-oss-puzzle-88B 1, a deployment-optimized derivative 

> 1

The model will become available on Hugging Face soon 

2optimized for both long- and short-context serving (Figure 2). Our primary objective is deployment on an 8 ×H100 node across both long- and short-context serving scenarios, with the long-context 128K setting being KV-cache–constrained. The resulting model improves max throughput by 1.63 × and 1.22 × in long- and short-context settings, respectively, and after post-training matches or slightly improves the parent model’s accuracy across the reasoning budgets. Our model also delivers throughput speedups of 2.82 × on a single NVIDIA H100 GPU. gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29 × higher request-level efficiency with 8.2% improvement of relative average accuracy at low reasoning effort. The effort length ratio is maintained in the same range as the parent model, thereby preserving a reliable user facing control to trade cost for quality (Figure 1). Since compression changes token counts across reasoning efforts, we also evaluate efficiency at the request level by accounting for tokens generated in practice, and track Effort Length Ratio , defined as the ratio of generation lengths under high versus low effort. We run Puzzle once with a multi-constraint objective spanning both long- and short-context serving scenarios. We score MoE pruning alternatives with an activation-based signal and score window-attention alternatives with a dedicated long-context reasoning signal. After selecting a compressed student model, we train it with knowledge distillation to recover quality lost from blockwise substitutions. We then perform reinforcement learning, training two complementary variants and merging them to further improve accuracy while keeping generation length low. Finally, we apply FP8 KV-cache quantization with max-calibrated KV scales to reduce KV footprint in the serving stack. 

Our contributions are: 

• Adapting Puzzle-style architecture search to MoE layers via heterogeneous expert removal under expert-parallel constraints. • Identifying which attention layers can be converted to window attention using long-context-aware scoring, yielding large KV-cache savings while preserving capability. • Using knowledge distillation to recover quality losses introduced during blockwise substitution. • Applying reinforcement learning to improve reasoning accuracy, training two complementary variants and merging them to maintain low generation length. • Applying FP8 KV-cache quantization with calibrated KV scales, enabling ∼ 2× KV-cache token capacity and faster attention modules for long-context serving. • Releasing gpt-oss-puzzle-88B , a deployment-optimized model derived from gpt-oss-120B. The remainder of this paper is organized as follows: Section 2 describes our Puzzle procedure; Section 3 details the training and evaluation setup; and Section 4 presents throughput and accuracy results. 

## 2 Puzzle Optimization 

Puzzle [Bercovich et al., 2025d] is a decomposed neural architecture search (NAS) framework for LLMs. Given a trained “ parent model ”, Puzzle searches for a derivative architecture that satisfies deployment efficiency constraints (e.g., memory footprint, latency, and throughput) while preserving the parent’s accuracy. It does so by (i) defining a discrete search space of alternative layer implementations pieces, (ii) estimating and assigning each alternative piece a quality score (comprising of its efficiency/accuracy profile), and (iii) solving a mixed-integer program (MIP) to select one alternative piece per layer under the target constraints. 

Blocks, subblocks, and search space. Following [Bercovich et al., 2025d], we refer to a transformer layer as a 

block , composed of two main subblocks : the attention module and the feed-forward module (in gpt-oss-120B, an MoE FFN). For each layer i, the search space combines an attention choice Ai = {ai, 1, . . . , a i,m } with an FFN choice 

Fi = {fi, 1, . . . , f i,n }, yielding Ai × F i. Because long-context inference is increasingly dominated by KV-cache size, our attention alternatives in this work focus on standard full-context attention and window attention [Beltagy et al., 2020]. Window attention keeps the KV cache bounded by a fixed window size, making its memory cost insensitive to the total sequence length. Our MoE FFN alternatives vary the number of experts, in a manner compatible with expert-parallel inference schemes. 

Replace-1-block scoring. To score the importance of each layer/subblock, we follow the activation-based scoring used in Nemotron-H [Blakeman et al., 2025b]. Given an input, we compute the intermediate activation tensor right before the LM head in the full parent model, and the corresponding tensor in a model where the particular layer is replaced with the alternative block variant. In our case, the alternative is an MoE subblock with a reduced number of experts. Layer importance is then the mean squared error (MSE) between these two activation tensors. We compute these MSE scores per sample, rank layers by importance, and average these rankings over a small random subset of the training data to 3obtain a reliable estimate of importance that takes into account sample variability. These are called replace-1-block 

scores [Bercovich et al., 2025d]. During search, candidate architectures are not evaluated directly; instead, their quality is estimated as the sum of the replace-1-block scores of their chosen subblocks. 

Optimized Scenarios. We run Puzzle once with a multi-constraint objective that targets two deployment scenarios on a single 8 ×H100 node: a long-context 64K/64K scenario that requires a 1.6 × throughput improvement over the gpt-oss-120B parent, and a short-context 4K/4K scenario that requires a 1.2 × improvement. Meeting these constraints pushes the optimization toward different architectural levers: for 64K/64K, where KV-cache I/O dominates decoding, Puzzle primarily improves efficiency by converting 8 out of the 18 global attention layers into window attention layers, achieving a KV-cache size that is 40% smaller compared to the parent model gpt-oss-120B. For 4K/4K, where KV-cache pressure is significantly lower, Puzzle primarily relies on MoE expert pruning (removing 25% of experts) to meet the target. Although the constraints are specified at the 8-GPU node level, they also translate into large gains on a single H100 GPU: as shown in Figure 6, the parent model is often memory-limited to batch size 1, while expert pruning and window attention free memory that enables larger effective batch sizes, leading gpt-oss-puzzle-88B to achieve a 2.82 ×

speedup on a single H100 GPU in a 64K/64K scenario, and a 2.44 × speedup in a 4K/4K scenario. In the following paragraphs, we detail the two main architectural components, MoE expert pruning and selective window attention, and how we implement and score them. 

Pruning mixture-of-experts layers. Our main method to tackle MoE layers is heterogeneous expert removal . For each MoE layer, we first rank the experts by their contribution to the output of the MoE module over a validation set, then construct a block library made of multiple variants of this layer, each keeping a different number of the original experts from the parent layer: 8, 16, 32, 64, 96 or 128 of the original 128 experts. Using this block library, we utilize Puzzle to determine how drastically each MoE layer in the model should be pruned, resulting in a heterogeneous architecture where the size of each MoE layer is determined by its estimated impact on model quality. Note how the earlier MoE layers appear to be far more important than the later ones (Figure 2). To rank the experts within each layer, we calculate expert contribution scores. We propagate samples through the model, and for each MoE layer we compare the output of the original layer vs the output of this layer with one expert removed: expert score i = Ex

MSE  f (x), f (i)(x) , where f is an MoE layer, f (i) is f with expert i removed, and x are the input hidden states to layer f . We found that using data compatible with the parent’s distribution is very important for correct importance estimation, and therefore used our generated LNPT-gpt-oss dataset (Section 3). To calculate expert contribution scores we used 12K packed sequences of length 8K tokens each, and to calculate replace-1-block scores we used 128 packed sequences of length 32K tokens each. 

Window attention for long-context inference. In long-context settings, the attention subblock becomes a primary bottleneck due to its dependence on the sequence length L: both the computation of attention scores and the storage/ac-cess of the key–value (KV) cache scale unfavorably as L grows. To mitigate this, several recent models adopt a mix of global attention layers and sliding window attention layers, including Gemma [Team et al., 2024, 2025] and gpt-oss [OpenAI, 2025]. Window attention restricts each token to attend only to the most recent W tokens, rather than the entire prefix. This modification reduces the effective attention span from L to W , yielding substantial savings in both compute and 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 

> Window 128
> Window 8192
> Global
> 64
> 128 Attention type
> Num experts
> Model Architecture
> Layer Index
> Attention Type
> Number of Experts

Figure 2: Our model architecture as chosen by Puzzle. Note how the earlier MoE layers appear to be far more important than the later ones. The parent model gpt-oss-120B has 128 experts per layer and alternates between sliding window attention with 128 tokens and global attention. 4KV-cache memory IO. In particular, the per-layer KV-cache footprint becomes proportional to W (instead of L), and the attention score computation similarly depends on W , which is especially beneficial when serving very long sequences. Previous works using Puzzle [Bercovich et al., 2025d,b,c] reported strong long-context retention even without dedicated long-context uptraining [Bercovich et al., 2025d]. However, those efforts mainly considered attention alternatives that preserve the global receptive field, such as grouped-query attention (GQA), or, in extreme cases, removing an attention layer altogether (which probably indicates that this attention layer contributed very little to begin with). While GQA reduces the KV-cache size by sharing key/value heads across query heads, its KV cache still grows linearly with sequence length. Window attention, in contrast, makes the KV cache effectively fixed-size and is therefore particularly attractive at very long contexts. However, converting all full-context attention layers to window attention typically incurs a serious quality degradation, since some layers rely on global context to support long-range dependencies. Therefore, our goal is to identify a subset 

of attention layers that are most amenable to window attention (i.e., “window-able”) and apply window attention only to those layers, while keeping the remaining layers as full-context attention. This selective replacement enables large efficiency gains in long-context scenarios while preserving the accuracy of the parent model. Our parent model, gpt-oss-120B, alternates between sliding window attention with 128 tokens and global attention. Our attention block library included all the original parent attention blocks, and an additional alternative block for each original global attention layer, where it is replaced by a sliding window attention layer with 8192 tokens. To determine which global attention layers are the most suitable to be transformed into window attention, we needed a dedicated scoring method that is long-context aware. While it is robust to a large variety of layer types and pruning methods, our standard method for calculating replace-1-block scores has an inherent locality bias, as it compares the model’s final hidden states used for next-token prediction, and language modeling is an inherently local task for most tokens. To capture long-term dependencies, when constructing the block library for attention layers, we instead measured accuracy scores on the Artificial Analysis Long-Context Reasoning benchmark (AA-LCR [Artificial Analysis, 2025]) for each block replacement. We conduct an ablation experiment comparing the two types of scoring in Appendix C. To further support long-context behavior after these architectural changes, we modify the YaRN positional encoding by increasing the RoPE scaling factor from 32 to 56. Although 32 matches the nominal 4K → 128K extension ratio used by the parent model, YaRN applies frequency-dependent scaling; at 128K, parts of the transition band can still experience substantial phase wrapping. Using factor 56 increases the effective RoPE periods and yields more stable long-range attention, improving long-context retrieval at 128K. We note that the long-context accuracy of the parent model may also be improved by tuning the RoPE scaling factor. 

## 3 Training, Quantization and Evaluation Setup 

Dataset. For expert contribution scores, MoE replace-1-block scores, knowledge distillation, and quantization, we used prompts from the nvidia/Llama-Nemotron-Post-Training-Dataset [Bercovich et al., 2025c], which is designed to enhance performance in mathematics, code generation, general reasoning, and instruction following. For each prompt, we generated responses from the parent model under both high and medium reasoning effort settings. We refer to the result as the LNPT-gpt-oss dataset. Block scoring and quantization were performed exclusively on the high reasoning effort subset, whereas knowledge distillation employed an equal mixture of high and medium effort responses to preserve accuracy across diverse usage patterns. 

Knowledge Distillation. Following the puzzle phase, we trained the model using a knowledge distillation objective on the LNPT-gpt-oss dataset for a total of 84B tokens, with the goal of improving inter-block compatibility and recovering any quality degradation introduced by blockwise substitution. During this stage, the MoE experts and router were kept frozen. Training was performed with a sequence length of 128K and a global batch size of 33M tokens, using the Megatron-LM framework [Shoeybi et al., 2019]. 

Reinforcement Learning. Following distillation, we run a reinforcement learning stage to further improve reasoning accuracy, while tracking how training choices affect generation length, since tokens per request directly determine end to end serving cost. We build on the repositories, multi-environment RL framework, and datasets of Blakeman et al. [2025c], training on the mathematics, coding, and general reasoning environments while excluding tool-use environments from the mixture. During this stage, the MoE experts and router were kept frozen. Across runs, we keep the recipe fixed (including a constant learning rate of 1e-6) and vary only the reasoning-effort composition of the RL data. Varying this single axis reveals the central tension of the RL stage: Training exclusively on high-effort reasoning data reliably improves reasoning accuracy, yet it lengthens generations across all efforts and shifts effort length ratio away 5Average              

> Score
> MMLU-
> Pro
> GPQA-
> Diamond
> HLE AALCR AIME25 IFBench SciCode RULER
> 128K
> 64K/64K
> 1×H100
> 64K/64K
> 8×H100
> 4K/4K
> 1×H100
> 4K/4K
> 8×H100
> 0
> 20
> 40
> 60
> 80
> 100
> 120
> Accuracy Retention (%)
> Accuracy Retention Throughput Speedup
> 100.8
> 103.9
> 108.2
> 98.3
> 99.3
> 100.8
> 97.3
> 103.3
> 104.9
> 87.0
> 108.2
> 125.2
> 82.7
> 86.1 87.2
> 104.2
> 111.2
> 121.2
> 101.9
> 108.0
> 128.6
> 93.8
> 95.9 101.4
> 118.7 119.4 122.3
> Low reasoning Medium reasoning High reasoning
> 0
> 1
> 2
> 3
> 4
> 5
> 6
> Relative throughput (× vs parent)
> 1.0 1.0 1.0 1.0
> 2.82×
> 1.63×
> 2.44×
> 1.22×
> gpt-oss-120B (KV FP8)
> gpt-oss-puzzle-88B (KV FP8)

Figure 3: Accuracy and throughput comparisons of gpt-oss-puzzle-88B with its parent, gpt-oss-120B (both with KV FP8). from the teacher range, making high effort disproportionately expensive (Table 3). At the other extreme, training on a balanced mixture of efforts acts as an implicit length regularizer, leading to lower verbosity at high reasoning effort. Generation length across efforts regress toward a shared mean and effort becomes less effective at steering generation length, weakening controllability. This regime also underperforms in accuracy at the same budget, suggesting it requires substantially more iterations to match the high effort focused setting. To reconcile these behaviors, we combine the two RL candidates via checkpoint weight averaging. The resulting model preserves near peak reasoning accuracy from the high effort trained policy while substantially reducing verbosity and restoring effort length ratio toward the teacher range, so users can reliably trade cost for quality. 

KV Quantization. In addition to keeping the MXFP4 quantization of the experts as in the parent model gpt-oss-120B, we employ FP8 KV quantization to reduce the memory footprint of the KV cache. This enabled both 2× token capacity in the KV cache and faster attention modules, which is especially beneficial in long context scenarios, such as our case with reasoning models. We first tried the common practice of not using KV scales. However, this resulted in subpar accuracy. Therefore, we computed KV scales based on our LNPT-gpt-oss dataset using max calibration, resulting in better accuracy. Furthermore, we rounded the scales up to a power of 2. This makes sure that rounding errors behave similarly to not having scales, but with a better adaptation of the dynamic range to mitigate underflow errors (all scales are below 1, so overflow errors seem to be less of a concern). Our inference benchmarks can be found in Section 4.1 and our accuracy benchmarks can be found in Section 4.2. 

Accuracy evaluations. We focused on reasoning benchmarks for accuracy evaluations. We used MMLU-Pro [Wang et al., 2024] and HLE [Phan et al., 2025] for reasoning and knowledge, GPQA-Diamond [Rein et al., 2023] for scientific reasoning, AIME-25 [Mathematical Association of America] for mathematical knowledge, SciCode [Tian et al., 2024] for coding and IFBench [Pyatkin et al., 2025] for general instruction following. Since we have used AA-LCR scores internally as a part of the Puzzle process, we’re using RULER [Hsieh et al., 2024] results as another benchmark to measure long context performance. We followed OpenAI’s reasoning efforts scheme and evaluated our model using 

high , medium and low reasoning efforts. In order to reduce variance in accuracy we calculated each benchmark several times and report averaged results. We have used the Nemo-Skills repo NVIDIA [2024] to run these benchmarks. Evaluation parameters are listed in Appendix B. 

Inference efficiency benchmarking. For each serving scenario we sweep tensor-parallel degree (TP ∈ { 1, 2, 4, 8})and a grid of batch sizes. For max-throughput numbers we select the best configuration per model (instead of fixing TP), ensuring a fair comparison. Each configuration is measured 3 times. The plots include ±1σ bands to visualize uncertainty. 

## 4 Results 

4.1 Inference Efficiency Why throughput (tok/s) is not enough for reasoning models. Per-token throughput (tok/s) and latency (ms/token) capture architectural efficiency, but for reasoning models they are not sufficient: different model variants and reasoning-6effort modes can generate different numbers of tokens per request, directly changing end-to-end latency and cost. Accordingly, we distinguish between (i) token throughput under fixed input/output lengths (this subsection) and (ii) 

request-level efficiency that also accounts for tokens generated in practice. Figure 1 summarizes this request-level trade-off by plotting accuracy against a relative request rate , computed as the max token throughput (best serving configuration per model) divided by the average tokens generated per request (and normalized to the gpt-oss-120B KV BF16 high-effort baseline); Appendix D reports the raw numbers underlying the figure. As an illustrative example, HyperNova-60B [Multiverse Computing, 2026] 2 is a third-party compressed derivative of gpt-oss-120B that targets architectural efficiency, but it generates longer reasoning traces, which can erase per-token throughput gains at the request level. Table 3 reports complementary generated-token statistics for our models. With this framing in mind, the remainder of this subsection focuses on architectural speedups: maximum token throughput and throughput–latency trade-offs in fixed 4K/4K and 64K/64K serving scenarios. We compare our optimized gpt-oss-puzzle-88B against the gpt-oss-120B parent. We focus on the specific constraints used during the optimization process and highlight the quantized gpt-oss-puzzle-88B configuration as the final optimized serving setup. 

Optimization Constraints. 

• Short Context (4K/4K): A target improvement of 20% . This scenario is typically MoE-dominant, driving the optimization of the Mixture-of-Experts (MoE) layers (expert pruning). • Long Context (64K/64K): A target improvement of 60% . This scenario is KV-cache-dominant, driving the optimization of the attention mechanism (selective window attention). 

Hardware. We report results on NVIDIA H100 (8 × H100 80GB HBM3 node). We first present a detailed analysis on H100, with a B200 case study in Appendix A. 

Scenarios and notation. We denote inference scenarios as input/output token lengths (e.g., 64K/64K means a 64K-token prompt followed by generating 64K tokens). The first number primarily stresses the prefill phase, while the second stresses the decode phase. 

4.1.1 H100 Results: Meeting the Dual Constraints 

Table 1 summarizes the speedups across our target deployment scenarios on a single node. The results demonstrate that 

gpt-oss-puzzle-88B exceeds both optimization targets: 1. In the 4K/4K scenario, we achieve a 1.22 × speedup, validating the efficiency gains from expert pruning in MoE dominant regimes. 2. In the 64K/64K scenario, we achieve a 1.63 × speedup, validating the impact of selective window attention in kv cache dominant regimes. Table 1: Inference scenarios (H100): gpt-oss-puzzle-88B vs gpt-oss-120B parent.                     

> Scenario Description gpt-oss-puzzle-88B gpt-oss-120B Speedup 4K/4K Max throughput on 8 ×H100 node 36.1K tok/s 29.6K tok/s 1.22 ×
> 64K/64K Max throughput on 8 ×H100 node 9.3K tok/s 5.7K tok/s 1.63 ×
> 4K/4K Max throughput on single H100 3.3K tok/s 1.4K tok/s 2.44 ×
> 64K/64K Max throughput on single H100 0.8K tok/s 0.3K tok/s 2.82 ×

Scaling and trade-offs. Figure 4 shows throughput scaling with batch size. Because gpt-oss-puzzle-88B reduces both weight bandwidth and KV-cache footprint, it sustains scaling to higher batch sizes, improving GPU utilization and reaching peak throughput across both MoE-dominant (4K/4K) and KV-dominant (64K/64K) regimes. Figure 5 illustrates the throughput–latency frontier for the 64K/64K scenario. The optimized model offers superior trade-offs; for example, when constrained to a decode latency (ITL) of 10ms, gpt-oss-puzzle-88B delivers approximately 

1.5 × higher throughput (6.4K vs 4.2K tok/s) compared to the parent model. 

> 2We evaluated HyperNova-60B only with KV FP16, using the Hugging Face checkpoint available as of Feb. 11, 2026.

71 2 4 8 16 32 64 128 256 

> Batch Size
> 0
> 2
> 4
> 6
> 8
> Throughput (K tok/s)
> max batch
> gpt-oss-120B
> gps-oss-puzzle-88B

(a) Long Context (64K/64K) 2 8 32 128 512  

> Batch Size
> 0
> 5
> 10
> 15
> 20
> 25
> 30
> 35
> Throughput (K tok/s)
> gpt-oss-120B
> gps-oss-puzzle-88B (MoE pruning only)

(b) Short Context (4K/4K) 

Figure 4: Throughput scaling with batch size on an 8× H100 node. Comparison of (a) long-context and (b) short-context scenarios. Both models (gpt-oss-120B and gpt-oss-puzzle-88B) use KV FP8. Shaded bands show ±1σ.0 10000 20000 30000 40000     

> TTFT Latency (ms)
> 0
> 2
> 4
> 6
> 8
> Throughput (K tok/s)
> max batch
> gpt-oss-120B
> gps-oss-puzzle-88B
> 510 15 20 25
> ITL Latency (ms)
> 0
> 2
> 4
> 6
> 8
> Throughput (K tok/s)
> max batch
> gpt-oss-120B
> gps-oss-puzzle-88B

Figure 5: Latency vs throughput trade-off (64K/64K). Both models (gpt-oss-120B and gpt-oss-puzzle-88B) use KV FP8. Shaded bands show ±1σ across repeated runs. 

Single GPU Efficiency. While our primary optimization constraints focused on full-node deployment, the architectural efficiency of gpt-oss-puzzle-88B is even more pronounced on constrained resources. On a single H100 GPU, we observe improvements of 2.44 × in the 4K/4K scenario and 2.82 × in the 64K/64K scenario compared to the parent model. Figure 6 shows that the parent model is quickly memory-limited as batch size increases, while expert pruning and selective window attention free memory that enables larger effective batch sizes and higher throughput. These single-device gains highlight the model’s ability to operate effectively under strict memory capacity limits where the parent model struggles with batch scaling. 81 2 4 8

> Batch Size
> 200
> 300
> 400
> 500
> 600
> 700
> 800
> Throughput (tok/s)
> max batch
> max batch
> gpt-oss-120B
> gps-oss-puzzle-88B (MoE pruning only)
> gps-oss-puzzle-88B

Figure 6: Single-H100 (TP=1) throughput scaling with batch size in the 64K/64K scenario. Compared to the gpt-oss-120B parent, MoE expert pruning and selective window attention reduce weight/KV-cache footprint and enable larger maximum batch sizes, improving utilization and throughput. All runs use KV FP8; shaded bands show ±1σ, and dashed markers indicate the maximum batch size that fits in memory. 

4.2 Accuracy Benchmarks 

We report accuracy results for our quantized and non-quantized gpt-oss-puzzle-88B models compared to the parent (gpt-oss-120B) in all reasoning efforts reported by OpenAI (low, medium and high) in Table 2. From Table 2, comparing the KV FP8 variants (our final serving configuration), gpt-oss-puzzle-88B achieves 100 .8% ,

103 .9% , and 108 .2% of the parent model’s suite-average accuracy at high, medium, and low reasoning effort, respec-tively, despite having ∼73% of the parent’s total parameter count. Retention varies across benchmarks, and some tasks remain below the parent. Table 2: Accuracy (%) for gpt-oss-120B and gpt-oss-puzzle-88B (KV BF16/KV FP8) at low, medium, and high reasoning effort. 

Model Average Accuracy 

MMLU-Pro GPQA-Diamond HLE AALCR AIME25 IFBench SciCode RULER 128K 

High reasoning 

gpt-oss-120B (KV BF16) 59.20 80.41 77.78 18.16 48.75 91.46 64.46 41.72 50.89 gpt-oss-120B (KV FP8) 58.19 80.60 77.34 18.86 46.75 89.58 65.76 40.83 45.82 gpt-oss-puzzle-88B (KV BF16) 59.44 79.32 75.13 17.52 42.25 92.92 67.77 40.83 59.80 gpt-oss-puzzle-88B (KV FP8) 58.67 79.19 75.25 16.40 40.75 93.33 67.01 41.42 56.02 

Medium reasoning 

gpt-oss-120B (KV BF16) 53.66 78.86 71.28 10.06 39.25 76.88 56.55 41.42 55.01 gpt-oss-120B (KV FP8) 52.89 78.71 68.88 9.68 41.50 77.92 58.67 42.75 45.04 gpt-oss-puzzle-88B (KV BF16) 56.64 78.03 69.70 10.57 36.00 86.88 65.56 39.64 66.71 gpt-oss-puzzle-88B (KV FP8) 54.93 78.18 71.15 10.47 35.75 86.67 63.35 40.09 53.77 

Low reasoning 

gpt-oss-120B (KV BF16) 45.41 75.18 62.75 4.17 35.25 50.00 44.13 39.20 52.61 gpt-oss-120B (KV FP8) 44.71 75.04 60.54 4.40 34.75 51.88 43.71 40.09 47.28 gpt-oss-puzzle-88B (KV BF16) 50.61 75.56 64.77 5.33 31.75 66.25 56.38 38.17 66.70 gpt-oss-puzzle-88B (KV FP8) 48.38 75.62 63.51 5.51 28.75 62.89 56.21 38.46 56.11 

Generated Token Count Analysis Table 3 reports the average number of generated tokens across seven benchmarks for multiple reasoning efforts, comparing RL intermediate checkpoints, the final gpt-oss-puzzle-88B, and its parent gpt-oss-120B. Notably, the final gpt-oss-puzzle-88B produces slightly fewer tokens than the average for the RL variants, 9RL Variants Reasoning Effort Pre-RL Balanced Mix High gpt-oss-puzzle-88B gpt-oss-120B Ratio Ours /Parent                   

> High 15.63 9.96 21.56 14.28 13.05 1.09 ×
> Medium 3.2 3.88 6.30 4.79 3.05 1.57 ×
> Low 1.4 2.01 1.56 1.73 1.35 1.28 ×

Table 3: Generated tokens (K) by reasoning effort, across RL checkpoints (Pre-RL, Balanced Mix - RL on all efforts, High - RL on high-effort only) and final models (gpt-oss-puzzle-88B vs parent gpt-oss-120B, with ratio). Averaged over MMLU-Pro, HLE, GPQA-Diamond, AIME-25, IFBench, SciCode, and AALCR. All models use KV FP8. and its effort length ratio remains close to the parent. Despite higher token counts than the parent, gpt-oss-puzzle-88B is more efficient at the request level due to architectural throughput gains, as seen in Figure 1. 

4.2.1 KV Scaling Impact 

As discussed in Section 3, we found that using KV scales gives better results. In Table 4, we detail the accuracy results for our KV quantization experiments. Table 4: Accuracy results for the gpt-oss-puzzle-88B model when using FP8 KV quantization with and without KV scales                                                              

> Model Average Accuracy
> MMLU-Pro GPQA-Diamond HLE AALCR AIME25 IFBench SciCode RULER 128K
> High reasoning
> No Scales 58.14 79.40 75.00 15.99 40.25 90.42 66.89 40.68 56.47 KV Scales 58.67 79.19 75.25 16.40 40.75 93.33 67.01 41.42 56.02
> Medium reasoning
> No Scales 55.47 77.71 70.58 11.54 35.50 87.08 64.97 41.12 55.25 KV Scales 54.93 78.18 71.15 10.47 35.75 86.67 63.35 40.09 53.77
> Low reasoning
> No Scales 48.01 75.46 63.19 5.14 26.25 65.21 55.02 38.61 55.17 KV Scales 48.38 75.62 63.51 5.51 28.75 62.89 56.21 38.46 56.11

## 5 Discussion 

This technical report introduced an extension to Puzzle, and applied it to optimize the gpt-oss-120B parent model. Start-ing from gpt-oss-120B, we derived gpt-oss-puzzle-88B by jointly targeting an 8 ×H100-node throughput improvement of 1.6 × in the long-context 64K/64K scenario and 1.2 × in the short-context 4K/4K scenario. gpt-oss-puzzle-88B achieves significant throughput improvement over the parent while matching the parent on suite-average accuracy across reasoning efforts (Figure 3). At the request level, which also accounts for tokens generated across reasoning efforts, gpt-oss-puzzle-88B improves along the accuracy–speed frontier (Figure 1). Our approach combines several complementary ingredients: (i) heterogeneous MoE expert pruning in which layers are pruned to different expert counts based on activation-based replace-1-block scores, (ii) selective window attention 

replacements scored with a long-context benchmark signal to preserve long-range behaviors, (iii) training complementary variants using reinforcement learning and merging them to further improve accuracy while keeping generation length low and (iv) FP8 KV-cache quantization with calibrated scales to further reduce KV-cache footprint. Together, these results show that post-training architecture search can substantially reduce the cost of both long- and short-context serving while matching or even improving quality. 

## References 

Artificial Analysis. Intelligence benchmarking methodology, 2025. URL https://artificialanalysis.ai/ methodology/intelligence-benchmarking . Accessed: 2025-12-18. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR , abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150 .10 Akhiad Bercovich, Mohammad Dabbah, Omri Puny, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Ehud Karpas, Itay Levy, Zach Moshe, Najeeb Nabwani, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, and Ran El-Yaniv. FFN fusion: Rethinking sequential computation in large language models. CoRR ,abs/2503.18908, 2025a. doi: 10.48550/ARXIV.2503.18908. URL https://doi.org/10.48550/arXiv.2503. 18908 .Akhiad Bercovich, Mohammad Dabbah, Omri Puny, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Ehud Karpas, Itay Levy, Zach Moshe, Najeeb Nabwani, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, and Ran El-Yaniv. FFN fusion: Rethinking sequential computation in large language models. CoRR ,abs/2503.18908, 2025b. doi: 10.48550/ARXIV.2503.18908. URL https://doi.org/10.48550/arXiv.2503. 18908 .Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, and Chris Alexiuk. Llama-nemotron: Efficient reasoning models, 2025c. URL https://arxiv.org/abs/2505.00949 .Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi Koren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny, Ran Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, and Ran El-Yaniv. Puzzle: Distillation-based NAS for inference-optimized llms. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025 . OpenReview.net, 2025d. URL https://openreview.net/forum?id=RY5MMBHRqo .Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Sepp ¨anen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan M. Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, and Markus Kliegl. Nemotron-h: A family of accurate and efficient hybrid mamba-transformer models. CoRR , abs/2504.03624, 2025a. doi: 10.48550/ARXIV.2504.03624. URL https://doi.org/10.48550/arXiv.2504.03624 .Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak 11 Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Sepp ¨anen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan M. Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, and Markus Kliegl. Nemotron-h: A family of accurate and efficient hybrid mamba-transformer models. CoRR , abs/2504.03624, 2025b. doi: 10.48550/ARXIV.2504.03624. URL https://doi.org/10.48550/arXiv.2504.03624 .Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, et al. Nvidia nemotron 3: Efficient and open intelligence. arXiv preprint arXiv:2512.20856 , 2025c. URL https://arxiv.org/ abs/2512.20856 .DeepSeek-AI. Deepseek-v3 technical report. CoRR , abs/2412.19437, 2024. doi: 10.48550/ARXIV.2412.19437. URL 

https://doi.org/10.48550/arXiv.2412.19437 .William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res. , 23:120:1–120:39, 2022. URL https://jmlr.org/papers/v23/ 21-0998.html .Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzm ´an, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur C¸ elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V ´ıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu 12 Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models. 2024. URL 

https://arxiv.org/abs/2407.21783 .Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, Hao Zhang, Hanwei Xu, Honghui Ding, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jingchang Chen, Jingyang Yuan, Jinhao Tu, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaichao You, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Tao Yun, Tian Pei, Tianyu Sun, Tao Wang, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean 13 Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nat. , 645(8081):633–638, 2025. doi: 10.1038/S41586-025-09422-Z. URL https://doi.org/10.1038/s41586-025-09422-z .Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. RULER: what’s the real context size of your long-context language models? CoRR , abs/2404.06654, 2024. doi: 10.48550/ARXIV.2404.06654. URL https://doi.org/10.48550/arXiv.2404.06654 .Mathematical Association of America. American invitational mathematics examination (aime). MAA website. URL 

https://maa.org/maa-invitational-competitions/ .Multiverse Computing. Hypernova-60b model card, 2026. URL https://huggingface.co/ MultiverseComputingCAI/HyperNova-60B . Hugging Face model card, accessed 2026-02-11. NVIDIA. Nemo-skills: A pipeline for improving skills of large language models, 2024. URL https://github.com/ NVIDIA-NeMo/Skills .OpenAI. gpt-oss-120b & gpt-oss-20b model card. CoRR , abs/2508.10925, 2025. doi: 10.48550/ARXIV.2508.10925. URL https://doi.org/10.48550/arXiv.2508.10925 .Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Daron Anderson, Tung Nguyen, Mobeen Mahmood, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Jessica P. Wang, Pawan Kumar, Oleksandr Pokutnyi, Robert Gerbicz, Serguei Popov, John-Clark Levin, Mstyslav Kazakov, Johannes Schmitt, Geoff Galgon, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Zachary Giboney, Gashaw M. Goshu, Joan of Arc Xavier, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, John Wydallis, Mark Nandor, Ankit Singh, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Darling Duclosel, Jungbae Nam, Jennifer Zampese, Ryan G. Hoerr, Aras Bacho, Gautier Abou Loume, Abdallah Galal, Hangrui Cao, Alexis C. Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Lianghui Li, Sumeet Motwani, Christian Schr ¨oder de Witt, Edwin Taylor, Johannes Veith, Eric Singer, Taylor D. Hartman, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Joshua Robinson, Aleksandar Mikov, Ameya Prabhu, Longke Tang, Xavier Alapont, Justine Leon Uro, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Julien Guillod, Yuqi Li, Joshua Vendrow, Vladyslav Kuchkin, and Ng Ze-An. Humanity’s last exam. CoRR , abs/2501.14249, 2025. doi: 10.48550/ARXIV.2501.14249. URL https://doi.org/10.48550/arXiv.2501.14249 .Valentina Pyatkin, Saumya Malik, Victoria Graf, Hamish Ivison, Shengyi Huang, Pradeep Dasigi, Nathan Lambert, and Hannaneh Hajishirzi. Generalizing verifiable instruction following. CoRR , abs/2507.02833, 2025. doi: 10.48550/ARXIV.2507.02833. URL https://doi.org/10.48550/arXiv.2507.02833 .David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. CoRR , abs/2311.12022, 2023. doi: 10.48550/ARXIV.2311.12022. URL https://doi.org/10.48550/arXiv.2311.12022 .Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings .OpenReview.net, 2017. URL https://openreview.net/forum?id=B1ckMDqlg .Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. CoRR , abs/1909.08053, 2019. URL 

http://arxiv.org/abs/1909.08053 .Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L ´eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram ´e, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozi ´nska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluci ´nska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng 14 Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin G ¨orner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, S ´ebastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at a practical size, 2024. URL https://arxiv.org/abs/2408.00118 .Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram ´e, Morgane Rivi `ere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Ga ¨el Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, Andr ´as Gy ¨orgy, Andr ´e Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Pluci ´nska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim P ˜oder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and L ´eonard Hussenot. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786 .Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu A. Huerta, and Hao Peng. Scicode: A research coding bench-mark curated by scientists. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Pa-quet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024 , 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 36850592258c8c41cecdaa3dea5ff7de-Abstract-Datasets_and_Benchmarks_Track.html .15 Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR , abs/2406.01574, 2024. doi: 10.48550/ARXIV.2406.01574. URL https://doi.org/10.48550/arXiv.2406.01574 .Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=NG7sS51zVF .An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR , abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. URL https://doi.org/10.48550/arXiv.2505.09388 .16 A B200 Inference Efficiency Case Study: 64K/64K 

Table 5 summarizes full-node inference throughput on an 8 ×B200 system across tested scenarios. In the 64K/64K setting, gpt-oss-puzzle achieves up to 1.4 × higher throughput than the parent gpt-oss-120B. Table 5: B200 full-node (8 × B200) inference throughput: gpt-oss-puzzle-88B vs Parent. Values are max throughput at the best configuration (TP/BS omitted for readability).          

> Scenario gpt-oss-puzzle-88B gpt-oss-120B (parent) gpt-oss-puzzle-88B/Parent 64K/64K 21.2K tok/s 15.2K tok/s 1.40 ×
> 4K/4K 73.6K tok/s 63.9K tok/s 1.15 ×

Figure 7 shows throughput scaling with batch size for the 64K/64K scenario. Across the sweep, the gpt-oss-puzzle-88B consistently outperforms the parent model, with gains increasing in more KV-cache–dominated regimes. 2 8 32 128 512 

> Batch Size
> 0
> 5
> 10
> 15
> 20
> Throughput (K tok/s)
> gpt-oss-120B
> gps-oss-puzzle-88B

Figure 7: B200 throughput scaling with batch size (64K/64K). 

## B Evaluation Benchmarks Parameters 

Table 6: Parameters Configuration for Accuracy Benchmarks 

Benchmark Temp Top k Top p Min p Tokens Reasoning Effort 

*all* 0.6 -1 0.95 0.0 128,000 high/medium/low All performance and accuracy measurements were conducted using vLLM v0.11.2 ; please note that results may vary with different versions. 

## C Ablation Study: Attention Scoring 

Layerwise agreement between AALCR- and activation-based replace-1 scores. Figure 8 compares, for each layer, two replace-1 block-quality signals: an AALCR-based gap and an activation-based MSE gap. Specifically, the 17 FullAttn Window MMLU-Pro GPQA AIME25 SciCode AALCR Reg AALCR Reg AALCR Reg AALCR Reg AALCR Reg AALCR                                             

> 94096 76.82 76.89 64.02 64.84 70.63 77.29 40.24 39.35 7.00 8.00
> 98192 78.32 78.92 68.69 73.42 79.17 88.12 41.12 40.24 8.00 14.00
> 12 4096 78.50 79.55 74.56 74.12 87.78 86.88 39.64 40.68 15.00 24.00
> 12 8192 79.26 79.40 73.67 74.43 88.33 90.00 40.53 40.09 17.00 37.00

Table 7: Regular scoring vs. AALCR-based scoring for selecting window-attention layers. “FullAttn” denotes the number of full-attention layers remaining in the final architecture (the parent model has 18 full-attention layers; 12 corresponds to a 33% reduction and 9 to a 50% reduction). “Window” is the candidate block window size ( 4K or 8K). “Reg” refers to the standard Puzzle replace-1 scoring signal. For each configuration and metric, the better score between Reg and AALCR is bolded. 

Figure 8: Layerwise AA-LCR and activation-MSE replace-1 scores. The AA-LCR score is computed as 

AALCR( parent ) − AALCR( replace-1 ), and the activation score is the replace-1 activation MSE between the parent and the modified model. Higher values in either metric indicate a larger gap from the parent and therefore a more irreplaceable layer under window-attention replacement. AALCR score is computed as AALCR( parent ) − AALCR( replace-1 ), while the activation score is the replace-1 MSE between the parent and modified model activations. In both cases, larger values indicate a larger deviation from the parent, suggesting that the corresponding layer is more irreplaceable by window attention. Overall, the two signals largely agree on the relative importance of layers, but they diverge in some cases (e.g., around the intermediate layers), highlighting that representational similarity and task-level degradation can emphasize different sensitivities. 

AALCR-scoring and Activation based scoring resulting architecture performance In this appendix we compare two block-quality scoring signals used to guide our attention-structure search: regular Puzzle replace-1 scoring versus AALCR-based scoring. The goal in both cases is to assign each candidate window-attention block (i.e., converting one full-attention layer to window attention of size W ) a scalar quality score, which Puzzle then uses to decide which layers to convert when synthesizing an improved architecture. In Puzzle replace-1 scoring, for a given layer, we replace the original block (in our case full-attention) with the candidate block (in our case window-attention with window size W ) while keeping all other layers unchanged. We then run a fixed validation set and measure how closely the modified model matches the parent’s internal behavior, using the discrepancy between the last hidden states of the modified model and the parent (e.g., an MSE-style distance). A candidate block is considered higher quality if this single-block replacement perturbs the parent’s representations less. In AALCR-based scoring, we again perform a single-block replacement, but we score block quality by the functional impact on downstream behavior: we compute the drop in AALCR caused by replacing that block, and use this performance degradation as the block-quality signal (smaller drop implies a better block). Once block-quality scores are computed (by either method), we run Puzzle and solve for an improved attention architecture by selecting which layers to convert to window attention under the search constraints. Table 7 summarizes the resulting model-level outcomes when using each scoring signal, enabling a direct comparison of regular replace-1 scoring versus AALCR-based scoring across configurations. 18 D Raw Results 

This appendix reports the raw numbers underlying Figure 1 for a 64K/64K serving scenario on (a) an 8 ×H100 node and (b) a single H100 GPU. We also include per-benchmark accuracy and average generated-token statistics for the benchmark suite used to compute the suite average. We include HyperNova-60B [Multiverse Computing, 2026] as an external compressed derivative of gpt-oss-120B. HyperNova-60B was evaluated only with KV FP16 (as provided), using the Hugging Face checkpoint available as of Feb. 11, 2026. 

Relative request rate. For each model and reasoning effort, relative request rate is computed as max token throughput (best serving configuration per model) divided by the average tokens generated per request, and normalized to the gpt-oss-120B (KV BF16, High reasoning effort) baseline in the corresponding hardware setting. 

Max token throughput. Tables 8 and 9 report the max token throughput values used in the relative request rate 

computation. Table 8: Max token throughput (64K/64K) at the best configuration per model on an 8 ×H100 node. The gpt-oss-puzzle-88B derivative achieves 1.63 × and 1.62 × higher throughput than the gpt-oss-120B parent under KV BF16 and KV FP8, respectively.             

> Model KV precision Max throughput (K tok/s) gpt-oss-120B KV BF16 4.0 gpt-oss-puzzle-88B KV BF16 6.5 HyperNova-60B KV BF16 6.9 gpt-oss-120B KV FP8 5.8 gpt-oss-puzzle-88B KV FP8 9.3

Table 9: Max token throughput (64K/64K) at the best configuration per model on a single H100 GPU. The gpt-oss-puzzle-88B derivative achieves 2.68 × and 2.86 × higher throughput than the gpt-oss-120B parent under KV BF16 and KV FP8, respectively.             

> Model KV precision Max throughput (K tok/s) gpt-oss-120B KV BF16 0.2 gpt-oss-puzzle-88B KV BF16 0.5 HyperNova-60B KV BF16 0.7 gpt-oss-120B KV FP8 0.3 gpt-oss-puzzle-88B KV FP8 0.8

Raw numbers used in Figure 1. Tables 10 and 11 report the suite-average accuracy and relative request rate points plotted in Figure 1. 

Per-benchmark accuracy and generation length. Table 12 reports per-benchmark accuracy and average generated tokens for the three compared models, matching the benchmark suite used in Figure 1. gpt-oss-120B and gpt-oss-puzzle-88B use KV BF16; HyperNova-60B was evaluated with KV FP16, as provided. 19 Table 10: Raw numbers for Figure 1(a): 8 ×H100 node (64K/64K). 

Effort KV precision Model Average accuracy (%) Relative request rate High KV BF16 gpt-oss-120B 59.20 1.000 gpt-oss-puzzle-88B 59.44 1.490 HyperNova-60B 51.86 1.288 KV FP8 gpt-oss-120B 58.19 1.401 gpt-oss-puzzle-88B 58.67 2.077 Medium KV BF16 gpt-oss-120B 53.66 4.280 gpt-oss-puzzle-88B 56.64 4.552 HyperNova-60B 46.23 6.032 KV FP8 gpt-oss-120B 52.89 5.991 gpt-oss-puzzle-88B 54.93 6.193 Low KV BF16 gpt-oss-120B 45.41 9.455 gpt-oss-puzzle-88B 50.61 11.975 HyperNova-60B 38.77 13.855 KV FP8 gpt-oss-120B 44.71 13.270 gpt-oss-puzzle-88B 48.38 17.179 

Table 11: Raw numbers for Figure 1(b): single H100 GPU (64K/64K). 

Effort KV precision Model Average accuracy (%) Relative request rate High KV BF16 gpt-oss-120B 59.20 1.000 gpt-oss-puzzle-88B 59.44 2.454 HyperNova-60B 51.86 2.934 KV FP8 gpt-oss-120B 58.19 1.624 gpt-oss-puzzle-88B 58.67 4.240 Medium KV BF16 gpt-oss-120B 53.66 4.280 gpt-oss-puzzle-88B 56.64 7.497 HyperNova-60B 46.23 13.741 KV FP8 gpt-oss-120B 52.89 6.945 gpt-oss-puzzle-88B 54.93 12.644 Low KV BF16 gpt-oss-120B 45.41 9.455 gpt-oss-puzzle-88B 50.61 19.724 HyperNova-60B 38.77 31.565 KV FP8 gpt-oss-120B 44.71 15.381 gpt-oss-puzzle-88B 48.38 35.076 

Table 12: Per-benchmark accuracy (%) and average tokens generated (K) for the three compared models with KV BF16 in Figure 1. 

Effort Model Average accuracy Average tokens (K) MMLU-Pro RULER 128K GPQA-Diamond AIME 25 IFBench SciCode AA LCR HLE High gpt-oss-120B 59.20 12.70 80.41 50.89 77.78 91.46 64.46 41.72 48.75 18.16 gpt-oss-puzzle-88B 59.44 13.87 79.32 59.80 75.13 92.92 67.77 40.83 42.25 17.52 HyperNova-60B 51.86 17.09 72.97 37.27 73.93 88.75 57.03 38.31 30.00 16.64 Medium gpt-oss-120B 53.66 2.97 78.86 55.01 71.28 76.88 56.55 41.42 39.25 10.06 gpt-oss-puzzle-88B 56.64 4.54 78.03 66.71 69.70 86.88 65.56 39.64 36.00 10.57 HyperNova-60B 46.23 3.65 71.03 37.09 67.42 76.25 50.00 34.02 25.00 8.99 Low gpt-oss-120B 45.41 1.34 75.18 52.61 62.75 50.00 44.13 39.20 35.25 4.17 gpt-oss-puzzle-88B 50.61 1.73 75.56 66.70 64.77 66.25 56.38 38.17 31.75 5.33 HyperNova-60B 38.77 1.59 66.16 42.70 59.22 44.17 40.65 31.66 20.75 4.87 

20