Title: Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision

URL Source: https://arxiv.org/pdf/2602.12164v1

Published Time: Fri, 13 Feb 2026 02:27:56 GMT

Number of Pages: 14

Markdown Content:
# Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision 

Xiaohan He * 1 2 Shiyang Feng * 1 Songtao Huang 1 2 Lei Bai 1 Bin Wang 2 Bo Zhang 1

## Abstract 

Large language models (LLMs) have demon-strated exceptional reasoning capabilities, and co-evolving paradigms have shown promising re-sults in domains such as code and math. How-ever, in scientific reasoning tasks, these models remain fragile due to unreliable solution evalu-ation and limited diversity in verification strate-gies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that en-ables models to self-evolve as both solver and verifier through a transition from sparse supervi-sion to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment an-chors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diver-sity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github. com/InternScience/Sci-CoE .

## 1. Introduction 

Self-evolving Reinforcement Learning (RL) has emerged as a transformative paradigm for Large Language Mod-els (LLMs) to refine their reasoning trajectories through feedback. A notable milestone is the Zero RL paradigm in-troduced by DeepSeek-R1 (Guo et al., 2025), which elicits sophisticated reasoning behaviors without prior supervised imitation learning. However, this approach remains funda-mentally dependent on annotated datasets for reward calcu-lation. To mitigate this reliance, self-play mechanisms have been adopted to facilitate autonomous evolution. In these 

> 1

Shanghai Artificial Intelligence Laboratory 2Fudan University. Correspondence to: Bo Zhang <zhangbo@pjlab.org.cn >.

Preprint. February 13, 2026. Electronic molar heat capacities in low-                   

> temperature heat capacity experiments :
> CeAg = 0.65 T
> CeCs = 2.38 T
> What is the thermal EMF at 1000 °C?
> Property Validation
> • Verify appl ictaion of the Seebeck effect .
> • Check dependence on temperature
> difference .
> • Validate units and physical scale .
> Scientific Question
> Reasoning Steps:
> • Relate heat capacity to Seebeck coefficient
> • Estimate temperature dependence
> • Compute EMF difference
> Generated Solution
> Final Answer:
> Verifiaction Strategies
> Reverse Calculation
> •Independent ly Recompute EMF .
> •Cross-check heat capacity difference
> integration.
> •Estimate EMF magnitude using known
> thermoelectric scales.

Figure 1. Examples of Scientific Question, Generated Solution and Verification Strategies. 

settings, the LLM concurrently assumes multiple roles, such as a challenger and a solver (Huang et al., 2025a; Zhao et al., 2025) or a solver and a verifier (Wang et al., 2025b), to drive co-evolution through mutual interaction. These self-play methods significantly enhance the autonomy of reasoning training by reducing the need for external supervision. However, existing self -evolution paradigms are primarily confined to domains such as coding and mathematics, where task quality can be assessed through clear verification sig-nals. In these domains, correctness can either be judged directly using ground - truth solutions or indirectly through explicit verification methods such as unit tests. In contrast, scientific reasoning tasks rarely provide such clear verifi-cation signals, which makes self -evolution in this domain far more challenging. Specifically, these tasks unfold in an open - world regime with multiple valid solution pathways and heterogeneous verification criteria. Furthermore, scien-tific reasoning tasks spans diverse disciplines where verifi-cation requires expert assessment of complex intermediate logic rather than simple answer matching. The prohibitive cost of curating such specialized supervision renders large-scale and data-intensive training approaches impractical. Motivated by these challenges, we raise a pivotal question: 

Can we develop a self -evolving RL framework for scientific reasoning tasks under limited supervision? 

In this work, we introduce Sci -CoE, a scientific co -evolving framework that consists of a Solver and a Verifier, both implemented within a single LLM. The Solver generates 1

> arXiv:2602.12164v1 [cs.AI] 12 Feb 2026 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision

candidate solutions, and the Verifier constructs strategies to evaluate their correctness. These two roles are optimized jointly through interactive reinforcement learning. Further-more, to maintain stability without ground -truth supervision, we propose a geometric reward mechanism that represents verification strategies in a latent geometric space. This mechanism encourages strategies to remain both reliable and diverse, which prevents consensus collapse and supports sustained self-evolution in open-ended scientific domains. Comprehensive experimental results demonstrate that Sci - CoE remains effective even when explicit verification signals are absent. Across diverse scientific domains, the framework achieves strong reasoning performance under limited supervision. Furthermore, the proposed reward mechanism enhances both the reliability and the diversity of verification strategies, leading to consistent performance gains. Our contributions are summarized as follows: • We proposed a co - evolving framework called Sci -CoE for scientific reasoning that integrates a Solver and a Verifier within a single LLM. This design en-ables the acquisition of both solution -generation and solution - verification capabilities, supporting self - evolution without ground - truth solutions or prede-fined verification procedures. • A geometric reward mechanism is developed to model verification strategies in a latent geometric space. By encouraging both reliability and diversity, this mech-anism prevents consensus collapse and enables stable unsupervised evolution. • Comprehensive experiments show that Sci - CoE frame-work not only improves reasoning accuracy and robust-ness but also cultivates effective verification behaviors capable of multi -perspective evaluation of scientific questions. 

## 2. Related Work 

2.1. Scientific Large Language Models 

Recent advancements in scientific LLMs span generalist ar-chitectures and domain-specific adaptations (Hu et al., 2025; Team et al., 2025; Fallahpour et al., 2025; Tan et al., 2025; Zhang et al., 2024). Intern-S1 utilizes a multimodal Mixture-of-Experts (MoE) architecture with Mixture-of-Rewards re-inforcement learning to outperform closed-source models in complex scientific tasks (Bai et al., 2025). SciReasoner aligns natural language with heterogeneous scientific repre-sentations to establish a reasoning foundation (Wang et al., 2025a), while SCI-Verifier introduces a unified reasoning-augmented framework for robust equivalence judgment in verification (Wang et al., 2025a). Regarding domain-specific innovations, ChemVLM integrates visual encoders to bridge molecular structures with text (Li et al., 2025). Med-R1 applies reinforcement learning for medical vision-language reasoning (Lai et al., 2025), and MindLLM em-ploys a subject-agnostic framework to decode fMRI signals directly into text (Qiu et al., 2025). AstroMLab 3 leverages high-quality data curation to enable a compact 8B-parameter model to match GPT-4o performance in astronomy (de Haan et al., 2024). 

2.2. Self-Evolving Large Language Models 

Early research in self-evolution LLMs explored the self-play between generation and verification, particularly within code domains. Sol-Ver introduces a self-play framework where models iteratively refine both code implementations and test cases (Lin et al., 2025). CURE leverages reinforce-ment learning to mutually enhance the LLM coder and unit tester through dynamic interaction (Wang et al., 2025b). Pushing autonomy further, subsequent frameworks learn to generate their own problems and adaptive curricula from scratch or minimal seeds. Absolute Zero demonstrates that reasoning capabilities can emerge purely through reinforced self-play without human priors (Zhao et al., 2025), whereas SERL bootstraps robust policies from limited data via itera-tive selection (Fang et al., 2025). Scaling this verification paradigm, Loong introduces an agent-environment loop that synthesizes large-scale training data with executable code-based ground truth, enabling Reinforcement Learn-ing with Verifiable Rewards (RLVR) across diverse dis-ciplines (Huang et al., 2025b). R-Zero employs internal consistency as a reward signal to facilitate self-evolution in general reasoning domains where external verifiers are absent (Huang et al., 2025a). Addressing the instability of such open-ended exploration, R-Few introduces a guided self-play mechanism to explicitly mitigate concept drift and diversity collapse (Yu et al., 2025). Distinct from these approaches, our work explores self-evolution for general scientific reasoning with limited data. We orchestrate the co-evolution of reasoning and rigorous verification strate-gies to eliminate dependencies on external verifiers while establishing a closed-loop reinforcement of scientific logic. 

## 3. Methodology 

3.1. Overview 

We propose Sci-CoE, a Scientific Co-Evolving Framework designed to systematically improve scientific reasoning ca-pabilities under minimal supervision, featuring a Solver and a Verifier. Training proceeds in two stages. In the first an-chored learning stage, Sci-CoE leverages a small amount of labeled data to establish anchored notions of correctness and verification reliability, providing a stable initialization for both roles (Section 3.3). In the second unsupervised co-evolution stage, the framework scales to large unlabeled 2Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision Scientific Question    

> Solver
> Verifier
> Verification
> Strategy
> Scientific
> Solution Chemistry Biology
> Physics Mathematics

## ···   

> Scientific Reasoning Tasks
> What is the complementary
> sequence of 5'-ATCG-3'?
> Solver
> Reward
> Verifier
> Reward
> Strategy
> Accuracy
> Solution
> Matching
> Solution
> Rollout *N
> Strategy
> Rollout *M
> Solution
> GT
> Stage 1:Anchored Learning
> Solver Reward
> Self
> Verified
> Strategy
> Embedding
> Stage 2:Unsupervised Co-evolution
> Solution
> Rollout *N
> Strategy
> Rollout *M
> consistency
> reliability
> diversity
> Geometric
> Modeling
> Verifier Reward
> Solution 1
> Solution 2
> Solution N

## ···    

> Strategy 1
> Strategy 2
> Strategy M

## ···   

> Rollout Rollout
> Solver Verifier LLM
> Reward
> Optimization Loop

Figure 2. The overall pipeline of Sci-CoE 

data, where Solver and Verifier mutually supervise each other through a consensus and geometric reward mecha-nism, enabling fully unsupervised co-evolution (Section 3.4). 

3.2. The Solver-Verifier Co-Evolving Mechanism 

The key challenge we address is that, in scientific domains, solution generation and answer verification are both difficult and mutually dependent, especially when reliable ground-truth annotations are scarce. Sci-CoE is built upon a central insight: robust scientific reasoning emerges from the co-evolution of solution generation and verification capability. Instead of treating solving and evaluation as separate com-ponents, Sci-CoE trains a single model to simultaneously assume two complementary roles: (1) Solver , which gener-ates candidate solutions for scientific questions; (2) Verifier ,which generates verification strategies to assess the correct-ness of the solutions. As illustrated in Figure 2, given a scientific question q, the model concurrently generates multiple solutions and multi-ple verification strategies: 

S(q) = {s1, . . . , s N }, V (q) = {v1, . . . , v M } (1) The Solver produces solutions si that contain explicit rea-soning steps and a final answer, while the Verifier generates natural-language verification strategies vj that evaluate so-lution correctness from diverse perspectives, such as logical consistency checks, physical constraints, or inverse deriva-tions. Each solution–verification pair (si, v j ) is evaluated by an external LLM acting as a judging model, which strictly follows the specified verification strategy to assess the solu-tion and outputs a binary result: Eval (si, v j ) ∈ { 0, 1} (2) These results form a verification matrix E ∈ { 0, 1}N ×M ,which serves as the core feedback signal for reinforcement learning. The Solver and Verifier share the same set of model parameters and are jointly optimized using Proximal Policy Optimization (PPO). This framework establishes a closed-loop co-evolving pro-cess: higher-quality solutions facilitate the learning of more discriminative verification strategies, while stronger verifi-cation strategies provide more reliable reward signals for solution generation in turn. 

3.3. Anchored Learning with Sparse Supervision 

We initiate the training process using a very small subset (1%-10%) of scientific questions annotated with ground-truth answers. The objective of this stage is not to maxi-mize performance under supervision, but to establish stable reference anchors for both problem solving and strategy generation. 

Solver Reward. For questions with ground-truth answers, the Solver is rewarded based on exact correctness: 

rsol  

> i

= G(si) ∈ { 0, 1} (3) If a generated solution is consistent with the reference an-swer, it is regarded as correct and receives a reward of 1; otherwise, it receives 0. This binary reward formulation encourages the Solver to generate solutions fully consistent with the reference. This supervision does not aim to ex-haustively teach domain knowledge, but instead provides a 3Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision 

minimal alignment signal that calibrates the model’s reason-ing trajectories scientifically. 

Verifier Reward. The goal of the Verifier is to generate verification strategies that are both discriminative and reli-able: an optimal verification strategy should pass all correct solutions while rejecting incorrect ones. First, we define the set of correct solutions as: 

S+(q) = {si ∈ S(q) | G(si) = 1 } (4) If a verification strategy passes all correct solutions, it is considered positively aligned. The sign of the verification reward is defined as: sign (vj ) = 

(

+1 , ∀si ∈ S +(q), Eval( si, v j ) = 1 ,

−1, otherwise . (5) The final reward function for verification is: 

rver  

> j

= sign (vj ) · Es∈S −(q) [1 − Eval( s, v j )] (6) where S−(q) = S(q) \ S+(q) is the set of incorrect solu-tions. This formulation assigns positive rewards to verifica-tion strategies that pass all correct solutions while rejecting a larger proportion of incorrect ones, and penalizes strategies that incorrectly reject correct solutions. 

Sequential Optimization. Since the Solver and Verifier share parameters, directly optimizing both objectives jointly at this stage may lead to unstable dynamics. We there-fore adopt a sequential optimization scheme after collecting rollout samples and their corresponding rewards for both solutions and strategies. Specifically, within each PPO iter-ation, we first update the model parameters using solution data, and then update the same parameters using strategy data. This ensures that the shared model parameters θ al-ternately integrate feedback signals from solving accuracy and strategy discriminability. By aligning the Solver with ground-truth correctness and training the Verifier to maxi-mally distinguish correct solutions from incorrect ones, this stage establishes a stable and reliable foundation for the subsequent unsupervised co-evolving stage. 

3.4. Unsupervised Co-evolution via Geometric Consensus. 

After anchored learning in Stage 1, the model acquires basic capabilities for generating candidate solutions and verifica-tion strategies. In Stage 2, we further scale training to a large corpus of unlabeled scientific questions, where no ground-truth answers are available. The key challenge in this stage is to provide reliable training signals for both the Solver and the Verifier without relying on external annotations. To address this challenge, we design a fully unsupervised co-evolving mechanism that leverages mutual consistency 

Algorithm 1 Stage 1: Anchored Learning 

Input: Sparse labeled dataset DGT = {(q, a ∗)}; shared policy πθ ; number of solutions N ; number of strategies 

M .

Initialize: Policy parameters θ.

for each training iteration do for each rollout sampled labeled question (q, a ∗) ∈DGT do 

Generate N solutions and M verification strategies. Evaluate each (si, v j ) using an external Judge Model: Eij = Eval( si, v j ) ∈ { 0, 1}.Compute solver reward using ground-truth: rsol  

> i

=

G(si, a ∗).Identify correct set: S+(q) = {si | rsol  

> i

= 1 }.Compute verifier reward: 

rver  

> j

= sign (vj ) · Es∈S −(q)

1 − Eval( s, v j )

end for 

Sequentially update θ using PPO with solution samples 

{(q, s i, r sol  

> i

)} and strategy samples {(q, v j , r ver  

> j

)}.

end for Output: Updated policy πθ .between solutions and verification strategies. The core idea is to replace absolute correctness signals with relative agree-ment and structural consensus, enabling the Solver and Ver-ifier to supervise each other. 

Solution Reward via Strategy Consensus. Without refer-ence answers, the Solver may reinforce incorrect reasoning through self-confirmation. To address this, we replace ab-solute correctness with relative consensus as the learning signal. Specifically, the quality of a solution is measured by its pass rate across various verification strategies. The solution reward is defined as: 

rsol  

> i

= 1

M

> M

X

> j=1

Eval (si, v j ) (7) This relative formulation mitigates noise from individual verification errors, encouraging the model to generate so-lutions that remain consistent across multiple verification perspectives. In practice, we introduce a threshold τ (e.g., 

τ =0.8) and treat solutions whose passing rate exceeds τ as high-consensus solutions. These solutions are collected into the set S+(q). The consistency-based component rcons  

> i

of the verification reward, is then computed based on this set via Eq.(6). 

Verification Reward via Geometric Modeling. To pre-vent the Verifier from maximizing consensus reward by gen-erating homogeneous or trivial verification strategies, we propose a reward framework based on geometric modeling, 4Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision 

evaluating verification strategies along three dimensions: consistency, reliability, and diversity. The consistency reward rcons  

> i

is derived from the high-consensus solution set S+(q) via Eq. (6) , encouraging strate-gies that accept high-consensus solutions while rejecting others. For the other two, we decouple the verification as-sessment from generated solutions and instead model the rewards based on the geometric structure of strategies in the latent representation space. Each natural-language verifica-tion strategy vj is mapped into a high-dimensional semantic vector zj = ϕ(vj ) ∈ Rd using a pretrained embedding model Qwen3-Embedding-8B (Zhang et al., 2025). We then perform K-means clustering over these embeddings {zj },obtaining clusters {C 1, . . . , CK } with corresponding centers 

{μ1, . . . , μ k}.

Reliability Reward. Strategies that lie closer to the clus-ter center are assumed to less likely to hallucinations or topic-drifting, representing more stable and trustworthy ver-ification logic. We therefore define the reliability reward: 

rrel  

> j

= 1 − dj

max k dk + ϵ (8) where dj = ∥zj − μc(j)∥2 is the Euclidean distance be-tween verification strategy zj and its cluster center, strate-gies closer to the center receive higher reliability rewards. 

Diversity Reward. Furthermore, to encourage the cov-erage of diverse verification perspectives, we introduce a diversity reward modeled in polar coordinates. Specifically, we use Principal Component Analysis (PCA) to project the decentralized vectors uj = zj − μk into a 2D sub-space, as ˜uj = ( xj , y j ) ∈ R2 Then compute the polar angle 

θj = atan2 (xj , y j ) for each strategy. In the ideal state, strategies should be uniformly distributed around the center from a geometric viewpoint. We promote this by reward-ing samples with significant angular deviations from others, strategies that are too close to others receive lower rewards: 

rdiv  

> j

= 1

|C k| − 1

X 

> j′̸=j

(1 − cos( θj − θj′ )) (9) The final verification reward is a weighted sum of consis-tency, reliability, and diversity: 

rver  

> j

= α r con  

> j

+ β r rel  

> j

+ γ r div  

> j

(10) In our experiments, we set the clustering coefficient k = 1 ,and α = 1 .0, β = 0 .5, γ = 0 .5.

Joint Optimization. Unlike the sequential training in An-chored Learning, this stage employs a joint optimization. Specifically, solution samples and strategy samples with their respective rewards are mixed within the same training 

Algorithm 2 Stage 2: Unsupervised Co-evolution 

Input: Unlabeled dataset DU = {q}; shared policy πθ ;embedding model ϕ(·); number of solutions N ; number of strategies M ; consensus threshold τ .

for each training iteration do for each rollout sampled question q ∈ D U do 

Generate N solutions and M verification strategies. Evaluate each (si, v j ) using an external Judge Model: Eij = Eval( si, v j ) ∈ { 0, 1}.Compute solution reward: rsol  

> i

= 1

> M

PMj=1 Eij 

Identify high-consensus solutions: S+(q) = {si |

ci ≥ τ }.Perform K-means clustering on {zj = ϕ(vj )} and obtain centers {μk}.Compute reliability reward and diversity reward (see Equation(8) and (9)). Compute final verifier reward: rver  

> j

= αr con  

> j

+

βr rel  

> j

+ γr div 

> j

end for 

Update θ using PPO with mixed samples 

{(q, s i, r sol  

> i

)} ∪ { (q, v j , r ver  

> j

)}.

end for Output: Updated policy πθ .batch. Consequently, during each PPO iteration, the model parameters θ receive gradient updates simultaneously from both the Solver and Verifier tasks. This enables the model to dynamically adjust its verification criteria while explor-ing new solution spaces, allowing solutions and verification strategies to mutually reinforce each other and progressively improve without ground-truth supervision. 

## 4. Experiments 

4.1. Experimental Setup Model and Optimization Configuration. We employ Qwen2.5-7B-Instruct and Qwen3-8B (Yang et al., 2025) as the base policy models to concurrently perform Solver and Verifier tasks. To provide high-quality verification feed-back signals during training , that is to judge whether a Solver’s solution passes the Verifier’s generated strategy, we utilize Qwen3-235B-A22B (Yang et al., 2025) as the external judging model. 

Data Construction. We integrate datasets including Mega-Science (Fan et al., 2025), Numinamath (Li et al., 2024), ScienceQA (Saikh et al., 2022), and CaseHold (Zheng et al., 2021), covering diverse scientific domains such as mathe-matics, physics, chemistry and biology. In Anchored Learning Stage, we sample 4k data from Mega-Science and Numinamath annotated with ground-truth ref-erence answers. In Unsupervised Co-evolution Stage, we 5Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision                                                                                                                                                                                                                      

> Table 1. Main Results on MMLU-Pro. We report the accuracy (%) on MMLU-Pro and its subsets. The best results within each column are highlighted in bold , and underline indicates the second best.
> Model Overall Bio. Bus. Che. C.S. Eco. Eng. Hea. His. Law Math Phi. Phy. Psy. Oth.
> Comparable Scale Model
> Llama-3.1-8B-Instruct 44.25 63.04 49.30 37.63 48.29 55.09 29.72 50.73 42.26 27.25 43.82 44.49 40.26 60.03 44.81 Ministral-8B-Instruct 37.93 59.00 39.42 26.41 44.88 49.29 23.12 43.28 43.83 25.98 41.15 36.47 29.18 51.63 40.15 Mathstrao-7B-v0.1 42.00 63.46 42.08 38.78 45.61 51.78 38.39 39.24 35.96 22.43 47.67 38.28 38.03 52.63 40.91 Yi-1.5-9B-Chat 45.95 66.67 54.25 39.49 50.00 60.19 33.23 43.52 40.94 26.61 52.48 40.08 41.42 59.40 44.91 Mistral-Small-Instruct 48.40 71.69 52.72 36.84 53.66 60.07 30.55 53.79 50.13 31.97 50.85 48.10 40.34 63.91 55.09
> Qwen2.5-7B-Instruct
> Base Model 57.39 72.11 64.89 57.16 60.49 68.84 39.94 56.85 48.29 32.52 71.87 49.90 58.35 65.91 54.33 Sci-CoE-Stage 1 57.68 74.34 67.17 56.89 60.73 68.72 40.04 56.60 47.77 33.33 72.09 48.50 59.05 65.54 53.90 Sci-CoE-Stage 2-18k 58.05 73.50 68.69 57.16 61.22 68.01 40.04 58.07 49.08 32.61 72.54 50.50 58.97 66.67 54.65 Sci-CoE-Stage 2-30k 58.51 73.92 68.19 55.39 61.71 70.62 42.31 58.19 48.82 34.06 72.76 50.50 59.35 67.17 54.87
> Qwen3-8B
> Base Model 63.19 78.80 69.71 68.02 66.10 72.27 53.04 62.47 51.97 31.52 78.53 51.50 67.67 69.30 55.95 Sci-CoE-Stage 1 63.27 78.94 69.20 66.78 65.85 72.63 53.77 63.08 50.92 32.61 78.09 52.51 68.44 69.42 55.41 Sci-CoE-Stage 2-18k 63.56 79.22 70.85 68.02 66.34 72.87 53.35 62.71 51.44 32.52 79.42 52.91 67.74 69.17 55.19 Sci-CoE-Stage 2-30k 64.34 80.20 70.72 68.20 68.05 73.93 54.59 63.33 54.07 33.42 79.79 53.51 68.36 70.30 56.06

further construct two unlabeled training sets of different scales of 18k and 30k to test scalability. Detailed dataset compositions and statistics for each training stage are pro-vided in the Appendix A.1. 

Benchmarks. To test scientific reasoning ability, we evalu-ate our approach on the following benchmarks: MMLU-Pro (Wang et al., 2024), GPQA-Diamond (Rein et al., 2024), and UGPhysics (Xu et al., 2025). These benchmarks col-lectively cover a wide range of scientific disciplines with challenging tasks, offering a stricter evaluation of complex reasoning abilities. For evaluation, we strictly employ the official evaluation scripts provided for each dataset to ensure accuracy and comparability. 

4.2. Main Results Performance on Scientific Reasoning Benchmarks. As shown in Table 1, Table 2 and Table 3, Sci-CoE consistently outperforms the corresponding base models on both general and domain-specific reasoning benchmarks. To highlight, our framework Sci-CoE with the Qwen3-8B outperforms the baseline by 4.04% on GPQA-Diamond dataset, raising the accuracy from 36.87 to 40.91. On the larger and broader MMLU-Pro benchmark, Sci-CoE also achieves 1.15% im-provement, from 63.19 to 64.34, demonstrating the general applicability of the learned reasoning and verification capa-bilities across scientific domains. For UGPhysics which focuses on undergraduate-level physics reasoning, Sci-CoE does not exhibit monotonic improvements across all sub-disciplines, likely due to do-main distribution mismatches between training data and specific physics topics. Nevertheless, Sci-CoE consistently  

> Figure 3. Performance of the model at different stages of the training process on GPQA-D evaluation data, the three broken lines represent the average accuracy of the generated solutions, the average accuracy of the generated validation strategies and Best-of-N(BoN) accuracy, using 16 generated solutions and 16 generated strategies. The baseline is Qwen2.5-7B-Instruct, and the rest of the model names represent the number of training steps in that stage.

improves the overall average accuracy, achieving increases of 1.97% and 1.34% repectively on the 7B and 8B base model. This suggests that Sci-CoE primarily learns general reasoning and verification patterns, rather than overfitting to specific subfields, enabling stable performance gains even when domain-specific supervision is extremely sparse. We compare Sci-CoE with several baselines of compara-ble model scale, including Llama-3.1-8B-Instruct (Dubey et al., 2024), Ministral-8B-Instruct-2410 (MistralAI, 2024), Mathstral-7B-v0.1 (Mistral, 2023), Yi-1.5-9B-Chat (Young et al., 2024) and Mistral-Small-Instruct-2409 (Jiang et al., 2023). As shown in in Table 2, Sci-CoE consistently outper-forms all same-scale baselines on both the general reasoning benchmark MMLU-Pro and the domain-specific benchmark UGPhysics. 6Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision                                                                                                                          

> Table 2. Main Results on UGPhysics. We report the accuracy (%) on the English subset of UGPhysics. The best results within each column are highlighted in bold , and underline indicates the second best. In case of ties, all tied results are marked. “Mec.”, “Elec.” and “Modern” stand for Mechanics & Thermodynamics, Electromagnetism, and Modern Physics subsets of UGPhysics.
> Model Data Scale Overall Acc Mec. and Ther. Elec. Modern Physics
> Comparable Scale Model
> Llama-3.1-8B-Instruct –14.66 12.64 14.35 16.80 Ministral-8B-Instruct-2410 –16.39 13.95 15.52 19.20 Mathstral-7B-v0.1 –17.45 14.82 17.77 19.94 Yi-1.5-9B-Chat –17.61 16.00 15.85 19.94 Mistral-Small-Instruct-2409 –25.72 22.71 22.70 29.97
> Qwen2.5-7B-Instruct
> Base Model –20.67 18.88 18.52 23.34 Sci-CoE-Stage 1 4k 21.07 20.14 19.81 22.51 Sci-CoE-Stage 2 18k 21.92 20.92 21.31 23.17 Sci-CoE-Stage 2 30k 22.64 21.84 23.13 24.91
> Qwen3-8B
> Base Model –31.76 30.73 29.98 33.51 Sci-CoE-Stage 1 4k 32.03 30.25 30.62 34.38 Sci-CoE-Stage 2 18k 32.46 30.21 33.30 34.38 Sci-CoE-Stage 2 30k 33.10 30.51 34.80 34.99
> Table 3. Main Results on GPQA-Diamond. We report the accuracy (%) on GPQA-Diamond and its subsets. The best results within each column are highlighted in bold , and underline indicates the second best.
> Model Data Scale Overall Acc Physics Chemistry Biology
> Qwen2.5-7B-Instruct
> Base Model –30.81 33.73 24.73 47.37 Sci-CoE-Stage 1 4k 31.31 34.88 24.73 47.37 Sci-CoE-Stage 2 18k 33.33 41.86 23.66 42.11 Sci-CoE-Stage 2 30k 35.35 41.86 26.88 47.37
> Qwen3-8B
> Base Model –36.87 39.53 33.33 42.11 Sci-CoE-Stage 1 4k 37.88 45.35 29.03 47.37 Sci-CoE-Stage 2 18k 38.89 41.86 33.33 52.63 Sci-CoE-Stage 2 30k 40.91 43.02 35.48 57.89

Scalability on Unlabeled Data. After introducing large-scale unlabeled data, Sci-CoE demonstrates strong scala-bility. As the scale of unlabeled data in Stage 2 increases from 18k to 30k, we observe continuous improvements in reasoning accuracy, without evident performance saturation. This indicates that increasing the diversity and quantity of unlabeled scientific problems enables Sci-CoE to discover more robust reasoning patterns. Crucially, our framework effectively bypasses the performance plateau commonly en-countered in self-training, maintaining a strong Scaling Law in the absence of ground-truth supervision. 

Evolutionary Trends and Co-evolving Iterations. The performance improvements achieved by Sci-CoE are pro-gressive, as illustrated in Figure 3, reflecting a stable and promising co-evolutionary process. The final model sub-stantially outperforms the baseline in solution, verification strategy, and Best-of-N (BoN) performance. In Stage 1, the rapid improvement in verification strategy accuracy equips the model with initial evaluation capabilities for reasoning processes, providing higher-quality feedback for the Solver in subsequent unsupervised co-evolution. Furthermore, the continuous improvement in BoN accuracy underscores the practical value of our designed Verifier in inference-time. Sci-CoE not only enhances the ability of generated candidate solutions but also constructs a reliable internal reward signal, allowing the model to accurately identify correct reasoning trajectories among multiple can-didates during inference. 

4.3. Ablation Study and Analysis Impact of Anchored Learning. We investigated the ne-cessity of the Anchored Learning by skipping Stage 1 and training directly on Stage 2 using unlabeled data. As shown in Table 4 Index 1, the model significantly lags behind Sci-CoE. In several benchmarks, it even underperforms the baseline. This confirms that although Stage 1 uses a rela-tively small amount of data, it provides essential training 7Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision (a) (b) (c) (d)                                             

> (e) (f) (g)
> Figure 4. Visualization of geometric reward and quantitative analysis of verification strategies. (a)-(d) display PCA projections of strategy embeddings in a polar coordinate system across different training stages, respectively corresponding to Baseline Model, Stage 1 only, Stage 2 with Naive Consensus Reward, and Stage 2 with Geometric Reward. The angular distribution of points indicates diversity, while the radial distance to the cluster center represents strategy reliability, with closer points indicating more stable. And the color represents the consistency score, with closer to green indicating higher scores. (e)-(g) illustrate mean consistency, reliability, and diversity reward scores of different models.
> Table 4. Ablation study on Scientific Reasoning benchmarks. Index Anchored Learning Stage1 Data Scale Geometric Reward GPQA-D MMLU-Pro UGPhysics Avg Acc Baseline –––36.87 63.19 31.76 43.94 1✗✗✗35.86 63.00 31.61 43.49 2✓0.4k ✗37.37 63.36 31.79 44.17 3✓4k ✗37.88 63.27 32.07 44.41 4✓4k ✓38.89 63.53 32.46 44.96

anchors that help the model establish an initial notion of correctness and verification reliability. Notably, in Stage 1, utilizing only 0.4k annotated samples, the model achieves overall performance improvement (Index 2). This confirms that a minimal set of high-quality anchor data is sufficient to successfully bootstrap the fundamental capabilities of both the Solver and Verifier, establishing a solid foundation for subsequent evolution. Furthermore, a comparison between Stage 1-0.4k and Stage 1-4k reveals that while more data yields a better starting point, the system can be successfully bootstrapped with as few as 0.4k samples, demonstrating the framework’s robustness. 

Effectiveness of Geometric Reward. We compared the ef-ficacy of the Naive Consensus Reward against our proposed Geometric Reward during Stage 2. As shown in Table 4 Index 3-4, the model utilizing the Geometric Reward sig-nificantly outperforms the Raw Reward version across all benchmarks. To provide a more intuitive explanation, we visualize the verification strategies by projecting their embedding vectors into a 2D space using PCA and representing them in polar coordinates as Figure 4 (a-d). The strategy points of the baseline model are mostly close to red in color, indicating low consensus scores. Meanwhile, they are unevenly dis-tributed and located far from the cluster center, suggesting that the generated strategies are neither reliable nor diverse. Compared to the baseline, the strategy points after Stage 1 exhibit an overall improvement in consensus. However, their angular distribution remains concentrated, indicating limited diversity. For Stage 2 with the Naive Reward, most strategy points are close to green and form several highly dense clusters that cover only a small angular range. This 8Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision 

reflects that the model repeatedly generates homogenized and overly simplistic strategies (e.g., simple format checks) to maximize consensus scores, which leads to a significant loss of diversity. In contrast, under the Geometric Reward, strategy points are uniformly distributed along the polar angle while maintaining high reliability (i.e., smaller radial distances). This demonstrates that the geometric reward successfully encourages the Verifier to explore orthogonal verification perspectives, thereby constructing a more robust evaluation system. 

Quantitative Dynamics of Reward Components. Beyond qualitative visualization, we analyze the quantitative metrics in Figure 4(e-g). The Naive Reward model achieves high consistency ( rcon ) but at the severe cost of diversity ( rdiv ). In contrast, the Geometric Reward mechanism achieves a high level of diversity and reliability while also maintaining a decent consistency score. This reveals that our geometric reward acts as a structural regularizer. By penalizing an-gular redundancy in the latent space, it prevents the model from falling into local optima where the Solver and Verifier prefer simple reasoning trajectories. The balanced improve-ment across consistency, reliability, and diversity is the key driver behind Sci-CoE’s superior generalization on complex scientific questions. 

## 5. Conclusion 

We present Sci-CoE, a scientific co-evolving framework that improves LLMs’ scientific reasoning under minimal supervision. A key insight is that verification strategies form a structured and learnable space, whose reliability and diversity can be encouraged through geometric modeling. Experimental results demonstrate that Sci-CoE improves reasoning accuracy and robustness, and scales effectively to large unlabeled data. We acknowledge the following lim-itations. Due to a limited budget, we only trained models with up to eight-billion parameters. Additionally, Sci-CoE currently relies on an external judging model to execute verification strategies, which introduces additional compu-tational cost and potential bias. We believe Sci-CoE rep-resents a meaningful step toward self-evolving scientific reasoning systems and opens new directions for learning reliable reasoning without supervision. 

## Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 

## References 

Bai, L., Cai, Z., Cao, Y., Cao, M., Cao, W., Chen, C., Chen, H., Chen, K., Chen, P., Chen, Y., et al. Intern-s1: A scientific multimodal foundation model. arXiv preprint arXiv:2508.15763 , 2025. de Haan, T., Ting, Y.-S., Ghosal, T., Nguyen, T. D., Acco-mazzi, A., Wells, A., Ramachandra, N., Pan, R., and Sun, Z. Astromlab 3: achieving gpt-4o level performance in as-tronomy with a specialized 8b-parameter large language model. arXiv preprint arXiv:2411.09012 , 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv e-prints , pp. arXiv–2407, 2024. Fallahpour, A., Magnuson, A., Gupta, P., Ma, S., Naimer, J., Shah, A., Duan, H., Ibrahim, O., Goodarzi, H., Maddison, C. J., et al. Bioreason: Incentivizing multimodal biolog-ical reasoning within a dna-llm model. arXiv preprint arXiv:2505.23579 , 2025. Fan, R.-Z., Wang, Z., and Liu, P. Megascience: Pushing the frontiers of post-training datasets for science reasoning. 

arXiv preprint arXiv:2507.16812 , 2025. Fang, W., Liu, S., Zhou, Y., Zhang, K., Zheng, T., Chen, K., Song, M., and Tao, D. Serl: Self-play reinforcement learn-ing for large language models with limited data. arXiv preprint arXiv:2505.20347 , 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-centivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. Hu, M., Ma, C., Li, W., Xu, W., Wu, J., Hu, J., Li, T., Zhuang, G., Liu, J., Lu, Y., et al. A survey of scientific large language models: From data foundations to agent frontiers. arXiv preprint arXiv:2508.21148 , 2025. Huang, C., Yu, W., Wang, X., Zhang, H., Li, Z., Li, R., Huang, J., Mi, H., and Yu, D. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004 , 2025a. Huang, X., Franke, G., Yang, Z., Bai, J., Bai, W., Bi, J., Ding, Z., Duan, Y., Fan, C., Fan, W., et al. Loong: Syn-thesize long chain-of-thoughts at scale through verifiers. 

arXiv preprint arXiv:2509.03059 , 2025b. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825 .9Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision 

Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th sym-posium on operating systems principles , pp. 611–626, 2023. Lai, Y., Zhong, J., Li, M., Zhao, S., Li, Y., Psounis, K., and Yang, X. Med-r1: Reinforcement learning for generaliz-able medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939 , 2025. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. 

Hugging Face repository , 13(9):9, 2024. Li, J., Zhang, D., Wang, X., Hao, Z., Lei, J., Tan, Q., Zhou, C., Liu, W., Yang, Y., Xiong, X., et al. Chemvlm: Explor-ing the power of multimodal large language models in chemistry area. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 39, pp. 415–423, 2025. Lin, Z., Shen, S., Shang, J., Weston, J., and Nie, Y. Learning to solve and verify: A self-play framework for code and test generation. arXiv preprint arXiv:2502.14948 , 2025. Mistral. Mathstral. https://mistral.ai/news/ mathstral/ , 2023. Accessed: 2024-09-23. MistralAI. Ministral model card, 2024. URL 

https://huggingface.co/mistralai/ Ministral-8B-Instruct-2410 .Qiu, W., Huang, Z., Hu, H., Feng, A., Yan, Y., and Ying, R. Mindllm: A subject-agnostic and versatile model for fmri-to-text decoding. arXiv preprint arXiv:2502.15786 ,2025. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling , 2024. Saikh, T., Ghosal, T., Mittal, A., Ekbal, A., and Bhat-tacharyya, P. Scienceqa: A novel resource for question answering on scholarly articles. International Journal on Digital Libraries , 23(3):289–301, 2022. Tan, Q., Zhou, D., Xia, P., Liu, W., Ouyang, W., Bai, L., Li, Y., and Fu, T. Chemmllm: Chemical multimodal large language model. arXiv preprint arXiv:2505.16326 , 2025. Team, N., Zhang, B., Feng, S., Yan, X., Yuan, J., Yu, Z., He, X., Huang, S., Hou, S., Nie, Z., et al. Novelseek: When agent becomes the scientist–building closed-loop system from hypothesis to verification. arXiv preprint arXiv:2505.16938 , 2025. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: A more robust and challenging multi-task language un-derstanding benchmark. Advances in Neural Information Processing Systems , 37:95266–95290, 2024. Wang, Y., Tang, C., Deng, H., Xiao, J., Liu, J., Wu, J., Yao, J., Li, P., Su, E., Wang, L., et al. Scireasoner: Laying the scientific reasoning ground across disciplines. arXiv preprint arXiv:2509.21320 , 2025a. Wang, Y., Yang, L., Tian, Y., Shen, K., and Wang, M. Co-evolving llm coder and unit tester via reinforcement learn-ing. arXiv preprint arXiv:2506.03136 , 2025b. Xu, X., Xu, Q., Xiao, T., Chen, T., Yan, Y., Zhang, J., Diao, S., Yang, C., and Wang, Y. Ugphysics: A comprehen-sive benchmark for undergraduate physics reasoning with large language models. arXiv preprint arXiv:2502.00334 ,2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 , 2025. Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Wang, G., Li, H., Zhu, J., Chen, J., et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652 , 2024. Yu, W., Liang, Z., Huang, C., Panaganti, K., Fang, T., Mi, H., and Yu, D. Guided self-evolving llms with minimal human supervision. arXiv preprint arXiv:2512.02472 ,2025. Zhang, D., Liu, W., Tan, Q., Chen, J., Yan, H., Yan, Y., Li, J., Huang, W., Yue, X., Ouyang, W., et al. Chem-llm: A chemical large language model. arXiv preprint arXiv:2402.06852 , 2024. Zhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie, P., Yang, A., Liu, D., Lin, J., et al. Qwen3 embed-ding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176 ,2025. Zhao, A., Wu, Y., Yue, Y., Wu, T., Xu, Q., Lin, M., Wang, S., Wu, Q., Zheng, Z., and Huang, G. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335 , 2025. Zheng, L., Guha, N., Anderson, B. R., Henderson, P., and Ho, D. E. When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings. In Proceedings of the eighteenth international conference on artificial intelligence and law , pp. 159–168, 2021. 10 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision 

## A. Appendix 

A.1. Training Data                   

> Table 5. Training Data Composition of Different Scales. The third column, Disciplines, represents the subset composition of disciplines in MegaScience.
> Scale MegaScience Disciplines NuminaMath Other
> 4k 3k Phy 1k, Bio 1k, Chem 1k 1k –18k 13k Phy 5k, Bio 2k, Chem 2k, Med 1k, Math 1k, CS 1k, Eco 1k 5k –30k 23k Phy 5k, Bio 4k, Chem 4k, Med 4k, Math 2k, CS 2k, Eco 2k 5k ScienceQA 1k, CaseHold 1k

A.2. Experiment Details 

At each sampling step during reinforcement learning, we generate rollouts for solutions and verification strategies using vLLM (Kwon et al., 2023) . 

Training Stages. Sci-CoE is trained in two stages: • Anchored Learning: trained for 300 optimization steps using sparse labeled data. • Unsupervised Co-evolution: – 18k-scale data setting: trained for 300 optimization steps. 

– 30k-scale data setting: trained for 500 optimization steps. 

Optimization. We adopt Proximal Policy Optimization (PPO) for joint Solver–Verifier training with the following settings: • Optimizer learning rate: 1 × 10 −6

• PPO updates per step: 1 • Training epochs per update: 1 • KL regularization enabled with coefficient 0.01 

• KL estimator: K3 estimator 

Sampling Configuration. At each optimization step, we sample scientific questions and generate multiple Solver and Verifier trajectories: • Number of sampled questions per step: 100 • Number of Solver rollouts per question: 10 • Number of Verifier rollouts per question: 10 • Sampling temperature: 1.0 

A.3. Prompt 

This is the prompt for solution generation: Solver Prompt                    

> You are ahelpful assistant help user solve problems. Please reason step by step, and put your final answer within \\boxed{}.

11 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision 

This is the problem you need to solve:{{problem}} 

This is the prompt for verification strategy generation: Verifier Prompt 

You are an intelligent assistant specialized in designing an effective verification strategy for various scientific problems. Given a problem, your task is NOT to solve the problem yourself or provide the final answer, but to generate ONE high-level verification strategy to check the correctness and quality of the provided solution. This is the problem:{{problem}} The strategy should aim to be: 1. Specific: Clearly define the input and expected output for one test scenario. 2. Actionable: Clearly describe how to perform the verification. 3. Discriminating: Capable of identifying subtle errors or confirming robust correctness. Before providing the strategy, you MUST think step-by-step about why the strategy is useful and how it can reveal potential flaws or confirm correctness. Finally, after generating the strategy and thinking thoroughly, you MUST output the strategy in the following format: **Strategy Type:**\n ``` (strategy type here) ``` \n\n**Strategy Description:**\n\n(A detailed, natural language description of the strategy design here.)\n The structure of the strategy requires the following: Consider reverse calculations, alternative solution methods, step-by-step logical checks, simplification, or specific mathematical property validations, etc. Think about checking final answer, checking units, applying fundamental laws, verifying against known principles, or consistency with expected experimental outcomes, etc. The plan should describe the logic for checking the solution, for example: 1. How to parse the input solution content. 2. What specific property, calculation, or logic to check./What specific theorem is used... 3. What the expected outcome of the check is. - The strategy type examples: boundary_test/core_functionality_test/answer_check/re ⌋

verse_calculation/step_check/unit_check/property_validation/... Crucially, your description must be in natural language only. DO NOT include any Python code. 

This is the prompt for ground-truth test judgment: Ground-truth Test Prompt 

You are a teacher specialized in evaluating solutions for scientific problems. Ineed you to judge whether the student's answer is correct given the ground truth answer. This is the problem:{{problem}} This is the reference correct solution of this problem: {{reference_solution}} This is a generated answer of this problem: {{solution}} Your task is to assess whether the student's answer captures the same meaning as the reference answer, even when expressed with different wording or format. Your tasks include: A. Identify Mathematical or Notational Equivalence: Pay special attention to any LaTeX expressions in both answers. Confirm that the mathematical relationships, variables, and operations conveyed are equivalent. 

12 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision 

B. Consider Physiccal Equivalence: Pay special attention to transferring the units of both answers and equivalent variables given in the problem description. Feel free to ignore some physical constants appropriately. **Output ONLY "True" if the solution is correct, otherwise output "False".** Do not include any other text or explanation. 

This is the prompt for solution-strategy judgment: Juding Model Prompt 

You are an expert reviewer specialized in evaluating solutions for scientific problems. This is the problem: {{problem}} This is a generated solution of this problem: {{solution}} This is a verification strategy design for this problem: strategy type: {{strategy_type}} strategy description: {{strategy_details}} Your task is to carefully review the solution based on the provided verification strategy and determine if the solution is correct. You must follow the steps or criteria of the strategy, instead of verifier the solution by yourself. Output ONLY "True" if the solution is correct according to the verification strategy, otherwise output "False". Do not include any other text or explanation. 

A.4. Case Study 

We show examples for Sci-CoE-7B as follows: Scientific Problem A benzene-toluene mixture with 40 mole% benzene is to be separated in a distillation column to produce a top product with 90 mole% benzene and a bottom product with ≤ 10 mole% benzene. The feed enters at its boiling point, and the column operates with a reflux ratio of 3 kmol/kmol product. Determine the number of theoretical plates required and the feed plate location. Generated verification strategy 1 

Strategy type: Step Check 

Strategy description: To effectively verify the correctness of the solution for the benzene-toluene distillation column problem, we break down the solution into several key steps. Here is a detailed approach: 1. H OW TO PARSE THE INPUT SOLUTION CONTENT :Verify input details: mole fractions in feed ( zF = 0 .40 ), top product ( xD = 0 .90 ), bottom product ( xW ≤ 0.10 ), and reflux ratio ( R = 3 ). 2. W HAT SPECIFIC PROPERTY , C ALCULATION , OR LOGIC TO CHECK :

Step 1: Feed Composition: Verify the benzene mole fraction is 0.40 .

Step 2: Product Specifications: Ensure xD = 0 .90 and xW ≤ 0.10 .

Step 3: Reflux Ratio: Confirm R = 3 kmol/kmol product. 

Step 4: Number of Theoretical Plates: Use the Fenske-Underwood-Gilliland (FUG) method or McCabe-Thiele plot to verify the count. 

Step 5: Feed Plate Location: Verify consistency using the Kirkbride equation or graphical McCabe-Thiele analysis. 3. W HAT THE EXPECTED OUTCOME OF THE CHECK IS :

Expected Mole Fractions: 0.90 (Top), ≤ 0.10 (Bottom). 

Expected Reflux Ratio: R = 3 .13 Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision 

Expected Number of Plates: Should fall within the calculated range for the given specifications. 

Expected Feed Plate Location: Must optimize the balance between vapor and liquid flow rates. This detailed step-check strategy ensures that all aspects of the distillation column design are thoroughly verified against fundamental principles. 

Generated verification strategy 2 

Strategy type: Property Validation 

Strategy description: To validate the correctness of the solution for the benzene-toluene separation problem, we will perform a property validation check focusing on the theoretical plate calculations and feed plate location. This strategy aims to ensure that the solution adheres to the principles of distillation tower design and separation throughput. 1. H OW TO PARSE THE INPUT SOLUTION CONTENT :The problem states that the feed contains 40 mole% benzene, and the column operates with a reflux ratio of 3 kmol/kmol product. The top product should have 90 mole% benzene, and the bottom product should have ≤ 10 

mole% benzene. The feed enters at its boiling point. 2. W HAT SPECIFIC PROPERTY , C ALCULATION , OR LOGIC TO CHECK :Verify the theoretical plate calculations by checking the number of theoretical plates required. Ensure the feed plate location meets the given operating conditions for the distillation column. 3. E XPECTED OUTCOME OF THE CHECK :The number of theoretical plates required should be consistent with the McCabe-Thiele method or balance equations. The location of the feed plate should be consistent with the feed rate and the composition of the feed. 

Step 1. Determine the Number of Theoretical Plates: 

Use the McCabe-Thiele method or a more advanced simulation tool to calculate the number of theoretical plates required. Check if the calculated number of plates matches the expected value based on the given reflux ratio and specified product compositions. Ensure that the top product has 90 mole% benzene and the bottom product has 

≤ 10 mole% benzene. 

Step 2. Verify the Feed Plate Location: 

Identify the feed plate by checking how the feed composition changes as it is distributed throughout the column. Ensure that the feed plate is positioned such that the top product is rich in benzene and the bottom product is rich in toluene. Verify that the feed plate location matches the feed composition of 40 mole% benzene. 

Step 3. Boundary Test: 

Double-check the case where the feed percentage of benzene is at the lower limit (40 mole%). Verify that the column operates correctly with the specified reflux ratio and product specifications. 

Step 4. Cross-Check with Alternative Methods: 

Use a different thermodynamic model or software (e.g., Aspen Plus, Distemod) to verify the theoretical plates and feed plate location. Compare the results from the alternative method with the solution provided to ensure consistency. By following this strategy, any potential flaws or discrepancies in the solution can be identified, ensuring that the final answer is robust and correct. 

14