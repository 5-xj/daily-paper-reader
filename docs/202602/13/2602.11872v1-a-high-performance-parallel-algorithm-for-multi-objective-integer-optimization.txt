Title: A High-Performance Parallel Algorithm for Multi-Objective Integer Optimization

URL Source: https://arxiv.org/pdf/2602.11872v1

Published Time: Fri, 13 Feb 2026 01:56:37 GMT

Number of Pages: 38

Markdown Content:
# A High-Performance Parallel Algorithm for Multi-Objective Integer Optimization 

# Kathrin Prinz ∗, Levin Nemesch, and Stefan Ruzika RPTU Kaiserslautern-Landau, Germany 

∗Corresponding author, prinz.kathrin@math.rptu.de February 13, 2026 

Multi-objective integer optimization problems are hard to solve, mainly because the number of nondominated images is often extremely large. We present the first exact algorithm, called PEA, that fully utilizes the multi-core architecture of modern hardware. By exploiting the structure of the parameter set of the underlying scalarization, PEA can use a high number of threads while avoiding the usual pitfalls of parallel computing. It is highly scalable and easy to implement. As a result, PEA can solve much larger in-stances than previous state-of-the-art algorithms. Besides, PEA has a sound theoretical foundation. Unlike other existing parallel algorithms, it always solves the same number of scalarization problems as comparable sequential algorithms. We demonstrate the potential of PEA in a computational study. 

# 1 Introduction 

Many industrial problems involve multiple, often conflicting objectives. Such problems can be modeled as multi-objective optimization problems , thus, optimizing all objectives simultaneously. As a result, there is no single optimal solution, but a whole set of so-called Pareto optimal or efficient solutions. The corresponding images under the objective functions are called nondominated . Consequently, ”solving“ a multi-objective optimization problem requires computing the entire nondominated set YN and imposes a huge computational burden: Most such problems are known to be intractable , i. e., the number of nondominated images is exponential in the instances size. State-of-the-art exact algorithms for multi-objective integer optimization problems work in the objective space and iteratively transform the problem into a series of single-objective problems via scalarizations , cf. [4, 6, 22]. This makes it possible to use highly 1

> arXiv:2602.11872v1 [math.OC] 12 Feb 2026

efficient single-objective problem solvers such as Gurobi [10] and CPLEX [13]. With very few exceptions, such algorithms are sequential. However, modern processors are not designed for purely sequential computing any-more: It is hard to find a processor, even in handheld devices, that does not incorporate multiple cores that can execute operations in parallel. So far, multi-objective opti-mization barely takes advantage of this potential. Meanwhile, the common practice of solving many single-objective scalarization problems already provides a good basis for parallelization. Why not enumerate the scalarization problems in a way that allows them to be solved independently and in parallel? Unfortunately, conventional algorithms such as in [6, 22] are not well-suited for this: They require complex data structures to avoid solving unnecessary scalarization problems. Thus, the scalarization problems cannot be solved independently and solving them in parallel requires costly information sharing between threads. We propose a shift in paradigm: Algorithm designs for multi-objective optimization should be inherently suited for parallel computing. We demonstrate that it is possible to design such a parallel algorithm for integer problems — without the need for com-plex data structures and with independently running scalarization problems. Threads can operate largely autonomously and without much communication. Hereby, a guided search in the parameter space of a suitable scalarization rather than in the objective space achieves superior performance. 

1.1 Literature 

We briefly review relevant literature on exact algorithms for multi-objective integer prob-lems. For a more extensive general overview of exact algorithms for multi-objective (mixed-)integer problems, we refer to the recent survey by Halffmann et al. [12]. Many methods to compute the entire nondominated set for multi-objective integer optimization problems work either by restricting the search to the part of the image space that may contain undiscovered nondominated images, the so-called search region ,or by searching through the parameter space defined by a scalarization. One possible approach to do this is to remove dominated areas in the image space by introducing additional constraints and variables for each discovered nondominated im-age. Thus, a method relying on such a description needs to iteratively solve increasingly complex single-objective problems [18, 21, 28]. Another possibility is to describe the search region by so-called local upper bounds. Here, a set of bounds limits the search region. This set is iteratively refined with each new nondominated image that is discovered (cf. [7, 8, 17]). Upper bound based methods can be combined with different scalarization, see Tamby and Vanderpooten [29] and D¨ achert et al. [6]. Methods working in the parameter space include the Quadrant Shrinking Method by Boland et al. [3], the L-Shape Method by Boland et al. [2], the algorithm by Kirlik and Sayın [16] and the recursive algorithm by Ozlen et al. [22]. However, between all these approaches, methods that are specifically designed for par-allelization are rare: Dhaenens et al. [7], extend the algorithm for bi-objective problems 2by Lemesre et al. [20] to any number of objectives. First, the search space is divided into equal areas according to one objective, which can be searched in parallel. Then, the rectangles between neighboring images are explored. Turgut et al. [30] present an exact parallel objective space decomposition algorithm that exploits regional dominance rela-tions between decomposed partitions for pruning. The most competitive exact parallel method so far has been developed by Pettersson et al. [23]. They introduce a permuta-tion parallelization technique, whereby each thread starts generating the nondominated set using different permutations of the objective functions and shares generated bounds with the other threads. The practical viability of their method is demonstrated in a com-putational study. The most recent algorithm is the tri-objective Parallel Enumeration Algorithm by Ruzika and Prinz [25], which show superior performance but is limited to only three objectives. 

1.2 Our Contribution 

We extend the Parallel Enumeration Algorithm (PEA) for tri-objective optimization problems by Prinz and Ruzika [25] to any number of objectives 1. The resulting algo-rithm is able to distribute scalarization problems among tasks working in parallel. Each task is able to independently generate their own follow-up scalarization problems, and different tasks rarely need to communicate. Our algorithm is the first of its kind for any number of objectives and marks an important milestone for the development of new parallel algorithms in multi-objective optimization. Unlike existing parallel algorithms, now matter how many threads are used, the number of scalarization problems solved by PEA is always the same, that is, PEA never does any additional or unnecessary work. PEA does not require any complex data structures, and its implementation is straight-forward. In addition, PEA is the first parallel algorithm that solves the same number of scalarization problems as state-of-the-art sequential algorithms, which is in O(|YN |⌊ k 

> 2⌋

)for problems with k objective functions, cf. [17]. The approach of PEA can be seen as follows: The parameters which are needed for a lexicographic variant of the epsilon-constraint scalarizations throughout the algorithm can be arranged as a directed tree. Each parameter represents a vertex in the tree, and the arcs between them represent a partial order. Traversing such a tree leads to an intuitive parallelization, since the knowledge of the root of a subtree is sufficient to start the traversal of this subtree independently from the rest of the larger tree. This approach enables PEA to solve multi-objective integer optimization problems of unprecedented size in a short amount of time, which is demonstrated in a computational study. This paper is organized as follows: In Section 2, we introduce our notation, well-known results and basic concepts. Section 3 presents our results for the special case of images in general position. We relate these results to the existing literature on local upper bounds. Then, in Section 4, we generalize these results. Our novel algorithm is presented in Section 5. Finally, the efficacy of the parallelization achieved by PEA is 

> 1Note that parts of this work have also appeared in the PhD thesis by Prinz [24]

3demonstrated by the numerical results presented in Section 6. 

# 2 Preliminaries 

In this section, we introduce concepts from the field of multi-objective optimization that we use in this paper. For a more comprehensive introduction, please refer to the book by Ehrgott [9]. We consider multi-objective integer optimization problems min f (x) = ( f1(x), . . . , f k(x)) ⊤

s. t. x ∈ X (MOIP )with n ∈ N variables and feasible set X ⊆ Zn. The vector-valued objective function f

maps each feasible solution x ∈ X to its image f (x). The image set Y := {f (x) : x ∈

X} ⊆ Rk subsumes all possible images. The vector spaces Rn and Rk are called the 

decision space and the image space , respectively. We use the following notation: For any 

y ∈ Rk and i ∈ { 1, . . . , k }, let y−i be the projection of y onto Rk−1 that excludes the i-th component, i. e., y−i := ( y1, . . . , y i−1, y i+1 , . . . , y k). Furthermore, for any i ∈ { 1, . . . , k },we use [ i] := {1, . . . , i }, y[i] = ( y1, . . . , y i) and y−[i] := ( yi+1 , . . . , y k). For a, b ∈ Rk, we denote the multi-dimension intervals by [ a, b ], ( a, b ), etc. Since there is no canonical ordering in the image space Rk, we utilize component-wise orders to define optimality: For images y, ¯y ∈ Rk, the weak component-wise order , the 

component-wise order , and the strict component-wise order are defined by 

y ≦ ¯y if and only if yi ≤ ¯yi for all i ∈ [k],y ≤ ¯y if and only if yi ≤ ¯yi for all i ∈ [k] and y̸ = ¯ y, y < ¯y if and only if yi < ¯yi for all i ∈ [k],

respectively. Then, a feasible solution x∗ ∈ X is called efficient if there does not exist a feasible solution x ∈ X such that f (x) ≤ f (x∗). The corresponding image y∗ = f (x∗) is called nondominated . The set of efficient solutions is called the efficient set and denoted by XE . The set of nondominated images is called the nondominated set and denoted by 

YN . In the following, we only consider MOIP instances with finite nondominated set. 

2.1 The Lexicographic Epsilon-Constraint Scalarization 

In the epsilon-constraint scalarization, one of the objectives is minimized while all others are bounded from above and turned into constraints. It was first introduced by Haimes et al. [11]. Optimal solutions of epsilon-constraint scalarization problems are not guaranteed to be efficient (they are only guaranteed to be weakly efficient , cf. Ehrgott [9]). In order to obtain efficient solutions of MOIP , we employ the lexicographic epsilon-constraint scalarization : For a given vector ε ∈ Rk−1, the corresponding lexicographic epsilon-constraint scalarization Π( ε) is defined as 4lex min (fk(x), f k−1(x), . . . , f 1(x)) s. t. f−k(x) < ε, x ∈ X. 

(Π( ε)) We allow εi = ∞ for i ∈ [k − 1] to indicate that the corresponding objective is uncon-strained. For clarity, we fix the ordering of the objectives. However, all results hold true for any ordering. We refer to the image y ∈ YN of an optimal solution x ∈ XE of Π( ε) as optimal for Π( ε). For the remainder of this paper, we assume that we have a black-box solver for Π( ε) at hand. If appropriate, such a black-box solver can be an inte-ger programming solver like CPLEX [13] or Gurobi [10]. Note that the strict inequality constraints f−k(x) < ε can be simulated by the constraints f−k(x) ≦ ε − (δ, . . . , δ ) for some δ > 0. Such a δ is guaranteed to exist, since the nondominated set is required to be finite. Furthermore, δ = 1 is a viable choice if the objective functions assume only integer values. It is well-known (cf. Laumanns et al. [19]) that for lexicographic epsilon-constraint scalarization problems the optimal image is always uniquely optimal. In addition, for every nondominated image y ∈ YN , there exists an ε ∈ Rk−1 such that y is optimal for Π( ε). Therefore, it is possible to obtain all nondominated images by repeatedly solving lexicographic epsilon-constraint scalarization problems for different values of ε.It is important to note that solving a lexicographic epsilon-constraint scalarization problem may require more effort than solving an epsilon-constraint scalarization prob-lem. Therefore, the scalarization problems that are solved by PEA might take longer than the ones of similar methods. However, the benefit from the parallelization of PEA makes up for this additional effort. 

2.2 Epsilon-Components 

In this section, we introduce epsilon-components , cf. [25], that we use to define an or-der on the parameters of the lexicographic epsilon-constraint scalarization: Similarly to weight set components for weighted sum scalarization problems (cf. Przybylski et al. [26]), we associate each nondominated image y with the set of all parameters ε ∈ Rk−1

for which y is optimal for Π( ε). 

Definition 1. For a nondominated image y ∈ YN , the epsilon-component is defined as 

E(y) := 

(

ε ∈ Rk−1 : y = arg lex min 

> ¯y∈YN

{(¯ yk, . . . , ¯y1) : ¯ y−k < ε }

)

.

The following four-objective example illustrates this concept. 

Example 2. We consider f = id and 

X = Y =

n

y1 = (4 , 1, 2, 1) ⊤, y 2 = (2 , 4, 3, 2) ⊤, y 3 = (1 , 3, 4, 3) ⊤o

.

5Let us first consider the nondominated image y1: This image is only feasible for Π( ε) if 

y1 

> −4

< ε . Thus, 

E(y1) ⊆ ε ∈ R3 : y1 

> −4

< ε = ε ∈ R3 : (4 , 1, 2) < ε .

Furthermore, y1 has the smallest fourth objective function value among all nondominated images. Thus, if y1 is feasible for a lexicographic epsilon-constraint scalarization problem, then it is also optimal. Therefore, it holds that 

E(y1) = ε ∈ R3 : (4 , 1, 2) < ε .

Similarly, y2 is only feasible for Π( ε) if y2 

> −4

< ε . However, y1 has a lower fourth objective function value than y2, i. e., y14 < y 24 . Consequently, if y1 is feasible, then y2 is not optimal. Thus, 

E(y2) = ε ∈ R3 : y2 

> −4

< ε \E(y1)= ε ∈ R3 : (2 , 4, 3) < ε ∩ ε ∈ R3 : (4 , 1, 2) ≮ ε .

Analogously, for any y ∈ YN , we get 

E(y) = 

n

ε ∈ Rk−1 : y−k < ε 

o

\ [  

> ¯y∈YN
> (¯ yk,..., ¯y1)<lex (yk,...,y 1)

E(¯ y). (1) The epsilon-components from this example are depicted in Figure 1. We observe the following structural properties of epsilon-components. Similar prop-erties (though in a slightly different context) are also given by Kirlik and Sayın [16], among others. We restate the properties here in the context of epsilon-components and include a proof for the sake of completeness. We use cl() to denote the closure of a set. 

Lemma 3. Let y ∈ YN and ε ∈ E(y). Then, the following holds: (i) For all ε∗ ∈ (y−k, ε ], it holds that ε∗ ∈ E(y).(ii) For all ε∗ ∈ [y−k, ε ], it holds that ε∗ ∈ cl( E(y)) .Proof. 

(i) It holds that y−k < ε ∗. Thus, y is feasible for Π( ε∗). Furthermore, if some y∗ ∈ Y

is feasible for Π( ε∗), then it is also feasible for Π( ε). Therefore, ε ∈ E(y) implies (yk, . . . , y 1) <lex (y∗

> k

, . . . , y ∗ 

> 1

). Hence, any image feasible for Π( ε∗) has a worse lexicographic objective function value than y. Therefore, it is ε∗ ∈ E(y). (ii) For each m ∈ N, we define ε(m) component-wise by 

ε(m) 

> i

:= 

(

ε∗ 

> i

+ 1 

> m

, if ε∗ 

> i

= yi,ε∗ 

> i

, else , for i ∈ [k − 1] .

Then, for sufficiently large m′ it holds that ε(m′) ∈ (y−k, ε ]. Therefore, by (i), we obtain ε(m) ∈ E(y) for all m ≥ m′. It is lim m→∞ ε(m) = ε∗, and, thus, 

ε∗ is an accumulation point of the sequence ( ε(m))m∈N. Therefore, it holds that 

ε∗ ∈ cl( E(y)). 61 2 4 1

34

2

3

4

y1

> −4

y2

> −4

y3

> −4

f1 f2

> f3

Figure 1: A visualization of Example 2: The projected nondominated images in R3 and their epsilon-components. 

2.3 Viable Combinations 

Next, we describe the set of all parameters for which we solve lexicographic epsilon-constraint scalarization problems to compute the nondominated set. Hereby, each pa-rameter is defined by k − 1 images. Furthermore, we use so-called dummy images to represent unconstrained objectives. 

Definition 4. For t ∈ [k], the t-th dummy image is given by 

dti := 

(

∞, i = t

−∞ , i̸ = t , for all i ∈ [k].

The dummy images can also be described by sufficiently small and large values, which is particularly useful for visualization purposes. The following observations are only for illustrative purposes and to give the reader an intuition. Thus, they are provided without proof. Combining Equation (1) and Lemma 3, we get that, for each y ∈ YN , there exist a certain number α(y) ∈ N of parameters ε(i) ∈ (R ∪ {∞} )k−1 such that E(y) can be written as 

E(y) = 

> α(y)

[

> i=1

(y−k, ε (i)].

Additionally, for each ε(i) and j ∈ [k − 1], there exists an image ¯ y ∈ YN ∪ { dj } such that ε(i) 

> j

= ¯ yj . Thus, each of these parameters can be defined by k − 1 images. Such an 7image ¯ y has a better lexicographic objective function value than y and an “adjacent” epsilon-components, i. e., ε(i) ∈ cl( E(¯ y)) ∩ E(y)̸ = ∅. Moreover, if we have found all the parameters ε(i) that describe the epsilon-components and solved a lexicographic epsilon-constraint scalarization problem for each, then we have explored the entire parameter space. That is, for each ε ∈ (R∪{∞} )k−1 we know which nondominated image is optimal for Π( ε). To summarize, we have a finite set of parameters we want to enumerate and for each such parameter ε there exist k − 1 images (nondominated or dummy) y1, . . . , y k−1 with 

ε ∈ Ei(yi) := cl( E(yi)) : εi = yii

for i ∈ [k − 1]. For any ε in such a set Ei(yi), it holds that the image yi itself is not feasible for Π( ε). Thus, solving the corresponding scalarization problem yields another nondominated im-age. The set of parameters PEA enumerates is given as follows. 

Definition 5. Let Y1, . . . , Yk−1 ∈ YN ∪ { d1, . . . , d k−1}. Then, Y = ( Y1, . . . , Yk−1) is a 

viable combination of YN , if k−1\

> i=1

Ei(Yi)̸ = ∅.

Each viable combination defines a viable parameter 

ε(Y) := 



Y11 , . . . , Yk−1

> k−1



.

We denote the set of all viable combinations of YN by V(YN ). For each viable combination Y it holds that that Tk−1 

> i=1

Ei(Yi) = {ε(Y)}, since any 

ε ∈ Tk−1 

> i=1

Ei(Yi) has k − 1 fixed components. We illustrate the concept of viable combinations and how they describe the parameter space/epsilon-components in the following example. 

Example 6. Consider the image set of Example 2. We use the dummy images 

d1 = (5 , 0, 0, 0) ⊤, d 2 = (0 , 5, 0, 0) ⊤, d 3 = (0 , 0, 5, 0) ⊤ and d4 = (0 , 0, 0, 5) ⊤.

Additionally, we extend the concept of epsilon-components (cf. Definition 1) and also refer to epsilon-components of dummy images. Hereby, E(d4) represents all parameters for which the lexicographic epsilon-constraint scalarization is infeasible. There are eleven viable combinations with viable parameters as depicted in Figure 2. Furthermore, we have 

E(y1) = ( y1

> −4

, ε (d1, d 2, d 3)] ,E(y2) = ( y1

> −4

, ε (y1, d 2, d 3)] and 

E(y3) = ( y3

> −4

, ε (y2, d 2, d 3)] ∪ (y3

> −4

, ε (y1, y 2, d 3)] .

8E(d4) can be described by the remaining seven viable parameters. Therefore, we need the viable combinations ( d1, d 2, d 3), ( y1, d 2, d 3), ( y2, d 2, d 3) and ( y1, y 2, d 3) to describe the epsilon-components of all nondominated images and the remaining seven viable combinations to describe E(d4). 050505 × ×

××

×

×

× d4

> −4

d1

> −4

d2

> −4

d3

> −4

f1 f2

> f3

Figure 2: The projected nondominated images and dummy images as well as their epsilon-components of Example 2 in R3. All viable parameters are marked. The four viable parameters that describe the epsilon-component of a nondom-inated image are marked bigger and in their respective color. The other seven parameters describe the epsilon-component of the dummy image d4, i. e., the part of the parameter space for which the respective epsilon-constraint scalar-ization problems are infeasible. So far we have only given an intuition why viable combinations describe the entire parameter space. We give the formal proofs and show how V(YN ) can be enumerated in Section 3 and Section 4. In Example 6, each viable combination describes a different viable parameter. This is because the image set is in general position , i. e., for all i ∈ [k] and y, ¯y ∈ YN with y̸ = ¯ y,it holds that yi̸ = ¯ yi. Furthermore, each viable parameter is necessary to describe one epsilon-component. This is not the case if YN is not in general position. In that case, it might happen that there are two viable combinations Y, Z ∈ V (YN ) with Y̸ = Z, but 

ε(Y) = ε(Z). Therefore, we first consider the simpler special case. 93 Viable Combinations under General Position 

In this section, we study properties of viable combinations and how they can be used to compute the nondominated set under the following assumption. 

Assumption 1. YN is in general position. This is a rather restrictive assumption in multi-objective optimization, but it makes the following proofs less technical. We show in Section 4 that similar results still hold, when YN is not in general position. 

Remark 7. Since we assume that YN is in general position, for two nondominated images y, ¯y, it holds that 

(yk, . . . , y 1) <lex (¯ yk, . . . , ¯y1) if and only if yk < ¯yk.

Therefore, a nondominated image y that is feasible for some Π( ε) is also optimal if no nondominated image ¯y with ¯yk < y k is feasible. Furthermore, no nondominated images share any objective function value. Therefore, for all i ∈ [k] it is yi ≯ ¯yi if and only if yi < ¯yi.

The theoretical results presented in this section form the foundation for the multi-objective PEA. The approach can be outlined as follows: We define a partial order on the set of all viable combinations such that it induces a directed tree. Each vertex represents a viable combination/parameter. Then, the nondominated image that is optimal for the corresponding lexicographic epsilon-constraint problem is used to explore outgoing arcs and obtain new viable combinations. We show that, for every nondominated image, there exists a viable combination such that it is optimal for the corresponding lexicographic epsilon-constraint problem. Consequently, enumerating all viable combinations and solving a lexicographic epsilon-constraint problem for each is sufficient to compute the entire nondominated set. Fur-thermore, under the general position assumption, we show that the parameters defined by viable combinations are projections of local upper bounds, cf. Klamroth et al. [17]. Therefore, the number of viable combinations is in O(|YN |⌊ k 

> 2⌋

) (as the number of local upper bounds is known to be in O(|YN |⌊ k 

> 2⌋

)). In the following, δ > 0 is the same δ that can be used to turn strict inequality constraints in the definition of Π( ε) into non-strict inequalities, i. e., 

δ < min  

> y, ¯y∈YN
> y̸=¯ y

min  

> i∈[k]

|yi − ¯yi|.

We show two conditions that hold for any viable combination and the optimal image of the corresponding scalarization problem. Afterwards, we use them to define an order on the set of viable combinations. The first condition gives a criterion that any viable combination must satisfy. The second condition states, that the k-th objective function value of the optimal image is strictly larger than the k-th objective function value of all images that appear in the viable combination. 10 Corollary 8. Let Y be a viable combination. Then, the following holds: (i) For all i, j ∈ [k − 1] with i̸ = j, it holds that Yji < Yii .(ii) Let y∗ be the optimal image of Π( ε(Y)) . Then, for any i ∈ [k − 1] , it holds that 

y∗ 

> k

> Yik.Proof. 

(i) Suppose there exist i, j ∈ [k − 1] with i̸ = j such that Yji > Yii . We show that this implies that ε(Y) /∈ Ej (Yj ) which contradicts that Y is a viable combination. It holds that Yj is not feasible for Π( ε∗), for any ε∗ ∈ Bδ(ε(Y)). This holds because for any such ε∗, it is Yji > Yii + δ = ε(Y)i + δ ≥ ε∗ 

> i

. Thus, Bδ(ε(Y)) ∩ E(Yj ) = ∅,which contradicts ε(Y) ∈ Ej (Yj ). Bδ(ε(Y)) denotes the δ-neighborhood of ε(Y). (ii) For ε∗ ∈ Bδ(ε(Y)) it holds that y∗ is feasible for Π( ε∗). Thus, for any i ∈ [k − 1] with y∗ 

> k

< Yik, we have Bδ(ε(Y)) ∩ E(Yi) = ∅ which contradicts ε(Y) ∈ Ei(Yi). We now define an order on the set of viable combinations. The idea is the follow-ing: Given a viable combination Y ∈ V (YN ), we solve a lexicographic epsilon-constraint scalarization and obtain a new nondominated image y∗. Then, we only use this in-formation to generate new viable combinations, i. e., information of Y1, . . . , Yk−1 and 

y∗. Therefore, we generate new viable combinations by replacing any of the Yi with 

y∗. The resulting image combination ( Y1, . . . , Yℓ−1, y ∗, Yℓ+1 , . . . , Yk) cannot be a viable combination if Corollary 8.(i) does not hold. Therefore, we require y∗ 

> ℓ

> Yiℓ for all i̸ = ℓ.

Definition 9. Let Y ∈ V (YN ) and let y∗ be the optimal image of Π( ε(Y)). Then, for every ℓ ∈ [k − 1], we call the combination ( Y1, . . . , Yℓ−1, y ∗, Yℓ+1 , . . . , Yk−1) the ℓ-th scion of Y if, for all i ∈ [k − 1] \{ ℓ}, it holds that y∗ 

> ℓ

> Yiℓ . We denote the ℓ-th scion by scion ℓ(Y). Conversely, we call Y a precursor of scion ℓ(Y). In view of Definition 9, the necessary condition in Corollary 8.(i) is also sufficient for generating new viable combinations. 

Theorem 10. Let Y ∈ V (YN ). Then, for all ℓ ∈ [k − 1] , it holds that the ℓ-th scion, if it exists, is a viable combination. Proof. In the following, y∗ is the optimal image of Π( ε(Y)). To show that the ℓ-th scion is a viable combination, we need to show that it is 

\

> i∈[k−1] \{ ℓ}

Ei(Yi) ∩ Eℓ(y∗)̸ = ∅.

To this end, we show that 

ε∗ := 



Y11 , . . . , Yℓ−1 

> ℓ−1

, y ∗ 

> ℓ

, Yℓ+1  

> ℓ+1

, . . . , Yk−1

> k−1



∈ Ei(Yi)11 for any i ∈ [k − 1] \{ ℓ} by contradiction. Showing that ε∗ ∈ Eℓ(y∗) works analogously. Suppose ε∗ /∈ Ei(Yi). Then, it is Bδ(ε∗) ∩ E(Yi) = ∅. In particular, it is 

εi := 



Y11 , . . . , Yℓ−1 

> ℓ−1

, y ∗ 

> ℓ

, Yℓ+1  

> ℓ+1

, . . . , Yk−1

> k−1



+ δe i /∈ E(Yi)where ei denotes the i-th unit vector. By Corollary 8.(i) and since y∗ 

> ℓ

> Yiℓ , Yi is feasible for Π( εi). Since it is feasible but not optimal, there exists a ˆ y with ˆ yk < Yik that is feasible for Π( εi). In addition, any image y̸ = Yi that is feasible for Π( εi) is also feasible for Π( ε(Y)), i. e., ˆ y is feasible for Π( ε(Y)). By Corollary 8.(ii), it holds that ˆ yik < Yik < y ∗

> k

. This contradicts that y∗ is optimal for Π( ε(Y)). Consequently, ε ∈ Ei(Yi). Thus, it is ( Y1, . . . , Yℓ−1, y ∗, Yℓ+1 , . . . , Yk−1) ∈ V (YN ).

We can use the same approach as in the proof of Theorem 24 for statements of the following form: 

Corollary 11. Let ε ∈ (R ∪ {∞} )k−1 such that there are images y1, . . . , y k−1 ∈ YN ∪{d1, . . . , d k−1} with εi = yii for all i ∈ [k − 1] . Let i ∈ [k − 1] such that yij < y jj for all 

j ∈ [k − 1] \{ i}, as is the case for a viable combination. Then, it holds that ε / ∈ Ei(yi)

implies that there exists a y∗ ∈ YN that is optimal for Π( ε) with y∗ 

> k

< y ik.

Next, we show that the scion order induces a directed tree that contains all viable combinations. The root of this tree is ( d1, . . . , d k−1). Therefore, we show that each viable combination, with the exception of ( d1, . . . , d k−1), has exactly one precursor. The idea of the proof is the following: For a viable combination Z ∈ V (YN ), we search for another viable combination Y ∈ V (YN ) with Z = scion ℓ(Y) for some ℓ ∈ [k − 1]. This means that the viable combinations Y and Z share k−2 images, i. e., for all i ∈ [k−1] \{ ℓ}

it holds that Zi = Yi. In addition, Zℓ is the optimal image of Π( ε(Y)). Therefore, by Corollary 8.(ii), it holds that Zℓk > Yik = Zik. That is, to find a precursor of Z, we find the image with largest k-th objective function value and swap it for another image. Hereby, the other image can be found by “shooting” a ray in direction eℓ until it “hits” the epsilon-component of an image. It is guaranteed to hit exactly one as E(dℓ) lies in that direction ( Zℓ̸ = dℓ since Z̸ = ( d1, . . . , d k) and dℓℓ = ∞) and the nondominated set is in general position. 

Theorem 12. Let Z ∈ V (Y ) with Z̸ = ( d1, . . . , d k−1). Then, it is 

n

Y ∈ V (YN ) : ∃ℓ ∈ [k − 1] with Z = scion ℓ(Y)

o

= 1 .

Proof. We start by showing that 

n

Y ∈ V (YN ) : ∃ℓ ∈ [k − 1] with Z = scion ℓ(Y)

o

≥ 1.

Let ℓ = arg max i∈[k−1] {Z ik} and 

y(Z) = arg min  

> y∈YN∪{ dℓ}

{yℓ : ( Z11 , . . . , y ℓ, . . . , Zk−1 

> k−1

) ∈ Eℓ(y) and yℓ > Zℓℓ )}.

12 Note that y(Z) is well defined since dℓ satisfies (Z11 , . . . , d ℓ, . . . , Zk−1 

> k−1

) ∈ Eℓ(dℓ) and dℓℓ > Zℓℓ .

Furthermore, as YN is in general position, the minimum is unique. We show that 

Y = ( Z1, . . . , Zℓ−1, y (Z), Zℓ+1 , . . . , Zk−1)is a viable combination and Zℓ = scion ℓ(Y). Therefore, we must show that 1. Zℓ is the optimal image of Π( ε(Y)) and 2. Y is a viable combination. First, we show that Zℓ is the optimal image of Π( ε(Y)). For the sake of contradiction, suppose that it is not. Then, there exists a y∗ ∈ YN \{ Zℓ} that is optimal for Π( ε(Y)) instead. Hence, it holds that y∗ 

> ℓ

< y (Z)ℓ, y∗ 

> i

< Zii for all i ∈ [k − 1] \{ ℓ} and y∗ 

> k

< Zℓk.Consequently, ε(Z) ∈ Eℓ(Zℓ) (which holds because Z ∈ V (YN )) implies that y∗ 

> ℓ

> Zℓℓ .In addition, by Lemma 3.(ii), it holds that (Z11 , . . . , Zℓ−1 

> ℓ−1

, y ∗ 

> ℓ

, Zℓ+1  

> ℓ+1

. . . , Zk−1 

> k−1

) ∈ Eℓ(y∗)and, thus, 

y∗ ∈

n

y ∈ YN ∪ { dℓ} : ( Z11 , . . . , y ℓ, . . . , Zk−1 

> k−1

) ∈ Eℓ(y) and yℓ > Zℓℓ

o

.

This contradicts the choice of y(Z) since y∗ 

> ℓ

< y (Z)ℓ.Next, we show that Y is a viable combination. Suppose this is not the case. For y(Z), that ε(Y) ∈ Eℓ(y(Z)) follows directly from 

y(Z) ∈ { y ∈ YN ∪ { dℓ} : ( Z11 , . . . , y ℓ, . . . , Zk−1 

> k−1

) ∈ Eℓ(y) and yℓ > Zℓℓ )}.

Hence, there exists an i ∈ [k − 1] \{ ℓ} with ε(Y) /∈ Ei(Zi). Corollary 11 implies that there exists a y∗ ∈ YN with y∗ 

> k

< Zik < Zℓk that is feasible for Π( ε(Y)). Hence, Zℓ is not optimal which is a contradiction. Thus, it holds that Y is a viable combination and, by Definition 9, Z = scion ℓ(Y). It remains to be shown that 

n

Y ∈ V (YN ) : ∃ℓ ∈ [k − 1] with Z = scion ℓ(Y)

o

≤ 1.

Again, let ℓ = arg max i∈[k−1] {Z ik}. Then, by Corollary 8.(ii), for any precursor U of Z

and all i ∈ [k − 1] \{ ℓ}, it holds that Ui = Zi. Let 

W, U ∈ 

n

Y ∈ V (YN ) : ∃ℓ ∈ [k − 1] with Z = scion ℓ(Y)

o

.

Suppose W̸ = U, i. e., Wi = Ui = Zi for i ∈ [k − 1] \{ ℓ} and Wℓ̸ = Uℓ. W. l. o. g., 

Wℓℓ < Uℓℓ . But then, by Corollary 8, Wℓ is feasible for ε(U) which implies that Zℓ is not optimal. Hence, Z is not a scion of U which is a contradiction. 13 (d1, d 2, d 3)

(y1, d 2, d 3) (d1, y 1, d 3) (d1, d 2, y 1)

(y2, d 2, d 3) (y1, y 2, d 3) (y1, d 2, y 2)

(y3, d 2, d 3) (y2, d 2, y 3) (y1, y 3, d 3) (y1, y 2, y 3)

Figure 3: The tree as induced by the order described in Definition 9, i. e., G = ( V, A )where V = V(YN ) and A = {(Y, Z) : Z = scion ℓ(Y) for some ℓ ∈ [k − 1] }, for the nondominated set given in Example 2. Thus, the order as defined in Definition 9 induces a directed tree with viable com-binations as vertices and root ( d1, . . . , d k−1). The induced tree for the instance from Example 2 is visualized in Figure 3. We now show that for each nondominated image y, there exists at least one viable combination Y such that y is optimal for Π( ε(Y)). Thus, if we enumerate all viable combination and solve a lexicographic epsilon-constraint scalarization problem for each, we are guaranteed to compute the entire nondominated set. The proof of the following theorem is visualized in Figure 4. We start with a parameter for which we know that y is optimal and — figuratively speaking — iteratively shoot rays in the directions e1, . . . , e k−1. Each time, until we hit the epsilon component of an image. The respective images then constitute a viable combination Y(y) and y is optimal for Π( ε(Y)). In addition, for all i ∈ [k − 1], it holds that Y(y)i 

> −[i]

< y −[i]. We later show that Y(y) is the only viable combination that has y as optimal image and satisfies this. Therefore, this adds a nice criterion when to save nondominated images without ever having to check for duplicates. 

Theorem 13. Let y ∈ YN . Then, there exist Y(y)1, . . . , Y(y)k−1 ∈ YN ∪ { d1, . . . , d k−1}

such that the following hold: (i) For all i ∈ [k − 1] , it holds that Y(y)i 

> −[i]

< y −[i].(ii) The optimal image of Π( ε(Y(y))) is y.(iii) Y(y) = ( Y(y)1, . . . , Y(y)k−1) is a viable combination. Proof. We iteratively define images Y(y)1, . . . , Y(y)k−1: For i ∈ [k − 1], we define 

Li(y) := {¯y ∈ YN ∪ { di} : yi < ¯yi, (Y(y)11, ... , Y(y)i−1

> i−1

, ¯yi, y i+1 + δ, ... , y k−1 + δ) ∈ Ei(¯ y)}

14 5

5 ε(Y(y4)) 

f1

f2

E(y1)

E(y2)

E(y3)

E(y4)

Figure 4: We consider a tri-objective problem for which the nondominated set is given by 

YN = y1 = (5 , 4, 1) ⊤, y 2 = (2 , 6, 2) ⊤, y 3 = (6 , 2, 4) ⊤, y 4 = (3 , 3, 5) ⊤ . Then, the construction of Y(y4) as in the proof of Theorem 13 can be interpreted as follows: Starting with ( y41 + δ, y 42 + δ), we shoot a ray in direction (1 , 0) until we “hit” the first epsilon-component, E(y3). Then, we “shoot” another ray in direction (0 , 1) until we hit another epsilon component, E(y1). Thus, 

Y(y4) = ( y3, y 1). and we set 

Y(y)i := arg min 

> ¯y∈L i(y)

{¯yi}.

For all i ∈ [k − 1] it holds that di ∈ L i(y). Thus, all Y(y)i are well defined and, since we assume that YN is in general position, also unique. In addition, from the construction of the Y(y)i it follows that Y(y)ij < Y(y)jj for all i, j ∈ [k − 1] with j̸ = i.(i) If Y(y)ij > y j for some i < j ≤ k, then (Y(y)11, . . . , Y(y)ii, y i+1 + δ, . . . , y k−1 + δ) /∈ Ei(Y(y)i)which contradicts Y(y)i ∈ L i(y). (ii) By construction, y is feasible for Π( ε(Y(y))). Suppose it is not optimal. Thus, there exists a y∗ ∈ YN with y∗ 

> i

< Y(y)ii for all i ∈ [k − 1] and y∗ 

> k

< y k. Furthermore, as 

y ∈ YN , there exists J ⊆ [k − 1] with y∗ 

> j

> y j for all j ∈ J. Let j = max J. Then, we have that ε(Y(y)) ∈ E(y∗) and (Y(y)11, . . . , Y(y)j−1

> j−1

, y ∗ 

> j

, y j+1 + δ, . . . , y k−1 + δ) ∈ [y∗, ε (Y(y)] .

Therefore, by Lemma 3.(ii), it is (Y(y)11, . . . , Y(y)j−1

> j−1

, y ∗ 

> j

, y j+1 + δ, . . . , y k−1 + δ) ∈ Ej (y∗)15 and y∗ ∈ L j (y) which contradicts the choice of Y(y)j .(iii) Suppose Y(y) is not a viable combination. Thus, for some j ∈ [k − 1], it holds that ε(Y(y)) /∈ Ej (Y(y)j ). Hence, by Corollary 11, there exists a y∗ ∈ YN with 

y∗ 

> k

< Y(y)jk and y∗ 

> i

< Y(y)ii for all i ∈ [k − 1] that is optimal for Π( ε(Y(y))). Thus, by (ii), it holds that y = y∗. But this implies that (Y(y)11, . . . , Y(y)jj , y j+1 + δ, . . . , y k−1 + δ) /∈ Ej (Y(y)j )since y is feasible for Π( ε) for all 

ε ∈ B δ

> 2

(( Y(y)11, . . . , Y(y)jj , y j+1 + δ, . . . , y k−1 + δ)) and yk = y∗ 

> k

< Y(y)jk. Thus, Y(y)j /∈ L j (y) which is a contradiction. We show that Y(y) is the only viable combination that satisfies Condition (i) and (ii) of Lemma 3. 

Corollary 14. Let y ∈ YN . Then, there exists exactly one Y ∈ V (YN ) such that y is optimal for Π( ε(Y)) and Yi 

> −[i]

< y −[i] for all i ∈ [k − 1] .Proof. Let Y(y) be the viable combination constructed in the proof of Theorem 13. We have already shown that y is optimal for Π( ε(Y(y))) and that Y(y)i 

> −[i]

< y −[i] for all 

i ∈ [k − 1]. Let Z be a viable combination such that y is optimal for Π( ε(Z)) and 

Zi 

> −[i]

< y −[i] for all i ∈ [k − 1]. We show that Zi = Y(y)i for all i ∈ [k − 1] by induction. 

• i = 1: We have that ( Z11 , y 2 + δ, . . . , y k−1 + δ) ∈ [Z1

> −k

, ε (Z)]. Thus, Lemma 3.(ii) states that ( Z11 , y 2 + δ, . . . , y k−1 + δ) ∈ E1(Z1). Therefore, Z1 ∈ L 1(y). Suppose 

Z1̸ = Y(y)1. Thus, Z11 > Y(y)11. Since Y(y)1 

> −1

< y −1 and y is feasible for Π( ε(Z)), it holds that Y(y)1 is feasible as well. Hence, as yk > Y(y)1 

> k

by Corollary 8.(ii), y

cannot be optimal which is a contradiction. Thus, it holds that Y(y)1 = Z1.

• i > 1: By induction, for all j < i it holds that Zj = Y(y)j . Furthermore, since Z

is a viable combination it holds that Zij < Zjj = Y(y)jj . Thus, (Y(y)1, . . . , Y(y)i−1, Zii , y i+1 + δ, . . . , y k−1 + δ) ∈ [Zi

> −k

, ε (Z)] .

Consequently, again by Lemma 3.(ii), it is Zi ∈ L i(y). Combining this with Zi̸ =

Y(y)i implies Y(y)ii < Zii . Thus, since Yi 

> −[i]

< y −[i], Y(y)i is feasible for Π( ε(Z)) which, again, contradicts that y is optimal. Thus, it holds that Zi = Y(y)i.16 Therefore, we already have the basic outline of PEA: We start with the viable com-bination ( d1, . . . , d k−1). Then, we solve the corresponding scalarization, generate scions and repeat. We only store optimal images if Corollary 14 is satisfied. Thus, each non-dominated image is stored exactly once, and we do not need to check for duplicates. What remains to be shown is that the cardinality of the set of all viable combinations is in O(|YN |⌊ k 

> 2⌋

). To this end, we link the set of viable combinations to the upper bound set of YN .

3.1 Viable Combinations and Upper Bounds 

In this section, we explore the connection between viable combinations and the repre-sentation of the search region - the subset of the image space that potentially contains further nondominated images, by so-called upper bounds, first introduced by Przybylski et al. [27]. Formally, for a set N ⊆ YN , the search region is given by 

S(N ) := 

n

z ∈ Rk : y≦̸ z for all y ∈ N

o

.

Then, local upper bounds that describe the search region can be defined as follows. 

Definition 15 (Klamroth et al. [17]) . Let N ⊆ Y be a finite set of images and u ∈ Rk

satisfy the following: (i) There is no y ∈ N such that y < u .(ii) There exist k points of N ∪ { d1, . . . , d k}, denoted by y1(u), . . . , y k(u), such that (

yii (u) = ui

yi

> −i

(u) < u −i

, i = 1 , . . . , k. 

Then, u is a local upper bound with respect to N . The set of all local upper bounds with respect to N is denoted by U (N ). The images y1(u), . . . , y k(u) are called the defining points of u.Every local upper bound u ∈ U (N ) defines a search zone C(u) ⊆ Rk:

C(u) = {z ∈ Rk : z < u }

and the search region can be written as 

S(N ) = [  

> u∈U(N)

C(u).

An upper bound based method to compute the nondominated set works as follows. Starting with N = ∅, such a method iteratively picks a local upper bound u ∈ U (N )and explores the associated search zone C(u) with an appropriate scalarization problem. Then, if a new nondominated image y is found, the updated local upper bound set 

U (N ∪ { y}) is computed and y is added to N . Otherwise, C(u) is empty. Hereby, 17 the update step is of crucial importance. Klamroth et al. [17] propose two different update strategies, one based on redundancy elimination and the other one based on redundancy avoidance. The latter uses defining points: When a new nondominated image y is identified u is replaced by new local upper bounds that are generated by verifying Definition 15.(ii). The search zones given by upper bounds in U (N ) intersect. Hence, there might be another u′ ∈ U (N ) such that y ∈ C(u′). In this case, u′ /∈

U (N ∪ { y}) and it needs to be replaced as well. D¨ achert et al. [8] propose the following neighborhood structure among local upper bound to efficiently identify the u′ ∈ U (N ) with y ∈ C(u′). 

Definition 16 (D¨ achert et al. [8]) . Let u, u ′ ∈ U (N ). Then, u and u′ are neighbors if they share k − 1 defining points, i. e., there exist ℓ, j ∈ [k] with ℓ̸ = j such that for all 

i ∈ [k]\{ ℓ, j } it holds that yi(u) = yi(u′) and yj (u) = yℓ(u′). Then, u is the j-neighbor of u′ and u′ is the ℓ-neighbor of u.The literature on computational geometry offers some insight on the cardinality of 

U (YN ): From Boissonnant et al. [1] and Bringmann [5] it can be derived that |U (YN )| ∈ O(|YN |⌊ k 

> 2⌋

). We show that we can map viable combinations onto U (YN ) via an injective function g. This provides insight into the cardinality of V(YN ) and relates the scion order to the order described in Definition 16. 

Theorem 17. Let Y ∈ V (YN ) and let Yk be the optimal image of Π( ε(Y)) . Then, 

(Y11 , . . . , Ykk ) ∈ U (YN ).Proof. Any y < u := ( Y11 , . . . , Ykk ) would be feasible for Π( ε(Y)) with yk < Ykk which contradicts that Yk is optimal for Π( ε(Y)). Furthermore, by Corollary 8, Y1, . . . , Yk

satisfy (

Yii = ui

Yi 

> −i

< u −i

, i = 1 , . . . , k. 

Thus, by Definition 15, u ∈ U (YN ). Theorem 17 implies that the function 

g : V(YN ) → U (YN ), Y 7 → (Y11 , . . . , Ykk ),

where Yk is the optimal image of Π( ε(Y)), is injective. Therefore, |V (YN )| ≤ | U (YN )|

and |V (YN )| ∈ O (|YN |⌊ k 

> 2⌋

). In addition, by Theorem 17, the viable parameters are pro-jections of the local upper bounds w. r. t. YN and the corresponding viable combination corresponds to the defining points y1(u), . . . , y k−1(u). Furthermore, for two viable com-binations Y, Z it holds that Y is the ℓ-th scion of Z, if and only if g(Y) is the ℓ-neighbor of g(Z) and g(Z) is the k-neighbor of g(Y). Thus, the tree induced by the scion order is equivalent to a subgraph of the graph induced by neighborhood structure among local upper bound w. r. t. Definition 16. However, while D¨ achert et al. [8] iteratively update the neighborhood structure and utilize it to efficiently update the search region, PEA directly enumerates the nondomi-nated images according to a subtree of the graph induced by the neighborhood structure 18 of the local upper bounds set of the entire nondominated set, i. e., the neighborhood structure once all nondominated images have already been found. In addition, while the generation of new local upper bounds and new viable combi-nations is similar there is a key difference: Upper bound based methods use parameters in Rk to solve scalarization problems. Meanwhile, PEA works with parameters in Rk−1.Let u be a local upper bound w. r. t. some N ⊆ YN . When a new image y is discovered in C(u) it means that no other nondominated image ¯ y with ¯ y ≥ y exists. Thus, no more scalarization problems need to be solved for any parameter u′ > y . That is, all 

u′ ∈ U (N ) with u′ > y need to be identified and replaced with local upper bounds w. r. t 

N ∪ { y}. In contrast, let Y be a viable combination of YN . When PEA solves Π( ε(Y)) and discovers a nondominated image it is not necessarily new, i. e., it might have been computed before. In addition, it only follows that y is optimal for all parameters in (y−k, ε (Y)]. That is, for all other viable combinations, solving a scalarization problem for the respective viable parameter might still yield a new nondominated images and PEA only generates new viable combinations and does not replace any. Thus, working with viable combinations is more suited for parallelization, or at least, requires less communication between different threads: The viable combinations can be processed independently while the local upper bounds cannot. 

# 4 Viable Combinations in the Generic Case 

In Section 3, we need the property that the nondominated set is in general position. However, while this assumption greatly helps in simplifying technical proofs, it is not a necessary condition for PEA. In this section, we consider the case where the nondomi-nated set YN is not in general position. The idea is the following: We construct a related nondominated set Φ( YN ) that is in general position. Hereby, Φ : YN → Rk is a function that slightly modifies each image. Since Φ( YN ) is in general position, all results from Section 3 hold. For each viable combination Φ( Y) of Φ( YN ), we consider the preimage 

Y. We show that it is a viable combination of YN and an image y is optimal for Π( ε(Y)) if and only if Φ( y) is optimal for Π( ε(Φ( Y))). Thus, since the enumeration of all viable combinations Φ( YN ) and solving a scalarization problem is sufficient to compute Φ( YN ), it is also sufficient to enumerate the preimages to compute YN .The set Φ( YN ) is only a means to an end and only used as a tool for the proofs in this section. It can be constructed as follows: Let K := |YN |. We assume that the nondominated images are ordered such that (y1

> k

, . . . , y 11 ) <lex (y2

> k

, . . . , y 21 ) <lex . . . < lex (yKk , . . . , y K 

> 1

). (2) We define Φ( YN ) := {Φ( y1), . . . , Φ( yK )} where for all i ∈ [K] and j ∈ [k] we set Φ( yi)j := 

(

yij + iδ K+1 , if there exists a s < i with ysj = yij ,yij , else. Under Φ, all nondominated images remain nondominated and the order as given in Equation (2) is preserved. 19 Proposition 18. For all y, ¯y ∈ YN with y̸ = ¯ y the following hold: (i) Φ( y) ≰ Φ(¯ y).(ii) (yk, . . . , y 1) <lex (¯ yk, . . . , ¯y1) implies (Φ( y)k, . . . , Φ( y)1) <lex (Φ(¯ y)k, . . . , Φ(¯ y)1).Proof. 

(i) Since both y and ¯ y are nondominated, there exists a j ∈ [k] with ¯ yj < y j . Then, Φ(¯ y)j ≤ ¯yj + Kδ K + 1 < ¯yj + δ < y j ≤ Φ( y)j .

Thus, it is Φ( y) ≰ Φ(¯ y). (ii) By construction, there exist s, t ∈ [K] with s < m such that the following hold for all i ∈ [k]: 

• If yi = ¯ yi, it holds that Φ( y)i ≤ yi + sδ K + 1 < ¯yi + tδ K + 1 = Φ(¯ y)i.

• If yi < ¯yi, it holds that Φ( y)i ≤ yi + sδ K + 1 < ¯yi ≤ Φ(¯ y)i.

Specifically, it holds that Φ( y)k < Φ(¯ y)k. Thus, it is (Φ( y)k, . . . , Φ( y)1) <lex (Φ(¯ y)k, . . . , Φ(¯ y)1).

Additionally, viable combinations of Φ( Y) and viable combinations of YN are strongly related. More precisely, the preimage of a viable combination of Φ( YN ) is a viable combination of YN . To proof this, we require the following lemma. 

Lemma 19. Let Φ( Y) be a viable combination of Φ( YN ). Then, for all i, j ∈ [k − 1] , it holds that Yji ≤ Y ii .Proof. In case i = j, the statement obviously holds. Otherwise, since Φ( Y) is a viable combination, it holds that Φ( Yj )i < Φ( Yi)i (Corollary 8.(i)). Suppose it is Yii < Yji .Then, by the construction of Φ, it holds that Φ( Yi)i ≤ Y ii + Kδ K + 1 < Yji ≤ Φ( Yj )i

which is a contradiction. 20 Theorem 20. Let Φ( Y) be a viable combination of Φ( YN ). Then, Y is a viable combi-nation of YN .Proof. We show that ε(Y) ∈ E1(Y1). Showing the same for Y2, . . . , Yk−1 works analo-gously. For each m ∈ N and for all i ∈ [k − 1], we define an ε(m) by 

ε(m) 

> i

:= 

(

Yii , if ( Yik, . . . , Yi

> 1

) <lex (Y1 

> k

, . . . , Y11 )

Yii + δm , else. Then, it holds that lim m→∞ ε(m) = ε(Y). We show that ε(m) ∈ E(Y1) for all m ∈ N. To this end, we first prove that Y1 is feasible for Π( ε(m)), i. e., Y1 

> i

< ε (m) 

> i

for all i ∈ [k − 1]. By Lemma 19, it holds that Y1 

> i

≤ Y ii . Thus, we need to show that Y1 

> i

< Yii = ε(m) 

> i

for all i ∈ [k − 1] with ( Yik, . . . , Yi

> 1

) <lex (Y1 

> k

, . . . , Y11 ). Suppose this is not the case, i. e., 

Y1 

> i

= Yii . Then, for some s, r with s < r it holds that Φ( Yi)i ≤ Y ii + sδ K + 1 < Y1 

> i

+ rδ K + 1 = Φ( Y1)i.

But this contradicts Corollary 8.(i). Thus, Y1 

> i

< Yii for all i ∈ [k−1] with ( Yik, . . . , Yi

> 1

) <lex 

(Y1 

> k

, . . . , Y11 ) and Y1 is feasible for Π( ε(m)). It remains to be shown that Y1 is also optimal for Π( ε(m)) for all m ∈ N. Suppose this is not the case. Then, there exists a y∗ with ( y∗

> k

, . . . , y ∗ 

> 1

) <lex (Y1 

> k

, . . . , Y11 ) that is feasible for Π( ε(m)) for some m ∈ N. Thus, by the definition of δ, it already holds that y∗ is feasible for Π( ε(m)) for all m ∈ N., i. e., Y1 is not optimal for any. We show that this contradicts that ε(Φ( Y)) ∈ E1(Φ( Y1)), i. e., it contradicts that Φ( Y)is a viable combination of Φ( YN ). To this end, let ¯ ε(m) be any sequence in E(Φ( Y1)) with limit ε(Φ( Y)). We show that for all i ∈ [k − 1] and for sufficiently large m′, it holds that Φ( y∗)i < ¯ε(m′) 

> i

, i. e., Φ( y∗) is feasible for Π( ε(m′)). This then contradicts that ¯ε(m) is a sequence in E(Φ( Y1)) since (Φ( y∗)k, . . . , Φ( y∗)1) <lex (Φ( Y1)k, . . . , Φ( Y1)1) by Proposition 18.(ii). To this end, we consider two distinct cases: 1. First, we consider all i ∈ [k − 1] with (Yik, . . . , Yi

> 1

) <lex (Y1 

> k

, . . . , Y11 ).

For these, we have that y∗ 

> i

< ε (m) 

> i

= Yii and, thus, Φ( y∗)i + δK+1 < Φ( Yi)i.Since lim m→∞ ¯ε(m) = ε(Φ( Y)), it is lim m→∞ ε(m) 

> i

= Φ( Yi)i. Consequently, for sufficiently large m′, it holds that Φ( y∗)i < ¯ε(m′) 

> i

.

2. Next, we consider all i ∈ [k − 1] with (Y1 

> k

, . . . , Y11 ) <lex (Yik, . . . , Yi

> 1

).

For these, it holds that y∗ 

> i

< ε (m) = Yii + δm and, therefore, y∗ 

> i

< Yii and (y∗

> k

, . . . , y ∗ 

> 1

) <lex (Y1 

> k

, . . . , Y11 ) <lex (Yik, . . . , Yi

> 1

).

Thus, it holds that Φ( y∗)i < Φ( Yi)i and, consequently, for sufficiently large m′, it is Φ( y∗)i < ¯ε(m′) 

> i

.21 Therefore, we get that, for sufficiently large m′, Φ( y∗) is feasible for Π( ε(m′)). This contradicts that ¯ ε(m) is a sequence in E(Φ( Y1)). The reverse of Theorem 20 does not hold as the following example shows. 

Example 21. Consider a tri-objective integer optimization problem with 

Y = YN =

n

y1 = (4 , 3, 2) ⊤, y 2 = (4 , 2, 3) ⊤, y 3 = (2 , 3, 4) ⊤o

.

Then, for δ = 1, we have Φ( YN ) = 

n

Φ( y1) = (4 , 3, 2) ⊤, Φ( y2) = (4 .5, 2, 3) ⊤, Φ( y3) = (2 , 3.75 , 4) ⊤o

It holds that Y = ( y2, y 3) is a viable combination of YN . But, E1(Φ( y2)) ∩E2(Φ( y3)) = ∅.Thus, (Φ( y2), Φ( y3)) is not a viable combination of Φ( YN ). A visualization is given in Figure 5. 55

f1

f2

> (a)

5

5

f1

f2

E(y1)

E(Φ( y1)) 

E(y2)

E(Φ( y2)) 

E(y3)

E(Φ( y3)) 

> (b)

Figure 5: The left images depicts the epsilon-components of the nondominated set YN

of Example 21. All viable parameters are marked. The right image depicts the same but for the set Φ( YN ). The parameter that is marked in green in the left image is defined by three different viable combinations, ( y1, y 3), (y2, y 1)and ( y2, y 3). Only two of these are also viable combinations of Φ( YN ), (Φ( y1), Φ( y3)) and (Φ( y2), Φ( y1)). (Φ( y1), Φ( y3)) and (Φ( y2), Φ( y1)) define different viable parameters, both are marked in green in the right image. Since Φ( YN ) is in general position, it is sufficient to solve a scalarization problem for each viable combination of Φ( YN ) to compute it. Thus, as each nondominated imagine in YN has a corresponding nondominated image in Φ( YN ), it stands to reason that it is also sufficient to solve a scalarization problem for all viable combinations of YN that map to viable combinations of Φ( YN ). We call these true combinations .22 Definition 22. Let Y be a viable combination of YN . Then, Y is a true combination of 

YN if Φ( Y) is a viable combination of Φ( YN ). We denote the set of all true combinations of YN by T (YN ). True combinations of YN and the corresponding viable combinations of Φ( YN ) “share optimal images”. 

Lemma 23. Let Y be a true combination of YN . A nondominated image y is optimal for Π( ε(Y)) if and only if Φ( y) is optimal for Π( ε(Φ( Y))) .Proof. Follows directly from the construction of Φ( YN ). Since there is a one-to-one correspondence between T (YN ) and V(Φ( YN )) and optimal images coincide, we can define an order on T (YN ) analogous to Definition 9 and The-orem 10. Note that there are two significant differences: We define the order for true combinations and not viable combinations. In addition, we have strict inequalities in Definition 9 while here we only have less or equals. 

Theorem 24. Let Y be a true combination and let y∗ be the optimal image of Π( ε(Y)) .Then, for all ℓ ∈ [k − 1] such that y∗ 

> ℓ

≥ Y iℓ for all i ∈ [k − 1] with i̸ = ℓ, it holds that 

(Y1, . . . , Yℓ−1, y ∗, Yℓ+1 , . . . , Yk−1) is a true combination. We call (Y1, . . . , Yℓ−1, y ∗, Yℓ+1 , . . . , Yk−1) the ℓ-th scion of Y and denote it by scion ℓ(Y).Proof. By Lemma 23 we have that Φ( y∗) is optimal for the viable combination Φ( Y). Furthermore, by construction, y∗ 

> ℓ

≥ Y iℓ for all i ∈ [k − 1] with i̸ = ℓ if and only if Φ( y∗)ℓ > Φ( Yi)ℓ for all i = [ k − 1] with i̸ = ℓ. Thus, we can apply Theorem 10 and get that (Φ( Y1), . . . , Φ( Yℓ−1), Φ( y∗), Φ( Yℓ+1 ), . . . , Φ( Yk−1)) is a viable combination. Therefore, by Theorem 20, it holds that ( Y1, . . . , Yℓ−1, y ∗, Yℓ+1 , . . . , Yk−1) is a true combination. 

Lemma 25. Let Y, Z ∈ T (YN ). Then, Z is the ℓ-th scion of Y if and only if Φ( Z) is the ℓ-th scion of Φ( Y).Proof. Follows directly from the proof of Theorem 24. Lemma 25 allows us to transfer the properties of viable combinations and scions as stated in Section 3 to true combinations. Therefore, the scion order has the desired properties even if the the nondominated set is not in general position. 

Theorem 26. 

(i) For each Z ∈ T (YN ) with Z̸ = ( d1, . . . , d k−1), it holds that 

n

Y ∈ T (YN ) : ∃ℓ ∈ [k − 1] with Z = scion ℓ(Y)

o

= 1 .

(ii) For y ∈ YN there exists exactly one true combination Y(y) ∈ T (YN ) such that y is optimal for Π( ε(Y(y))) and for all i ∈ [k − 1] , it holds that Y(y)i 

> −[i]

≤ y−[i].

23 (iii) It holds that |T (YN )| ∈ O (|YN |⌊ k 

> 2⌋

).

Remark 27. If the nondominated set is not in general position, there is no one-to-one correspondence between local upper bounds, defining points and viable/true combinations. The mapping described in Theorem 17 only maps combinations to so-called quasi-upper bounds w. r. t. YN (cf. D¨ achert et al. [8]). In addition, upper bounds can have multiple defining points and not all correspond to viable/true combinations. 

# 5 Algorithm 

In this section, we formally describe PEA for computing the nondominated set of multi-objective integer optimization problems. The idea of PEA is as follows: First, we ini-tialize the dummy images. Then, we start with the true combination ( d1, . . . , d k−1), the root of the directed tree induced by the scion order as described in Theorem 24. Then, the directed tree is explored with depth-first search. A complete listing can be found in Algorithm 1. 

Algorithm 1: PEA 

input : Objective function f , feasible set X.

output: Nondominated set YN . 

> 1

function exploreSubtree( Y = ( Y1, . . . , Yk−1)): 

> 2

YN ← ∅ ; 

> 3

y∗ ← solveModel(Π( ε(Y)));  

> 4

if Yi 

> −[i]

≤ y∗−[i] for all i ∈ [k − 1] then  

> 5

YN ← YN ∪ { y∗}; 

> 6

end  

> 7

for j ∈ [k − 1] do  

> 8

if y∗ 

> j

≥ Y ij for all i ∈ [k − 1] \{ j} then  

> 9

YN ← YN ∪ exploreSubtree( Y1, . . . , Yj−1, y ∗, Yj+1 , . . . , Yk−1); 

> 10

end  

> 11

end  

> 12

return YN ; 

> 13

for i ∈ [k − 1] do  

> 14

dij ←

(

∞, i = j

−∞ , i̸ = j , for j ∈ [k];  

> 15

end  

> 16

YN ←exploreSubtree( d1, . . . , d k−1) ; 

> 17

return YN ;The correctness of Algorithm 1 follows directly from Section 4. By Theorem 26.(i), the scion order on the set of true combinations defines a tree. Algorithm 1 first calls the function exploreSubtree for the root ( d1, . . . , d k−1) of said tree in line 16, and solves 24 the corresponding lexicographic epsilon-constraint scalarization problem in line 3. We assume that solveModel invokes a black-box solver that correctly returns the optimal image or that the scalarization is infeasible. Then, Theorem 24 guarantees that all scions are identified in lines 7–11 and exploreSubtree is called recursively for each. Thus, Algorithm 1 goes through all true combinations. Furthermore, by Theorem 26.(ii), each nondominated image is added to YN in lines 4–6 exactly once. 

Theorem 28. PEA generates the entire nondominated set and solves O(|YN |⌊ k 

> 2⌋

) lexi-cographic epsilon-constraint scalarization problems. 

The true strength of PEA lies in its ability to be parallelized in a straight-forward way: Once called, exploreSubtree can work independently until the whole subtree is traversed. Hence, any call to exploreSubtree can be outsourced to a new thread, and this thread can then also schedule the tasks it generates in the recursion to different threads. The speed-up observed when parallelizing PEA is quite significant and scales nearly linear for many instances and numbers of threads, see Section 6. 

5.1 Improvements 

It is well known that integer programming solvers typically take longer to prove infea-sibility than solving a feasible problem. In addition, providing a feasible solution can speed up the resolution of problems. See also Boland et al. [2] and Tamby and Van-derpooten [29] for a discussion on both points. Therefore, in this section, we focus on both: We show how to avoid infeasible scalarization problems unless the multi-objective problem is already infeasible and how to provide a feasible solution for all but the very first scalarization problem. To this end, we show that there exists a subset of nondominated images so that for any lexicographic epsilon-constraint scalarization problem this set contains a feasible image if such an image exists. This set can be constructed by applying a permutation to the objectives functions and solving a related problem. This related problem uses lexicographic epsilon-constraint scalarization problem with all k objectives but minimizes them in a different order and constraints on k − 2 objectives. Since these scalarization problems can be seen as lexico-graphic epsilon-constraint scalarization problems for k − 1 objectives functions, we refer to the construction of the previously mentioned sets as a k − 1 dimensional problem. This procedure can be applied recursively until we end up with a single-objective problem with one optimal solution, which is then feasible for all scalarization problems needed to solve the two dimensional problem, etc. The aforementioned sets are the following: 

Definition 29. Let r ∈ [k − 1]. We define 

YN (k) := YN and YN (r) := y ∈ YN (r + 1) : ∄¯y ∈ YN with ¯ y[r] ≤ y[r] .

25 By definition, it holds that YN (1) ⊆ · · · ⊆ YN (k) = YN .In order to compute the sets YN (r), we permute the objective functions, i. e., we apply a permutation σ sigma on the objective functions and given a vector ε ∈ Rk−1 solve the following permuted lexicographic epsilon-constraint scalarization problem 

lex min (fσ(k)(x), f σ(k−1) (x), . . . , f σ(1) (x)) s. t. f−σ(k)(x) < ε, x ∈ X. 

(Π σ(ε)) For i ∈ [k], we use εσ(i) to access the constraint on the objective function fσ(i). All previous results and definitions still hold as the order on the objectives was arbitrary and just fixed for simpler notation in the first place. We adjust the notation as follows: We denote the epsilon-component of a nondominated image y by Eσ(YN ) and the set of all true combinations of YN by Tσ(YN ). Furthermore, a true combination Y ∈ T σ(YN )has the form Y = ( Yσ(1) , . . . , Yσ(k−1) ) where Yσ(i) 

> σ(i)

defines ε(Y)σ(i). We illustrate the sets defined in Definition 29 and how to compute them using permuted lexicographic epsilon-constraint scalarization problems in the following tri-objective example. 

Example 30. We consider f = id and 

X = Y =

n

y1 = (5 , 4, 2) ⊤, y 2 = (2 , 6, 3) ⊤, y 3 = (6 , 2, 4) ⊤, y 4 = (3 , 3, 5) ⊤,y5 = (2 , 5, 5) ⊤, y 6 = (5 , 2, 6) ⊤o

.

Then, it is YN (2) = y4, y 5, y 6 . We observe that for all ε ∈ R2 the lexicographic epsilon-constraint scalarization Π( ε) is feasible, if and only if there is a y ∈ YN (2) that is feasible. Hence, YN (2) can be used to avoid infeasible scalarization. Furthermore, since we are “ignoring” the third objective function, calculating YN (2) can be done by permuting the order on the objective functions and only adjusting the bound on one objective, i. e., solving a bi-objective problem (where we still need to take a lexicographic minimum of all objective functions). Consider the permutation σ = (3 , 1, 2) . Then, for each y ∈ YN (2) there exists an ε ∈ R2 with εσ(1) = ε3 = ∞ such that y is optimal for Π σ(ε). Additionally, all results from previous sections hold for any order on the objective functions. Hence, for each y, we can find a true combination Y ∈ T σ(YN ) with Yσ(1) = dσ(1) such that 

y is the optimal image. However, which permutation we use is important. Specifically 

f3 needs to have the lowest priority in the lexicographic minimization. Otherwise, for example for σ = (1 , 3, 2) and Y = ( d1, d 3), i. e., unbounded objectives, we have that y3

is optimal for Π σ(ε(Y)). Even though, y3[2] = (6 , 2) ≤ (5 , 2) = y6[2] and, thus, y3 /∈ YN (2). This procedure can be repeated recursively: We can use YN (1) = {y5} to avoid infeasible scalarizations when computing YN (2). Here, we “ignore” two objective functions, use the permutation σ = (3 , 2, 1) and do not vary any of the parameters. The epsilon-components w. r. t. to the different permutations are depicted in Figure 6. We now formalize the observations from Example 30. First, for all r ∈ [k] we show 26 5

5

f1

f2    

> (a) σ= (1 ,2,3)

5

5

f1

f3    

> (b) σ= (3 ,1,2)

5

5

f3

f2    

> (c) σ= (3 ,2,1)

Eσ(y1)

Eσ(y2)

Eσ(y3)

Eσ(y4)

Eσ(y5)

Eσ(y6)

YN (2) 

YN (1) 

Figure 6: The epsilon-components of the images of Example 30 for different permuta-tions. The epsilon-components of the images in YN (2) and YN (1) are high-lighted by different patterns. how YN (r) can be computed. To this end, we use the permutations σr given by 

σr(j) = 

(

k − j + 1 , if j ≤ k − rr − k + j, if j > k − r , for all j ∈ [k].

Hence, σr = ( k, k − 1, . . . , r + 1 , 1, 2, . . . r, ). Specifically, σk = id = (1 , . . . , k ) and 

σ1 = ( k, k − 1, . . . , 1). We now show that if we consider permuted problems with σr and leave k − r objective 27 functions unbounded, we can compute YN (r). We do this by applying Theorem 26. 

Theorem 31. Let y∗ ∈ YN and r ∈ [k]. Then, y∗ ∈ YN (r) if and only if there exists a true combination Y ∈ T σr (YN ) with (Yσr (1) , . . . , Yσr (k−r)) = ( dσr (1) , . . . , d σr (k−r)) such that y∗ is optimal for Πσr (ε(Y)) .Proof. Note that ( σr(1) , . . . , σ r(k − r)) = ( k, k − 1, . . . , r + 1). First, let y∗ ∈ YN (r) ⊆ YN . Thus, by Theorem 26, there exists a true combination 

Y(y∗) ∈ T σr (YN ) such that y∗ is optimal for Π σr (ε(Y∗)). Additionally, for i ∈ [k − 1] it holds that 

Y(y∗)σr (i)   

> σr(i+1) ,...,σ r(k)

≤ y∗  

> σr(i+1) ,...,σ r(k)

(this is the equivalent of Y(y∗)i 

> −[i]

≤ y∗−[i] under the permutation σr). Specifically, by the construction of σr, for i ≤ k − r we get that Y σr (i)[k−i] ≤ y∗

> [k−i]

. That means that 

Yσr (1) [k−1] ≤ y∗

> [k−1]

, . . . , Yσr (k−r)[r] ≤ y∗

> [r]

. Hence, since y∗ ∈ YN (r) ⊆ YN (r + 1) · · · ⊆ YN (k), it holds that Yσr (i) = dσr (i).Conversely, let Y ∈ T σr (YN ) with 



Yσr (1) , . . . , Yσr (k−r)

=



dσr (1) , . . . , d σr (k−r)

such that y∗ is optimal for Π σr (ε(Y)) be given. Suppose there exists a y ∈ YN with 

y[r] ≤ y∗

> [r]

. As the objectives r + 1 , . . . , k are unbounded, this means that y is also feasible for Π σr (ε(Y)). Furthermore, (yr, . . . , y 1, y r+1 , . . . , y k) <lex (y∗ 

> r

, . . . , y ∗ 

> 1

, y ∗

> r+1

, . . . , y ∗

> k

),

that is, (yσr (k), . . . , y σr (1) ) <lex (y∗ 

> σr(k)

, . . . , y ∗ 

> σr(1)

).

Hence, y∗ could not have been optimal for Π σr (ε(Y)) which is a contradiction. In addition, the existence of a y with y[i] ≤ y∗ 

> [i]

for some i > r would also contradict the optimality of y∗. Thus, y∗ ∈ YN (r). In the following, for r ∈ [k], we denote by ˆTσr (YN ) the set of all true combinations Y ∈ Tσr (YN ) with ( Yσr (1) , . . . , Yσr (k−r)) = ( dσr (1) , . . . , d σr (k−r)). Note, that it is ˆTσk (YN ) = 

T (YN ). We now show for every r = 2 , . . . , k and all Y ∈ ˆTσr (YN ) that the correspond-ing lexicographic epsilon-constraint scalarization Π σr (ε(Y)) is feasible if and only if a 

y ∈ YN (r − 1) is feasible. Hence, as by Theorem 31 it is sufficient to consider true combinations in ˆTσr (YN ) to compute YN (r), we can avoid infeasible scalarizations and obtain feasible solutions to speed up integer programming solvers by iteratively comput-ing YN (1) , . . . , Y N (k). 

Theorem 32. Let r = 2 , . . . , k and Y ∈ ˆTσr (YN ). Then, Πσr (ε(Y)) is feasible if and only if a y ∈ YN (r − 1) is feasible. 

28 Proof. Clearly, if a y ∈ YN (r − 1) is feasible for Π σr (ε(Y)), Π σr (ε(Y)) is feasible. Conversely, let Π σr (ε(Y)) be feasible. Then, by Theorem 31, the optimal image y∗ is an element of YN (r). Consequently, either y ∈ YN (r −1) or there exists a ¯ y ∈ YN (r −1) with ¯y[r−1] ≤ y[r−1] . Thus, since ε(Y)i = Yii = dii for all i > r , ¯ y is feasible for Π σr (ε(Y)). 

# 6 Computational Study 

To investigate the running time characteristics of PEA in practice, we conduct a small exploratory computational study. PEA is implemented in C++17 and compiled with gcc 14.2.1. For solving the lexico-graphic epsilon-constraint scalarization problems, CPLEX 22.1.1. is used. All CPLEX parameters are kept unchanged, except a lowered MIP tolerance to 10 −6 and limiting the number of threads for CPLEX to one. Furthermore, we use the oneTBB library [14] for the parallelization of PEA. We provide the code of our PEA implementation under gitlab.rhrk.uni-kl.de/xug28mot/pea-parallel-enumeration-algorithm. For building PEA with all necessary compiler flags, we also provide a cmake build script there. We compare PEA to an established parallelized algorithm and choose the algorithm AIRA from [23] as benchmark. However, issues arise in the implementation from [23] when using five or more objectives. Since a reimplementation is out of scope for our paper, we only compare PEA to AIRA on problems with four objectives. For a higher number of objectives, we only look at the running times of PEA and how these running times scale with the number of threads used. We do not compare PEA to sequential algorithms. For tri-objective problems, Prinz and Ruzika [25] already demonstrated that sequential algorithms cannot compete with PEA when multiple threads are available. To test PEA and AIRA, we use instances of the multi-objective knapsack problem (KP) and instances of multi-objective integer linear programs (ILP). For both classes of instances, we take the instance from the sets provided by Kirlik and Sayın [15]. The KP instances as well as the multi-objective assignment problem instances (AP) described in [15] are commonly used in computational studies for multi-objective algorithms, e. g., [2, 6, 16]. We do not use the AP instances in our study since the aforementioned past studies show that both KP and AP instances have similar running time behavior on a qualitative level. The number of objectives ranges from 4 to 10, with varying sizes (defined by number of variables) of the instances for each number of objectives. The number of variables ranges for the KP instances from 10 to 100, for the ILP instances with four objectives from 10 to 80, and for the ILP instances with five or more objectives from 10 to 50. Increments are in steps of size 10. For the ILP instances, the number of constraints is exactly half the number of variables. For each combination of problem, size and number of objectives, ten instances are used. Newly generated instances are used whenever the original instances sets from [15] do not contain instances of such size or number of objective, our generation scheme is identical to that in [15]. Our complete set of instances can be found under gitlab.rhrk.uni-kl.de/xug28mot/pea-parallel-enumeration-algorithm. 29 For each single instance, PEA and AIRA were run once for each number of threads τ

from the set {1, 4, 16 , 24 , 64 , 120 }, with the exception of AIRA for 64 and 120 threads on the four objective instances. This is because AIRA can only use at most k! threads. Therefore, for four objective instances, AIRA is limited to 24 threads. This is also why we use 24 and 120 threads, even though they are not powers of two, unlike the rest. They allow for a direct comparison between PEA and AIRA (ignoring the aforementioned issues with AIRA for five or more objectives). The experiments were run on a compute server with two AMD EPYC 9554 processors, each with 64 physical cores, and 1.51 terrabyte RAM, and Gentoo Linux as operating system. To explore a huge number of instance and number-of-threads combinations we imposed a rather strict time limit of 1/2 hours. Furthermore, we used the following logic to reduce the start of runs that would likely time out: Let the number of threads τ

that PEA/AIRA uses be fixed. Furthermore, for a fixed instance size n and number of objectives k, let every single run of PEA/AIRA with τ threads for instances of size n

and number of objectives k have timed out. In this case, we assumed that if we try to solve an instance of larger size or with more objectives with PEA/AIRA and τ threads, it would also time out. We directly skipped such constellations in the computational study and consider them as timed out in our analysis. To give an example, assume that for PEA with 4 threads all ILP instances with 10 variables and 5 objectives timed out. Then any ILP instance with at least 10 or more variables and at least 5 or more objectives was directly seen as timed out when using 4 threads. However, for all runs of PEA with a different number of threads than 4, this has no effect. 

6.1 Results 

To measure how the running times of PEA and AIRA scale with the number of threads, we cannot use typical measures such a speedup or efficiency . Those require the running time of the algorithms when using only a single thread as baseline. Due to the large sizes of most of the considered instances, no single threaded algorithm would terminate in a reasonable time, even with a much higher running time limit than 1/2 hours. Thus, we do not have the running times of PEA/AIRA with a single thread for many instances. Consequently, we consider the “inverse” of the speedup and define the slowdown as follows. For a fixed instance size, the slowdown measures the average running time of PEA or AIRA with a fixed number of threads relative to the baseline of PEA with 120 threads. More formally, let IA(n) be the set of all KP/ILP instances of size n that 

A ∈ { AIRA , PEA } finished within the running time limit when using using τ threads. The slowdown then is defined by 

sl A(n) := 1

|I A(n)|

X

> I∈I A(n)

tA(I)

tPEA120 (I) ,

where tA(I) is the running time of A with τ threads for instance I, and tPEA120 (I) is the running time of PEA with 120 threads for instance I. A value of sl A(n) = 2, for 30 Instance 1 thread 4 threads 16 threads 24 threads 64 threads 120 threads k n |YN | #P Time (s) Time (s) Time (s) Time (s) Time (s) Time (s) 4 10 11.6 43.4 0.14 0.09 0.11 0.12 0.17 0.29 20 136.8 663.5 10.71 2.92 1.2 1.08 1.12 1.26 30 397.6 2012.2 53.93 14.02 4.54 3.72 2.85 2.95 40 1808.6 9978.2 456.21 115.45 31.44 22.31 12.06 10.15 50 2881.1 16172.0 (9) 218.2 57.54 40.71 20.36 15.56 60 6393.8 36504.0 (5) (9) 168.1 114.84 51.96 37.54 70 15067.5 87501.0 - (5) (9) 385.32 160.82 109.97 80 25513.4 150422.8 - - (8) (9) 329.68 218.34 90 26235.3 151832.6 - - (9) 690.6 282.99 191.97 100 (82983.6) (498594.4) - - (1) (1) (5) (9) 5 10 16.2 120.6 0.44 0.17 0.17 0.18 0.24 0.34 20 161.2 1884.4 42.41 10.87 3.38 2.67 1.92 1.91 30 1058.7 16600.0 (9) 173.55 45.01 30.97 14.60 10.88 40 4278.4 77618.5 (3) (8) 335.01 225.60 94.31 63.74 50 9990.9 183462.4 - (3) (9) (9) 271.45 179.13 60 (28222.7) (576806.1) - - (1) (2) (6) (7) 70 (28961.5) (557116.0) - - (1) (3) (5) (6) 80 (28856.0) (518751.0) - - - - (1) (1) 90 (59402.0) (1113779.0) - - - - - (1) 6 10 19.7 256.6 1.44 0.45 0.29 0.29 0.35 0.47 20 300.7 11176.8 324.96 81.41 21.33 14.85 7.11 5.50 30 1927.2 92473.3 (3) (8) 337.12 225.85 92.51 61.21 40 (6920.3) (384086.4) - - (6) (6) (9) (9) 50 (16192.0) (935894.2) - - - - (3) (4) 60 (15844.0) (781350.0) - - - - (1) (1) 7 10 31.3 1230.7 13.99 3.62 1.18 0.94 0.72 0.78 20 459.5 38554.7 (7) 352.73 89.39 60.36 25.60 17.64 30 4557.7 839874.3 (1) (1) (2) (4) (8) 650.47 40 (7038.7) (1120506.0) - - - - (2) (3) 8 10 29.2 2130.2 34.38 8.73 2.45 1.79 1.11 1.05 20 573.6 223966.9 (4) (7) (9) (9) 178.98 115.80 30 (3379.3) (1188202.3) - - - (1) (2) (3) 9 10 39.0 28329.5 (9) 138.68 34.86 23.41 9.78 6.65 20 (691.3) (588866.3) - (2) (7) (7) (9) (9) 30 (472.0) (59723.0) - - - (1) (1) (1) 10 10 41.0 31658.8 (9) 196.30 49.23 33.15 13.82 9.36 20 (1108.4) (1543770.2) - - (1) (1) (5) (5) 

Table 1: Results of PEA on the KP instances. The instance size is denoted by n, and results are aggregated over all instances of a fixed combination of size and number of objectives. The columns |YN | and #P give the average number of nondominated images and scalarizations solved by PEA, respectively. If PEA could not solve an instance within the time limit with any number of threads, both columns are in brackets and give the averages over the remaining instances that could be solved. The columns for the running time give the average running time in seconds or, if the number is in brackets, the number of instances that could be solved within the time limit. 31 Instance 1 thread 4 threads 16 threads 24 threads 64 threads 120 threads k n |YN | #P Time (s) Time (s) Time (s) Time (s) Time (s) Time (s) 4 10 38.4 159.1 1.25 0.45 0.34 0.35 0.41 0.51 20 190.1 953.7 33.32 8.93 3.46 3.21 3.07 3.21 30 451.0 2457.2 287.19 73.47 21.82 18.16 13.1 12.77 40 571.6 3084.2 (8) (8) 222.14 156.5 106.7 107.73 5 10 189.0 2642.9 27.44 7.09 2.11 1.59 1.05 1.04 20 684.2 10968.3 (9) 160.96 43.08 29.19 14.17 12.11 30 (506.8) (7752.7) (9) (9) (9) (9) (9) (9) 40 (1449.3) (25291.3) (4) (5) (8) (8) (9) (9) 6 10 134.5 4220.4 48.21 12.36 3.61 2.71 1.72 1.62 20 1065.0 65453.9 (6) (9) (9) 199.64 83.6 57.7 30 (2007.9) (102662.5) (3) (5) (6) (6) (8) (8) 40 (4085.5) (275838.5) (1) (3) (5) (5) (5) (8) 7 10 402.7 54622.5 (8) 196.85 49.73 33.47 13.60 8.84 20 (2504.9) (577646.3) (2) (2) (5) (8) (9) (9) 30 (2467.9) (398181.7) (1) (3) (4) (4) (6) (7) 40 (2387.5) (331054.0) - - - - (2) (2) 8 10 544.8 274255.3 (6) (7) 292.17 194.27 75.09 45.4 20 (1705.2) (724967.0) - (1) (4) (4) (5) (6) 30 (1578.0) (634203.0) - - (1) (1) (3) (3) 40 (1046.5) (398243.5) - - - - (1) (2) 9 10 117.0 26342.1 477.89 119.22 30.66 20.86 8.97 6.13 20 (955.4) (1056189.8) - (1) (3) (3) (4) (5) 10 10 301.5 504702.5 (4) (7) (9) (9) 166.6 98.23 20 (107.0) (75623.0) - (1) (1) (1) (1) (1) 

Table 2: Results of PEA on the ILP instances. The instance size is denoted by n, and results are aggregated over all instances of a fixed combination of size and number of objectives. The columns |YN | and #P give the average number of nondominated images and scalarizations solved by PEA, respectively. If PEA could not solve an instance within the time limit with any number of threads, both columns are in brackets and give the averages over the remaining instances that could be solved. The columns for the running time give the average running time in seconds or, if the number is in brackets, the number of instances that could be solved within the time limit. 32 example, indicates that A with τ threads runs on average twice as long as PEA with 120 threads. We list the average absolute running times of PEA in Table 1 for KP instances and in Table 2 for ILP instances. For four objectives, the slowdown for PEA and AIRA is visualized in Figure 7, and for PEA for five and six objectives it is visualized in Figure 8. Note that the cardinality of the nondominated sets varies between instances, in particular for ILP instances. Interested readers can find the exact running times of PEA and AIRA for each instance under gitlab.rhrk.uni-kl.de/xug28mot/pea-parallel-enumeration-algorithm for a more comprehensive picture. We first discuss the results for the four objective instances. For all but the smallest instances, PEA with 120 threads consistently is the fastest combination of algorithm and number of threads. For any fixed number of threads τ , PEA with τ threads is faster than AIRA with τ threads, with the only exception being the single-threaded runs on the ILP instances. Although PEA does not scale completely proportional to the number of threads, for KP instances every increase in number of threads leads to a clear improvement in running time, and for ILP instances clear improvements show for at least 64 threads. Note that this is because the ILP instances have fewer nondominated images. Thus, less scalarization problems are solved and there is less for PEA to parallelize. In contrast, AIRA can not provide much improvement in running time above 16 threads on both instance sets. The results show that PEA outperforms AIRA both in the running time for fixed numbers of threads, and in scaling with the number of threads. For the instances with five or more objectives, we only discuss how the running time of PEA scales with the number of threads, since we do not have AIRA as benchmark algorithm to compare against here. Overall, PEA scales just as well as it does for four objectives, though more instances time out. In addition, for a high number of objectives, the number of scalarization problems PEA solves is high even for instances with few nondominated images. Therefore, we can observer clear improvements for up to 120 threads, even on instances with few nondominated images. Altogether, the results show that PEA can take advantage of high numbers of threads. For many instances, using 120 threads provides a clear improvement in running time. Compared to computational studies that exist in the literature (e.g. in [6, 23, 29]), we are able to solve far larger instances in reasonable time. To the best of our knowledge, for the largest of our instances, this is the first time that instances of this size are even considered in a computational study. 

# 7 Conclusions 

We introduced a new order for parameters of epsilon-constraint scalarizations. This order arranges the scalarizations in a directed tree. Traversing this tree is a new algorithmic approach to find all nondominated images of a multi-objective integer problem. With PEA, we presented the first algorithm using this approach. The computational study 33 10 20 30 40 50 60 70 80 90 100  

> size n
> 1
> 2
> 4
> 8
> 16
> 32
> 64
> 120
> sl A(n)

(a) KP 10 20 30 40 50 60 70 80  

> size n
> 1
> 2
> 4
> 8
> 16
> 32
> 64
> 120
> sl A(n)

(b) ILP (c) legend 

Figure 7: The slowdown of AIRA and PEA for KP and ILP instances with 4 objectives. The naming scheme PEA τ (or AIRA τ ) describes the combination of algorithm and number of threads τ . Note that the y-axes are scaled logarithmic. The shaded area shows the range of the slowdown over all instances that could be solved within the time limit. 34 10 20 30 40 50 60 70 80 90 100 

size n

> 1
> 2
> 4
> 8
> 16
> 32
> 64
> 120
> sl A(n)

(a) KP: k = 5 10 20 30 40 50 60 70 80 90 100 

size n 

> 1
> 2
> 4
> 8
> 16
> 32
> 64
> 120
> sl A(n)

(b) KP: k = 6 10 20 30 40 

size n

> 1
> 2
> 4
> 8
> 16
> 32
> 64
> 120
> sl A(n)

(c) ILP: k = 5 10 20 30 40 

size n 

> 1
> 2
> 4
> 8
> 16
> 32
> 64
> 120
> sl A(n)

(d) ILP: k = 6 

(e) Legend. 

Figure 8: The slowdown of AIRA and PEA for KP and ILP instances with 5 and 6 objectives. The naming scheme PEA τ (or AIRA τ ) describes the combination of algorithm and number of threads τ . Note that the y-axes are scaled loga-rithmic. The shaded area shows the range of the slowdown over all instances that could be solved within the time limit. 35 shows that it greatly speeds up the computation of nondominated images on practical instances. Hence, the significance of PEA is two-fold: First, it gives practitioners a new tool to utilize computational resources efficiently and to speed up many real-world applications. Second, it proposes a new approach for the design of multi-objective optimization al-gorithms. We hope that future research is able to build upon PEA to engineer faster variants or to enable it to be used for even more problem classes. Here, we want to remark that although we described PEA for integer problems, it can be applied to any multi-objective problem with finite nondominated set, as long as a solver for the lexico-graphic epsilon-scalarization problems is available. 

# Acknowledgments 

The authors gratefully acknowledge the funding by the Deutsche Forschungsgemein-schaft (DFG, German Research Foundation) — GRK 2982, 516090167 “Mathematics of Interdisciplinary Multiobjective Optimization”, the Carl Zeiss Foundation — Project number P2019-01-005, and the Deutsche Forschungsgemeinschaft (DFG, German Re-search Foundation) — Project number 508981269. In addition, we would like to thank William Pettersson for his technical support with his implementation of AIRA. 

# References 

[1] J. Boissonnat, M. Sharir, B. Tagansky, and M. Yvinec. Voronoi diagrams in higher dimensions under certain polyhedral distance functions. Discret. Comput. Geom. ,19(4):485–519, 1998. [2] N. Boland, H. Charkhgard, and M. W. P. Savelsbergh. The L-shape search method for triobjective integer programming. Math. Program. Comput. , 8(2):217–251, 2016. [3] N. Boland, H. Charkhgard, and M. W. P. Savelsbergh. The quadrant shrinking method: A simple and efficient algorithm for solving tri-objective integer programs. 

Eur. J. Oper. Res. , 260(3):873–885, 2017. [4] N. Boland, H. Charkhgard, and M. W. P. Savelsbergh. A new method for optimizing a linear function over the efficient set of a multiobjective integer program. Eur. J. Oper. Res. , 260(3):904–919, 2017. [5] K. Bringmann. Bringing order to special cases of klee’s measure problem. In K. Chatterjee (ed.): Mathematical Foundations of Computer Science 2013 , pp. 207– 218. Springer Berlin Heidelberg, 2013. [6] K. D¨ achert, T. Fleuren, and K. Klamroth. A simple, efficient and versatile objective space algorithm for multiobjective integer programming. Math. Methods Oper. Res. ,100:351–384, 2024. 36 [7] K. D¨ achert and K. Klamroth. A linear bound on the number of scalarizations needed to solve discrete tricriteria optimzation problems. J. Glob. Optim. , 61(4): 643 – 676, 2015. [8] K. D¨ achert, K. Klamroth, R. Lacour, and D. Vanderpooten. Efficient computation of the search region in multi-objective optimization. Eur. J. Oper. Res. , 260(3): 841–855, 2017. [9] M. Ehrgott. Multicriteria Optimization (2. ed.) . Springer, 2005. [10] L. Gurobi Optimization. Gurobi optimizer reference manual. URL: https://www. gurobi.com/ , 2023. [11] Y. Y. Haimes, L. S. Lasdon, and D. A. Wismer. On a bicriterion formulation of the problems of integrated system identification and system optimization. IEEE Trans. Syst. Man Cybern. Syst , SMC-1(3):296–297, 1971. [12] P. Halffmann, L. E. Sch¨ afer, K. D¨ achert, K. Klamroth, and S. Ruzika. Exact algorithms for multiobjective linear optimization problems with integer variables: A state of the art survey. Journal of Multi-Criteria Decision Analysis , 29:341–363, 2022. [13] IBM. Cplex (v22.1.1). URL: https://www.ibm.com/docs/en/icos , 2023. [14] intel. oneAPI threading building blocks (oneTBB). URL: https://github.com/ uxlfoundation/oneTBB/ , 2024. [15] G. Kirlik and S. Sayin. Computing the nadir point for multiobjective discrete optimization problems. J. Glob. Optim. , 62(1):79–99, 2015. [16] G. Kirlik and S. Sayın. A new algorithm for generating all nondominated solutions of multiobjective discrete optimization problems. Eur. J. Oper. Res. , 232(3):479–488, 2014. [17] K. Klamroth, R. Lacour, and D. Vanderpooten. On the representation of the search region in multi-objective optimization. Eur. J. Oper. Res. , 245(3):767–778, 2015. [18] D. Klein and E. Hannan. An algorithm for the multiple objective integer linear programming problem. Eur. J. Oper. Res. , 9(4):378–385, 1982. [19] M. Laumanns, L. Thiele, and E. Zitzler. An efficient, adaptive parameter variation scheme for metaheuristics based on the epsilon-constraint method. Eur. J. Oper. Res. , 169(3):932–942, 2006. [20] J. Lemesre, C. Dhaenens, and E. Talbi. Parallel partitioning method (PPM): A new exact method to solve bi-objective problems. Comput. Oper. Res. , 34(8):2450–2462, 2007. 37 [21] B. Lokman and M. K¨ oksalan. Finding all nondominated points of multi-objective integer programs. J. Glob. Optim. , 57(2):347–365, 2013. [22] M. Ozlen, B. A. Burton, and C. A. G. MacRae. Multi-objective integer program-ming: An improved recursive algorithm. J. Optim. Theory Appl. , 160(2):470–482, 2014. [23] W. Pettersson and M. Ozlen. Multiobjective integer programming: Synergistic parallel approaches. INFORMS J. Comput. , 32(2):461–472, 2020. [24] K. Prinz. Parallelization In Multi-Objective Optimization Based On The Epsilon-Constraint Scalarization . doctoralthesis, Rheinland-Pf¨ alzische Technische Univer-sit¨ at Kaiserslautern-Landau, 2026. [25] K. Prinz and S. Ruzika. The parallel epsilon algorithm for tri-objective integer optimization problems. INFORMS J. Comput. , 2025. ahead of print. [26] A. Przybylski, X. Gandibleux, and M. Ehrgott. A recur sive algorithm for finding all nondominated extreme points in the outcome set of a multiobjective integer programme. INFORMS J. Comput. , 22(3):371–386, 2010. [27] A. Przybylski, X. Gandibleux, and M. Ehrgott. A two phase method for multi-objective integer programming and its application to the assignment problem with three objectives. Discret. Optim. , 7(3):149–165, 2010. [28] J. Sylva and A. Crema. A method for finding the set of non-dominated vectors for multiple objective integer linear programs. Eur. J. Oper. Res. , 158(1):46–55, 2004. [29] S. Tamby and D. Vanderpooten. Enumeration of the nondominated set of mul-tiobjective discrete optimization problems. INFORMS J. Comput. , 33(1):72–85, 2021. [30] O. Turgut, E. Dalkiran, and A. E. Murat. An exact parallel objective space de-composition algorithm for solving multi-objective integer programming problems. 

J. Glob. Optim. , 75(1):35–62, 2019. 38