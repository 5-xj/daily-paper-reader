Title: Automated Model Design using Gated Neuron Selection in Telecom

URL Source: https://arxiv.org/pdf/2602.10854v1

Published Time: Thu, 12 Feb 2026 02:11:09 GMT

Number of Pages: 9

Markdown Content:
# Automated Model Design using Gated Neuron Selection in Telecom 

Adam Orucu †‡ , Marcus Medhage ∗, Farnaz Moradi †, Andreas Johnsson †§ , and Sarunas Girdzijauskas ‡† Ericsson Research, Stockholm, Sweden 

‡ KTH Royal Institute of Technology, Stockholm, Sweden 

§ Uppsala University, Uppsala, Sweden 

Abstract —The telecommunications industry is experiencing rapid growth in adopting deep learning for critical tasks such as traffic prediction, signal strength prediction, and quality of service optimisation. However, designing neural network archi-tectures for these applications remains challenging and time-consuming, particularly when targeting compact models suit-able for resource-constrained network environments. Therefore, there is a need for automating the model design process to create high-performing models efficiently. This paper introduces TabGNS (Tabular Gated Neuron Selection), a novel gradient-based Neural Architecture Search (NAS) method specifically tai-lored for tabular data in telecommunications networks. We eval-uate TabGNS across multiple telecommunications and generic tabular datasets, demonstrating improvements in prediction per-formance while reducing the architecture size by 51–82% and reducing the search time by up to 36x compared to state-of-the-art tabular NAS methods. Integrating TabGNS into the model lifecycle management enables automated design of neural networks throughout the lifecycle, accelerating deployment of ML solutions in telecommunications networks. 

Index Terms —Machine Learning, Neural Architecture Search, Tabular Data, Model Life Cycle Management 

I. I NTRODUCTION 

The rapid evolution of telecommunication networks has accelerated the adoption of Artificial Intelligence (AI) and Machine Learning (ML), advancing toward the AI-native vision [1]. Neural networks are increasingly leveraged for tasks such as traffic prediction [2], received signal strength estimation [3], and service performance prediction [4]. These models are expected to be deployed across diverse environ-ments, ranging from the User Equipment (UE) to the Radio Access Network (RAN) and Core, each with distinct con-straints on computational resources and requirements on model performance and inference latency. Designing neural network architectures that meet these requirements is a complex and time-consuming process, often requiring significant manual effort and domain expertise. It is further pronounced by the need for continuous model maintenance and retraining as part of the model Life Cycle Management (LCM) — which should be fully automated to ensure sustainability over time. Automating the model design is essential to achieve fully automated model LCM. 

∗ Work performed while at Ericsson Research. This work has been accepted for publication in IEEE/IFIP Network Operations and Management Symposium 2026. The final published version will be available via IEEE Xplore. Data collection  ML testing  AI/ML deployment  

> AI/ML inference
> ML training
> Neural architecture search
> Neural
> network training
> Network Infrastructure
> Training phase
> Deployment phase Inference phase
> TabGNS

Fig. 1. Workflow of the operational steps in the ML model lifecycle management [5] augmented with TabGNS for automated neural architecture search and training. 

Neural Architecture Search (NAS) [6] is a promising ap-proach for automating the discovery of high-performing neural network architectures. It employs learning algorithms to ex-plore a predefined search space that selects a high-performing architecture while respecting constraints such as, inference latency, and model size [7]. NAS has demonstrated success in domains such as computer vision and natural language processing. However, existing research for telecom has mainly focused on non-tabular data [8], [9], despite the abundance of tabular data in telecom systems [10]. In this paper, we introduce Tabular Gated Neuron Selection (TabGNS) 1, a novel gradient-based NAS method tailored for tabular data in telecommunications. TabGNS can seamlessly be integrated into existing ML model LCM frameworks in 3GPP or O-RAN based networks. An illustrative example is provided in Fig. 1. During the training phase, TabGNS performs joint architecture search and model training. The resulting model is deployed and used for inference. Continuous monitoring of model performance can then trigger retraining, for example due to changes in data distribution or resource availability, enabling re-optimization over both architecture and data. To the best of our knowledge, TabGNS is the first NAS approach designed specifically for tabular telecom data and the first to apply gradient-based architecture search to Multilayer Perceptrons (MLPs), through a neuron-level gating mecha-nism. In this paper, we have chosen to focus on MLPs as 

> 1

https://github.com/EricssonResearch/tabgns    

> arXiv:2602.10854v1 [cs.LG] 11 Feb 2026 1.0x 1.6x 12.3x
> Relative Search Time
> 15.25
> 15.50
> 15.75
> 16.00
> 16.25
> 16.50
> 16.75
> 17.00
> MSE
> TabGNS (Ours)
> 23k params
> TabNAS
> 92k params
> AgE
> 182k params Fig. 2. TabGNS outperforms previous tabular NAS methods on all of the critical dimensions; prediction error, architecture size, and search time. Scatter-point size represents architecture size. Results for VoD dataset.

they remain widely used for tabular data in production telecom systems due to their efficiency, and proven performance. This choice ensures a uniform and controlled search space for fair comparison across all NAS methods, and simplifies the search space, making it easier to interpret and analyse the NAS algorithm’s behaviour. We note that our gating mechanism is architecture-agnostic and can be extended to other layer types in future work. At its core, TabGNS employs a progressive growth strategy, starting from small architectures and gradually widening them using the learning algorithm to efficiently discover compact models with high predictive performance. More specifically, TabGNS improves the architectural design of MLPs for tab-ular data along three key dimensions: (1) fast and efficient architecture discovery, (2) resulting compact model size, and (3) high predictive performance. Fig. 2 provides an indicative example (search time, mean square error, and model size) of the superiority of TabGNS compared to other state-of-the-art approaches. Our contributions are as follows:  

> •

Identification of the need for automated MLP design in telecom networks, with particular interest in tabular data.  

> •

Introduction of TabGNS, a method for gradient-based neural architecture search through a neuron-level gating mechanism.  

> •

Comprehensive evaluation on multiple datasets demon-strating significant improvements compared to state-of-the-art — matching or surpassing prediction performance while reducing model size by 51 − 82% , and accelerating search time by up to 36 times. The remainder of this paper is organized as follows: Sec-tion II reviews related work in NAS and its applications in tabular and telecom data. Section III formalizes the prob-lem description. Section IV presents our proposed method, TabGNS, in detail. Sections V and VI describe our datasets, experimental setup, and evaluation methodology, and present our results and comparative analysis. Finally, Section VII concludes the paper and discusses future research directions. II. R ELATED WORK 

Finding deep learning architectures for tabular data that can outperform tree-based ensemble methods has been an active research area and several deep learning architectures such as transformer based architectures [11], [12] have been devel-oped. Although these architectures show promising results, they do not always outperform vanilla MLP models while requiring significantly higher computational resources [13]. Recently MLP based ensemble architectures have shown com-petitive results to tree based methods while being more effi-cient than more advanced and complicated architectures [13], [14]. In this paper, we focus on automatically finding the best MLP architecture for tabular telecom data using NAS. As a future work one could enhance the selected MLP with ensemble methods to further improve the model performance. NAS has achieved remarkable success in finding models with state-of-the-art accuracy across various tasks, particularly for image and text data [15]. Using NAS in telecom has also received some attention. Wang et al. [8] use NAS to find the best convolutional neural network for traffic classification, and NAS-AMR [9] is proposed for automatic modulation recogni-tion using simulated signal datasets. Cooperatively optimising data collection and NAS for IoT devices and image data was studied by Yin et al. [16]. While these papers explored NAS for telecom use cases, they all used unstructured datasets such as images and signals. However, the majority of the ML tasks for network management are based on tabular datasets [10], which is our focus in this paper. According to Yang et al. [17], NAS for tabular data has received limited attention due to a lack of understanding of promising architectures and a lack of relevant datasets. Although few, there is some work in the area, including AgEBO-tabular [18] and TabNAS [17], both of which have been used as baselines in this paper. Similar to AgEBO-tabular, Xing et al. [19] use Aging Evolution NAS for tabular data and [20] is designed with focus on fairness; therefore are out of scope for this paper. We utilise the NAS part of AgEBO-tabular and not Bayesian Optimisation (BO) for hyperparameter tuning therefore we refer to it as AgE. DARTS [21], being a gradient descent based method, has inspired numerous extensions that leverage continuous search spaces for faster search times. Several works use Gumbel-Softmax (GS) [22], or Straight-Through Estimator (STE) [23] functions to enable gradient-based optimisation: Proxyless-NAS [24], SNAS [25], Dong et al. [26], and FBNet [27] use GS to select between different layer types, while GS-NAS [28] applies it to select layer widths for Deep Belief Networks. However, these DARTS-based methods have primarily focused on computer vision and natural language processing tasks. Our work contributes to the state of the art by being the first gradient-based NAS method specifically designed for tabular data problems. Other distantly related works are Compete to compute [29] which turns off neurons given its performance relative to its neighbour and Dropout [30] which randomly drops neurons Architecture search and weight training 

# . . .         

> popen = 1 .0
> popen = 0 .0
> a)
> b)
> c) d)
> Fig. 3. Representation of the learning process of TabGNS. (a) Gate for each neuron is initialised to a be open at a low probability. (b) Architecture and weights are iteratively updated until convergence. (c) Neurons with gates with probability popen >0.5are extracted together with their weights. (d) Final trained fully connected neural network, can be further fine tuned if needed.

during training. Neither of these works are applied to archi-tecture search. Pruning techniques [31] also aim to minimise the architectures parameter count while maintaining good performance. Specifically, Pruning-as-Search [32] aims to find a pruned architecture using NAS. However, these type of methods start from a trained model and prune weights instead of neurons. Our method, on the other hand, trains the models while selecting the architecture and keeps the final model fully-connected which is a structure more suitable for GPUs. III. P ROBLEM DESCRIPTION 

Deploying effective neural networks in telecom systems requires careful architecture selection to balance competing requirements such as limited computational resources and high predictive performance — a task traditionally performed manually. To address this time-consuming and inefficient process, there is a need for an automated architecture selection process that can efficiently determine a high-performing neural network structure without human intervention. This process should optimise ML models as part of fully automated LCM, to achieve high accuracy while minimising computational overhead both during architecture search and at model infer-ence. Given an ML task, we define a search space S parametrised by the number of hidden layers L and the maximum width 

W of fully connected layers. S corresponds to the largest possible MLP that contains W L unique smaller architectures. The objective is to find an architecture A ⊆ S that is significantly smaller than S and that achieves comparable or superior predictive performance to S when both are trained. Thus, the optimisation objective is, 

min A⊆S Lval (w∗(A), A )

s.t. w∗(A) = argmin w Ltrain (w, A ) (1) where w are the weights of the model, L is the loss of the model that is calculated on the training or validation data. The optimization process aims to select a subset of neurons within 

S to construct A, reducing computational complexity while maintaining or enhancing model performance. In this paper we introduce a novel search algorithm that can efficiently find a small architecture A that is optimised for Eq. (1). IV. M ETHOD 

This section provides the necessary background and presents Tabular Gated Neuron Selection (TabGNS) — aNAS method that automates parts of the model lifecycle management. 

A. Background 

NAS involves searching for an optimal architecture by selecting from choices of layers and how they are to be con-nected. Since these choices are discrete, traditional gradient-based methods cannot directly handle them. There are several approaches that can enable the use of gradient-based methods on discrete variables. In this section, we introduce the methods used in this paper. 

Gumbel-Softmax: Gumbel-softmax [22] is a differentiable approximation of a categorical sampling. It allows backpropa-gation through random variables, which are otherwise discrete choices. Each category’s probability is represented by a logit which is summed with an independent random variable from a Gumbel distribution. The sum is divided by a temperature term 

τ , which is used to control the sharpness of the distribution (the “randomness” in the samples). The softmax of this term is used to provide probabilities for each category. The process is defined by Eq. (2), 

yi = exp ((log ( πi) + oi) /τ )

Pkj=1 exp ((log ( πj ) + oj ) /τ ) for i = 1 , . . . , k (2) where πi is the logit of the category i out of k categories and oi is a random sample from the Gumbel distribution. As the temperature τ approaches zero, the distribution of 

y approaches a categorical distribution. Gumbel-softmax is frequently used together with the Straight-Through Estimator (STE). 

Straight Through Estimator: STE [23] uses discrete vari-ables such as argmax during the forward propagation, while in the backward propagation the gradient is approximated by assuming the operations are continuous. This approximation enables the use of standard backpropagation, despite the problem being discrete. 

Gumbel Softmax with Straight Through Estimator: In this variant of Gumbel-softmax, a discrete operation (one-hot en-coding of arg-max of y) is used during the forward pass. During the backward pass, meanwhile, continuous values of y

(pre-encoding) are used. Given, 

zi = one-hot 



arg max  

> j

yj



(3) the gradient is approximated as, 

∂L ∂π i

≈ ∂L ∂z i

· ∂y i

∂π i

(4) where L is the loss, ∂L/∂z i is gradient flowing into the hard output zi from subsequent layers and ∂y i/∂π i is the gradient of the Gumbel-softmax with respect to the logits. 

Differentiable Architecture Search: DARTS [21] is a well-established method for architecture search. It uses a continuous relaxation of the architecture search space to find a high-performing architecture. The architecture search space is de-fined by various neural network operations, ranging from layer types (identity, convolution, linear) to layer-specific properties (kernel size, stride length, layer width). While these are discrete options, DARTS makes them continuous by weighting them and propagating the weighted outputs of each operation. The weight of each option is characterised by a parameter which together are passed through a softmax function. These parameters are later updated using traditional gradient-based learning algorithms until they converge. Therefore, one of the multiple operations is chosen as the final operation. This process is done for every choice in the architecture search space. In a DARTS learning process, the algorithm optimises the weights of the entire architecture, a SuperNet [33] containing all possible architectures, on the training data-split and the architecture on the validation data-split. Storing all possible ar-chitectures in one SuperNet means that a portion of the weights between two different architectures are likely to overlap. This method of storing and training architectures, called weight sharing , is frequently used due to its efficiency caused by not needing to retrain all the weights for every architecture being tested. The training process iteratively alternates between weight optimisation and architecture optimisation, with each step performed on individual data batches. Training continues until the architecture converges, with one operation for each selection of operations. DARTS has proven effective for architectures such as ConvNets but faces some challenges when applied to fully connected networks for tabular data. Convolutional layers are agnostic to the input size thanks to the “sliding-window” behaviour of the convolution operation. This is not the case for fully connected layers, which require explicit definitions of the input and output dimensions. Furthermore, this approach misses efficiency opportunities; for instance, when considering two candidate layers of widths 5 and 7, the weights in the width-5 layer represent a subset of those in the width-7 layer, yet conventional DARTS would train these independently without leveraging potential weight sharing. 

B. TabGNS: Tabular Gated Neuron Selection 

We propose shifting from layer-level to neuron-level gradient-based architecture search. Our method begins with a fully connected SuperNet that includes all candidate neurons. The final architecture is derived based on each neuron’s contri-bution to predictions, effectively selecting a high-performing subset of neurons from the SuperNet. Since different archi-tectures are subsets of the SuperNet, many of the weights are shared and reused across the architectures. This overlap reduces the need to store and train separate sets of weights, improving training efficiency. Moreover, this approach pro-vides finer control over the search space, enabling more precise decisions than traditional layer-based approaches. We implement the selection process by associating each neuron in the search space with a “gate” that combines a learnable parameter and an activation function. These gates control whether neurons are active or inactive. Although many activation functions can be used, we select Gumbel-softmax with Straight-Through Estimator (GS-STE) due to its widespread adoption and well-established theoretical under-standing. Thus, TabGNS provides a learnable mechanism for sampling whether a neuron is included. During the training process, the probabilities of different gates change, reflecting the contribution of the corresponding neuron. At the end of the training process, the gates are binarised to determine which neurons are included in the final architecture. Each gate requires only one parameter, g(i,j ), representing the “on” state of the neuron; the “off” state is fixed to zero. The fixed value still participates in the stochastic process through the Gumbel distribution’s randomness. Consequently, the GS-STE formulation, with many categories in Eq. (2), requires only two options. The activation probability of a single gate in position (i, j ) in the neural network can be expressed as; 

p(i,j ) = exp    log  g(i,j ) + o1

 /τ 

exp    log  g(i,j ) + o1

 /τ  + exp ( o2/τ ) (5) where o1 and o2 are the samples from the Gumbel distri-bution which are separately sampled for every gate. The search space of TabGNS, S, defines the largest possible neural network. As visualised in Fig. 3, each hidden neuron is assigned a corresponding gate, where each gate contains a parameter g(i,j ) that determines the probability of activating its associated neuron. The complete set of these parameters, denoted as g, defines our searchable architecture space. Algorithm 1: TAB GNS 

Input : Neural network S with weights and biases w,and training data (Xtrain , Y train ), validation data (Xvalid , Y valid )

Output: Architecture A and its weights // Initialise gate parameters g = {g(i,j )} for each neuron at position (i, j )

while not converged do 

// Create batches from training and validation data 

{Bitrain } ← CreateBatches (Xtrain , Y train )

{Bivalid } ← CreateBatches (Xvalid , Y valid )

for each batch (Bitrain , B ivalid ) do 

// Step 1: Optimise network weights 

g.trainable ← False 

w.trainable ← True 

Ltrain ← ForwardPass (w | g, B itrain )

Update weights w ← w − ηw∇wLtrain 

// Step 2: Optimise architecture (gates) 

g.trainable ← True 

w.trainable ← False 

Lvalid ← ForwardPass (g | w, B ivalid )

Update gates g ← g − ηg ∇gLvalid 

// Extract final architecture. p calculated using Eq. (5) 

A ← { (i, j ) | p(i,j ) 

> open

≥ 0.5}

TabGNS follows the standard DARTS training process of alternating optimization explained in background section. Weights of the model are updated using training data while gates remain frozen, then gates are updated using validation data while weights remain frozen. This iterative process en-sures that architectural decisions (gates) and parameter learn-ing (weights) are optimised independently. Pseudo code for the training process is provided in Algorithms 1 and 2. To explain this process more intuitively: During each forward pass, neurons are stochastically sampled based on their gate probabilities ( popen ), and information flows only through the selected active neurons. The alternating optimization then updates either the weights or gates of these active neurons, depending on the current training phase. V. D ATASETS 

We evaluate TabGNS using six tabular datasets: four telecom-specific datasets representative of telecom use cases of well-known prediction tasks and two generic large tabular datasets for comprehensive benchmarking. Detailed informa-tion about these datasets is provided in the following subsec-tions and Table I, respectively. 

A. Received Signal Strength (RSS) Prediction 

Three datasets address the challenge of predicting received signal strength (Reference Signal Received Power (RSRP) or path loss) across multiple frequency bands using mea-surements from a single primary carrier. Each dataset repre-sents distinct network configurations, and frequency scenarios 

Algorithm 2: FORWARD PASS 

Input : Weights w, biases b, gate parameters g, input data X, target data Y and loss function loss fn 

Output: Loss value L

// Initialise with input data 

h ← X

for each layer l ∈ { 1, 2, ..., L } do 

// Linear transformation 

z ← w[l] · h + b[l]

// Apply activation function 

a ← σReLU (z)

// Apply neuron gating (Eq. (5)) 

h ← a ⊙ σGS-STE (g[l]) 

// Compute loss 

L ← loss fn (h, Y )

return L

generated using different simulators. Across all datasets, the prediction task follows a consistent framework: given a UE location, signal strength metrics from the primary carriers across all cells are used to predict the signal strength for the secondary carriers. As an example, for DeepMIMO, given 18 cells in the network each with one primary and three secondary carriers there are 18 × 1 = 18 input features and 18 × 3 = 54 

prediction tasks. Prediction tasks are formed as a regression problem. These results can be used to predict the strongest cell on the secondary carrier. This task has previously been studied by Masood et al. [3]. 

1) DeepMIMO: The dataset 2 originates from a deployment scenario where we predict path loss for high-frequency sec-ondary carriers (28 GHz, 60 GHz, and 140 GHz) using primary carriers (3.5 GHz) path loss measurements. It uses the Outdoor Urban Microcellular scenario to simulate 18 base stations and approximately 31,000 unique UE locations. 

2) Sim-A: The dataset models a cellular layout with three base stations with three cells each. It includes 100 simula-tion snapshots, each containing 1,000 randomly placed UEs, totalling 100,000 UE locations. In this task, we use RSRP measurements from an LTE carrier at 900 MHz to predict RSRP for a secondary NR carrier at 4.5 GHz. 

3) Sim-B: The dataset simulates a network with three base stations, each with three cells. It includes 50,000 UE location samples generated using 100 distinct UE identities. Here, the RSS task involves predicting RSRP values for secondary carriers at 1800 MHz, 2600 MHz, and 3500 MHz based on primary carrier RSRP at 800 MHz. 

B. Service Performance Prediction 

The fourth telecom dataset focuses on video-on-demand (VoD) frame rate (FR) prediction. It consists of server-side resource utilisation metrics; such as, disk I/O statistics, net-work statistics, CPU core utilisation, memory, and swap space 

> 2https://www.deepmimo.net/

utilisation, from a cluster of nine high-performance servers offering VoD services. The cluster receives a load according to a sinusoid function ranging 20–120 clients/minute generated using a Poisson process. The goal is to predict the frame rate experienced by end users based on backend resource metrics, providing a proxy for user experience monitoring in service assurance tasks. The task is a regression problem. For further information, we refer to Yanggratoke et al. [4]. 

C. Non-telecom Tabular Datasets 

As we are proposing a general tabular NAS method, we additionally evaluate our method on two large-scale generic tabular datasets that are commonly used in the deep learn-ing literature: CoverType, which involves forest cover type classification, and Higgs, which contains simulation data for predicting the Higgs boson particle in high-energy physics experiments. 3                       

> TABLE I SUMMARY OF DATASETS .
> Dataset Task type Input / Output Samples
> VoD (FR) Regression 46 / 150,000 DeepMIMO (RSS) Regression 18 / 54 31,419 Sim-A (RSS) Regression 9 / 999,999 Sim-B (RSS) Regression 9 / 27 50,100 CoverType Classification 54 / 7 581,012 Higgs Classification 24 / 2 940,160

VI. E XPERIMENTS 

In this section, we analyse TabGNS to demonstrate its be-haviour and the reasoning behind choices we have made during its development. Further, since we present a new tabular NAS method, we compare it against the two existing tabular NAS methods — AgE and TabNAS. Since TabGNS also functions as a training technique that achieves high accuracy, we have included a comparison with a large MLP configured to repre-sent the largest possible architecture within our search space. We do these evaluations on four different telecom datasets, presented in Section V. Additionally, since our method can be applied to tabular data more generally, we evaluated it on two more datasets that are frequently used when evaluating ML models for tabular data. We begin the section with details on the implementation of the methods and the experiments to ensure reproducibility. 

A. Implementation Details Experimental setup: To ensure a fair comparison between NAS methods, we standardised the search space and evaluation procedures across all approaches and datasets. The architecture search is constrained to MLPs with five layers and a maximum width of 512 units per layer. This configuration was chosen as we observed diminishing returns in performance beyond this size. AgE and TabNAS require a predefined set of allowable layer widths. For these methods, we have selected 20 discrete  

> 3Both datasets were downloaded from openml.org .

options between 2 and 512. In contrast, our method does not require a predefined list and is free to select any width between 1 and 512. We did not extend the search spaces of AgE and TabNAS to include all possible widths as doing so would further increase their already substantial computational cost. All methods train and evaluate selected architectures using a learning rate of 0.001 , with a maximum of 300 epochs and early stopping with a patience of 20. GPU experiments were conducted on an NVIDIA H100 for the RSS datasets and an NVIDIA A30 for the remaining datasets. AgE and TabNAS select an architecture as a result of the search process. We report the final test performance of these architectures by creating new randomly initialised models and training them until convergence. TabGNS, on the other hand, provides both an architecture and its weights. Therefore, we are not required to retrain the architecture from random initialisation. Instead, we warm start a new model from these weights and train for a small number of additional epochs. 

TabGNS: While tuning hyperparameters for each dataset individually could yield better results, we opt for simplicity and use the same hyperparameters for TabGNS across all datasets. We set τ = 1, the architecture learning rate of 

0.05 , and initialise our gate parameters to −3, giving them approximately 4.7% probability of being open — starting the search from small architectures. Traditional DARTS methods often use “unrolling” to look ahead to the next architecture state when training the weights of the models. We have only seen increase in computation time when using unrolling, therefore do not use it in TabGNS. 

Other NAS methods: For our comparative study, we use the DeepHyper 4 implementation of AgE. TabNAS does not have public code; therefore, we implemented it ourselves in PyTorch same as TabGNS. One advantage of AgE is its ability to parallelise computation over multiple devices. We were not able to parallelise over many GPUs; therefore, the experiments have been done in parallel on 20 CPU cores. This does not change the method and, therefore, its predictive performance. To ensure a fair comparison with other NAS methods with regard to search time, we report all time measurements for CPUs as well. For TabNAS, we have set the number of pre-training epochs to be one-fourth of the total epochs, as described by the authors. While a key feature of TabNAS is its ability to constrain the maximum number of parameters in the architecture, we do not utilise this feature in our comparisons to ensure that all methods are evaluated on the same search space. We use 2048 Monte-Carlo samples, a 0.001 RL learning rate and a momentum of 0.9. For AgE, we use a population of 100, a sample size of 10 and 300 iterations. 

B. Analysis of TabGNS 

Our experiments highlight the significant influence of gate initialisation on NAS outcomes. We found that initialising the gate probabilities to be mostly open or mostly closed has an impact on the size of the final architecture. As seen              

> 4https://github.com/deephyper/deephyper 020 40 60 80 100
> Iterations
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Number of Parameters
> 1e6
> popen ≈ 0.88
> popen ≈ 0.12 Fig. 4. Size of the architecture during a search process given initialisation of gates with low/high probability of being open, for the VoD dataset. Initialisation of the gates determines the starting size of the architectures and consequentially the size of the final architecture. 0.0 0.2 0.4 0.6 0.8
> Initial probability of gate being open ( popen )
> 15.2
> 15.4
> 15.6
> 15.8
> 16.0
> 16.2
> 16.4
> 16.6
> 16.8
> MSE
> AgE (MSE)
> TabNAS (MSE)
> 10 4
> 10 5
> Number of Parameters
> Fig. 5. Architecture size and prediction error given initial gate state on VoD dataset. With low initial gate probability, TabGNS is able to find small architectures with low error.

in Fig. 4, initialising all gates to be open with a small probability ( popen ≈ 12% ) starts the architecture search from a small size and slowly grows it, while initialising to a large probability ( popen ≈ 88% ) starts from a large architecture and shrinks it. TabGNS uses early stopping based on validation loss, terminating the search when validation performance stops improving. This causes the final architecture sizes to be biased towards their starting architecture size. More comprehensive experimentation in Fig. 5 provides the final architecture size and test error for different neural architecture searches with different gate initialisations. In-creasing the initial probability of gates being open results in a larger final architecture. Furthermore, we see that there isn’t a substantial difference in the test error of these different initialisations. Specifically, all tested initialisations outperform other NAS methods. There is, however, a preferred range for the initialisation where we can obtain an architecture that both achieves low error while having a low number of parameters — in this case, popen ∼ 4 − 12% .

C. Comparative Results 

As we have previously discussed, there are three dimensions of optimisation that are important for autonomously designing neural network architectures for telecom use cases, namely prediction error, architecture size, and search time. For a comprehensive evaluation, we compare TabGNS on all the datasets against state-of-the-art tabular NAS methods and a large MLP. We use a large MLP as a naive baseline model that is the largest architecture in the search space and, therefore, much larger than anything the NAS methods end up selecting. We report prediction performance (mean-squared error or accuracy depending on the task) and final model size as the number of parameters (weights) in the selected architectures in Table II. Bold formatting indicates the best results that are statistically significant based on the t-test ( p < 0.05 ). Previous tabular NAS methods manage to find architectures that are smaller than the large MLP; however, they cannot improve the predictive performance. TabGNS, on the other hand, can find architectures that match or exceed large MLP prediction performance while containing 1-2 orders of mag-nitudes fewer parameters. Both TabNAS and AgE experience situations where they either don’t converge or can’t find a good architecture, causing a high variance in results. Compared to previous NAS methods, TabGNS not only achieves lower error for most datasets but also achieves more consistently good results (lower standard deviation). Architectures selected by TabGNS are also smaller than the ones selected by previous tabular NAS methods. This makes them more suitable for many telecom use cases where there are limited computational resources or requirements on low inference latency. Search time measurement for the NAS methods showcase the advantage of using gradient descent compared to Re-inforcement Learning (RL) and Evolutionary Search (ES), especially while running on GPUs (see Table III). RL and ES based NAS methods require training and evaluating many architectures to be able to learn from them — they are sample inefficient. Gradient-based methods, on the other hand, are able to learn by optimising the architectures given the loss landscape. This process is, therefore, much more efficient than the alternatives. This is especially true when utilising GPUs due to gradient-based optimisations’ inherent parallelis-ability. Our experiments show that evolutionary search, AgE, is significantly slower than the other two methods due to its requirement of completely training each architecture that is picked during the search, even though it uses 20 CPU cores to run the process in parallel. Speed comparisons of TabNAS and TabGNS on GPUs clearly show the advantage of gradient-based optimisation versus comparatively inefficient RL-based optimisation. Furthermore, since TabGNS trains the architecture during the search, there is no requirement to retrain the final architecture. Instead, we can use the final weights of the method and extract the weights corresponding to the selected architecture. We use these weights to warm-start the architecture and train for a few additional epochs, which achieves an even better performance. TABLE II COMPARISON OF TAB GNS WITH A LARGE MLP AND OTHER NAS METHODS .                                                                                                                                                   

> Type Dataset Metric Naive NAS Method
> Large MLP TabNAS AgE TabGNS (Ours)
> Telecom VoD (FR) MSE ↓15.477 ±0.417 15.949 ±0.456 16.759 ±3.539 15.246 ±0.622
> # Parameters ↓1,048,576 56,576 ±41,616 182,886 ±207,455 21,071 ±5,018 DeepMIMO (RSS) MSE ↓34.070 ±1.981 81.593 ±64.113 36.026 ±1.411 30.555 ±0.367
> # Parameters ↓1,048,576 63,590 ±54,593 1575,488 ±170,563 141,337 ±16,378 Sim-A (RSS) MSE ↓20.297 ±1.582 57.426 ±69.821 20.579 ±1.781 20.719 ±0.270
> # Parameters ↓1,048,576 110,526 ±97,567 1489,882 ±103,872 86,929 ±11,567 Sim-B (RSS) MSE ↓7.692 ±0.735 68.359 ±53.491 9.372 ±0.978 7.666 ±0.197
> # Parameters ↓1,048,576 60,736 ±65,297 1470,357 ±205,914 89,615 ±9,372 Non-Telecom Covertype Accuracy ↑95.353 ±0.193 94.691 ±0.668 95.528 ±0.161 95.241 ±0.192 # Parameters ↓1,048,576 142,131 ±73,451 529,429 ±121,975 69,313 ±4,784 Higgs Accuracy ↑74.303 ±0.148 74.603 ±0.245 69.822 ±10.459 75.090 ±0.139
> # Parameters ↓1,048,576 138,525 ±82,722 19,354 ±9,944 130,198 ±5,176
> 1Parameter count ignored from bolding due to its significant detriment to prediction performance.
> ↓↑ Downward ( ↓) and upward ( ↑) pointing arrows show whether smaller or larger values are better respectively.

TABLE III COMPARISON OF SEARCH TIMES (WALL -CLOCK TIME IN HOURS ) FOR DIFFERENT METHODS ON A CPU AND A GPU.                                                                                                            

> Dataset Device Method
> TabNAS AgE 1TabGNS (Ours)
> VoD CPU 1.32 ±0.43 9.95 ±4.20 0.81 ±0.36
> GPU 1.46 ±0.86 –0.04 ±0.01 DeepMIMO CPU 0.76 ±0.27 31.98 ±16.02 1.26 ±0.39 GPU 0.69 ±0.26 –0.07 ±0.01 Sim-A CPU 4.51 ±1.10 59.28 ±18.94 1.72 ±0.62
> GPU 1.80 ±0.86 –0.15 ±0.05 Sim-B CPU 1.55 ±0.38 41.76 ±23.27 1.57 ±0.45
> GPU 0.97 ±0.45 –0.08 ±0.02 Covertype CPU 7.72 ±2.64 232.84 ±82.88 21.40 ±6.71 GPU 8.06 ±1.80 –1.52 ±0.11 Higgs CPU 13.51 ±6.65 106.39 ±23.19 22.37 ±15.08
> GPU 16.88 ±5.39 –2.10 ±0.53
> 1AgE was run on 20 parallel CPU cores. No parallelisation for TabGNS and TabNAS.

It is worth noting that although the range of possible architectures in the search spaces of the selected NAS methods is the same, because TabGNS has a more granular control over the architecture the number of possible choices it has to pick from is larger. As described in Section VI-A, TabNAS and AgE can choose out of 20 options per layer ( 20 5 = 3 , 2 · 10 6

unique architectures), while TabGNS chooses an architecture out of 512 different widths per layer ( 512 5 ≈ 3, 5·10 13 unique architectures). In conclusion, TabGNS enables starting the search from small architectures and growing them to select architectures that are smaller than state-of-the-art tabular NAS methods while having a better prediction performance. TabGNS is able to converge to a final architecture much faster than previous methods. Additionally, there is no need to retrain the final selected architecture since the weights of the architecture are optimised in conjunction with the architecture search. VII. C ONCLUSION 

We address the challenge of automating neural network de-sign for telecom data, where the manual process of architecture selection has been time-consuming and often suboptimal due to the need to balance prediction performance with resource constraints. We introduce TabGNS, the first gradient-based architecture–search approach tailored for tabular data. Our evaluations show that TabGNS delivers superior performance across three dimensions: it matches or exceeds state-of-the-art predictive accuracy on 5 out of 6 datasets, while requiring only 18 –49% of the parameters used by prior methods, and achieving a 5–36 × reduction in search time. By automating the architecture–search process, TabGNS eliminates the bottle-necks of manual design, enabling faster retraining cycles and rapid adaptation to evolving network conditions. The resulting architectures exhibit a substantially smaller footprint, reducing hardware demands and minimising inference latency, a critical factor for time-sensitive telecommunication tasks. Finally, TabGNS attains these efficiency gains while simultaneously improving predictive accuracy. Our future work will extend the evaluation to cover more datasets within network management and telecom and explore the applicability of gradient-based gated neuron selection to other types of layers and architectures, such as convolutional or recurrent layers. ACKNOWLEDGEMENT 

This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. We would like to thank Dinand Roeland at Ericsson Research, for his constructive feedback which greatly improved the clarity and quality of the work. REFERENCES [1] M. Iovene, L. Jonsson, D. Roeland, M. D’Angelo, G. Hall, and M. Erol-Kantarci, “A detailed study of the AI Native concept,” 2023. [Online]. Available: https://www.ericsson.com/en/reports-and-papers/white-papers/ai-native [2] C. Zhang, P. Patras, and H. Haddadi, “Deep Learning in Mobile and Wireless Networking: A Survey,” IEEE Communications Surveys & Tutorials , vol. 21, no. 3, pp. 2224–2287, 2019. [3] H. Farooq, J. Forgeat, S. Bothe, K. Cyras, and M. Moin, “Multi-Task Learning as Enabler for General-Purpose AI-Native RAN,” in 

IEEE International Conference on Communications Workshops (ICC Workshops) , 2024, pp. 779–785. [4] R. Yanggratoke, J. Ahmed, J. Ardelius, C. Flinta, A. Johnsson, D. Gill-blad, and R. Stadler, “Predicting service metrics for cluster-based services using real-time analytics,” in 11th International Conference on Network and Service Management (CNSM) , 2015, pp. 135–143. [5] ETSI, “Study on Artificial Intelligence/Machine Learning (AI/ML) man-agement,” European Telecommunications Standards Institute, Tech. Rep. ETSI TR 128 908 V18.0.0, May 2024. [6] B. Zoph and Q. Le, “Neural Architecture Search with Reinforcement Learning,” in International Conference on Learning Representations ,2017. [7] A. Orucu, F. Moradi, M. Ebrahimi, and A. Johnsson, “On Multi-Objective Neural Architecture Search for Modeling Network Perfor-mance,” in 15th International Conference on Network of the Future (NoF) , 2024. [8] X. Wang, X. Wang, L. Jin, R. Lv, B. Dai, M. He, and T. Lv, “Evo-lutionary Algorithm-Based and Network Architecture Search-Enabled Multiobjective Traffic Classification,” IEEE Access , vol. 9, pp. 52 310– 52 325, 2021. [9] X. Zhang, H. Zhao, H. Zhu, B. Adebisi, G. Gui, H. Gacanin, and F. Adachi, “NAS-AMR: Neural Architecture Search-Based Automatic Modulation Recognition for Integrated Sensing and Communication Systems,” IEEE Transactions on Cognitive Communications and Net-working , vol. 8, no. 3, pp. 1374–1386, Sep. 2022. [10] M. Chui, J. Manyika, M. Miremadi, N. Henke, R. Chung, P. Nel, and S. Malhotra, “Notes from the AI frontier: Insights from hundreds of use cases,” McKinsey Global Institute , vol. 2, pp. 1–31, 2018. [11] Y. Gorishniy, I. Rubachev, V. Khrulkov, and A. Babenko, “Revisiting Deep Learning Models for Tabular Data,” Oct. 2023. [12] S. O. Arik and T. Pfister, “TabNet: Attentive Interpretable Tabular Learning,” Dec. 2020. [13] Y. Gorishniy, A. Kotelnikov, and A. Babenko, “TabM: Advancing tabular deep learning with parameter-efficient ensembling,” in The Thirteenth International Conference on Learning Representations , Oct. 2024. [14] I. Rubachev, N. Kartashev, Y. Gorishniy, and A. Babenko, “TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks,” Oct. 2024. [15] T. Elsken, J. H. Metzen, and F. Hutter, “Neural Architecture Search: A Survey,” The Journal of Machine Learning Research , vol. 20, no. 55, pp. 1–21, 2019. [16] B. Yin, Z. Chen, and M. Tao, “Dynamic Data Collection and Neural Architecture Search for Wireless Edge Intelligence Systems,” IEEE Transactions on Wireless Communications , vol. 22, no. 1, pp. 688–703, 2022. [17] C. Yang, G. Bender, H. Liu, P.-J. Kindermans, M. Udell, Y. Lu, Q. V. Le, and D. Huang, “TabNAS: Rejection Sampling for Neural Architec-ture Search on Tabular Datasets,” in Advances in neural information processing systems , vol. 35, 2022, pp. 11 906–11 917. [18] R. ´ Egel´ e, P. Balaprakash, I. Guyon, V. Vishwanath, F. Xia, R. Stevens, and Z. Liu, “AgEBO-tabular: joint neural architecture and hyperpa-rameter search with autotuned data-parallel training for tabular data,” in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis , 2021, pp. 1–14. [19] N. Xing, S. Cai, Z. Luo, B. Ooi, and J. Pei, “Anytime Neural Architec-ture Search on Tabular Data,” 2024. [20] R. Das and S. Dooley, “Fairer and More Accurate Tabular Models Through NAS,” 2023. [21] H. Liu, K. Simonyan, and Y. Yang, “DARTS: Differentiable Architecture Search,” in International Conference on Learning Representations , 2019. [22] E. Jang, S. Gu, and B. Poole, “Categorical Reparameterization with Gumbel-Softmax,” in International Conference on Learning Represen-tations , 2017. [23] Y. Bengio, N. L´ eonard, and A. Courville, “Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation,” 2013, arXiv preprint arXiv:1308.3432. [24] H. Cai, L. Zhu, and S. Han, “ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,” in International Conference on Learning Representations , 2019. [25] S. Xie, H. Zheng, C. Liu, and L. Lin, “SNAS: stochastic neural architec-ture search,” in International Conference on Learning Representations ,2019. [26] X. Dong and Y. Yang, “Searching for a Robust Neural Architecture in Four GPU Hours,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. [27] B. Wu, K. Keutzer, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, and Y. Jia, “FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. [28] T. Pang, S. Zhao, J. Han, S. Zhang, L. Guo, and T. Liu, “Gumbel-Softmax based Neural Architecture Search for Hierarchical Brain Net-works Decomposition,” Medical Image Analysis , vol. 82, 2022. [29] R. K. Srivastava, J. Masci, S. Kazerounian, F. Gomez, and J. Schmid-huber, “Compete to compute,” in Advances in neural information processing systems , vol. 26, 2013. [30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-dinov, “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” Journal of Machine Learning Research , vol. 15, no. 1, pp. 1929–1958, 2014. [31] H. Cheng, M. Zhang, and J. Q. Shi, “A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommenda-tions,” IEEE Transactions on Pattern Analysis and Machine Intelligence ,vol. 46, no. 12, pp. 10 558–10 578, Dec. 2024. [32] Y. Li, P. Zhao, G. Yuan, X. Lin, Y. Wang, and X. Chen, “Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization,” in Thirty-First International Joint Conference on Artificial Intelligence , vol. 4, Jul. 2022, pp. 3236–3242. [33] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, “Efficient Neural Architecture Search via Parameters Sharing,” in Proceedings of the 35th International Conference on Machine Learning , 2018, pp. 4095–4104.