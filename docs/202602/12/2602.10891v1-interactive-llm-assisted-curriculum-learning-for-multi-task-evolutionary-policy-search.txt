Title: Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search

URL Source: https://arxiv.org/pdf/2602.10891v1

Published Time: Thu, 12 Feb 2026 02:15:01 GMT

Number of Pages: 14

Markdown Content:
# Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search 

## Berfin Sakallioglu 

MIGe - University of Trieste Trieste, Italy berfin.sakallioglu@phd.units.it 

## Giorgia Nadizar 

University Toulouse Capitole, IRIT -CNRS UMR5505 Toulose, France giorgia.nadizar@irit.fr 

## Eric Medvet 

DIA - University of Trieste Trieste, Italy emedvet@units.it 

Optimizer LLM 

Start 

Reasoning 

Optimization with 1 case: 

1

Reasoning 

Optimization with 2 cases: ,

Reasoning 

Reasoning 

Optimization with ùëõ cases: , , , . . . , 

0 Contextualization 

Response 0 2 Feedback 1 

3 Response 1 Feedback 2 

Response 2 

Response ùëõ 

Feedback (prompt) 

Response  

> Quality on the last case : 0.02 Quality on previous cases : 0.01, 0.06

Quality metrics  

> Quality plot:

Qual. progression [opt.]  

> Behavior visualization:

Behavior viz. [opt.]  

> Recall the format, return ONE JSON object with the following keys: . . .

Resp. format recap   

> {"case": ". . . ";

The new case  

> "understood": "Last case gave good quality (0.02) rapidly within 200 evaluations; the trajectory plot shows . . . ";

Explanation 

> "reasoning": "With consistently good performance across cases, this next case will include a wall . . . " }

Strategy 

Figure 1: An overview of our fully unsupervised pipeline for performing curriculum learning (CL) policy search using a curriculum generated by an LLM based on the feedback on the optimization process. At the beginning ( ) the optimizer 0

sends a contextualization prompt to the LLM describing the general context, the subsequent interactions, and the format of feedback and response messages, and asking for an initial training case. Then, the optimizer repeats the following steps, starting with a set comprising the initial LLM-provided training case: 1 it performs an optimization run, 2 it sends a summary of this run to the LLM, asking for a new case, 3 it gets a response with a new case and adds it to the training set. After ùëõ 

iterations of 1 2 3 , the pipeline stops ( ) and the optimizer returns the best policy found in the process. On the right, we sketch the structure of the feedback prompt and the LLM response, highlighting the salient components. 

Abstract 

Multi-task policy search is a challenging problem because poli-cies are required to generalize beyond training cases. Curriculum learning has proven to be effective in this setting, as it introduces complexity progressively. However, designing effective curricula is labor-intensive and requires extensive domain expertise. LLM-based curriculum generation has only recently emerged as a po-tential solution, but was limited to operate in static, offline modes without leveraging real-time feedback from the optimizer. Here we propose an interactive LLM-assisted framework for online curricu-lum generation, where the LLM adaptively designs training cases based on real-time feedback from the evolutionary optimization process. We investigate how different feedback modalities, rang-ing from numeric metrics alone to combinations with plots and behavior visualizations, influence the LLM ability to generate mean-ingful curricula. Through a 2D robot navigation case study, tackled with genetic programming as optimizer, we evaluate our approach against static LLM-generated curricula and expert-designed base-lines. We show that interactive curriculum generation outperforms static approaches, with multimodal feedback incorporating both progression plots and behavior visualizations yielding performance competitive with expert-designed curricula. This work contributes to understanding how LLMs can serve as interactive curriculum designers for embodied AI systems, with potential extensions to broader evolutionary robotics applications. 

CCS Concepts 

‚Ä¢ Computing methodologies ‚Üí Genetic programming; Active learning settings ; ‚Ä¢ Information systems ‚Üí Language models .

Keywords 

Large language models, Quality diversity, Genetic programming 

> arXiv:2602.10891v1 [cs.NE] 11 Feb 2026 Berfin Sakallioglu, Giorgia Nadizar, and Eric Medvet

1 Introduction and related works 

Multi-task (MT) learning trains a single model to solve several related tasks jointly instead of in isolation, encouraging shared representations that improve generalization to new but similar tasks [ 7, 48 ]. In robotics, for example, a visuomotor policy might be trained across families of manipulation tasks such as pushing objects with different shapes, opening doors and drawers with varying kinematics, or placing items into containers at different poses; these tasks share perception and embodiment but differ in contact dynamics and success criteria [ 10 , 45 ]. Learning them in an MT setting is particularly important for real-world deployment, where agents might be called to rapidly adapt to novel tasks that are close variants of those seen during training, rather than repeatedly relearning from scratch [16]. Evolutionary policy search provides a general framework for optimizing MT policies without relying on gradients. By iteratively selecting, mutating, and recombining candidate solutions, evolu-tionary policy search can optimize arbitrary policy representations, including interpretable [ 8 , 33 ] or structured policies discovered via genetic programming (GP) [ 40 ]. This flexibility makes evolution-ary optimization particularly interesting for MT settings, where diverse [ 31 , 32 ] or structured [ 9] solutions may be needed. For example, GP has been applied in MT visual learning domains [ 18 ]‚Äî such as evolving controllers that perform well across multiple Atari games or diverse image processing tasks [ 5, 6]‚Äîdemonstrating that symbolic policies can capture shared structure while remaining potentially human-interpretable. The iterative nature of evolutionary policy search naturally in-teracts with curriculum learning (CL) [ 20 , 21 , 24 ]: by controlling how and when tasks (or data from each task) are presented, cur-ricula can guide populations toward solutions that capture shared structure more efficiently [ 4]. In MT settings, curricula often stage tasks so that simpler or information-rich tasks bootstrap learning of features or structures that later support harder problems [35]. However, designing effective curricula poses a fundamental challenge. Crafting representative, difficulty-calibrated training instances or auxiliary tasks typically requires substantial domain expertise [ 4 , 37 ]. In robotics, this often entails manually creating synthetic trajectories or intermediate challenges to target specific capability gaps, frequently without access to real-world data [ 12 ]. Moreover, even when suitable tasks are available, scheduling them effectively is nontrivial [27]. Adaptive curricula offer a promising alternative by adjusting task sampling online based on learning progress, validation performance, or model uncertainty, focusing training on tasks that are neither too easy nor too difficult [ 14 , 41 ]. Such approaches can improve transfer, aligning training pressure with the agent evolving capabilities to enhance generalization [ 19 , 27 , 42 , 44 ]. Yet jointly designing task content and scheduling‚Äîespecially when done by humans‚Äîadds significant complexity, requiring careful supervision and continual intervention during training [37]. In this context, large language models (LLMs) provide a promis-ing path toward fully automated curriculum design. They can gen-erate diverse tasks, environments, reward functions, or curricula, either statically [ 26 , 36 ] or adaptively [ 11 , 22 , 23 ]. LLMs have been used to propose initial environments for training [ 23 ], dynamically adjust difficulty based on agent performance [ 22 ], and even produce structured teaching curricula in educational settings [ 47 ]. Interac-tive LLM-based CL approaches, such as EnvGen [ 46 ], combine task generation with feedback from the agent, allowing the curriculum to progress alongside the learner. Building on these advances, we propose interactive LLM-assisted curriculum learning for MT evolutionary policy search. Our ap-proach uses an LLM to generate candidate tasks or task variants, receive feedback from evolutionary optimization signals, and it-eratively refine the curriculum to match the population evolving capabilities. Unlike prior methods [ 46 ], we never train agents on hand-designed test cases. In addition, our method supports poten-tially interpretable controller representations and explores different feedback modalities, including multimodal signals, which can fur-ther improve learning [17]. We test our approach on a 2D navigation task using a symbolic equation-based controller and find that interactive LLM-generated curricula outperform static LLM-generated curricula, with richer feedback further boosting performance. Overall, our method achieves results comparable to expert-designed curricula with zero human effort, demonstrating that it can efficiently guide MT learning, even for interpretable representations. Moreover, this also opens the door to larger-scale applications, as LLMs can inherently generate a wide variety of environments [ 11 ] and can even be fine-tuned to produce specialized ones [1, 38]. 

2 Interactive LLM-assisted curriculum learning 

We propose a fully unsupervised pipeline for performing CL policy search using a curriculum generated by an LLM based on the feed-back concerning the optimization process. The pipeline, sketched in Figure 1, involves two actors, the optimizer and the LLM , interacting iteratively through messages for a given number of times. We assume that the problem being tackled can be instantiated in cases , i.e. , problem instances on which a candidate policy can be evaluated. Moreover, we assume that the optimizer can perform the optimization on a bag of one or more cases, called training cases. The optimizer-LLM interaction starts with a 0 contextualiza-tion message sent by the optimizer to the LLM to provide basic information and context about the overall interaction and to ask for an initial training case ùëê 0. This message serves for instructing the LLM about the subsequent interactions, to describe the format of feedback messages, and to prescript the format of responses, namely on how to encode new cases. After this first exchange, the interaction works as follows, start-ing from a bag ùê∂ = {ùëê 0 } containing the single initial training case provided by the the LLM. For a given number ùëõ stage of times, 1 the optimizer runs an optimization run on ùê∂ , 2 the optimizer sends a feedback message to the LLM containing the feedback about the optimization run, 3 the LLM responds with a message containing a new case ùëê , then the bag of training cases is augmented with ùëê ,

ùê∂ ‚Üê ùê∂ ‚äï { ùëê }. We call stage an iteration of the three steps above. We call curriculum the sequence of training bags obtained by the optimizer at the end of the process. The precise format of messages ( i.e. , the corresponding prompts )sent by the optimizer is problem-specific. We describe them in detail later for the case study which we consider for assessing our LLM-assisted CL for MT evolutionary policy search 

proposal. Figure 1 (on the right) sketches the structure of these messages, highlighting the main components; Appendix A shows them in detail. We remark that, at each stage, the bag ùê∂ includes the cases of all the previous stages: we opt for accumulating training cases, rather than training at each stage on a single case, for avoiding catastrophic forgetting, i.e. , having the optimizer producing a policy that solves the current case but ‚Äúforgets‚Äù how to solve the previous ones, eventually hindering generalization [ 13 ]. As an aside, this relives the user from the burden of choosing an optimizer and a policy representation which are robust to catastrophic forgetting. 

2.1 Feedback modality 

We investigate three feedback modalities that progressively increase information richness: 

Numeric feedback (N) contains only quantitative performance metrics, namely, the quality scores of the policy found by the optimizer assessed on each case in ùê∂ .

Progression-augmented feedback (N+P) extends N by adding a visualization of optimization convergence. This plot shows the quality of the best policy across optimization iterations (ùë• -axis: iteration number; ùë¶ -axis: quality metric), one line for each case in ùê∂ , revealing how quickly and effectively the optimization process improved. 

Behavioral-augmented feedback (N+P+B) further extends N+P by including a visualization of the policy behavior on the training cases. This provides the LLM with direct insight into the policy behavioral characteristics and how it inter-acts with the problem constraints. In the case study consid-ered here (see Section 3), behavior is visualized as a spatial trajectory showing the path taken by the agent overlaid on the environment layout. 

2.2 Requirements and limitations 

Our pipeline is general and agnostic with respect to the inner work-ing of the optimizer, the nature of the policy, and the considered problem. Nevertheless, there are some requirements concerning the optimizer and the problem. The optimizer must: (a) accept multiple training cases that accu-mulate over stages, (b) produce a numeric quality measure quan-tifying the policy performance on each case in ùê∂ , and (c) if N+P or N+P+B feedback is employed, track and report the quality pro-gression across optimization iterations for each case. Moreover, it is convenient, but not strictly required, that the optimizer can start an optimization from the results of a previous optimization run: this makes the entire process more efficient and effective. The problem must: (a) allow for a case representation which can be encoded in a format the LLM can generate and (b) if N+P+B feedback is employed, allow for a visual representation of the agent behavior on a case, given a policy. The first problem requirement might appear hard to meet, as the syntax describing a case might be custom and not known to the LLM. However, current LLMs are becoming increasingly better at complying with provided guide-lines [ 39 , 43 ]. And there are also some LLMs which have been customized for specific domains, e.g., MarioGPT [ 38 ] used for gen-erating levels of the Super Mario Bros game in [ 21 ]. Moreover, in many cases one can develop a procedure that automatically fixes small issues in LLM-generated cases. In the problem considered as case study in this work, the LLM was in general able to generate syntactically correct cases. 

3 Case study: 2D navigation 

We consider as use case the problem of learning a policy that per-mits a simulated robotic agent to navigate any 2D arena avoiding obstacles and reaching a target which can perceive by the agent through sensors. 

3.1 Model, policy, and optimization 

3.1.1 Agent and environment. The agent is a simulated differential-drive robot with two wheels, five proximity sensors, and two sen-sors for sensing the distance to and the relative direction of the target. The environment is a 2D arena defined by a robot starting position, a target position, and some obstacles in the form of walls placed inside the arena. The size of the arena is of 1 m √ó 1 m ; the robot has a circular shape with a radius of 0.02 m .We simulate the movement of the robot in the arena in discrete time with a time step of 0.1 s . At each time step, the robot perceives the distance to the closest obstacle (arena internal and external walls) along five directions equally spread on the front side of the robot ( i.e. , ‚àí ùúã  

> 2

, ‚àí ùúã  

> 4

, 0, ùúã  

> 4

, and ùúã  

> 2

), the distance to the target, and the relative direction to the target. The proximity sensors and the distance-to-target sensor have a range of 0.5 m : if there are no objects in the range, the sensors sense 0.5 m ; the domain of these sensors readings is hence [0, 0.5]. The angle-to-target sensor domain is [‚àí ùúã, ùúã ].Upon the processing of the sensory input, the robot sets inde-pendently the rotational speed of the two wheels in [‚àí ùúî max , ùúî max ].We set ùúî max such that the maximum linear speed of the robot is 

0.01 m s ‚àí1. When the robot collides with an obstacle, the linear speed goes to zero but we allow it to rotate. 

3.1.2 Policy representation. Given the sensors and actuators of the robot, its controller is in general a dynamical system with R7

as observation space and R2 as action space. We implement the controller as an outer fixed part wrapping an inner variable part, which is the one we actually optimize. The outer part pre-processes the observation for the wrapped in-ner part and post-processes the latter output before using it as actua-tion values for the wheels. In the pre-processing, it takes the original observation ùíê (ùëò ) ‚àà R7 and (i) normalizes each element in the proper domain ( i.e. , [0, 0.5] or [‚àí ùúã, ùúã ]) and (ii) augments it with trend and average value during the last 0.5 s : i.e. , for each ùëú (ùëò ) 

> ùëñ

, the augmented observation ùíê ‚Ä≤( ùëò ) ‚àà [‚àí 1, 1]21 contains normalized ùëú (ùëò ) 

> ùëñ

, ùëú (ùëò ) 

> ùëñ

‚àíùëú (ùëò ‚àí4) 

> ùëñ

,and 15

√çùëó =4 

> ùëó =0

ùëú (ùëò ‚àí ùëó ) 

> ùëñ

. In the post-processing, the outer part takes the output ùëé ‚Ä≤ ‚àà R of the inner part and translates it to ùíÇ (ùëò ) ‚àà[‚àí ùúî max , ùúî max ], with ùëé (ùëò ) 

> 1

= ùëé (ùëò ) 

> left

= ùúî max 



1 + 2 min 



0, tanh ùëé ‚Ä≤( ùëò )

 

and ùëé (ùëò ) 

> 2

= ùëé (ùëò ) 

> right

= ùúî max 



1 ‚àí 2 max 



0, tanh ùëé ‚Ä≤( ùëò )

 

. This way, if 

ùëé ‚Ä≤( ùëò ) ‚Üí ‚àí‚àû then ùíÇ (ùëò ) = (‚àí ùúî max , ùúî max ) and the robot rotates to the left, if ùëé ‚Ä≤( ùëò ) ‚Üí +‚àû then ùíÇ (ùëò ) = (ùúî max , +ùúî max ) and it rotates to the right, if ùëé ‚Ä≤( ùëò ) = 0 then ùíÇ (ùëò ) = (ùúî max , ùúî max ) and it goes straight at the maximum speed. In the intermediate cases, the robot follows a Berfin Sakallioglu, Giorgia Nadizar, and Eric Medvet 

curvy path, with the radius of the curve depending on |ùëé ‚Ä≤( ùëò ) |. Note that the outer part of the controller is stateful in the pre-processing, as it maintains a memory of the last 5 observations, and stateless in the post-processing. As inner part of the controller we use a function in R21 ‚Üí R

which we encode through a syntax regression tree, suitable to be optimized with GP. Namely, we use the following 11 labels for the non-terminal nodes: ‚Ä¢ + ‚Ä¢ , ‚Ä¢ ‚àí ‚Ä¢ , ‚Ä¢ √ó ‚Ä¢ , ‚Ä¢ √∑ ‚àó ‚Ä¢, log ‚àó ‚Ä¢, max (‚Ä¢ , ‚Ä¢) ,

min (‚Ä¢ , ‚Ä¢) , tanh ‚Ä¢, ‚Ä¢ > ‚Ä¢, ‚Ä¢ < ‚Ä¢, ‚Ä¢?‚Ä¢ : ‚Ä¢, where bullets represent the children of the corresponding node and hence dictate its arity. 

√∑‚àó and log ‚àó represent the protected versions of the corresponding operators. The comparison operators > and < return the sign of the difference of the two arguments; ‚Ä¢?‚Ä¢ : ‚Ä¢ corresponds to the ternary operator where the first argument is compared against 0.For the terminal nodes we use as labels the observation variables 

ùëú ‚Ä≤( ùëò ) 

> 1

, . . . , ùëú ‚Ä≤( ùëò ) 

> 21

and ephemeral constants in [‚àí 5, 5].Summarizing, we drive the robotic agent with a GP tree encod-ing a function from R21 to R wrapped within a dynamical system which augments sensor readings with ‚Äúhistorical‚Äù information and simplifies the action space. We call policy the inner tree. 

3.1.3 Tasks. The overall goal is to find a policy allowing a robot to navigate any arena, i.e. , able to solve the general problem of reaching a target while avoiding collisions. We attempt to achieve this goal by making candidate policies experience different arenas, each one constituting a training case . We call episode a simulation of a robot with a policy within an arena. In our settings, an episode is deterministic. Given an arena with a starting position for the robot and a position for the target, we run an episode for 60 s (simulated time, 

i.e. , for 600 time steps). At the beginning of the episode, we set the direction of the robot to the right, regardless of the position of the target. At the end of episode, we record the full trajectory of the robot. Out of this, we compute several metrics, including the final (i.e. , at ùëò = 600 ) and average distance of the robot to the target. We remark that this 2D navigation problem has been widely used in previous work as a basic, yet not trivial, benchmark for policy search, often for assessing quality diversity (QD) search algorithms [ 3, 15 , 25 , 28 ]. However, in most of the cases, policies were assessed only on the arena they were trained on. Instead, here we aim at obtaining a policy able to navigate, in general, any arena, which is a much harder problem. I.e. , we cast this navigation problem as a multi-task (MT) problem where a single policy is required to solve many (similar) tasks, each one defined by an arena. To assess the effectiveness of a policy to solve the MT navigation problem, we consider the set ùê∂ test of six test arenas of Figure 2. We remark that these six test arenas are never known to the optimizer, nor to the LLM. 

3.1.4 Evolutionary optimizer. Based on the recent literature involv-ing the 2D navigation problem, we use MAP-elites (ME) as optimizer. ‚ÄúClassic‚Äù ME [ 30 ] evolves a population of candidate solutions using only mutation as variation operator. Solutions compete to occupy a cell in a discrete structure called archive: ME maps solutions to cells using two descriptors which provide numerical coordinates which are then discretized. Here we use a grid-based archive, where 

Figure 2: The six test arenas used for assessing policies. The red region shows the robot starting position (which is placed at that center of the region), the green region shows the target position. 

discretization corresponds to matching continuous values of each descriptor against equally sized bins in the corresponding domain. Given a search space ùëÜ (here, the set of GP trees used as policies), a fitness function ùëì : ùëÜ ‚Üí R, and two descriptors ùëë 1 : ùëÜ ‚Üí ùê∑ 1 ‚äÇ R

and ùëë 2 : ùëÜ ‚Üí ùê∑ 2 ‚äÇ R, our ME works as follows‚Äîwe assume that ùëì 

has to be minimized. First, we initialize a population of ùëõ pop candidate solutions and insert each solution in the archive. To this end, for each solution ùë† ,we compute its coordinates ùíÖ =  ùëë 1 (ùë† )| ùê∑ 1,ùëõ bins , ùëë 2 (ùë† )| ùê∑ 2,ùëõ bins 

 in the archive, where ùë• |ùëã ,ùëõ =

j

ùëõ ùë• ‚àímin ùë• ‚Ä≤ ‚ààùëã     

> max ùë• ‚Ä≤ ‚ààùëã ‚àímin ùë• ‚Ä≤ ‚ààùëã

k

‚àà { 0, . . . , ùëõ } denotes the discretization of ùë• using ùëõ equally sized bins in ùëã ‚äÇ R. If the cell is empty, we put ùë† in the archive. If the cell contains a solution ùë† ‚Ä≤, we replace it with ùë† only if ùëì (ùë† ) ‚â§ ùëì (ùë† ‚Ä≤). After the initialization, we repeat the following steps until some termination criterion is met. (i) We generate an offspring of ùëõ batch new solutions by extracting, randomly with repetitions, ùëõ batch solutions from the archive and mutating each one of them. (ii) We possibly insert each new individual in the archive, as for the initial population. At the end of the evolution, the archive contains a set of solutions, i.e. ,policies, which are diverse based on the descriptors and are good based on ùëì .For initializing the population at the first stage, we use the ramped-half-and-half procedure, commonly used in GP, with tree depth enforced to be in [4, 10 ]. For initializing the population at sub-sequent stages, we take the best 14 ùëõ pop solutions of the last iteration population of the previous stage, one mutated solution obtained from each of them, and generate the remaining 12 ùëõ pop solutions randomly with the ramped-half-and-half procedure: this way, we seed the initial population of the ùëñ -th stage with the outcome of the optimization of the ùëñ ‚àí 1-th stage. As mutation, we use with equal probability the GP subtree mu-tation or a Gaussian perturbation of the terminal nodes labeled with numerical constants, with a ùúé = 0.25 : when mutating trees, we enforce a limit on the tree depth ‚â§ 10 .

Fitness function and descriptors. Given a bag ùê∂ of training cases (here, arenas) and a candidate solution ùë† (here, a tree), we compute the fitness as the weighted average of the final and average distance of the robot to the target, averaged across arenas, i.e. , ùëì (ùë† ; ùê∂ ) = 

> 1
> |ùê∂ |

√çùëê ‚ààùê∂ ùëì single (ùë† ; ùëê ), where: 

ùëì single (ùë† ; ùëê ) = 0.9 ùíô (600 ) 

> robot

‚àí ùíô target + 0.1 1600 

> ùëò =600

‚àëÔ∏Å 

> ùëò =1

ùíô (ùëò ) 

> robot

‚àí ùíô target 

is the weighted average on a single arena ùëê , with ùíô (ùëò ) 

> robot

being the position of the robot equipped with ùë† at step ùëò of an episode in the arena ùëê and ùíô target being the target position in ùëê . We choose LLM-assisted CL for MT evolutionary policy search 

to use an average of the final and average distance to the target, and not just the final distance, for driving the optimization, as this leads to ‚Äúsmarter‚Äù behaviors, awarding policies resulting in shorter trajectories to the target. However, we assign a relatively low weight ( 0.1) to the average distance as we verified that larger values make the search harder, as certain arenas may require to increase the length of the path (and hence worsen the average distance) to overcome a local optimum. As ME descriptors, we considered the gap along the ùë• and ùë¶ axes of the robot to the target at the end of the episode, averaged across arenas, i.e. , ùëë 1 (ùë† ; ùê∂ ) = 1 

> |ùê∂ |

√çùëê ‚ààùê∂ 



ùë• (600 ) 

> 1,robot

‚àí ùë• 1,target 



and ùëë 2 (ùë† ; ùê∂ ) = 

> 1
> |ùê∂ |

√çùëê ‚ààùê∂ 



ùë• (600 ) 

> 2,robot

‚àí ùë• 2,target 



. These descriptors correspond to the adaptation of the classic ME descriptors used for the 2D navigation problem on a single arena to the case where a policy is assessed on many arenas. In the classic case, the final coordinates of the robot are used as descriptors: this would not make sense here as different arenas have in general different target position, obstacles, and starting position. We hence consider the relative position to the target and average it across arenas. We set the descriptor domains to ùê∑ 1 = ùê∑ 2 = [‚àí 0.5, 0.5].

Parameters and remark. Based on experience, previous work, and exploratory experiments, we set the parameters of the optimizer as follows: ùëõ pop = ùëõ batch = 100 , ùëõ bin = 10 , and, as termination criterion, the number of fitness evaluations being performed (which we set to 10 000 or more depending on the experiment). We recall that a single fitness evaluation corresponds to performing |ùê∂ | episodes. We verified experimentally that this combination of policy rep-resentation and optimizer is effective enough to tackle the MT 2D navigation problem. In particular, we compared it against combi-nations of genetic algorithm (GA) as optimizer, artificial neural networks (ANNs) as policies, and different ME descriptors based on both the behavior or on policy features: we found that ME with the descriptors based on the relative final position and GP trees for representing the policy is the best combination. We remark, how-ever, that the main contribution of this work is the optimizer-LLM interaction scheme, which is by design agnostic to these lower level details. 

3.2 LLM interaction in the 2D navigation case 

The overall interaction scheme described in Section 2 is general, and many components of the prompt remain domain-agnostic. However, certain elements are problem-dependent and require customization for the 2D navigation task. Here we describe these problem-specific adaptations. As shown in Figure 1, we begin with a contextualization prompt that differs from subsequent feedback prompts. This initial prompt establishes the 2D navigation domain, defining the differential drive robot task where the agent must navigate from a start position to a target while avoiding walls. The prompt specifies the role of the LLM as a stage designer, defines the quality metric (distance to target), describes the feedback format it will receive, enforces validity constraints, and provides an example arena. We give the complete contextualization prompt in Appendix A. We communicate arena configurations through character string encodings. We assume that an arena is described by tiles and each tile is encoded as a character: s represents the start position, t the target, w denotes walls, and e indicates empty tiles. The LLM gener-ates 15 √ó 15 character grids with rows separated by the | character. A substantial portion of the prompt enforces structural constraints, including exactly one start and one target position, guaranteed reachability via a continuous path, consistent row lengths of 15 

characters, and internal validation of connectedness. The complete format specification is visible in the prompt shown in Appendix A.. Subsequent feedback prompts vary by modality. For N, we pro-vide quality scores including the best policy distance to target on the current case, historical performance on previous cases, and average quality across all accumulated cases. N+P extends this with a convergence plot (PNG format) showing distance to target (ùë¶ -axis) over fitness evaluations ( ùë• -axis), with separate lines for each accumulated case to reveal optimization dynamics. A legend identifies each case line in the plot. N+P+B further adds trajectory visualizations‚Äî2D plots (SVG format) similar to Figure 3 displaying walls, start position, target, and the spatial path taken by the best-performing policy at the end of training for each case. A feedback example showing the convergence plot and the trajectory visual-ization is shown in Section 4.4; a complete interaction sequence is presented in Appendix A. 

4 Experimental Evaluation 

We evaluate the proposed pipeline through a series of experiments comparing different feedback modalities and baseline approaches. Our experimental design addresses the following research ques-tions: RQ1 Does the pipeline enable learning, with policies improv-ing across curriculum stages and generalizing to unseen cases? How do LLM-generated curricula compare to expert-designed, random, and static baselines? Which feedback modality is the most effective among the three investigated conditions? RQ2 Are LLM-generated curricula actual curricula? Do their cases work equally well if administrated in a single batch instead of progressively? RQ3 Does the LLM really ‚Äúunderstand‚Äù the feedback? Does it exploit it when creating new cases? 

4.1 Experimental procedure and baselines 

We compare three versions of our pipeline corresponding to the feedback modalities described in Section 2.1 against three baselines without feedback. 

Expert Represents an expert-designed static curriculum of eight cases (see Figure 3) created by a domain expert (one of the authors of this paper), serving as our gold standard. We generated this curriculum based on our (quite long) experience on the navigation problem and evolutionary approaches for solving it; we also knew the test arenas. 

Static Consists of an LLM-generated curriculum which we obtain by asking the LLM to build eight cases in a single batch without iterative feedback, using a modified prompt. 

Random Comprises randomly generated cases with randomized wall segments of varying lengths, numbers, and orienta-tions, as well as random start and target positions. Berfin Sakallioglu, Giorgia Nadizar, and Eric Medvet 59.90s o 59.90s o 59.90s ‚Üì59.90s o 59.90s o 59.90s o 59.90s ‚Üë59.90s o 

Figure 3: The expert-designed curriculum with the trajecto-ries obtained with a policy optimized on this curriculum. 

All curricula have the same number of cases (eight). We administer all of them progressively, in eight stages: first stage with the first arena, second with the first and the second arenas, and so on. We repeat the execution of each method 10 times to assess sta-tistical significance and consistency. For the variants (both with and without feedback) involving the LLM, we execute the entire process 10 times, resetting the LLM session at each repetition. For the random baseline, we generate 10 random curricula. For the expert baseline, we execute 10 sequences of optimization runs with different random seeds on the same expert-designed curriculum. We use JGEA [ 29 ] for performing the optimization with ME and for simulating the navigation episodes. We use Claude Sonnet 4.5 [ 2] with extended thinking capabilities as the LLM‚Äîwe leave to future work an investigation on the impact of using other LLMs. We forwarded the messages between the optimizer (JGEA) and the LLM (Claude) manually, i.e. , by copy-pasting JGEA output within templated prompt and inserting LLM responses within JGEA ex-periment description file. This way we are able to monitor in real time the correct execution of the interaction. The execution of one interaction with a variant with feedback lasts on average 20 min : ‚âà 10 % is taken by the LLM to respond (with no significant variance across feedback modalities), the remaining by the optimizer‚Äîwe run it on a machine equipped with an Intel ¬Æ

Core ‚Ñ¢ i5 14500T vPro ¬Æ CPU ( 14 cores at 4.8 GHz , with 20 threads) and 16 GB of RAM. The code for reproducing our experiments is available at https: //redacted.for.review. 

4.2 General effectiveness (RQ1) 

Figure 4 presents the learning dynamics across all methods over eight curriculum stages, showing both training performance (top row) and test generalization (bottom row). Namely, the figure shows the value of ùëì (weighted average of final and average distance to tar-get) computed for the best policy ùë† ‚òÖ in the population respectively on the train ( ùê∂ train ) or test ( ùê∂ test ) arenas‚Äîthe lower, the better. The left column shows progression across stages as line plots (median and interquartile range over ten repetitions), while the right column shows distribution of the final training and test performance. The line plots reveal two patterns. For training performance, performance ùëì  ùë† ‚òÖ; ùê∂ train 

 generally increases across stages for all methods except Random , indicating that curricula progressively introduce harder cases as intended. For test performance, perfor-mance ùëì  ùë† ‚òÖ; ùê∂ test 

 decreases across stages for all methods except Random , demonstrating successful learning and generalization to unseen environments. All LLM-generated curricula (active N, N+P, N+P+B, and Static) significantly outperform the Random baseline and demonstrate clear learning. Random does not show improvement in test performance across stages, suggesting that structured curriculum    

> 0
> 0.2
> 0.4
> 0
> 0.2
> 0.4
> 0.6
> 2468
> 0
> 0.5
> 1
> 0.2
> 0.4
> 0.6
> 0.8
> 1
> Training  ùëì   ùë† ‚òÖ; ùê∂ train
> 

Progression      

> ‚òÖ‚òÖ‚òÖ ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ ‚òÖ‚òÖ

Final distribution Stages Gener.  ùëì   ùë† ‚òÖ; ùê∂ test     

>  ‚òÖ‚òÖ‚òÖ ‚òÖ‚òÖ‚òÖ ‚òÖ‚òÖ‚òÖ ‚òÖ‚òÖ‚òÖ‚òÖ ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ ‚òÖ‚òÖ

W/ feedback: N N+P N+P+B W/o feedback: Static Random Expert 

Figure 4: Progression (right plots) and final distribution (left plots) of the performance of the best policy ùë† ‚òÖ in the popula-tion measured on the train arenas (top plots) and on the test arenas (bottom plots), for the three interactive modalities and the three baselines. In the progression plots, the line corresponds to the median value across the 10 repetitions, the shaded area to the interquartile range. In the distribution plots, stars above the boxes show significant differences: e.g., an orange star over the red box indicates that N is signifi-cantly different than Random. 

design (whether from the LLM or expert) is essential for effective learning. LLM-generated curricula successfully create meaningful learning progressions. While all LLM methods show learning, none surpass the Expert curriculum in final performance. However, N+P and N+P+B achieve performance statistically equivalent to the expert baseline (Mann-Whitney U test, ùëù > 0.05 ), indicating that LLM-assisted curriculum generation with appropriate feedback modalities can match expert-level design. Two variants with feedback outperform Static generation. N+P and N+P+B achieve better test performance than Static (Mann-Whitney U test, all ùëù < 0.05 ). This demonstrates that pro-viding iterative performance feedback enables the LLM to adapt curriculum difficulty and design more effective learning progres-sions for these versions. These results validate the core hypothesis: active, feedback-informed curriculum generation produces better learning outcomes than static, one-shot curriculum design. 

Feedback modality. N+P and N+P+B outperform N . Both progression-augmented (N+P) and behavioral-augmented (N+P+B) feedback yield better test performance than numeric-only feed-back (N) (Mann-Whitney U test, ùëù < 0.05 ). The addition of the convergence plot visualization appears to provide the LLM with actionable insights about optimization dynamics, enabling more LLM-assisted CL for MT evolutionary policy search    

> 2468
> 0.1
> 0.2
> 0.3
> 0
> 0.1
> 0.2
> 0.3
> 0.4

Stages 

> min ùë†  ùëì  (ùë† ; ùê∂ test )

Progression ‚òÖ ‚òÖ‚òÖ ‚òÖ‚òÖ ‚òÖ‚òÖ ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ 

Final distribution 

W/ feedback: N N+P N+P+B W/o feedback: Static Random Expert 

Figure 5: Progression (right plots) and final distribution (left plots) of the performance of the policy ùë† ‚òÖ with the best perfor-mance on the test arenas, for the three interactive modalities and the three baselines. 

informed curriculum design (see, e.g., the last stage response of the LLM in the complete interaction shown in Appendix A). N+P and N+P+B show no significant difference. This suggests that while adding convergence visualization to numeric feedback improves performance, incorporating trajectory visualizations does not provide substantial additional benefits. Analysis of the LLM  

> understood

and reasoning response fields reveals that it correctly interprets the trajectory visualizations and describes observed be-haviors accurately. However, this additional information may also introduce prompt complexity without adding decision-relevant in-sights beyond what convergence plots already provide. The longer, more complex prompts in N+P+B may also introduce cognitive load that offsets potential benefits [34]. 

4.2.1 Generalization and QD. As we use a QD optimizer (namely, ME), we obtain a set of diverse and good policies at the end of each optimization step, rather than just one good policy. In particular, we use the final gap in ùë• and ùë¶ axes between the target and the robot position at the end of simulation. Intuitively, different policies in the ME archive are hence diverse in the direction they approach the target from. Indeed, we use these descriptors exactly because they can facilitate the finding of policies that truly generalize the target-reaching behavior beyond the examples provided by training arenas‚Äî i.e. , to really solve the MT 2D navigation task. To demonstrate that ME actually exploits this potential in achiev-ing generalization, we show in Figure 5 the performance in the test arenas of the best solution in the ME archive, i.e. , the progression and distribution of min ùë† ùëì (ùë† ; ùê∂ test ).It can be seen that for all methods there is at least one policy in the archive which scores much better than the best solution chosen based on the performance on the training arenas: e.g., the median min ùë† ùëì (ùë† ; ùê∂ test ) of the final policy obtained with the N+P+B feedback is ‚âà 0.13 ( in Figure 5), while the median of ùëì  ùë† ‚òÖ; ùê∂ test 



is ‚âà 0.22 ( in Figure 4). However, the largest difference is in the populations evolved with the Expert curriculum, particularly visible by comparing the progression of the index ( ). 

> 0.2
> 0.4
> 0.6
> 0.8
> 1
> **Gener.  ùëì   ùë† ‚òÖ; ùê∂ test
> 

Progressive: Batch: N NN+P N+P N+P+B N+P+B 

Figure 6: Final distribution of the performance of the best policy ùë† ‚òÖ in the population measured on the test arenas for each modality with progressive and batch administration. An arch over a pair of boxes denotes statistically significant difference. 

4.3 Progressive vs. batch cases (RQ2) 

To verify that the LLM-generated cases form a meaningful cur-riculum progression rather than arbitrary arena collections, we compare training with progressive administration of cases against training with batch administration of all cases. For each of our variant (N, N+P, N+P+B), we take each of the 10 curricula ob-taned with feedback and perform a single optimization round with a ùê∂ train containing all the eight curriculum cases. To ensure fair comparison in terms of total episodes, we run the batch training longer, with 40 000 fitness evaluations as stopping criterion: this way we perform the same number of episodes in both conditions (8 ¬∑ 40 000 = √çùëñ =8 

> ùëñ =1

10 000 ). We show the performance of the obtained policies in Figure 6. The results demonstrate that N+P and N+P+B truly benefit from curriculum structure: progressive training significantly out-performs the batch variants (Mann-Whitney U test, ùëù < 0.05 ). Conversely, there is no difference from progressive and batch administration of curricula obtained with N. This confirms that the augmented feedback provided by progression plots and behavior visualization allows the LLM to generate sequences being genuine curricula, i.e. , such that case ordering and gradual accumulation facilitate learning, rather than simply diverse training sets. 

4.4 Analyses of LLM responses (RQ3) 

Figure 7 illustrates one stage of one interaction loop using the N+P+B modality, where 3 cases have already been generated and evaluated. The LLM receives numeric metrics, convergence plots, and trajectory visualizations, then produces the next case along with explanatory text. The complete sequence of prompts and LLM responses across all stages of this interaction is provided in Appen-dix A. By observing the understood field of the LLM response in Fig-ure 7, it can be seen that the LLM demonstrates basic competence in interpreting the provided visualizations. It correctly identifies con-vergence patterns from the progression plots, including the starting quality value, approximate evaluation count at convergence, and relative convergence speed. For trajectory plots, it accurately lo-cates start and target positions and describes the general path shape (e.g., smooth curve path , diagonal path ). The LLM also correctly uses the legend to associate performance curves with specific cases in multi-line plots. Berfin Sakallioglu, Giorgia Nadizar, and Eric Medvet 

2 Feedback after stage 3 59.90s ‚Ü∑59.90s ‚Üô59.90s ‚Ü∑

Arenas (and ùë† ‚òÖ behavior) after stage 3 1 optimization     

> Metrics
> - quality of the best policy on the last stage : 0.023 - quality of the best policy on previous stages : 0.003, 0.003, 0.023 - quality on average of all stages: 0.0097
> Progression of the quality Policy behavior 59.90s ‚Ü∑59.90s ‚Üô59.90s ‚Ü∑
> understood
> Stage 3 achieved quality 0.023 after starting at 0.37 and converging by evaluation 2000; the trajectory shows a smooth S-curve path from left-middle start navigating upward through horizontal wall barriers to reach the upper-right target, demonstrating successful zigzag maneuvering.
> reasoning
> With consistently strong performance across stages, this next arena introduces a vertical corridor structure on the left side with the target inside it, plus additional horizontal barriers in the lower-right area where the start is located, requiring the policy to navigate through a more constrained passage while maintaining path diversity from previous horizontal-dominated layouts.

3 Response 3 59.90s ‚Üó59.90s ‚Üó59.90s ‚Üó59.90s ‚Üó

Arenas (and ùë† ‚òÖ behavior) after stage 4 1 optimization 

Figure 7: A visualization of a chunk of one run of the inter-action with the N+P+B feedback: steps 1 2 3 of stage 3 and step 1 of stage 4. The complete interaction including these stages is shown in Appendix A. 

From the reasoning field in Figure 7, we see that the LLM is able to exploit the understanding of the feedback for providing and motivating the new case. By looking more broadly at other interactions, we see that when policies achieve low quality scores consistently, the LLM introduces additional obstacles, increases spatial complexity, or varies object positions to maintain challenge progression. Conversely, when performance degrades significantly, it explicitly acknowledges the difficulty spike and generates simpler arenas. Comparing the LLM textual descriptions of intended arena layouts with the actual 2D environments rendered from its character strings reveals reasonable consistency; the generated structures generally match the stated design intentions. These observations suggest that the LLM can extract relevant information from multimodal feedback and adjust curriculum diffi-culty accordingly, though its designs represent functional rather than optimal curriculum strategies. 

5 Concluding remarks 

We introduced an interactive large language model (LLM)-assisted framework that automates curriculum generation for multi-task (MT) evolutionary policy search by leveraging real-time optimiza-tion feedback. Experiments on 2D robotic navigation demonstrated that interactive LLM curricula outperform static LLM and ran-dom baselines, with progression-augmented feedback (N+P) and behavior-augmented feedback (N+P+B) achieving performance comparable to expert-designed curricula. Our findings reveal that feedback richness matters: adding con-vergence plots to numeric metrics improves curriculum quality, though behavioral visualizations offer more modest incremental gains. Analysis of LLM responses confirmed meaningful interpre-tation of multimodal feedback and adaptive difficulty calibration. Crucially, curricula generated with richer feedback function as genuine learning progressions, as sequential administration out-performs batch training. This framework is agnostic to optimizer, policy representation, and problem domain, opening pathways for broader evolutionary robotics applications. Future work should investigate scaling to complex robotic tasks, alternative LLMs, and whether incorporat-ing the evolved policies or generalization metrics (e.g., validation set performance) into the feedback loop enables LLMs to better anticipate curriculum impact on unseen cases. This work demon-strates that LLMs can effectively automate curriculum design for embodied artificial intelligence (AI), lowering the expertise barrier for MT learning in evolutionary systems. 

References 

[1] Fuma Aki, Riku Ikeda, Takumi Saito, Ciaran Regan, and Mizuki Oka. 2024. Llm-poet: Evolving complex environments using large language models. In Pro-ceedings of the Genetic and Evolutionary Computation Conference Companion .243‚Äì246. [2] Anthropic. 2025. Introducing Claude Sonnet 4.5. https://www.anthropic.com/ news/claude-sonnet-4-5 [3] Ryan Bahlous-Boldi, Maxence Faldor, Luca Grillotti, Hannah Janmohamed, Lisa Coiffard, Lee Spector, and Antoine Cully. 2025. Dominated novelty search: Rethinking local competition in quality-diversity. In Proceedings of the Genetic and Evolutionary Computation Conference . 104‚Äì112. [4] Yoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning . 41‚Äì48. [5] Ying Bi, Bing Xue, and Mengjie Zhang. 2021. Learning and sharing: A multitask genetic programming approach to image feature learning. IEEE Transactions on Evolutionary Computation 26, 2 (2021), 218‚Äì232. [6] Ying Bi, Bing Xue, and Mengjie Zhang. 2022. Multitask feature learning as multiobjective optimization: A new genetic programming approach to image classification. IEEE Transactions on Cybernetics 53, 5 (2022), 3007‚Äì3020. [7] Rich Caruana. 1997. Multitask learning. Machine learning 28, 1 (1997), 41‚Äì75. [8] Camilo De La Torre, Giorgia Nadizar, Yuri Lavinas, Herv√© Luga, Dennis G Wilson, and Sylvain Cussat-Blanc. 2025. Evolution of Inherently Interpretable Visual Control Policies. In GECCO‚Äô25: Genetic and Evolutionary Computation Conference .ACM, 358‚Äì367. doi:10.1145/3712256.3726332 [9] Karol Desnos, Nicolas Sourbier, Pierre-Yves Raumer, Olivier Gesny, and Maxime Pelcat. 2021. Gegelati: Lightweight artificial intelligence through generic and evolvable tangled program graphs. In Workshop on Design and Architectures for Signal and Image Processing (14th edition) . 35‚Äì43. [10] Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. 2017. Learning modular neural network policies for multi-task and multi-robot transfer. In 2017 IEEE international conference on robotics and automation (ICRA) .IEEE, 2169‚Äì2176. [11] Maxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune. 2024. Omni-epic: Open-endedness via models of human notions of interestingness with environments programmed in code. arXiv preprint arXiv:2405.15568 (2024). [12] Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. 2017. Reverse curriculum generation for reinforcement learning. In LLM-assisted CL for MT evolutionary policy search 

Conference on robot learning . PMLR, 482‚Äì495. [13] Robert M French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3, 4 (1999), 128‚Äì135. [14] Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. 2017. Automated curriculum learning for neural networks. In 

international conference on machine learning . Pmlr, 1311‚Äì1320. [15] Luca Grillotti and Antoine Cully. 2023. Kheperax: a lightweight jax-based robot control environment for benchmarking quality-diversity algorithms. In Proceed-ings of the Companion Conference on Genetic and Evolutionary Computation .2163‚Äì2165. [16] Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado Van Hasselt. 2019. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3796‚Äì3803. [17] Yuxiao Huang, Wenjie Zhang, Liang Feng, Xingyu Wu, and Kay Chen Tan. 2025. How multimodal integration boost the performance of llm for optimization: Case study on capacitated vehicle routing problems. In 2025 IEEE Symposium for Multidisciplinary Computational Intelligence Incubators (MCII) . IEEE, 1‚Äì7. [18] Wojciech Ja≈õkowski, Krzysztof Krawiec, and Bartosz Wieloch. 2008. Multitask visual learning using genetic programming. Evolutionary computation 16, 4 (2008), 439‚Äì459. [19] S√©bastien Jean, Orhan Firat, and Melvin Johnson. 2019. Adaptive scheduling for multi-task learning. arXiv preprint arXiv:1909.06434 (2019). [20] Steven Jorgensen, Giorgia Nadizar, Gloria Pietropolli, Luca Manzoni, Eric Medvet, Una-May O‚ÄôReilly, and Erik Hemberg. 2024. Large language model-based test case generation for gp agents. In Proceedings of the genetic and evolutionary computation conference . 914‚Äì923. [21] Steven Jorgensen, Giorgia Nadizar, Gloria Pietropolli, Luca Manzoni, Eric Medvet, Una-May O‚ÄôReilly, and Erik Hemberg. 2025. Policy Search through Genetic Programming and LLM-assisted Curriculum Learning. ACM Transactions on Evolutionary Learning (2025). [22] Xiaoxu Li, Zifan Ye, Yi Xia, and Ruck Thawonmas. 2025. Dynamic difficulty adjustment using a large language model: A case study in magic: The Gathering. 

Entertainment Computing (2025), 100997. [23] William Liang, Sam Wang, Hung-Ju Wang, Osbert Bastani, Dinesh Jayaraman, and Yecheng Jason Ma. 2024. Environment curriculum generation via large language models. In 8th Annual Conference on Robot Learning .[24] Saining Liu, Yi Mei, and Mengjie Zhang. 2025. Curriculum Learning in Genetic Programming Guided Local Search for Large-scale Vehicle Routing Problems. 

arXiv preprint arXiv:2505.15839 (2025). [25] Harald M Ludwig, Ane Espeseth, and Eric Medvet. 2025. Trace-Elites: Better Quality-Diversity with Multi-point Descriptors. In International Conference on the Applications of Evolutionary Computation (Part of EvoStar) . Springer, 338‚Äì353. [26] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931 (2023). [27] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. 2019. Teacher‚Äì student curriculum learning. IEEE transactions on neural networks and learning systems 31, 9 (2019), 3732‚Äì3740. [28] Eric Medvet, Samuele Lippolis, and Giorgia Nadizar. 2025. Quality-diversity in problems with composite solutions: a case study on body‚Äìbrain robot optimiza-tion. Genetic Programming and Evolvable Machines 26, 2 (2025), 22. [29] Eric Medvet, Giorgia Nadizar, and Luca Manzoni. 2022. JGEA: a modular java framework for experimenting with evolutionary computation. In Proceedings of the genetic and evolutionary computation conference companion . 2009‚Äì2018. [30] Jean-Baptiste Mouret and Jeff Clune. 2015. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909 (2015). [31] Jean-Baptiste Mouret and Glenn Maguire. 2020. Quality diversity for multi-task optimization. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference . 121‚Äì129. [32] Giorgia Nadizar, Eric Medvet, and Dennis Wilson. 2024. Searching for a Diver-sity of Interpretable Graph Control Policies. In Proceedings of the Genetic and Evolutionary Computation Conference . 933‚Äì941. doi:10.1145/3638529.3653987 [33] Giorgia Nadizar, Eric Medvet, and Dennis G Wilson. 2024. Naturally Interpretable Control Policies via Graph-Based Genetic Programming. In European Conference on Genetic Programming (Part of EvoStar) . Springer, 73‚Äì89. doi:10.1145/3643688 [34] Sania Nayab, Giulio Rossolini, Giorgio C Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. 2024. Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost. CoRR (2024). [35] Anastasia Pentina, Viktoriia Sharmanska, and Christoph H Lampert. 2015. Cur-riculum learning of multiple tasks. In Proceedings of the IEEE conference on computer vision and pattern recognition . 5492‚Äì5500. [36] Kanghyun Ryu, Qiayuan Liao, Zhongyu Li, Payam Delgosha, Koushil Sreenath, and Negar Mehr. 2025. Curricullm: Automatic task curricula design for learning complex robot skills using large language models. In 2025 IEEE International Conference on Robotics and Automation (ICRA) . IEEE, 4470‚Äì4477. [37] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. 2022. Curriculum learning: A survey. International Journal of Computer Vision 130, 6 (2022), 1526‚Äì 1565. [38] Shyam Sudhakaran, Miguel Gonz√°lez-Duque, Matthias Freiberger, Claire Glanois, Elias Najarro, and Sebastian Risi. 2023. Mariogpt: Open-ended text2level genera-tion through large language models. Advances in Neural Information Processing Systems 36 (2023), 54213‚Äì54227. [39] Zhensu Sun, Xiaoning Du, Zhou Yang, Li Li, and David Lo. 2024. Ai coders are among us: Rethinking programming language grammar towards efficient code generation. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis . 1124‚Äì1136. [40] Quentin Vacher, Stephen Kelly, Ali Naqvi, Nicolas Beuve, Tanya Djavaherpour, Micka√´l Dardaillon, and Karol Desnos. 2025. Maple: Multi-action programs through linear evolution for continuous multi-action reinforcement learning. In 

Proceedings of the Genetic and Evolutionary Computation Conference . 1062‚Äì1071. [41] Neeraj Varshney, Swaroop Mishra, and Chitta Baral. 2022. Let the model decide its curriculum for multitask learning. arXiv preprint arXiv:2205.09898 (2022). [42] Linji Wang, Zifan Xu, Peter Stone, and Xuesu Xiao. 2025. GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring. In 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) .IEEE, 591‚Äì596. [43] Yanlin Wang, Tianyue Jiang, Mingwei Liu, Jiachi Chen, Mingzhi Mao, Xilin Liu, Yuchi Ma, and Zibin Zheng. 2025. Beyond functional correctness: Investigating coding style inconsistencies in large language models. Proceedings of the ACM on Software Engineering 2, FSE (2025), 690‚Äì712. [44] Ziping Xu and Ambuj Tewari. 2022. On the statistical benefits of curriculum learning. In International Conference on Machine Learning . PMLR, 24663‚Äì24682. [45] Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. 2020. Multi-task rein-forcement learning with soft modularization. Advances in Neural Information Processing Systems 33 (2020), 4767‚Äì4777. [46] Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, and Mohit Bansal. 2024. Envgen: Generating and adapting environments via llms for training embodied agents. 

arXiv preprint arXiv:2403.12014 (2024). [47] Xueqiao Zhang, Chao Zhang, Jianwen Sun, Jun Xiao, Yi Yang, and Yawei Luo. 2025. Eduplanner: Llm-based multi-agent systems for customized and intelligent instructional design. IEEE Transactions on Learning Technologies (2025). [48] Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE transac-tions on knowledge and data engineering 34, 12 (2021), 5586‚Äì5609. Berfin Sakallioglu, Giorgia Nadizar, and Eric Medvet 

A Example of a complete interaction 

We show here a complete execution of our pipeline with the N+P+B feedback. Namely, we show the 0 contextualization prompt, which the optimizer sends before the first stage, and the three key steps for every stage: 1 the optimization, 2 the feedback sent by the optimizer to the LLM, and 3 the LLM response. For the 1 optimization, we show the behavior ( i.e. , the trajec-tory) of the best policy at the end of the evolution on the train cases (i.e. , arenas) of the current stage and the plot of the progression during the optimization of the performance of the best policy on the train cases. We also show the behavior of the same individual on the test cases, for showing how the generalization of the policy improves during the entire process: we recall, however, that this behavior and, more broadly, the performance on the test cases are never known by the optimizer and the LLM. For the 2 feedback, we show the exact prompt the optimizer submits to the LLM: in the actual interaction, we also attached the image of the behavior of the best policy on the train cases and the plot of the progression of the performance (the very same images shown here in the preceding optimization step). For the 3 response, we show the exact response given by the LLM. 

A.1 Before stage 1 

A.1.1 0 Contextualization.  

> 1

CONTRACT : STAGE - GRID v1 .0  

> 2

You are a STAGE DESIGN assistant for CURRICULUM LEARNING in evolutionary control . I am optimizing the policy of adifferential drive robot in a 2D arena . The robot must START at 

's ', REACH the target 't ', and CANNOT PASS over walls 'w '. I am expecting to obtain via curriculum learning , a general policy for ANY arena , I will evaluate GENERALIZATION with test arenas .  

> 3

## PURPOSE & USE  

> 4

You will provide STAGES ( arenas ) for a sequential curriculum learning process . Each stage will be used to assess candidate policies during optimization , which proceeds in sequential STAGES ( curriculum steps ). You DO NOT assist with optimization ; you only output the NEXT STAGE and a brief explanation (" understood " and " reasoning ").  

> 5

## OBJECTIVE QUALITY  

> 6

I assess candidate policies on the stages you provide via an objective score called " quality ". The goal is to minimize the quality score ( distance to target ). While achieving quality <0.1 represents optimal performance , policies with scores below 0.3 also demonstrate solid capabilities and meaningful progress .  

> 7

## PROVIDED FEEDBACK  

> 8

Before asking for each new stage , I will provide some feedback about the previous stage and the progress in general . Namely , I‚Äô ll provide :  

> 9

- quality of the best policy on the last stage : {{ current_quality }} // lower = better  

> 10

- quality of the best policy on previous stages : {{ history_summary }} // compact stats over recent stages / qualities  

> 11

- quality on average of all stages : {{ average_quality }} // lower = better  

> 12

- a PLOT ( for your reference , not to be produced ) showing the evolution of the best individual 's quality over the number of evaluations for each stage (s0 , s1 , s2 , . ..). It indicates how quickly and how well the population improved during training . Use this trend to assess the current stage and adjust the next stage ‚Äôs complexity accordingly .  

> 13

- a second plot ( for your reference , not to be produced ) showing the ARENA ( previous stage ) and the TRAJECTORY of the best individual . It includes the start ( pink square ) , target ( green square ) , walls ( black segments ) , and the trajectory ( pink line ). Use what you observe in this image to analyze the arena ‚Äôs layout , difficulty , and robot behavior before generating the next stage . Treat it as a visual reference for the previous arena ‚Äôs layout and the robot ‚Äôs trajectory , this is very important information . This plot should help you understand the MAPPING between the STRING named grid and the VISUAL LAYOUT of the arena , USE this understanding . Use it to infer spatial relationships , but you don ‚Äôt need to describe it verbatim , simply integrate your observations naturally into your explanation in " understood ".  

> 14

- When analyzing the trajectory plot , note which wall configurations created meaningful challenges versus simple obstacles . Use successful patterns to inform varied new designs , not repetitive ones .  

> 15

## WHAT TO RETURN  

> 16

Return ONE JSON object with the following keys :  

> 17

- " grid ": the next stage as a string encoding a matrix . The grid must follow the GRID FORMAT rules exactly (15 x 15 , rows joined by |, cells in {w ,s ,t ,e }) .  

> 18

- " understood ": a string with your one - sentence interpretation of all the feedback I provided . ( generic on first call ). INCLUDING YOUR INTERPRETATION OF BOTH PLOTS ( briefly summarize key visual observations from the image , such as the robot ‚Äôs path shape , proximity to walls , or how it approached the target )  

> 19

- " reasoning ": a string with a short motivation for why this stage is appropriate as the next step in a curriculum .  

> 20

## GRID FORMAT  

> 21

- The grid is a single string with vertical bar ( '| ') as separator between rows .  

> 22

- Size : 15 rows and 15 columns . All the rows have to have the SAME LENGTH (15) .  

> 23

- Allowed characters :  

> 24

- w = wall  

> 25

- s = starting position ( exactly once )  

> 26

- t = target position ( exactly once )  

> 27

- e = empty cell  

> 28

Do not use '|' as a cell character . Allowed cells are only {w , s, t , e }.  

> 29

## HARD CONSTRAINTS ( must be logically satisfied before output )  

> 30

- Exactly one 's ' and one 't '. 

> 31

- There must be AT LEAST ONE CONTINUOUS OPEN PATH of 'e '

cells connecting 's ' to 't '!!! ( walls 'w ' block movement ). INTERNAL VALIDATION PROCEDURE : Simulate a 4- connected flood - fill or BFS ( moves : up / down / left / right only ) starting from 's '.Confirm that 't ' can be reached without crossing any 'w '. Only grids that pass this internal check are valid .  

> 32

- VARY the positions of 's ' and 't ' across different stages .  

> 33

- All the rows HAVE TO have the SAME LENGTH .  

> 34

- Don ‚Äôt include frames in the grid , it is provided automatically by default .  

> 35

Output only after rechecking that all HARD CONSTRAINTS are met .  

> 36

## VALIDATION CHECK ( must pass before output )  

> 37

Before outputting , verify the given hard constraints were met .  

> 38

If any condition fails , regenerate the grid and recheck .  

> 39

Only output once all checks pass .  

> 40

## OUTPUT FORMAT  

> 41

CRITICAL : Each row = EXACTLY 15 characters . COUNT before output .  

> 42

{ 

> 43

" grid ": "< grid - string - with - rows - joined -by -| >" ,  

> 44

" understood ": "< your brief interpretation ( generic on first call ) >",  

> 45

" reasoning ": "< why this stage is a sensible next step >"  

> 46

} 

> 47

## COMPLETE EXAMPLE ( DO NOT COPY LITERALLY ; FOLLOW THE STRUCTURE ) 

> 48

{ 

> 49

" grid ": " eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeeeeeeteeeeee | eeeeeeeeeeeeeee | wwwwwwwwweeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeeeeewwwwwwww | eeeeeeeeeeeeeee | eseeeeeeeeeeeee | eeeeeeeeeeeeeee | wwwwweeeeeeeeee | eeeeeeeeeeeeeee ",  

> 50

" understood ": " No prior feedback provided ; initializing curriculum with a solvable maze ." ,  

> 51

" reasoning ": " Start and target are well - separated with multiple corridors ; it supports early exploration while ensuring reachability ."  

> 52

} 

> 53

## FINAL INSTRUCTION  

> 54

Return ONLY a valid JSON object as specified above . No extra text , no code fences . 

A.1.2 3 Response 0.  

> 1

{ 

> 2

" grid ": " eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeseeeeeeeeeete | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eewwwwwweeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeeeeeweeeeeee | eeeeeeeweeeeeee | eeeeeeeweeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee ",  

> 3

" understood ": " No prior feedback provided ; initializing the curriculum with a simple yet structured arena to establish baseline performance ." ,  

> 4

" reasoning ": " This first stage places start and target on the same row with moderate separation , introducing a single L - shaped wall obstacle that requires basic navigation around it . The layout is deliberately simple to allow the initial population to learn fundamental movement and target - seeking behaviors while ensuring a clear solvable path exists ."  

> 5

}LLM-assisted CL for MT evolutionary policy search 

A.2 Stage 1 

A.2.1 1 Optimization. 59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Ü∑59.90s o 59.90s o 

A.2.2 2 Feedback. 

1 CONTRACT : STAGE - GRID v1 .0 applies . 

2 ## PROVIDED FEEDBACK 

3 - quality of the best policy on the last stage : {0.000} 

4 - quality of the best policy on previous stages : {0.000} 

5 - quality on average of all stages : {0.000} 

6 ## REMINDER ( do not change ) 

7 - Grid : 15 x15 ; rows joined by '|' ( do not use '|' as a cell ). 

8 - Size : exactly 15 rows x 15 columns ; every row must be length 15. 

9 - Alphabet : {w , s , t , e }; exactly one 's ' and one 't '.

10 - Variety : vary the positions of 's ' and 't ' across stages . 

11 - Reachability : at least one 4- connected path of 'e ' from 's ' to 

't ' ( no diagonals ). 

12 - Include : " understood " and " reasoning ". 

13 Don ‚Äôt forget the constraints and validation check . If any output would violate CONTRACT : STAGE - GRID v1 .0 , regenerate internally and only emit a valid JSON object . 

14 - the two plots are attached . The two plots are for reference only ; do not reproduce them in the output . 

15 ## ROW LENGTH VALIDATION ( CRITICAL ) 

16 1. Each row = exactly 15 characters 

17 2. Total of 15 rows 

18 3. Validation method : COUNT each row before output 

19 4. If count != 15 -> REJECT and regenerate entire grid 

A.2.3 3 Response. 

1 {

2 " grid ": " eeeeeeeeeeeeeee | eeeeeweeeeeeeee | eeseeweeeeeeeee | eeeeeweeeeeeeee | eeeeeweeeeeeeee | eeeeeeeeeeeeeee | eeeeeeeewwwwwwe | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | ewwwwweeeeeetee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeeeweeeeeeeee | eeeeeweeeeeeeee | eeeeeeeeeeeeeee ", 

3 " understood ": " Stage 0 achieved optimal quality (0.000) rapidly within 200 evaluations ; the trajectory plot shows anearly direct path from left - side start to right - side target with minimal deviation around the simple L - shaped walls ." , 

4 " reasoning ": " Since the previous stage was solved perfectly and quickly , this stage introduces three distinct wall obstacles forming partial barriers at different vertical positions , requiring the robot to navigate around multiple corners while maintaining a clear solvable path from the left - middle start to the right - middle target ." 

5 }

A.3 Stage 2 

A.3.1 1 Optimization. 59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Ü∑59.90s o 59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Ü∑

A.3.2 2 Feedback. 

1 CONTRACT : STAGE - GRID v1 .0 applies . 

2 ## PROVIDED FEEDBACK 

3 - quality of the best policy on the last stage : {0.000} 

4 - quality of the best policy on previous stages : {0.000 , 0.000} 

5 - quality on average of all stages : {0.000} 

6 ## REMINDER ( do not change ) 

7 - Grid : 15 x15 ; rows joined by '|' ( do not use '|' as a cell ). 

8 - Size : exactly 15 rows x 15 columns ; every row must be length 15. 

9 - Alphabet : {w , s , t , e }; exactly one 's ' and one 't '.

10 - Variety : vary the positions of 's ' and 't ' across stages . 

11 - Reachability : at least one 4- connected path of 'e ' from 's ' to 

't ' ( no diagonals ). 

12 - Include : " understood " and " reasoning ". 

13 Don ‚Äôt forget the constraints and validation check . If any output would violate CONTRACT : STAGE - GRID v1 .0 , regenerate internally and only emit a valid JSON object . 

14 - the two plots are attached . The two plots are for reference only ; do not reproduce them in the output . 

15 ## ROW LENGTH VALIDATION ( CRITICAL ) 

16 1. Each row = exactly 15 characters 

17 2. Total of 15 rows 

18 3. Validation method : COUNT each row before output 

19 4. If count != 15 -> REJECT and regenerate entire grid 

A.3.3 3 Response. 

1 {

2 " grid ": " eeeeeeeeeewweee | eeeeeeeeeewwete | eeeeeeeeeewweee | eeeeeeeeeeeeeee | eewwwwwwwwwweee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeewwwwwwwweee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | ewwwwweeeeeeeee | eeeeeeeeeeeeeee | eseeeeeeeeeeeee | eeeeeeeeeeeeeee ", 

3 " understood ": " Both stages achieved optimal quality (0.000) ; stage 1's progression shows initial struggle ( starting near 1.0) before converging around evaluation 800 , and the trajectory displays a smooth diagonal path from upper - left to lower - right navigating around the three scattered wall segments ." , 

4 " reasoning ": " With two consecutive optimal solutions , this stage introduces a more complex multi - barrier layout with three horizontal wall segments at different vertical levels ( top , middle , lower - middle ) creating a zigzag navigation requirement from bottom - left start to top - right target , testing the policy 'sability to handle sequential obstacle avoidance ." 

5 }Berfin Sakallioglu, Giorgia Nadizar, and Eric Medvet 

A.4 Stage 3 

A.4.1 1 Optimization. 59.90s ‚Ü∑59.90s ‚Üô59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Üì59.90s ‚Ü∑

A.4.2 2 Feedback. 

1 CONTRACT : STAGE - GRID v1 .0 applies . 

2 ## PROVIDED FEEDBACK 

3 - quality of the best policy on the last stage : {0.023} 

4 - quality of the best policy on previous stages : {0.003 , 0.003 , 0.023} 

5 - quality on average of all stages : {0.0097} 

6 ## REMINDER ( do not change ) 

7 - Grid : 15 x15 ; rows joined by '|' ( do not use '|' as a cell ). 

8 - Size : exactly 15 rows x 15 columns ; every row must be length 15. 

9 - Alphabet : {w , s , t , e }; exactly one 's ' and one 't '.

10 - Variety : vary the positions of 's ' and 't ' across stages . 

11 - Reachability : at least one 4- connected path of 'e ' from 's ' to 

't ' ( no diagonals ). 

12 - Include : " understood " and " reasoning ". 

13 Don ‚Äôt forget the constraints and validation check . If any output would violate CONTRACT : STAGE - GRID v1 .0 , regenerate internally and only emit a valid JSON object . 

14 - the two plots are attached . The two plots are for reference only ; do not reproduce them in the output . 

15 ## ROW LENGTH VALIDATION ( CRITICAL ) 

16 1. Each row = exactly 15 characters 

17 2. Total of 15 rows 

18 3. Validation method : COUNT each row before output 

19 4. If count != 15 -> REJECT and regenerate entire grid 

A.4.3 3 Response. 

1 {

2 " grid ": " eeeeeeeeeeeeeee | eweeeeeeeeeeeee | eweeeeeeeeeweee | eweeeeeeeeeweee | eweeeeeeeeeweee | eweeeeteeeeweee | eweeeeeeeeeweee | eweeeeeeeeeweee | ewwwwwwwwwwweee | eeeeeeeeeeeeeee | eeeeewwwwweeeee | eeeeeeeeeeewwwe | eeeeeeeeeeeeese | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee ", 

3 " understood ": " Stage 2 achieved quality 0.023 after starting at 0.37 and converging by evaluation 2000; the trajectory shows a smooth S - curve path from left - middle start navigating upward through horizontal wall barriers to reach the upper - right target , demonstrating successful zigzag maneuvering ." , 

4 " reasoning ": " With consistently strong performance across stages , this next arena introduces a vertical corridor structure on the left side with the target inside it , plus additional horizontal barriers in the lower - right area where the start is located , requiring the policy to navigate through a more constrained passage while maintaining path diversity from previous horizontal - dominated layouts ." 

5 }

A.5 Stage 4 

A.5.1 1 Optimization. 59.90s ‚Üó59.90s ‚Üó59.90s ‚Üó59.90s ‚Üó59.90s ‚Üô59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Ü∑

A.5.2 2 Feedback. 

1 CONTRACT : STAGE - GRID v1 .0 applies . 

2 ## PROVIDED FEEDBACK 

3 - quality of the best policy on the last stage : {0.119} 

4 - quality of the best policy on previous stages : {0.013 , 0.007 , 0.013 , 0.119} 

5 - quality on average of all stages : {0.0382} 

6 ## REMINDER ( do not change ) 

7 - Grid : 15 x15 ; rows joined by '|' ( do not use '|' as a cell ). 

8 - Size : exactly 15 rows x 15 columns ; every row must be length 15. 

9 - Alphabet : {w , s , t , e }; exactly one 's ' and one 't '.

10 - Variety : vary the positions of 's ' and 't ' across stages . 

11 - Reachability : at least one 4- connected path of 'e ' from 's ' to 

't ' ( no diagonals ). 

12 - Include : " understood " and " reasoning ". 

13 Don ‚Äôt forget the constraints and validation check . If any output would violate CONTRACT : STAGE - GRID v1 .0 , regenerate internally and only emit a valid JSON object . 

14 - the two plots are attached . The two plots are for reference only ; do not reproduce them in the output . 

15 ## ROW LENGTH VALIDATION ( CRITICAL ) 

16 1. Each row = exactly 15 characters 

17 2. Total of 15 rows 

18 3. Validation method : COUNT each row before output 

19 4. If count != 15 -> REJECT and regenerate entire grid 

A.5.3 3 Response. 

1 {

2 " grid ": " eeeeeeeeeeeeeee | eeeewwwweeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | teeeeeeeeeeeeee | eeeeeeeewwwwwwe | eeeeeeeeeeeeeee | wwwwweeeeeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeweeeee | eeeeeeeeeweeeee | eeeeeeeeewwwwwe | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | seeeeeeeeeeeeee ", 

3 " understood ": " Stage 3 reached quality 0.119 after starting at 1.0 and converging around evaluation 5000 , showing a struggle compared to previous near - optimal stages ; the trajectory reveals a curved path from lower - right start navigating leftward around the vertical corridor structure to reach the upper - middle target , indicating the constrained passage was challenging ." , 

4 " reasoning ": " Given the increased difficulty in stage 3 (quality 0.119 vs ~0.01 in prior stages ) , this stage consolidates learning with moderate complexity by placing start at bottom -left and target at top - left on the same vertical edge , introducing scattered horizontal wall segments at different heights that require navigating around but without tight corridor constraints , allowing the policy to refine navigation skills before escalating difficulty further ." 

5 }LLM-assisted CL for MT evolutionary policy search 

A.6 Stage 5 

A.6.1 1 Optimization. 59.90s ‚Üó59.90s ‚Üó59.90s ‚Üó59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üë

A.6.2 2 Feedback. 

1 CONTRACT : STAGE - GRID v1 .0 applies . 

2 ## PROVIDED FEEDBACK 

3 - quality of the best policy on the last stage : {0.048} 

4 - quality of the best policy on previous stages : {0.006 , 0.009 , 0.017 , 0.027 , 0.048} 

5 - quality on average of all stages : {0.0212} 

6 ## REMINDER ( do not change ) 

7 - Grid : 15 x15 ; rows joined by '|' ( do not use '|' as a cell ). 

8 - Size : exactly 15 rows x 15 columns ; every row must be length 15. 

9 - Alphabet : {w , s , t , e }; exactly one 's ' and one 't '.

10 - Variety : vary the positions of 's ' and 't ' across stages . 

11 - Reachability : at least one 4- connected path of 'e ' from 's ' to 

't ' ( no diagonals ). 

12 - Include : " understood " and " reasoning ". 

13 Don ‚Äôt forget the constraints and validation check . If any output would violate CONTRACT : STAGE - GRID v1 .0 , regenerate internally and only emit a valid JSON object . 

14 - the two plots are attached . The two plots are for reference only ; do not reproduce them in the output . 

15 ## ROW LENGTH VALIDATION ( CRITICAL ) 

16 1. Each row = exactly 15 characters 

17 2. Total of 15 rows 

18 3. Validation method : COUNT each row before output 

19 4. If count != 15 -> REJECT and regenerate entire grid 

A.6.3 3 Response. 

1 {

2 " grid ": " eeeeeeeeeeeeeee | ewwwwwwweeeeeee | eeeeeeeeeeeeeee | eeeeeeeweeeeeee | eeeeeeeweeeeeee | eeteeeewwwwwwwe | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | ewwwwweeeeeeeee | eweeeeeeeeeeeee | eweeeeeeeweeeee | eweeeeeeeweeeee | eweeeeeeeweeeee | eweeeeeeewwwwwe | eeeeeeeeeeeeese ", 

3 " understood ": " Stage 4 achieved quality 0.048 after starting at 0.21 and stabilizing around evaluation 2500; the trajectory shows a complex curved path from lower - right area to upper - right target , navigating around a large rectangular enclosure and scattered horizontal barriers , demonstrating successful adaptation to constrained spatial navigation ." , 

4 " reasoning ": " With quality gradually increasing from 0.006 to 0.048 across stages , this next arena introduces a mixed wall configuration : an upper horizontal barrier , a central corridor structure combining vertical and horizontal segments , and alower - left L - shaped corner obstacle , with start positioned in the bottom - left corner and target in the upper - left area to test diagonal navigation through multiple distinct obstacle types ." 

5 }

A.7 Stage 6 

A.7.1 1 Optimization. 59.90s ‚Üó59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Üë59.90s ‚Ü∑59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üë

A.7.2 2 Feedback. 

1 CONTRACT : STAGE - GRID v1 .0 applies . 

2 ## PROVIDED FEEDBACK 

3 - quality of the best policy on the last stage : {0.007} 

4 - quality of the best policy on previous stages : {0.020 , 0.008 , 0.003 , 0.010 , 0.031 , 0.007} 

5 - quality on average of all stages : {0.0129} 

6 ## REMINDER ( do not change ) 

7 - Grid : 15 x15 ; rows joined by '|' ( do not use '|' as a cell ). 

8 - Size : exactly 15 rows x 15 columns ; every row must be length 15. 

9 - Alphabet : {w , s , t , e }; exactly one 's ' and one 't '.

10 - Variety : vary the positions of 's ' and 't ' across stages . 

11 - Reachability : at least one 4- connected path of 'e ' from 's ' to 

't ' ( no diagonals ). 

12 - Include : " understood " and " reasoning ". 

13 Don ‚Äôt forget the constraints and validation check . If any output would violate CONTRACT : STAGE - GRID v1 .0 , regenerate internally and only emit a valid JSON object . 

14 - the two plots are attached . The two plots are for reference only ; do not reproduce them in the output . 

15 ## ROW LENGTH VALIDATION ( CRITICAL ) 

16 1. Each row = exactly 15 characters 

17 2. Total of 15 rows 

18 3. Validation method : COUNT each row before output 

19 4. If count != 15 -> REJECT and regenerate entire grid 

A.7.3 3 Response. 

1 {

2 " grid ": " eeeeeeeeeeeeeee | eewwwwwwweewwwe | eeeeeeeeeeeeeee | eeeeeeeweeeeeee | wwwwweeweeeeeee | eeteeeeweeeeeee | eeeeeeeweeeeeee | eeeeeeeweeeseee | eeeeeeeweeeeeee | eeeeeeeweeeeeee | eeeeeeeeeeeewwe | eeeeeeeeeeeeeee | ewwwwwweeeeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee ", 

3 " understood ": " Stage 5 achieved near - optimal quality 0.007 with rapid convergence by evaluation 1500; the trajectory shows a smooth diagonal path from upper - left to right - middle navigating around the complex multi - level corridor and L - shaped structures , demonstrating strong navigation capabilities through the varied obstacle configurations ." , 

4 " reasoning ": " With consistent strong performance ( average quality 0.0129) , this stage introduces a vertical corridor running through the central - right area with the start embedded within it , requiring the robot to first navigate out of the corridor and then reach the target on the left side , while upper and lower horizontal barriers create additional routing constraints to test escape - and - navigate behavior ." 

5 }Berfin Sakallioglu, Giorgia Nadizar, and Eric Medvet 

A.8 Stage 7 

A.8.1 1 Optimization. 59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Üô59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üë59.90s ‚Ü∑59.90s ‚Ü∑

A.8.2 2 Feedback. 

1 CONTRACT : STAGE - GRID v1 .0 applies . 

2 ## PROVIDED FEEDBACK 

3 - quality of the best policy on the last stage : {0.553} 

4 - quality of the best policy on previous stages : {0.000 , 0.008 , 0.002 , 0.005 , 0.025 , 0.007 , 0.553} 

5 - quality on average of all stages : {0.0856} 

6 ## REMINDER ( do not change ) 

7 - Grid : 15 x15 ; rows joined by '|' ( do not use '|' as a cell ). 

8 - Size : exactly 15 rows x 15 columns ; every row must be length 15. 

9 - Alphabet : {w , s , t , e }; exactly one 's ' and one 't '.

10 - Variety : vary the positions of 's ' and 't ' across stages . 

11 - Reachability : at least one 4- connected path of 'e ' from 's ' to 

't ' ( no diagonals ). 

12 - Include : " understood " and " reasoning ". 

13 Don ‚Äôt forget the constraints and validation check . If any output would violate CONTRACT : STAGE - GRID v1 .0 , regenerate internally and only emit a valid JSON object . 

14 - the two plots are attached . The two plots are for reference only ; do not reproduce them in the output . 

15 ## ROW LENGTH VALIDATION ( CRITICAL ) 

16 1. Each row = exactly 15 characters 

17 2. Total of 15 rows 

18 3. Validation method : COUNT each row before output 

19 4. If count != 15 -> REJECT and regenerate entire grid 

A.8.3 3 Response. 

1 {

2 " grid ": " eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | wwwwweeeeeewwwe | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | eeeewwwwweeeeee | eeeeeeeeeeeeeee | eseeeeeeeeeteee | eeeeeeeeeeeeeee | eeeewwwwweeeeee | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee | wwwwweeeeeewwwe | eeeeeeeeeeeeeee | eeeeeeeeeeeeeee ", 

3 " understood ": " Stage 6 showed quality 0.553 with a completely flat progression curve remaining at that level across all 10 ,000 evaluations , indicating the vertical corridor structure was too challenging ; the trajectory plot reveals the robot struggled to escape the constrained passage to reach the left - side target ." , 

4 " reasoning ": " Given the significant difficulty spike in stage 6 ( quality jumped from 0.007 to 0.553 with no learning progress ) , this stage steps back in complexity by providing an open arena with symmetrically placed horizontal wall segments at the top , middle , and bottom levels , requiring basic obstacle avoidance without tight constraints , allowing the policy to rebuild confidence and navigation skills before attempting more complex corridor - based challenges ." 

5 }

A.9 Stage 8 

A.9.1 1 (Last) optimization. 59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Üô59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üó59.90s ‚Ü∑59.90s ‚Üë59.90s ‚Ü∑59.90s ‚Ü∑