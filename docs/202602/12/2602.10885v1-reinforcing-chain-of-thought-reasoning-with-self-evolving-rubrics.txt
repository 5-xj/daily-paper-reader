Title: Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics

URL Source: https://arxiv.org/pdf/2602.10885v1

Published Time: Thu, 12 Feb 2026 02:14:24 GMT

Number of Pages: 21

Markdown Content:
# Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics 

## Leheng Sheng 1,2,∗,‡, Wenchang Ma 1,∗,†, Ruixin Hong 1,

## Xiang Wang 3, An Zhang 3,†, Tat-Seng Chua 2

> 1

Seed, ByteDance , 2National University of Singapore ,

> 3

University of Science and Technology of China 

> ∗

Equal Contribution , ‡Work done at ByteDance Seed , †Corresponding authors 

## Abstract 

Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose RLCER (Reinforcement 

Learning with CoT Supervision via Self-Evolving Rubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.  

> Date:

February 12, 2026  

> Correspondence:

An Zhang at an_zhang@ustc.edu.cn Wenchang Ma at mawenchang.99@bytedance.com  

> Project Page:

https://alphalab-ustc.github.io/rlcer-alphalab/ 

(a) Average performance dynamics. (b) Performance comparison. 

Figure 1 Performance across three math datasets on the 7B model. Training with RLCER leads to a higher performance ceiling, and the self-evolving rubrics further enhance reasoning performance. 

1

> arXiv:2602.10885v1 [cs.AI] 11 Feb 2026

## 1 Introduction    

> Figure 2 Key idea of reinforcement learning with CoT supervision via self-evolving rubrics (RLCER). The policy model πθacts as both the reasoner and the rubrics generator, self-generating and self-evolving the rubrics for CoT supervision, where the evolving direction is shaped towards the correlation with the rubrics satisfaction and the final answer correctness.

Chain-of-thought (CoT) [ 27 ] reasoning is essential for large language models (LLMs) to solve complex tasks, where its quality strongly affects the final-answer correctness [ 3, 9, 15 , 39 ]. However, CoT is rarely rewarded or optimized explicitly, even often regarded as a by-product of outcome-centric optimization objectives in large-scale reinforcement learning with verifiable rewards (RLVR) [ 7 , 13 , 23 ]. This yields an underconstrained learning signal: for the same final answer, many distinct reasoning trajectories receive identical rewards, allowing optimization to drift toward shortcut or brittle strategies [ 33 , 40 ]. Consequently, the model can converge to sub-optimal reasoning behaviors, limiting robustness and overall capability [16]. However, while important, directly rewarding or supervising CoTs remains challenging in practice, mainly for two reasons: (i) training additional reward models (RMs) for CoT supervision typically requires labor-intensive and fine-grained annotations [ 6, 16 ], and (ii) the policy’s CoT distribution shifts during training, yielding non-stationary and potentially biased supervision. Can the policy model self-propose rubrics as CoT supervision criteria and self-evolve them during training with no human annotations? If so, this could enable a new paradigm of self-improving reasoning, shifting RL from optimizing what LLMs answer to autonomously improving how LLMs think, potentially yielding “free-lunch” reasoning gains. We find possibilities of achieving this from the recent advances in the research line of self-evolving training methods [ 10 , 24 ], where a policy model πθ can improve itself with little to no human intervention during training. On the one hand, πθ itself can self-generate training data, such as verifiable question-answer pairs for RL or high-quality CoT trajectories for cold-starting reasoning [ 17 , 19 , 32 , 37 ]. On the other hand, πθ

itself can self-assign reward signals by acting as a judge to evaluate its own solutions [ 4, 8, 29 , 31 ]. A key idea shared by these methods is to let a single policy model πθ play multiple roles under different prompts, serving not only as a reasoner that produces solutions but also as a data generator or a reward provider [ 31 , 37 ], with all roles optimized jointly (e.g., via techniques such as multi-agent reinforcement learning [ 37 ]). Together, these capabilities make it plausible to self-propose supervision criteria (e.g., rubrics) for assessing CoT quality and to self-evolve as training progresses. However, how to adaptively propose and evolve such criteria in the self-evolving training paradigm remains largely underexplored. To fill this gap, we propose RLCER (Reinforcement Learning with CoT Supervision via Self-Evolving Rubrics), empowering RLVR with self-proposed and self-evolving rubrics to supervise CoT reasoning. The core idea is to let the policy model πθ self-propose CoT supervision criteria as natural-language rubrics (i.e., describing desirable reasoning properties like “avoid tangential explorations”) [ 11 , 35 ], rewarding CoTs by their rubric satisfaction, and simultaneously improving the model’s rubric-generation ability throughout training. An overview of the key process is illustrated in Figure 2. Specifically, a single policy model πθ is instantiated 2with different prompts to play two roles: a reasoner πRea θ that generates CoTs ˆC and final answers ˆA, and a rubricator πRub θ that generates a set of candidate rubrics ˆR ≜ {ˆτk}Kk=1 , where K is the number of rubrics 

ˆτk. The reasoner is rewarded by both the final answer correctness and the CoT quality, which is measured by satisfaction of all valid rubrics ˆτ valid k . Here, we deem a rubric ˆτk valid if its satisfaction indicator vk is strongly correlated with final-answer correctness z across multiple sampled rollouts for the same question Q

(i.e., corr (vk, z) > α ), since a high correlation means that satisfying the rubric aligns with producing correct answers. Additionally, we encourage the rubricator to self-evolve its rubric generation capability by rewarding the fraction of valid rubrics among its proposals (i.e., |{ ˆτ valid k }|  

> K

). Generally, RLCER enables RLVR to move beyond outcome-centric optimization by rewarding CoTs with supervision criteria that are self-proposed and self-evolved, without any human annotation. We further conduct comprehensive experiments to examine the effectiveness of the proposed RLCER. First, we conduct a primary study showcasing that self-proposed rubrics can be reliable and learnable even when training is driven solely by rubric-based rewards, without any outcome reward (cf. Section 5.2). Based on this insight, RLCER generally outperforms the naive RLVR (cf. Section 5.3). Additionally, introducing self-evolving training for the rubricator further improves the reasoning performance (cf. Section 5.4). Moreover, the generated rubrics can even serve as in-prompt hints to enhance reasoning at inference time (cf. Section 5.5). Overall, our results suggest a new paradigm in which models autonomously generate CoT supervision criteria to strengthen RL training, pointing toward more self-improving and self-evolving reasoning systems. 

## 2 Related Works 

2.1 LLM Reasoning with RL 

Reinforcement learning has become an essential technique for incentivizing chain-of-thought (CoT) reasoning in LLMs [ 36 , 38 ]. A milestone paradigm is reinforcement learning with verifiable rewards (RLVR) [ 7, 13 , 23 ], where LLMs are trained on verifiable tasks such as math problems [ 20 ], puzzle solving [ 5], and code execution [ 14 ], by rewarding final-answer correctness using task-specific verifiers. Since such tasks are intrinsically difficult to reward-hack, RLVR tends to encourage LLMs to produce longer CoTs before answering, thereby improving reasoning performance [ 7, 13 ]. However, despite its effectiveness in incentivizing reasoning, most RLVR training remains outcome-centric and provides no direct supervision over the CoT itself, lacking guidance on how the model should reason. While several efforts have tried to mitigate this by assigning process rewards using additional reward models (RM) [ 16 ], training such RMs requires labor-intensive training data collection [ 6, 16 , 39 ], and a static RM often fails to keep pace with the evolving CoT distribution during training [ 7 ]. Therefore, such challenges hinder the combination of RLVR training with CoT supervision criteria for rewarding in practice. 

2.2 Self-Evolving in LLMs 

A critical step toward general artificial intelligence lies in the ability to self-evolve with little or no human intervention [ 10 ]. Recent work has made progress via self-training and self-play, where a single policy model 

πθ can self-generate training data [ 17 , 19 , 37 ], or self-provide reward signals during learning [ 4 , 8, 31 ]. The self-evolving capabilities can further be strengthened with techniques such as multi-role reinforcement learning, where a single policy model πθ is prompted to play different roles (e.g., reasoner, data generator, or reward provider) and trained with role-specific rewards [ 17 , 34 , 37 ]. Therefore, these advances in such self-evolving methods suggest the possibilities of self-proposing criteria for CoT supervision, while this direction remains largely under-explored. 

2.3 Reinforcement Learning with Rubrics 

Recently, reinforcement learning with rubrics has emerged as a promising way to provide fine-grained and interpretable reward signals [ 2, 11 , 12 , 35 ]. Here, rubrics are structured, checkable evaluation criteria (i.e., explicit requirements for assessing outputs) [ 11 ] that decompose high-level objectives into multi-facet supervision signals [ 22 ], enabling RL to extend towards non-verifiable and open-ended domains [ 11 , 40 ]. However, existing work largely applies rubrics to score response quality, leaving their effectiveness for CoT 3supervision underexplored. Moreover, most approaches rely on predefined or static rubrics, which may become mismatched as model behaviors evolve during training [ 22 ]. To address these gaps, we introduce a framework that self-proposes rubrics as CoT supervision criteria and continually evolves them throughout training, integrating rubric-based RL for CoT supervision with self-evolving training. 

## 3 Preliminaries 

In this section, we briefly review the necessary background that forms the foundation of our work, including incentivizing LLM reasoning with RL in Section 3.1 and multi-role RL under a single policy in Section 3.2. 

3.1 Incentivizing LLM Reasoning with RL 

In this work, we focus on incentivizing LLMs to reason with RL, where the policy model is optimized by rewarding the high-quality rollouts.  

> Rollout.

Given a question Q, the policy model πθ attempts to solve it by generating a response that contains a CoT ˆC followed by an answer ˆA, which can be formulated as: 

ˆC, ˆA ∼ πθ (· | Q ). (1)  

> Rewarding.

After rollouts generation, rewards are assigned based on response quality, which can depend on: (i) the CoT ˆC and (ii) the answer ˆA. In the widely adopted RLVR paradigm, rewards typically depend solely on the correctness of the predicted answer ˆA:

r = ψ(I(A, ˆA)) , (2) where I(·, ·) is an answer correctness indication function which returns 1 if two answers A and ˆA are equivalent and 0 otherwise, and a reward assignment function ψ maps this correctness signal I(A, ˆA) to a binary reward 

r. Through such RLVR training, the policy model πθ gradually learns to answer questions, which is often associated with producing longer CoTs [ 7]. However, under the RLVR paradigm, the CoT ˆC itself is rarely rewarded directly due to practical implementation challenges [ 7], while the lack of such supervision may induce sub-optimal reasoning strategies [16]. 

3.2 Multi-Role RL Under a Single Policy Model 

Recently, studies have shown that a single policy model πθ can effectively learn multiple roles by assigning role-specific rewards [18, 37]: 

rrole i → πrole i 

> θ

, (3) where rrole i denotes the reward assigned to role i, and πrole i 

> θ

represents the same underlying policy model πθ

instantiated under the prompt Prole i corresponding to that role. A typical application under this framework is to let the model simultaneously act as a reasoner and a data generator, achieving RL training starting from no human-labeled data [ 19 , 24 , 37 ]. More broadly, this multi-role, single-policy setup provides a natural substrate for self-evolving training: different roles instantiated from the same model can compete [ 17 , 37 ] or collaborate [ 18 , 34 ], continuously improving the learning signals and behaviors [ 26 ]. Building on this view, we investigate whether the same policy can simultaneously serve as a reasoner and a CoT rubrics generator (i.e., rubricator). With role-specific rewards, the rubricator is encouraged to produce useful CoT supervision criteria, which in turn enables the self-evolution of CoT rewarding. In this case, the reasoner and rubricator collaborate together for better reasoning. 

## 4 Methodology 

In this section, we introduce RLCER (Reinforcement Learning with CoT Supervision via Self-Evolving Rubrics) for bootstrapping chain-of-thought reasoning. In the remainder of this section, we first briefly introduce the key idea of RLCER in Section 4.1 to facilitate a high-level understanding. Then, we discuss the multiple roles under the same policy model in Section 4.2. After that, we discuss how to incentivize reasoning with 4self-proposed and rubrics in Section 4.3. We then discuss how to self-evolve such rubrics in Section 4.4. Finally, we present how to optimize the policy πθ under the two roles based on role-specific rewards in Section 4.5. 

4.1 Key Idea  

> Figure 3 The RLCER loop. Format reward is ignored for brevity. One single policy model self-proposes rubrics for rewarding CoTs, and self-evolves the rubrics via rewarding generation capabilities.

The key idea of RLCER is to go beyond rewarding what LLMs answer (i.e., final-answer correctness) to explicitly and autonomously reward how LLMs think (i.e., CoT quality) using self-proposed, self-evolving rubrics. The rubrics are evolved by tracking how well their satisfaction correlates with the final answer correctness. To achieve this goal, we introduce two key roles in the multi-role RL framework for optimization: the rea-soner πRea θ for answering the question and the rubri-cator πRub θ for generating rubrics supervising CoTs. Here, the reasoner πRea θ (·) = πθ (· | P Rea ) and the rubricator πRub θ (·) = πθ (· | P Rub ) share the same pol-icy parameter θ, while their roles are distinguished by different prompts PRea and PRub respectively. The prompts can be found in Appendix B. Additionally, a verifier πϕ is used for judging the satisfaction of rubrics, which is another fine-tuned and frozen model parameterized by ϕ.As shown in Figure 3, we explicitly refine how the reasoner πRea θ thinks, by rewarding the CoT quality beyond the answer correctness, with the CoT quality evaluated by the verifier πϕ based on rubrics generated by the rubricator. We further enable rubric generation to self-evolve for more autonomous and effective CoT supervision by rewarding the quality of generated rubrics, guided by the correlation between rubric satisfaction degree and answer correctness. We jointly optimize the policy model πθ under two roles by assigning different rewards for each role. After computing role-specific rewards, we optimize the shared policy model πθ for both roles, with advantages computed separately for each role [17]. 

4.2 Two Roles in One Policy: Reasoner and Rubricator 

In this section, we take a closer look at the different roles of the reasoner πRea θ and the rubricator πRub θ , within the RLCER framework.  

> Reasoner

πRea θ . The reasoner answers the question by generating a CoT ˆC and a final answer ˆA. Given the question Q, the rollout sampling process of the reasoner can be formulated as: 

ˆC, ˆA ∼ πRea θ

 · | Q , (4) where ˆC denotes the CoT process, and ˆA denotes the predicted final answer.  

> Rubricator

πRub θ . The rubricator generates a set of K rubrics for evaluating the CoT qualities based on the question Q and one CoT ˆC. Given the question Q and a generated CoT ˆC, the rollout sampling process of the rubricator can be formulated as: 

ˆR ∼ πRub θ

 · | Q , ˆC, ˆR ≜ {ˆτk}Kk=1 , ˆτk ≜ (ˆ ck, ˆsk). (5) The generated textual output ˆR contains K specific rubrics, each denoted as ˆτk. Here, ˆck is the textual criterion (e.g., “avoids tangential explorations post-interval validation (3-∞) by focusing on count aggregation”) of rubric ˆτk and ˆsk ∈ R is the score of rubric ˆτk reflecting its importance, which can be both positive or negative. The rubrics generated under the rubricator role will then be used for judging the CoT quality through an external verifier πϕ.54.3 Rewarding How to Think via Self-Proposed Rubrics 

In this section, we discuss how to autonomously reward how LLMs think via self-proposed rubrics.  

> Rewarding CoT with self-proposed rubrics.

Specifically, we reward a CoT based on how well it satisfies the valid rubrics proposed by the rubricator πRub θ . The CoT reward is computed by aggregating the scores of the satisfied rubrics: 

rRea cot = norm 

 X  

> ˆτk∈ˆRvalid

πϕ



ˆck, ˆC



· ˆsk

 , (6) where ˆRvalid denotes the set of valid rubrics from the rubricator, πϕ( ˆck, ˆC) ∈ { 0, 1} is a binary value produced by the verifier πϕ indicating whether the CoT ˆC satisfies rubric ˆck, and norm (·) is a normalization function that maps the aggregated score to the interval [0 , 1] . The function norm (·) conducts the min–max normalization as norm (x) = ( x − MinValue )/(MaxValue − MinValue ), where MinValue and MaxValue denote the minimum and maximum aggregated rubric scores attainable under the valid rubrics ˆRvalid , respectively. 

> Valid rubrics

. Here, a rubric ˆτk is considered valid if it is informative as a rewarding criterion: (i) its satisfaction indicator vk is positively correlated with final-answer correctness z across N sampled rollouts of the same question Q (i.e., corr (vk, z) > α ), and (ii) it is discriminative across different CoTs and would not yield consistent satisfaction (i.e., std (vk) > 0). We define vk and z over these N rollouts as: 

vk ≜

h

πϕ(ˆ ck, ˆC0), . . . , π ϕ(ˆ ck, ˆCN )

i

,

z ≜

h

I(A, ˆA0), . . . , I(A, ˆAN )

i

.

(7) The intuition is that higher correlation implies that satisfying the rubric is consistently associated with correct final answers and can produce informative rewarding signals. Here α is set as 0.2 by default in our paper.  

> Outcome reward.

The CoT reward rRea cot can be used as an auxiliary reward alongside the outcome reward 

rRea outcome , which is calculated as follows: 

rRea outcome =

(

1, is_equiv (A, ˆA)

−1, Otherwise. (8) Following DAPO [ 30 ], we assign a binary outcome reward based on final answer correctness: rRea outcome =1 if the predicted answer ˆA is equivalent to the ground truth answer A, and rRea outcome =-1 otherwise.  

> Total reward for reasoner.

Finally, the overall reward for the reasoner is the combination of the outcome reward and the CoT reward: 

rRea = rRea outcome + rRea cot . (9) Such a reward enables autonomously optimizing what LLMs answer and how LLMs think simultaneously. 

4.4 Rubrics Self-Evolving for Better Supervision 

While the two-role design enables a single policy model to self-propose rubrics for CoT supervision, we argue that such rubrics may get saturated: once consistently satisfied by all rollouts, they lose discriminative power over CoT quality and no longer provide reward signals, as shown in Section 4.3. Therefore, we further propose to self-evolve the rubric generation capability, so as to progressively produce more informative and challenging rubrics that mitigate saturation.  

> Self-evolving with validity reward.

To achieve rubricator self-evolving, we reward the fraction of the number of valid rubrics among all generated rubrics as follows: 

rRub evolving = Kvalid 

K , (10) 6Figure 4 Illustration of the reward calculation process in RLCER. For question Q, the reasoner generates N responses each with CoT ˆCk and the final answer ˆAk . After that, the rubricator generates Kn specific rubrics (i.e., 

ˆRn ≜ {ˆτk }Kn

> k=1

). The outcome reward is applied first by matching the generated answer ˆAk with the ground-truth answer A. All the valid rubrics (i.e., corr (vk , z) > α and std (vk ) > 0) are collected for rewarding CoTs. And the fraction of valid rubrics for the k-th rubricator generation (i.e., |{ ˆτ valid n,k }|  

> Kn

) is used for rewarding the rubricator to self-evolve. 

where K and Kvalid ≜ |{ ˆτ valid k }| are the numbers of all generated rubrics and valid rubrics, respectively. Such rewards encourage the rubricator to generate more informative rubrics that are positively correlated with answer correctness (i.e., corr (vk, z) > α ) and discriminative (i.e., std (vk) > 0), thereby preventing rubric saturation. This reward provides a simple way for evolving the quality of generated rubrics, by connecting the satisfaction of the rubrics with the final answer correctness.  

> Format reward.

Additionally, to ensure the proposed rubrics can be stably parsed, we reward the rubricator with a format reward, rewarding the correct format as follows: 

rRub f ormat =

(

1, The format is correct. 

0, Otherwise. (11)  

> Total reward for rubricator.

Finally, the total reward for the rubricator is calculated as the combination of the quality reward and the format reward: 

rRub = rRub evolving + rRub f ormat . (12) Such a reward design encourages the rubricator to continuously refine its proposed rubrics, consistently providing informative rewarding signals for CoT supervision. 

4.5 Two-Role Optimization under a Single Policy 

After computing the rewards for the reasoner and the rubricator, we jointly optimize the shared policy model πθ under both roles in an end-to-end manner. Specifically, we compute role-specific advantages ˆARea t

and ˆARub t from their respective rewards rRea and rRub , and use them to update the policy via a standard policy-gradient objective. Gradients from both roles are aggregated to update the same set of parameters 

θ, allowing the model to learn from role-specific feedback while maintaining a unified policy. The overall optimization procedure is illustrated in Algorithm 1. 7Algorithm 1: Policy Optimization with RLCER 

Input: Question Q, answer A, the rollout sampling number N , and the maximum training steps T .

Initialization: initialize policy model πθ , training step t ← 0.

while t ≤ T do 

> // sample reasoner rollouts

{ ˆCn, ˆAn}Nn=1 ∼ πRea θ (· | Q )

> // sample rubricator rollouts

{ ˆRn}Nn=1 ∼ πRub θ (· | Q , ˆCn)

> // calculate reasoner rewards

{rRea n }Nn=1 = {rRea outcome,n + rRea cot,n }Nn=1 

> // calculate rubricator rewards

{rRub n }Nn=1 = {rRub evolving,n + rRub f ormat,n }Nn=1 

> // calculate role-specifc advantages

Calculate ˆARea t and ˆARub t

> // policy optimization

Update policy πθ via J (θ)

t ← t + 1 

end while 

return πθ .

> Role-specific rollout
> Rewarding
> Policy updating

With role-specific advantages ˆARea t and ˆARub t , the optimization objective can be formulated as follows: 

J (θ) = E(Q,A)∼D Rea , o∼πRea θold (·|Q )

h

min 



ρt(θ) ˆARea t , clip  ρt(θ), 1 − ϵ, 1 + ϵ ˆARea t

i 

,

+ E(Q, ˆC)∼D Rub , o∼πRub θold (·|Q , ˆC)

h

min 



ρt(θ) ˆARub t , clip  ρt(θ), 1 − ϵ, 1 + ϵ ˆARub t

i 

,

(13) where ρt(θ) is the policy likelihood ratio between πθ and πθold at step t, and ϵ is the clipping hyperparameter (e.g., 0.2). It is worth noting that GRPO is unsuitable for our algorithm, since the rubricator operates under different contexts, preventing its rollouts from being grouped under the same context required by GRPO [ 23 ]. 

## 5 Experiments 

• RQ1: Can self-proposed rubrics provide reliable rewarding signals for RL training? 

• RQ2: How does RLCER enhance the reasoning capabilities of LRMs? 

• RQ3: How does the self-evolving enable the rubricator to gradually propose better rubrics? 

• RQ4: How effective are generated rubrics at facilitating LLM reasoning when used as in-prompt hints? 

5.1 Experiment Setup 

We briefly introduce the experiment setup of our paper, and more details can be found in Appendix A. 

LLMs. We conduct experiments on two open-source Qwen models of different sizes (8B and 4B) [ 28 ]. We first cold-start the pre-trained models (i.e., Qwen3-8B-Base and Qwen3-4B-Base) by performing supervised fine-tuning on correctly formatted data obtained via reject sampling from a stronger teacher model, as we observe that even post-trained versions of such small-sized models (e.g., Qwen3-8B) cannot always strictly follow the required instructions when acting as the rubricator and reasoner. More details can be found in Appendix A. All subsequent RL experiments are conducted based on these carefully cold-started models. 

Training and evaluation. We train models based on the DAPO-Math-17k dataset [ 30 ], which contains 17k high-quality math questions. We evaluate models on various reasoning benchmarks. For math reasoning, we evaluate math-reasoning benchmarks including AIME24 [ 20 ], AIME25 [ 20 ], and AMC23 [ 1 ]. For general knowledge 8reasoning, we evaluate on GPQA-Diamond [ 21 ] and SuperGPQA [ 25 ]. For the large size of SuperGPQA, we evaluate on three subsets, each including 100 questions, namely SuperGPQA-Eng, SuperGPQA-Med, and SuperGPQA-Sci. For each testing question, we sample N =16 responses with sampling temperature as 0.7. We report pass@1 as the accuracy metric (i.e., the average pass rate among 16 responses). For all RL training, we fix the max training steps to 1500. 

5.2 (RQ1) Reliability of Self-Proposed Rubrics     

> (a) Accuracy on AMC23. (b) Accuracy on AIME25.
> Figure 5 Accuracy dynamics when only rewarding CoTs. Even when rewarding CoTs with self-proposed rubrics without any outcome reward can bring consistent performance gain.

A key factor that determines the success of our method lies in whether the self-proposed and self-evolving rubrics can provide meaningful learning signals for RL. To verify this, we first conduct a preliminary experiment, considering an outcome-free setting that trains the model by only reward-ing CoTs based on self-proposed rubrics without any outcome rewards (i.e., rRea = rRea cot ). If the model still learns to reason effectively under this setup, it would indicate that self-proposed rubrics can yield meaningful RL signals. We re-port the accuracy dynamics on reasoning tasks during the training in Figure 5, where the bold lines are the rolling averaged performance with a window size of 3 and the slim and shallow lines are the original performance. We have the following observation:  

> Self-proposed rubrics can provide meaningful CoT rewarding signals for RL training.

As shown in this figure, by only rewarding CoTs with the self-proposed rubrics (i.e., Rubric Only in the figure), the reasoning capability of the LLM shows consistent improvement during training. In contrast, when we assign random values between 0 and 1 as rubric rewards (i.e., Rubric Only (Random) in the figure), the LLM fails to achieve performance improvements and even exhibits sudden performance drops during training around 200 steps. Such results show that the self-proposed and self-evolving rubrics can yield meaningful CoT rewarding signals, which lay an important foundation for our approach RLCER. 

5.3 (RQ2) Effectiveness of RLCER 

Based on the reliability of self-proposed rubrics, we further test the effectiveness of combining both rRea outcome 

and rRea cot in RLCER. We compare RLCER with the vanilla RLVR which only considers the outcome reward, and report the performance comparison in Table 1. Here, both RLCER and RLVR start from the SFT checkpoint. We have the following observations:  

> RLCER outperforms the outcome-centric RLVR training.

As shown in Table 1, training with RLCER outperforms the vanilla RLVR across multiple datasets and multiple model sizes on average. Even when trained only on math datasets, RLCER also generalizes well on general reasoning tasks (i.e., GPQA series.) Additionally, we observe that the performance gain is more obvious on larger size models (i.e., 8B models benefit more from RLCER than 4B models). Such phenomena validate the effectiveness of RLCER, which provides a “free-lunch” for improving the RL performance, by enhancing it with self-proposed and self-evolving rubrics. Case studies about the generated rubrics can be found in Appendix C. 

5.4 (RQ3) Mechanism of Rubrics Self-Evolving 

In this section, we study how the rubrics self-evolving affects training within the RLCER framework (i.e., the role of the reward rRub evolving ). To study this, we conduct an ablation study on the 8B-sized model by abandoning the evolving reward rRub evolving , and the rubricator is only rewarded with the format reward. We report the training dynamics in Figure 1 and Figure 6 including the averaged performance (i.e., averaged accuracy among AIME2024, AIME2025, and AMC2023), the averaged correlation between rubrics satisfaction and final answer correctness (i.e., corr (vk, z)), and the CoT reward of the reasoner (i.e., rRea cot ). The performance is reported 9Table 1 The performance comparison across diverse reasoning benchmarks.                                                                 

> Datasets Scale Method AIME2024 AIME2025 AMC2023 GPQA-Diamond SuperGPQA-Eng SuperGPQA-Med SuperGPQA-Sci
> 4B Base 11.25 6.46 31.09 7.77 18.50 14.25 15.44 SFT 17.29 18.96 59.53 24.43 31.69 28.25 27.81 + RLVR 29.38 30.21 79.53 44.16 39.75 28.50 42.88
> + RLCER 29.38 30.63 81.88 44.91 40.19 31.63 41.81 8B Base 11.46 9.58 42.34 24.37 33.06 24.69 23.63 SFT 22.29 23.75 66.41 31.72 36.00 33.75 35.19 + RLVR 34.79 32.50 84.53 46.56 42.94 38.31 48.81 + RLCER 37.50 33.33 86.41 48.77 45.00 36.50 50.25

as the rolling average of 5 scores for a clearer demonstration, where the bold lines report the rolling average scores and the shallow and slim lines report the raw scores. We have the following observations: 

(a) Correlation dynamics. (b) rRea cot dynamics. 

Figure 6 The effect of self-evolving on correlation and CoT-reward dynamics. 

Self-evolving rubrics enable better performance. 

As shown in Figure 1, by rewarding with rRub evolving ,RLCER exhibits a more stable learning curve and gradually outperforms the ablation version (i.e., RLCER (w/o evolving)). Additionally, both RLCER and RLCER (w/o evolving) outperform the naive RLVR method as it quickly converges to a sub-optimal performance. Such phenomena show the effectiveness of self-evolving in enhanc-ing reasoning. 

Self-evolving with rRub evolving enables more infor-mative rubrics. We further analyze whether self-evolving enables the policy model to propose better rubrics. As shown in Figure 6a, the averaged correlation between rubric satisfaction and final answer correctness (i.e., corr (vk, z)) increases along the training process, while the ablation baseline shows an unchanged correlation. This indicates that self-evolving enables the policy model to gradually propose more informative rubrics that better align with the final answer correctness. Additionally, with such evolved rubrics, it gradually becomes harder to gain the CoT reward rRub evolving as the Figure 6b shows a decreasing trend during the RLCER training. While the ablation baseline’s rubrics can be gradually satisfied by πRea θ as rRub evolving 

increases. Such results highlight the importance of rubrics self-evolving. 

5.5 (RQ4) Effectiveness of Rubrics as In-prompt Reasoning Hints 

Figure 7 Performance with rubrics as in-prompt hints. 

In this section, we attempt to understand how the generated rubrics can guide the model to-ward better reasoning. While we have verified that self-proposed and self-evolved rubrics can generate valid signals for rewarding CoTs, the mechanism through which these rubrics implic-itly steer reasoning processes remains elusive, largely because the underlying reward signals are difficult to analyze. To this end, we explicitly employ these self-proposed rubrics as in-prompt hints, and evaluate the model’s reasoning per-formance under this setup. If incorporating the rubrics into the prompt yields performance improvements, it demonstrates that the rubrics offer valid guidance for the reasoning path. We report the performance of Qwen3-8B with a max response length of 20480 in Figure 7. As shown in the figure, when using generated rubrics as in-prompt hints, the performance improves, 10 indicating that these rubrics can indeed guide better reasoning. Additionally, the performance of BoN (N=16) improved even further on the AIME datasets, indicating that the rubrics introduced as hints may incentivize LLMs to conduct more exploration. 

## 6 Limitations 

There are still several limitations in this paper. On the one hand, the introduction of the rubricator role increases the rollout burden and thereby requires more training time. On the other hand, our method is still quite limited to the RLVR domain, leaving the effectiveness of rewarding with self-proposed rubrics on non-verifiable domains unknown. 

## 7 Conclusion 

Outcome-centric RLVR improves final-answer accuracy but largely ignores direct supervision for CoTs, risking suboptimal reasoning strategies. However, directly rewarding CoT is challenging since reward models are costly to train and brittle under distribution shift and reward hacking. To address these issues, we proposed RLCER, which rewards CoTs with self-proposed, self-evolving rubrics without human annotation. Experiments showed that RLCER consistently outperforms outcome-only RLVR across datasets and LLM backbones, highlighting that such self-proposed and self-evolving rubrics can provide meaningful CoT supervision signals pushing the upper bound of LLM reasoning. Our future work will include exploring the generalization of RLCER to non-verifiable domains. 11 References 

[1] AI-MO. American mathematics contest. https://huggingface.co/datasets/AI-MO/aimo-validation-amc ,2023. [2] Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, and Karan Singhal. Healthbench: Evaluating large language models towards improved human health. CoRR, abs/2505.08775, 2025. [3] Edward Y. Chang, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. CoRR, abs/2502.03373, 2025. [4] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. In ICML. OpenReview.net, 2024. [5] François Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. ARC-AGI-2: A new challenge for frontier AI reasoning systems. CoRR, abs/2505.11831, 2025. [6] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards. CoRR, abs/2502.01456, 2025. [7] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [8] Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kai-Xuan Chen, Mingli Song, and Dacheng Tao. Serl: Self-play reinforcement learning for large language models with limited data. CoRR ,abs/2505.20347, 2025. [9] Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, and Anthony Hartshorn. What characterizes effective reasoning? revisiting length, review, and structure of cot. CoRR, abs/2509.19284, 2025. [10] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenhailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, and Mengdi Wang. A survey of self-evolving agents: On path to artificial super intelligence. CoRR, abs/2507.21046, 2025. [11] Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. CoRR, abs/2507.17746, 2025. [12] Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, Xijun Gu, Peiyi Tu, Jiaxin Liu, Wenyu Chen, Yuzhuo Fu, Zhiting Fan, Yanmei Gu, Yuanyuan Wang, Zhengkai Yang, Jianguo Li, and Junbo Zhao. Reinforcement learning with rubric anchors. CoRR, abs/2508.12790, 2025. [13] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, 

12 Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. [14] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In ICLR. OpenReview.net, 2025. [15] Gangwei Jiang, Yahui Liu, Zhaoyi Li, Qi Wang, Fuzheng Zhang, Linqi Song, Ying Wei, and Defu Lian. What makes a good reasoning chain? uncovering structural patterns in long chain-of-thought reasoning. CoRR, abs/2505.22148, 2025. [16] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In ICLR, 2024. [17] Bo Liu, Chuanyang Jin, Seungone Kim, Weizhe Yuan, Wenting Zhao, Ilia Kulikov, Xian Li, Sainbayar Sukhbaatar, Jack Lanchantin, and Jason Weston. SPICE: self-play in corpus environments improves reasoning. CoRR ,abs/2510.24684, 2025. [18] Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, and Natasha Jaques. Chasing moving targets with online self-play reinforcement learning for safer language models. CoRR, abs/2506.07468, 2025. [19] Hongliang Lu, Yuhang Wen, Pengyu Cheng, Ruijin Ding, Haotian Xu, Jiaqi Guo, Chutian Wang, Haonan Chen, Xiaoxi Jiang, and Guanjun Jiang. Search self-play: Pushing the frontier of agent capability without supervision. CoRR, abs/2510.18821, 2025. [20] Mathematical Association of America. American invitational mathematics examination (aime), February 2024. URL https://maa.org/math-competitions/american-invitational-mathematics-examination-aime .[21] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. [22] Rulin Shao, Akari Asai, Shannon Zejiang Shen, Hamish Ivison, Varsha Kishore, Jingming Zhuo, Xinran Zhao, Molly Park, Samuel G Finlayson, David Sontag, et al. Dr tulu: Reinforcement learning with evolving rubrics for deep research. arXiv preprint arXiv:2511.19399, 2025. [23] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR ,abs/2402.03300, 2024. [24] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017. [25] M-A-P Team. Supergpqa: Scaling LLM evaluation across 285 graduate disciplines. CoRR, abs/2502.14739, 2025. [26] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and Hoang D. Nguyen. Multi-agent collaboration mechanisms: A survey of llms. CoRR, abs/2501.06322, 2025. [27] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. [28] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai 

13 Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. [29] Zhiling Ye, Yun Yue, Haowen Wang, Xudong Han, Jiadi Jiang, Cheng Wei, Lei Fan, Jiaxin Liang, Shuowen Zhang, Ji Li, Chunxiao Guo, Jian Wang, Peng Wei, and Jinjie Gu. Self-rewarding rubric-based reinforcement learning for open-ended reasoning. CoRR, abs/2509.25534, 2025. [30] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an open-source LLM reinforcement learning system at scale. In NeurIPS, 2025. [31] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. In ICML, 2024. [32] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. In NeurIPS, 2022. [33] Charlie Zhang, Graham Neubig, and Xiang Yue. On the interplay of pre-training, mid-training, and rl on reasoning language models. arXiv preprint arXiv:2512.07783, 2025. [34] Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme, Daniel Khashabi, Jason Weston, and Hongyuan Zhan. The alignment waltz: Jointly training agents to collaborate for safety. CoRR, abs/2510.08240, 2025. [35] Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, and Lifeng Jin. Chasing the tail: Effective rubric-based reward modeling for large language model post-training. CoRR, abs/2509.21500, 2025. [36] Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, and Bowen Zhou. A survey of reinforcement learning for large reasoning models. CoRR, abs/2509.08827, 2025. [37] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. CoRR ,abs/2505.03335, 2025. [38] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023. [39] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. In ACL, 2025. [40] Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Jiale Zhao, Jingwen Yang, Jianwei Lv, Kongcheng Zhang, Yihe Zhou, Hengtong Lu, Wei Chen, Yan Xie, and Mingli Song. Breaking the exploration bottleneck: Rubric-scaffolded reinforcement learning for general LLM reasoning. CoRR, abs/2508.16949, 2025. 

14 Appendix 

## A Implementation Details 

We conduct the cold-start training with the LlamaFactory 1 and all the RL experiments based on the verl 2

framework.  

> Cold-start.

For the cold-start stage, we collect 40k training data from the responses of Doubao-Seed-1.6-thinking 3, including 20k math reasoning trajectories and 20k rubricator trajectories, which are collected via reject sampling. We then perform full-parameter supervised fine-tuning (SFT) on the 40k dataset using Qwen3 prompt template, with a sequence cutoff length of 32768, per-device train batch size = 1, gradient accumulation steps = 1, base learning rate = 2e-5 (cosine scheduler with min lr = 2e-6), weight decay = 0.1, warmup ratio = 0.01 and total training epochs = 5.0. For the verifier, we distill a small-sized model Qwen3-4B-Base based on the verification responses from Doubao-Seed-1.6-thinking, using the same SFT recipe as in the cold-start stage.  

> Hyperparameter Value

Prompt template Qwen3 Sequence cutoff length 32768 Per-device train batch size 1Gradient accumulation steps 1Learning rate (cosine scheduler) 2e-5 (min lr = 2e-6) Weight decay 0.1 Warmup ratio 0.01 Epochs 5.0  

> Table 2

Key training hyperparameters used in the cold-start SFT. 

> 1https://github.com/hiyouga/LlamaFactory
> 2https://github.com/volcengine/verl
> 3https://openrouter.ai/bytedance-seed/seed-1.6

15 RL training. For the RL training, we train all the models with the DAPO-Math-17K data, with prompt max length = 16384, response max length = 12288 (overlong buffer enabled, length = 4096), no KL reward/loss, clip ratio = 0.2, learning rate for actor = 1e-6, learning rate for critic = 1e-5, temperature = 1.0, top_p = 1.0, train batch size = 32, rollout number N = 8, mini batch size = 64, weight decay=0.1, grad clip=1.0, lr warmup steps=10. For all RL training, we fix the max training steps to 1500. The response length and prompt length are configured in this setting because the outputs generated by the reasoner will serve as the input prompts for the rubricator. Therefore, the max prompt length should be greater than the response length, and the sum of these two values must not exceed 32k tokens.   

> Hyperparameter Value
> Algorithm

PPO Training dataset DAPO-Math-17K Max prompt length 16384 Max response length 12288 Overlong buffer 4096 KL reward/loss None Clip ratio 0.2 Learning rate (actor) 1e-6 Learning rate (critic) 1e-5 Sampling temperature 1.0 Top_p 1.0 Train batch size 32 Rollout number ( N ) 8Mini batch size 64 Weight decay 0.1 Gradient clip 1.0 LR warmup steps 10 Max steps 1500  

> Table 3 Key training hyperparameters used in RL training.

16 B Prompts 

In this section, we report the prompts used as the reasoner πRea θ and the rubricator πRub θ . It is worth mentioning that during our initial attempts to directly prompt Doubao-Seed-1.6-thinking model to generate rubrics based on the question and a reference answer, the model was unable to generate rubrics that are not already presented in the reference. Therefore, we modified the prompt to instruct the model to generate a set of rubrics where the current response score falls below the middle value, and explicitly reinforced this in the format reward. We observed that this practice triggers the model to actively refine rubrics during reasoning to generate meaningful rubrics for room of self-improvement. The 20k rubricator trajectories were then collected using this specific prompt with rejection sampling under such format reward. 

## PROMPT: 

> {question} Please reason step by step, and put your final answer within \boxed{}.

Figure 8 The prompt of the reasoner role. 

17 PROMPT: 

You are an expert in educational assessment and rubric design. Your task is to analyze a given question–answer pair and generate comprehensive evaluation rubrics that can be used to assess response quality for this question. The answer is only a reference answer from the student(not necessarily a good response), so your rubric system should consider the merits already presented in the response, and most importantly, room for further improvements. # Input Data [Question]: {question} [Response]: {response} # Task Instructions Based on the provided question and answer, generate a comprehensive rubric that’s suitable with multiple evaluation criteria. Each criterion should be: 1. Specific and Measurable: Clearly define what constitutes meeting or not meeting the criterion 2. Binary Evaluable: Can be assessed as true/false by an LLM evaluator 3. Comprehensive Coverage: Together, all criteria should cover the key aspects of a high-quality response # Required Rubric Categories Generate criteria to cover these aspects: 1. Effectiveness of Problem Decomposition & Planning Definition: The systematic breakdown of a complex problem into logically ordered, manageable sub-tasks, and the formulation of a strategic roadmap for solving them. Core Aspects: Identifying key components, establishing step dependencies, sequencing operations, and anticipating challenges before execution. 2. Effectiveness of BackTracking / Self-Validation / Error Handling Definition: The continuous monitoring of reasoning validity through consistency checks, proactive error detection, and adaptive revision of flawed steps. Core Aspects: Implementing verification checkpoints (e.g., unit analysis/estimation), diagnosing inconsistencies, recovering from dead ends via path correction (not restarting), and mitigating error propagation. 3. Reasoning Clarity & Flow Definition: The coherent structuring and communication of logical progression where each step explicitly follows from prior conclusions and leads to subsequent insights. Core Aspects: Hierarchical argument organization under clear objective, unambiguous terminology, explicit logical connectors (e.g., "thus," "since," "conversely"), and seamless transitions between ideas. 4. Reasoning Focus & Efficiency Definition: The maintenance of strict alignment with problem objectives while optimizing cognitive resources through elimination of redundancies and irrelevant explorations. Core Aspects: Sustained direction toward core goals, pruning of tangential paths, and avoidance of overthinking/repetition during reasoning. 5. Other Question-Specific Aspects Definition: The strategic selection and application of domain-relevant methods that align with both problem constraints and solution goals. Core Aspects: Leveraging context-optimal approaches (e.g., dimensional analysis in physics, elimination methods for multiple-choice), avoiding technique mismatches, and adapting tools to exploit problem structure (e.g., symmetry in geometry). # Output Format Return a JSON object with the following structure: “‘json {"question_domain": "math/calculus", "rubrics": [ { "category": "Effectiveness of Problem Decomposition & Planning" "criterion": "XXX", "points": 5}, { "category": "Reasoning Clarity & Flow" "criterion": "XXX", "points": 3 }, { "category": "Reasoning Focus & Efficiency" "criterion": "XXX", "points": 4 }, { "category": "Other Question-Specific Aspects" "criterion": "XXX", "points": -3 }, { "category": "Reasoning Focus & Efficiency" "criterion": "XXX", "points": -2 } // ... additional criteria ], "maximum_score": a(sum of all positive metrics), "minimum_score": b(sum of all negative metrics), "current_score": x(must be smaller than (a+b)/2, otherwise revise your rubric system to get better room of improvement) }“‘ # Important Guidelines - First output the domain of the question in two levels, then generate 5 - 15 criteria in total, ensuring comprehensive coverage - Points should reflect the relative importance of each criterion (supports positive scores from 1 to 5 for reward criteria, and negative scores from -5 to -1 for penalty criteria). Specifically, a penalty criteria should be a statement that expresses flaw/failure in the reasoning process, while a reward criteria should be a statement that express merits/goal accomplishment in the reasoning process. Some of the final things that your should CHECK BEFORE YOUR FINAL RESPONSE: - Try to make your rubrics specific to the question, but not so detailed as some of the overly detailed metrics may not apply to all the response(e.g. "fail to apply XXX theorem" would be a acceptable metric, but "fail to reflect after miscalculating using XXX theorem in step25" would be a very bad metric to avoid, since another students response may not apply XXX theorem, nor having step 25). In other words, do not let the reference answer constrain your rubric system! Again, DO NOT let the reference answer constrain your rubric system!!! To do this, try to propose a set of metrics pretending the reference answer is not there, and reconsider adding some of them. - Remember, the answer above is only a reference answer from the student(not necessarily a good response), so your rubric system should consider the merits already presented in the response, and most importantly, room for further improvements. To enforce this, the reference response should have a score strictly below the average of your proposed rubric system. In order to check for this, you need to calculate the maximum&minimum score from you rubric system, and score the given response at the end of your json. If you found that the rubrics you presented the first time gives a overly-high score, brainstorm more rubrics pretending the reference answer is not there in orderly to find room for improvements and add them to your rubric systems. - Return only the JSON object without additional commentary. 

Figure 9 The prompt of the rubricator role. 

18 PROMPT: 

You are an expert in evaluating student responses to math problems. Your task is to assess a given response based on a set of binary rubrics and compute a final score. [Task Input]: - Question: The math problem the student is solving. - Response: The student’s step-by-step solution. - Rubrics: A list of criteria for evaluation. Each criterion has: - category: The aspect of reasoning being assessed. - criterion: A binary statement (either a merit to reward or a flaw to penalize). - points: The points assigned (positive for rewards, negative for penalties). [Task Instruction]: Evaluate each criterion: - For each criterion in the rubrics list (in the given order), determine if the criterion is satisfied by the response. - Output True if the criterion is met (for a reward) or if the flaw is present (for a penalty). - Output False otherwise. - Base your judgement solely on the content of the response and the specific wording of the criterion. - Compute the final score: Sum the points of every criterion that is evaluated as True (this includes both positive and negative points). [Output Format] Return a JSON object with the following structure: “‘json {"judgement": [boolean, boolean, ...], // List of True/False for each criterion in the same order as the rubrics "final_score": number // Sum of points for True judgements }“‘ [Important Guidelines] - Be objective: Do not consider any external knowledge beyond the provided question, response, and rubrics. - Binary decision: Each criterion must be evaluated as strictly True or False. There are no partial credits. - Order matters: The judgement list must exactly match the order of the rubrics provided. - Independent evaluation: Assess each criterion separately. The presence or absence of one flaw/merit does not influence the evaluation of others (unless the criterion explicitly references another). - Penalty criteria: For a criterion with negative points, True means the flaw is present (so the negative points are added). False means the flaw is absent (so 0 is added for that criterion). - Reward criteria: For a criterion with positive points, True means the merit is present (so positive points are added). False means the merit is absent (so 0 is added). Now, proceed to evaluate the following: Question: {question} Response: {resp} Rubrics: {rubrics} Your final output should contain only the JSON object. Do not include any additional text or explanations. 

Figure 10 The prompt of the verifier. 

19 C Case Study 

In this section, we present a case to illustrate how the rubricator proposes meaningful metrics conditioned on the reasoner’s response. This example shows that the rubricator successfully identifies the key problem( Employs ad-hoc manual listing for small digit sets instead of systematic permutation formula application, increasing error risk ) that caused miscalculation for d = 0 and d = 1 subsets, leading to the final error of 131. It also gives a hint to efficiently solving the problem and reducing reasoning redundancy ( Avoids redundant recounting by categorizing sets with minimum permutation > 2013 upfront, reducing per-set analysis effort ). Therefore, the self-proposed rubrics can effectively guide the model to solve the specific problem, serving as valuable auxiliary reward for the reasoner. 

Question: 2013 is the first year since the Middle Ages that consists of four consecutive digits. How many such years are there still to come after 2013 and before 10000? 

Ground Truth: 149 

Prediction: 131 

Generated Rubrics: 

{"question_domain":"math/digit-sequence-analysis", "rubrics": [{"category":"Effectiveness of Problem Decomposition & Planning", "criterion":"Explicitly outlines digit set ranges, leading zero constraints, and year boundary limits before permutation calculations", "points":3 }, {"category":"Reasoning Clarity & Flow", "criterion":"Uses hierarchical structuring (e.g., bullet points) for each digit set with explicit permutation counts and exclusion justifications", "points":3 }, {"category":"Reasoning Focus & Efficiency", "criterion":"Avoids redundant recounting by categorizing sets with minimum permutation > 2013 upfront, reducing per-set analysis effort", "points":3 }, {"category":"Effectiveness of BackTracking, Self-Validation & Error Handling", "criterion":"Fails to implement edge case validation (e.g., overcounting leading-zero permutations) via secondary checklist reviews", "points":-4 }, {"category":"Other Question-Specific Aspects", "criterion":"Employs ad-hoc manual listing for small digit sets instead of systematic permutation formula application, increasing error risk", "points":-2 }], "maximum_score":9, "minimum_score":-6 }

Figure 11 The rubricator’s generated rubrics 

20 Reasoner’s Response: 

Got it, let’s try to figure out this problem: we need to find how many years after 2013 and before 10000 have four consecutive digits. First, let’s make sure I understand what "four consecutive digits" means in a year (which is a four-digit number, right? Wait, years like 2013 are four-digit, 10000 is five-digit). So a four-digit year with four consecutive digits—wait, does that mean the digits are consecutive in order, like 2013 is 2,0,1,3? Wait no, 2013 isn’t consecutive digits. The problem says "2013 is the first year since the Middle Ages that consists of four consecutive digits"—wait, maybe it’s digits that are consecutive integers, but not necessarily in order? Wait, wait, let’s recheck: "consists of four consecutive digits"—probably, it means the set of digits is four consecutive integers, maybe in any order? Wait, 2013: digits are 2,0,1,3—are those four consecutive? 0,1,2,3 are four consecutive, but 2013 has 0, so 0 is there, then 1,2,3 but missing 2? Wait no, 0,1,2,3 are four consecutive numbers, but 2013 has digits 2,0,1,3—wait maybe I misread: 2013’s digits: 2,0,1,3—wait 0,1,2,3 are four consecutive, but is 2013 a permutation of four consecutive digits? Let’s list permuted four-digit numbers with digits being four consecutive integers. First, what are four consecutive integers? Let’s denote four consecutive digits as d, d+1, d+2, d+3 where d is from 0 to 6 (since d+3 ≤ 9, because digits go up to 9). So possible d: 0 (0-3), 1 (1-4), 2 (2-5), 3 (3-6), 4 (4-7), 5 (5-8), 6 (6-9). Wait let’s list the sets: d=0: {0,1,2,3} d=1: {1,2,3,4} d=2: {2,3,4,5} d=3: {3,4,5,6} d=4: {4,5,6,7} d=5: {5,6,7,8} d=6: {6,7,8,9} These are the possible sets of four consecutive digits (digits are 0-9, so four consecutive can’t include 7-10, etc.). Now, a four-digit year can’t start with 0, so we need to count permutations of each set that are four-digit numbers (so first digit ̸ = 0) and are between 2013 and 10000 (so starting from 2014 up to 9999). Wait 2013 is the first since Middle Ages—so we need years after 2013, so starting from 2014. Let’s take each set and count valid permutations (four-digit, no leading zero, >2013): Set {0,1,2,3} (d=0): How many four-digit permutations? Total permutations: 4! = 24, minus those starting with 0: 3! = 6, so 18. Now we need permutations >2013. Let’s list or calculate: First digit: can be 1,2,3 (since 0 is invalid). If first digit is 1: then the rest are permutations of 0,2,3: 3! = 6 numbers (1023, 1032, 1203, 1230, 1302, 1320). The smallest in this first digit group is 1023, which is less than 2013, so we can ignore all first digit 1 permutations. If first digit is 2: then permutations starting with 2: the remaining digits are 0,1,3. We need permutations >2013. The smallest permutation starting with 2 is 2013, which is the given year. So permutations starting with 2 and >2013: remaining digits 0,1,3–-how many are >2013? The smallest is 2013 (which is equal), next would be 2031, 2103, 2130, 2301, 2310. Wait let’s count: For first digit 2, second digit: can be 0,1,3 (since 2 is already used). If second digit is 0: then third and fourth digits are permutations of 1,3: 2 numbers (0132 invalid? Wait no, first digit 2, second 0: 2 0 x x–-so numbers are 2013, 2031 (since 1 and 3 are digits, so 2013 and 2031). 2013 is equal, so 2031 is >2013. If second digit is 1: then third and fourth digits are permutations of 0,3: numbers are 2103, 2130 (both >2013). If second digit is 3: then third and fourth digits are permutations of 0,1: numbers are 2301, 2310 (both >2013). So for first digit 2: number of permutations >2013: let’s count: 2031 (yes), 2103 (yes), 2130 (yes), 2301 (yes), 2310 (yes). That’s 5 numbers from set {0,1,2,3}. Set {1,2,3,4} (d=1): Four-digit numbers, no leading zero (so first digit 1-4). All permutations are valid since first digit can’t be 0. Total permutations: 4! = 24. We need years after 2013. The smallest number in this set is 1234, which is less than 2013, so many numbers here are below 2013. Wait first digit 1: numbers start with 1, so 1xxx, which are all <2000, so ignore. First digit 2: permutations starting with 2: total 6 (since second digit: 3! for remaining 3 digits). These are from 2134 upwards? Wait list them: 2134, 2143, 2314, 2341, 2413, 2431. All these are >2013, right? Yes, since smallest is 2134, which is >2013. So all 6 permutations starting with 2 in this set are valid. ==> Employs ad-hoc manual listing for small digit sets instead of systematic permutation formula application, increasing error risk 

Set {2,3,4,5} (d=2): First digit can be 2-5 (since no 0 here). So permutations: total 4! =24, minus leading zero? Wait no, no 0, so all 24 are valid four-digit (since first digit 2-5). We need years after 2013. Let’s list first digits: 2,3,4,5. First digit 2: permutations starting with 2: 6 numbers (2 _ _ _). The smallest is 2345? Wait no, digits are 2,3,4,5–-wait permutations starting with 2: second digit can be 3,4,5 (since 2 is used). So numbers: 2345, 2354, 2435, 2453, 2534, 2543. The smallest in first digit 2 is 2345, which is greater than 2013. So all 6 permutations starting with 2 are valid. First digit 3: permutations starting with 3: 6 numbers (all >2345, so >2013), same for 4 and 5: all 6 each. So total for this set: 6 + 6 + 6 + 6 = 24. Set {3,4,5,6} (d=3): Similar to above, digits 3-6, no 0/1. So all four-digit permutations are valid (first digit 3-6). All numbers here are ≥ 3456, which is way above 2013, so all 4! =24 permutations are valid. Set {4,5,6,7} (d=4): Even higher, 4xxx and above, all permutations valid: 24 numbers. Set {5,6,7,8} (d=5): 5xxx and above: 24 numbers. Set {6,7,8,9} (d=6): 6xxx and above: 24 numbers. ==> Avoids redundant recounting by categorizing sets with minimum permutation > 2013 upfront, reducing per-set analysis effort 

Now let’s add them up: Set {0,1,2,3}: 5Set {1,2,3,4}: 6Set {2,3,4,5}: 24 Set {3,4,5,6}: 24 Set {4,5,6,7}: 24 Set {5,6,7,8}: 24 Set {6,7,8,9}: 24 Calculations: 5 + 6 = 11; 11 + 24*5 = 11 + 120 = 131 .

Figure 12 The reasoner’s response 

21