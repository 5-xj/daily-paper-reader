# Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics
# 通过自演进准则增强思维链推理

**Authors**: Leheng Sheng, Wenchang Ma, Ruixin Hong, Xiang Wang, An Zhang, Tat-Seng Chua \\
**Date**: 2026-02-11 \\
**PDF**: https://arxiv.org/pdf/2602.10885v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 6.0 \\
**Evidence**: Self-evolving rubrics for autonomous reasoning align with evolution of heuristics and efficient automatic algorithms \\

---

## Abstract
Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

## 摘要
尽管思维链（CoT）在大型语言模型（LLM

---

## 速览摘要（自动生成）

**问题**：CoT推理训练依赖高成本人工标注，且静态奖励模型难以应对分布演变和奖励作弊。

**方法**：提出RLCER框架，利用模型自主生成并不断演进的评分标准（Rubrics）为推理过程提供奖励信号。

**结论**：RLCER在无结果奖励时仍优于传统强化学习，且该标准作为提示词能显著提升推理性能。