Title: Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization

URL Source: https://arxiv.org/pdf/2602.10794v1

Published Time: Thu, 12 Feb 2026 02:06:05 GMT

Number of Pages: 10

Markdown Content:
# Transport, Don’t Generate: Deterministic Geometric Flows for Combinatorial Optimization 

Benjy Friedmann 1 Nadav Dym 1

# Abstract 

Recent advances in Neural Combinatorial Opti-mization (NCO) have been dominated by diffu-sion models that treat the Euclidean Traveling Salesman Problem (TSP) as a stochastic N × N

heatmap generation task. In this paper, we pro-pose CycFlow , a framework that replaces iterative edge denoising with deterministic point transport. CycFlow learns an instance-conditioned vector field that continuously transports input 2D coordi-nates to a canonical circular arrangement, where the optimal tour is recovered from this 2N dimen-sional representation via angular sorting. By lever-aging data-dependent flow matching, we bypass the quadratic bottleneck of edge scoring in favor of linear coordinate dynamics . This paradigm shift accelerates solving speed by up to three or-ders of magnitude compared to state-of-the-art diffusion baselines, while maintaining competi-tive optimality gaps. 

# 1. Introduction 

Euclidean routing problems, such as the Traveling Salesman Problem (TSP), are central to Operations Research, with diverse applications in logistics, circuit design, and resource allocation. TSP is an NP-hard combinatorial optimization problem (Karp, 1972). It can be solved to optimality us-ing time-consuming branch-and-cut solvers like Concorde (Applegate et al., 2009). Alternatively, fast but approximate solutions can be obtained via heuristics like 2-OPT (Lin & Kernighan, 1973) and the Christofides (Christofides, 1976) algorithm. The emergence of Neural Combinatorial Optimization (NCO) represents a paradigm shift toward data-driven solvers, where modern neural architectures learn to exploit 

> 1

Faculty of Mathematics, Department of Applied Math-ematics, Technion - Israel Institute of Technology, Haifa, Israel. Correspondence to: Benjy Friedmann <ben-jamin.fri@campus.technion.ac.il >.

Preprint. February 12, 2026. 

Figure 1. Comparison of the prevailing NCO paradigm (Left), which views TSP as stochastic heatmap edge denoising, versus CycFlow (Right), which treats TSP as a deterministic geometric flow. Our method transports points to a target manifold rather than classifying edges, accelerating inference by orders of magnitude. 

the underlying structure of problem instances to optimize the accuracy-speed tradeoff. Initially, NCO has relied on autoregressive heuristics or constructive policies. Recently, the field has gravitated towards diffusion models, which approach TSP as a heatmap generation task. While these methods achieve strong performance by leveraging paral-lel denoising, they introduce a significant computational overhead: they replace sequential decoding with iterative Langevin dynamics, requiring multiple refinement steps to resolve high-fidelity heatmaps. This inefficiency is not an artifact of implementation, but a direct consequence of treating a deterministic problem as a stochastic one. The TSP is inherently a deterministic geometric mapping: for a given input set, the optimal cy-cle is a unique well-defined structure determined entirely by the metric geometry. Formulating TSP as a genera-tive task—sampling from a highly uncertain distribution 

p(π|X)—forces the solver to employ stochastic denoising, an approach that is misaligned with the problem’s nature. Current state-of-the-art methods effectively sacrifice com-putational efficiency for distribution estimation, solving a deterministic optimization problem with probabilistic ma-chinery designed for high-variance data generation. In this work, we introduce CycFlow, a framework that views TSP through the lens of Deterministic Point Trans-1            

> arXiv:2602.10794v1 [cs.LG] 11 Feb 2026 Transport, Don’t Generate
> Figure 2. Visualizing the Deterministic Linear Flow. The panels illustrate the evolution of point sets from the initial random configuration
> x0(t= 0 ) to the target solution manifold x1(t= 1 ). The overlaid vector field indicates the flow direction x1−x0, guiding the nodes into a structured circle. As shown in the final panel, the optimal node permutation is recovered by sorting the transported points based on their angular position θrelative to the origin.

port. Rather than generating a solution from Gaussian noise, CycFlow learns to transport the unordered set of nodes 

X ⊂ R2 onto a canonical target manifold—specifically 

S1—where the ordering becomes trivial (Figure 2). By coupling the input distribution directly to this solution mani-fold, our method induces flow trajectories that are straighter and simpler than those in diffusion processes. Crucially, this design shifts the computational burden from denoising 

N × N edge probabilities to evolving N coordinate points, enabling linear-complexity inference. Figure 1 illustrates the difference between the two approaches. 

Contributions 

(i) Transport, Not Classification: We reformulate TSP as evolving a set of N coordinates in R2 rather than classifying an N × N adjacency matrix. This leverages the Euclidean inductive bias, treating the problem as a continuous geometric transformation rather than a discrete graph search. (ii) Deterministic Flow Matching: Utilizing Flow Match-ing, we regress the vector field between the input met-ric space and the solution geometry. This results in straight, deterministic flow paths that are easier to learn than diffusion reverse processes. (iii) Orders-of-Magnitude Speedup: We empirically demonstrate that CycFlow attains competitive optimal-ity gaps while running 2-3 orders of magnitude faster than leading diffusion models, validating the efficiency of linear coordinate dynamics. 

# 2. Related Work 

The evolution of Neural Combinatorial Optimization (NCO) has been defined by a dialectic between expressiveness —the ability to model multimodal posterior distributions—and 

tractability —the computational cost of inference. While early learning-based approaches prioritized speed via con-structive heuristics, the recent literature has shifted toward generative paradigms to close the optimality gap with clas-sical solvers. 

2.1. Constructive and Autoregressive Baselines 

The foundational era of NCO treated optimization as a se-quence generation task. Motivated by the rigidity of exact solvers, architectures such as Pointer Networks (Bello et al., 2016) and the Attention Model (AM) (Kool et al., 2019) pa-rameterized the policy as an autoregressive encoder-decoder. These methods, typically trained via Reinforcement Learn-ing or behavior cloning, offer highly efficient inference com-pared to search-based baselines. Subsequent works focused on stabilizing the high variance inherent in RL training; no-tably, POMO (Kwon et al., 2020) leveraged the underlying symmetries of TSP to construct a low-variance baseline. However, constructive approaches suffer from a fundamen-tal limitation: error propagation inherent in greedy decod-ing. The sequential nature of decoding means that early, irreversible missteps permanently trap the model in local op-tima (Joshi et al., 2020). While beam search offers marginal improvements, it scales poorly with problem size, failing to capture the global structural rearrangements required for high-quality solutions on large-scale instances. 

2.2. The Generative Shift: Diffusion and Consistency 

To overcome the limitations of autoregression, the field pivoted toward non-autoregressive, iterative refinement. DI-FUSCO (Sun & Yang, 2023) established a new paradigm by formulating TSP as a graph-based diffusion process, treat-ing optimization as a heatmap denoising task. By modeling the joint probability of edges, these methods enable global refinement and capture multimodal optima, significantly reducing the optimality gap. This expressiveness, however, incurs a severe inference bot-2Transport, Don’t Generate        

> Figure 3. Geometric Coupling. We construct a specific target Yfor each input Xbased on the optimal tour, creating a deterministic, data-dependent coupling (x0, x 1).

tleneck. Integrating the probability flow ODE often involves hundreds of function evaluations, creating a significant com-putational overhead. Recent ”distilled” solvers have at-tempted to resolve this computational bottleneck. Fast-T2T (Li et al., 2024) employs consistency distillation to map noise directly to solutions in a single or few steps, while DISCO (Zhao et al., 2024) constrains the diffusion process to a ”residue” manifold to drastically reduce the search space. 

2.3. Geometric Decoupling and the Case for Flow 

While consistency models and residue constraints address latency, they inherit a critical structural limitation from their predecessors: the geometric decoupling of the solution space. Most state-of-the-art generative solvers (including DIFUSCO, Fast-T2T, and DIMES (Qiu et al., 2022)) operate on the discrete N × N edge adjacency matrix. This formulation approaches the problem indirectly. Recov-ering a structured geometric object (a permutation) from unstructured Gaussian noise on an edge manifold presents a challenging denoising task. This process focuses compu-tational effort on resolving dense edge probabilities, rather than directly evolving the linear geometry of the tour itself. CycFlow addresses this disconnect by adopting a simulation-free alternative to diffusion that regresses a deterministic vector field, based on Flow Matching (Lipman, 2022). Un-like prior works that apply the flow to the edge space, we propose an instance-conditioned flow on the node coordi-nates. By transporting the input coordinates to a canonical circular arrangement, we align the generative process with the topological structure of the TSP, ensuring both linear-time tractability and geometric consistency. 

# 3. Method 

We address the Euclidean TSP problem, where the input is a collection of N points in R2, denoted by the point cloud 

x0 ∈ RN ×2 (throughout this paper, we use the terms “point cloud,” “set of coordinates,” and “nodes” interchangeably to refer to this input structure). The goal is to find the shortest Eulerian cycle which traverses through all the points. While this cycle was typically represented as a N × N permutation matrix in previous work, a key innovation of our method is that we represent the optimal cycle geometrically via another point cloud x1 ∈ RN ×2 whose entries all reside on a circle, forming a cycle. CycFlow solves TSP by transporting inputs x0 ∈ RN ×2 to a canonical target x1 ∈ RN ×2 via a learned ODE. This section details the data-dependent coupling that defines the boundary conditions ( Section 3.1), the conditional flow matching objective used to learn the dynamics ( Sec-tion 3.2), and our architectural choice to learn the velocity (Section 3.3). 

3.1. Data-Dependent Geometric Couplings Target Construction. Let x0 ∈ RN ×2 be the unordered input coordinates, and assume we know the optimal tour 

π∗ (which is the case in the training phase). We construct the target x1 by embedding the optimal TSP tour onto a circle, scaled such that its Frobenius norm matches that of the input (i.e., ∥x1∥F = ∥x0∥F ). The nodes are placed on this circle in the exact sequence of the optimal permutation 

π∗, with arc lengths strictly proportional to the edge weights of the original TSP coordinates relative to the full optimal cycle length (Figure 3). By explicitly constructing x1 from the intrinsic geometry of x0, we establish a deterministic relationship between the source and target. 

Coupling Alignment. Our definition of x1 still has a global 3Transport, Don’t Generate 

Table 1. TSP-50 and TSP-100 Results. CycFlow matches the optimality of heavy iterative baselines while outperforming diffusion-based inference speeds by orders of magnitude. Baseline results are sourced from (Li et al., 2024). TSP-50 TSP-100 METHOD GAP (%) TIME GAP (%) TIME 

Exact & Heuristics 

CONCORDE 0.00 3M 0.00 12 M

LKH3 (H ELSGAUN , 2017) 0.00 3M 0.00 33 M

2O PT (L IN & K ERNIGHAN , 1973) 2.95 – 3.54 –

RL & Constructive Baselines 

AM (K OOL ET AL ., 2019) 1.76 2S 4.53 6S

GCN (J OSHI ET AL ., 2019) 3.10 55 S 8.38 6M

TRANSFORMER (B RESSON & L AURENT , 2021) 0.31 14 S 1.42 5S

POMO (K WON ET AL ., 2020) 0.64 1S 1.07 2S

SYM -NCO (K IM ET AL ., 2022) – – 0.94 2S

Diffusion & Iterative Refinement 

IMAGE DIFFUSION (G RAIKOS ET AL ., 2022) 1.23 – 2.11 –DIFUSCO (S PEED ) (S UN & Y ANG , 2023) 12.84 16 S 20.20 20 S

DIFUSCO (A CCURACY ) (S UN & Y ANG , 2023) 0.41 18 M 1.16 18 M

T2T & Fast T2T (Recent SOTA) 

T2T (S PEED ) (L I ET AL ., 2023) 8.15 55 S 16.09 1M

T2T (A CCURACY ) (L I ET AL ., 2023) 0.03 26 M 0.11 42 M

FAST T2T (S PEED ) 0.31 11 S 1.31 16 S

FAST T2T (A CCURACY ) 0.01 3M 0.03 3M

CYC FLOW (O URS ) 0.08 0.004 S 0.35 0.01 S

rotation ambiguity. To minimize the complexity of the trajectory connecting x0 and x1, we perform Procrustes alignment. We explicitly seek the optimal rotation R∗ that minimizes the aggregate squared distance between each in-put node (x0)i and its mapped target vertex (x1)i. This objective is mathematically equivalent to minimizing the squared Frobenius norm of the difference matrix: 

R∗ = argmin 

> R∈SO (2)

PNi=1 ∥(x1)iR − (x0)i∥2 ≡ argmin 

> R∈SO (2)

∥x1R − x0∥2

> F

(1) The minimizer R∗ is easily found using the Kabsch algo-rithm (Kabsch, 1978). The aligned pair (x0, x 1R∗) consti-tutes a correlated sample from our joint distribution. This coupling strategy ensures that the global displacement is minimal, significantly reducing transport cost and prevent-ing the formation of complex, high-curvature trajectories characteristic of independent couplings (Bose et al., 2024). By explicitly minimizing global displacement, this align-ment acts as a structural ”head start,” simplifying the re-gression task from resolving long-range entanglements to learning local, low-velocity adjustments. 

3.2. Deterministic Conditional Flow Matching 

We reformulate the generation process as learning the con-ditional velocity field of a deterministic probability flow (Lipman, 2022). We utilize the framework of (Albergo et al., 2023) in its noiseless limit, effectively constructing a direct transport map from the input to the solution. 

The Linear Interpolant. We define a time-dependent tra-jectory xt that linearly interpolates between the unordered input coordinates x0 at t = 0 and the target manifold x1 at 

t = 1 . Following the formulation of (Albergo et al., 2023) with the noise coefficient set to zero ( γt = 0 ), the path is given by: 

xt = (1 − t)x0 + tx 1, t ∈ [0 , 1] . (2) This construction leverages the input x0 as a “structurally informative prior” for the target, rather than starting from uninformative Gaussian noise. This defines a direct bridge between the problem and its solution. 

Conditional Velocity Field. The flow of this interpolant is governed by the simple Ordinary Differential Equation (ODE) 

ddt xt = u(x0, x 1), where u(x0, x 1) = x1 − x0. (3) This corresponds to the Optimal Transport ODE (OT-ODE) limit of the Schr ¨odinger Bridge (Liu et al., 2023), where the dynamics become purely deterministic. 

Regression Objective. To learn the transport dynamics, we parameterize a conditional neural velocity field vθ (t, x t|x0)

to approximate the ground-truth flow. We minimize the flow matching objective (Lipman, 2022), which regresses the neural velocity field vθ onto this target 4Transport, Don’t Generate     

> Figure 4. Inference Latency vs. Optimality Gap (TSP-50 to TSP-1000). We plot the time–accuracy Pareto frontier on a log–log scale (lower-left is better); marker sizes indicate problem size N.CycFlow (red stars) occupies a previously unreachable sub-second regime, achieving competitive optimality gaps while reducing inference latency by orders of magnitude compared to diffusion-based solvers (e.g., DIFUSCO), constructive methods (POMO), and exact solvers (Concorde). This demonstrates an expansion of the efficiency–accuracy Pareto frontier rather than a simple trade-off between speed and solution quality.

vector field. This is strictly equivalent to a square loss re-gression over the data-dependent couples (x0, x 1) sampled from the joint distribution ρ constructed in Section 3.1: 

LCFM (θ) = Et∼U (0 ,1) ,

> (x0,x 1)∼ρ

h

∥vθ (t, x t|x0) − u(x0, x 1)∥2i

(4) By minimizing this objective, vθ learns to mimic the optimal linear path that pushes unordered nodes x0 directly to their canonical solution arrangement x1.

Inference. At test time, given a new problem instance x0,we generate the solution in two steps: 1. Integration: We approximate the flow map by numer-ically integrating the learned ODE starting at X0 = x0

and using a simple Euler solver with K steps: 

Xk+1 = Xk + 1

K vθ (tk, X k|X0), k = 0 . . . K − 1

This yields the transported points x1 := Xk, which lie on the target manifold (the canonical circle). 2. Recovery: We recover the discrete permutation ˆπ by sorting the N points in x1 based on their polar angle relative to the origin. This solution is then refined using the local greedy 2OPT algorithm. 

The Role of Couplings in Generative Frameworks. 

Standard generative models conventionally select a simple base density, such as a standard Gaussian, primarily for its analytical tractability and ease of sampling (Ho et al., 2020; Song et al., 2020). This results in an independent coupling ,where the starting distribution is agnostic to the target data structure, often forcing the model to learn complex trajec-tories to bridge two disparate distributions. In contrast, our approach leverages the framework of stochastic interpolants with data-dependent couplings (Liu et al., 2023; Albergo et al., 2023; Bose et al., 2024). Instead of treating the input and target as independent, we explicitly construct a depen-dent pairing: for every unordered input set x0, we generate a unique, corresponding target geometry x1 derived directly from the optimal solution. Formally, this defines a sharp conditional distribution ρ1(x1|x0), creating a valid data-dependent coupling ρ(x0, x 1) = ρ0(x0)ρ1(x1|x0) that per-mits efficient learning via flow matching (Lipman, 2022). 

3.3. Architecture: Canonicalize-Process-Restore 

In this subsection we describe our architecture for the veloc-ity field vθ . We design our architecture to be size-agnostic, allowing the same model to be applied to TSP problems of different sizes, and strictly equivariant to permutation, rotation and translation of the input’s pose. We attain equiv-ariance using the Canonicalize-Process-Restore pipeline. This strategy decouples symmetry constraints from the net-work architecture. Unlike methods that rely on invariant aggregation—which often obscures fine-grained relative spa-tial structure—canonicalization allows us to utilize standard Transformers. This enables the model to remain “geomet-rically aware,” capturing precise relative node placement within a standardized frame while guaranteeing global equiv-ariance. 

Canonicalization We employ a variant of the canonical-ization scheme proposed by Friedmann & Werman (2025). To resolve permutation ambiguity, we induce a determinis-tic sequence order based on the intrinsic geometry of the point cloud. We construct a full graph with Gaussian kernel weights Wij = exp( −∥ xi − xj ∥2/σ 2) and compute the normalized Graph Laplacian Lsym = I − D−1/2W D −1/2.The points are sorted according to the values of the Fiedler vector (the eigenvector corresponding to the second smallest eigenvalue). To resolve the inherent sign ambiguity of this eigenvector v, we enforce positive skewness: if P v3 

> i

< 0,we flip v ← − v. This ensures a consistent traversal direc-tion across structurally similar instances. Once ordered, we resolve rigid E(2) ambiguities by fix-ing a canonical reference frame. First, we enforce transla-tion invariance by centering the point cloud at the origin: 

xi ← xi − ¯x, where ¯x = 1

> N

PNj=1 xj . Next, we compute a weighted orientation vector u = PNi=1 wixi, where the weights wi vary linearly from −1 to 1 along the spectral sequence. The point cloud is rotated such that u aligns with the positive vertical axis. Finally, reflection ambiguity is re-5Transport, Don’t Generate 

solved by examining the latter half of the sum: we calculate the aggregate x-coordinate of the points in the second half of the sequence (where wi > 0). If this sum is negative, we reflect the point cloud across the y-axis. 

Processing and Restoration. The canonicalized sequence is fed into a Transformer backbone. We utilize Rotary Posi-tional Embeddings (RoPE) to explicitly encode the relative positions of points along the spectral curve, allowing the attention mechanism to capture fine-grained geometric de-pendencies. Furthermore, the flow time t is injected into the model via Adaptive Layer Normalization (AdaLN), which modulates the scale and shift parameters of each block based on the current timestep. The network predicts the velocity field vcan in this standardized frame. Finally, we apply the in-verse of the canonical rotation and permutation to transport the update back to the original input geometry. 

# 4. Experiments 

We evaluate CycFlow on Euclidean TSP benchmarks with 

N ∈ { 50 , 100 , 500 , 1000 }. Our main finding is that Cy-cFlow achieves competitive accuracy with an inference time of several milliseconds, which is a 2 − 3 order of magnitude speedup with respect to other diffusion baselines. 

4.1. Experimental Protocol 

We adhere to a rigorous protocol to ensure fair comparison of runtime and accuracy: 

Metrics. We report the Approximation Gap relative to the optimal tour length Lopt (computed using Concorde (Applegate et al., 2009)): Gap (%) = 100 · Lmethod − Lopt 

Lopt 

(5) Runtime is reported as the median per-instance wall-clock time (in seconds), including the full inference stack (nu-merical integration, angular recovery, and parallel 2-opt refinement). 

Baselines. We compare against a broad spectrum of solvers: exact methods (Concorde, LKH3), constructive heuristics (AM, POMO), and state-of-the-art iterative models (DI-FUSCO, T2T, Fast-T2T). For baseline papers which re-ported results of several variants, we report both “best speed” and “best accuracy” variants to expose the full trade-off pro-file. To ensure fairness, we compare against baseline check-points that utilize 2-opt post-processing whenever such re-sults are available. 

Reproducibility. Code will be released with the final ver-sion of the paper. 

4.2. Main Results: The Latency-Accuracy Pareto Frontier 

Our results on smaller TSP problems with N = 50 , 100 

points are reported in Table 1, and results for larger TSP instances N = 500 , 1000 are reported in Table 2. The two tables show that CycFlow achieves sub-second latency even on large-scale instances. For TSP-1000, our method’s inference time is 0.22 s , whereas accuracy-focused diffusion baselines require minutes (e.g., Fast T2T requires 516 s). This represents a speedup of 2-3 orders of magnitude, validating that linear coordinate dynamics eliminate the quadratic bottleneck of edge-denoising. In terms of accuracy, on TSP-50 CycFlow attains a very low 0.08% gap in just 4 ms , as the number of points N

increases we observe a more significant efficiency-accuracy tradeoff. On our largest instance, TSP-1000, CycFlow yields a 9.89% gap at 0.22 s . This accuracy is competitive with many previous methods, but is substantially higher than computationally intensive global solvers, as well as Fast T2T’s lower gap which is 2-3 orders of magnitude slower. Thus, CycFlow should not be considered as a drop-in re-placement for solvers where time is infinite and optimality is paramount; rather, it provides a viable neural solver for real-time applications requiring competitive solutions at scale. Figure 4 gives another visualization of these results in the efficiency-accuracy plane. The figure shows that CycFlow is ’Pareto Optimal’, in the sense that it is significantly faster than previous methods, while retaining competitive accu-racy. 

4.3. Ablations and Analysis 

We conduct targeted ablations on TSP50 to isolate the source of our performance gains. 

Necessity of Flow Matching (Table 3). To isolate the con-tribution of the transport dynamics, we compared CycFlow against two single-shot baselines. We first evaluated a naive 

Angular Sort , which simply orders the raw input points based on their polar angle. While this yields a high gap of 9.38% on TSP50, it is far better than random, confirming that mapping points to a circle is a geometrically sound premise. Next, we tested Direct Angular Regression , where the model attempts to predict the final target angles in a single forward pass. This method failed to generalize ef-fectively (3.75% gap), suggesting that the mapping from random 2D coordinates to a precise cycle is too complex and non-linear for a one-step prediction. CycFlow achieves a 0.09% gap because the iterative ODE integration breaks this difficult transformation down into a continuous, smooth path, allowing the model to gradually steer points to their optimal positions. 6Transport, Don’t Generate   

> Table 2. TSP-500 and TSP-1000 Results. Scalability comparison. CycFlow maintains sub-second inference on TSP-1000, whereas constructive baselines (POMO, DIMES) suffer from prohibitive runtime growth and iterative models (T2T) require minutes to converge. Baseline results are sourced from (Li et al., 2024).

TSP-500 TSP-1000 METHOD GAP (%) TIME GAP (%) TIME 

Exact & Heuristics 

CONCORDE 0.00 37.7 M 0.00 6.65 H

LKH3 (H ELSGAUN , 2017) 0.00 46.3 M 0.00 2.57 H

RL & Constructive Baselines 

AM (K OOL ET AL ., 2019) 20.99 1.5 M 34.75 3.2 M

GCN (J OSHI ET AL ., 2019) 79.61 6.7 M 110.29 28.5 M

POMO (K WON ET AL ., 2020) 48.22 11.6 H 114.36 63.5 H

DIMES (Q IU ET AL ., 2022) 14.38 1.0 M 14.97 2.1 M

Diffusion & Iterative Refinement 

DIFUSCO (S UN & Y ANG , 2023) 1.5 4.5 M 1.89 14.4 M

T2T & Fast T2T (Recent SOTA) 

T2T (S PEED ) (L I ET AL ., 2023) 4.28 36 S – –T2T (A CCURACY ) (L I ET AL ., 2023) 5.61 6.4 M 9.04 19.4 M

FAST T2T (S PEED ) 1.23 15 S 1.42 57 S

FAST T2T (A CCURACY ) 0.39 2.2 M 0.58 8.6 M

CYC FLOW (O URS ) 6.84 0.06 S 9.89 0.22 S

Backbone and Robustness (Table 4). To isolate the effect of our overall framework and our specific choice of velocity model vθ described in Section 3.3, we experimented with replacing our architecture with other backbone architectures, conducting extensive hyperparameter sweeps for each ar-chitecture to ensure a fair comparison. We first evaluated Equivariant GNNs (Satorras et al., 2021) on TSP50. De-spite their theoretical rotation invariance, they yielded the highest optimality gap (0.34%). We hypothesize that the iterative message-passing paradigm may degrade the repre-sentation of the fine-grained global geometry required for high-precision TSP solving. Next, we evaluated standard Transformer variants. While these models capture global interactions better than GNNs, they lack inherent rotation equivariance. This forces the model to expend capacity on learning approximate symme-tries from the data, limiting its ability to focus purely on the combinatorial optimization task. Ultimately, our cycFlow achieved the best performance (0.09% gap) by employing the spectral canonicalization strategy described in Section 3.3. We attribute the superi-ority of this method to two factors: first, canonicalization removes the burden of learning rigid symmetries, simpli-fying the geometric task. Second, the resulting spectral ordering assigns structural meaning to the sequence po-sitions, allowing the model to effectively utilize standard Rotary Positional Embeddings (RoPE), where the specific frequencies naturally align with the spectral properties of the data. 

# 5. Conclusion and Limitation 

We have presented CycFlow , a framework that redefines the Euclidean TSP as a deterministic geometric transport problem rather than a stochastic heatmap generation task. By learning a continuous vector field that transports input coordinates to a canonical circular manifold, we replace the quadratic complexity of iterative edge scoring with linear coordinate dynamics . Empirically, this approach acceler-ates inference by orders of magnitude, with an inference time ≪ 1 sec even for TSP1000, while maintaining compet-itive optimality gaps. CycFlow demonstrates that aligning the generative process with the problem’s inherent geometry enables real-time, high-fidelity optimization. 

Limitations and Future Work. Our current framework operates within the supervised learning paradigm, requir-ing access to optimal solutions during training—a char-acteristic shared by leading diffusion-based NCO solvers (Sun & Yang, 2023). Developing unsupervised or reinforce-ment learning variants to eliminate this dependency remains an open frontier. Additionally, CycFlow is explicitly de-signed to exploit the continuous inductive bias of Euclidean space. Extending this transport-based methodology to non-Euclidean or general graph-based routing problems will require architectural adaptations to encode discrete edge 7Transport, Don’t Generate 

Table 3. Impact of Flow Matching (TSP-50). Comparison of our iterative transport against single-shot baselines. The Angular Sort baseline shows that zero-shot inference using our circular geometric prior yields meaningful but sub-optimal results. The failure of Direct Regression demonstrates that a single-step predic-tion cannot resolve the complex point permutations, whereas our iterative ODE approach successfully closes the gap. METHOD GAP (%) ANGULAR SORT 9.38 DIRECT ANGULAR REG . 3.75 

CYC FLOW (O URS ) 0.09 

Table 4. Ablation on Backbone Architecture (TSP-50). We compare our architecture against extensive hyperparameter sweeps of equivariant GNNs and Transformer variants. EGNNs struggle with fine geometry, while standard Transformers suffer from a lack of rotation equivariance. BACKBONE GAP (%) EQUIVARIANT GNN 0.34 TRANSFORMER 0.20 TRANSFORMER + F OURIER 0.27 TRANSFORMER + ROPE 0.18 

CYC FLOW (O URS ) 0.09 

metric structures in addition to geometric structure, repre-senting a promising direction for future research in geomet-ric flow matching. Ultimately, we believe that our fundamental ideas of using geometric flows, with a geometric cyclic representation of the optimal cycle, can be highly effective for fast TSP solving, in both supervised and unsupervised settings, and in the many TSP instances where Euclidean coordinates are informative, even if the distances are non-Euclidean. 

# Impact Statement 

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 8Transport, Don’t Generate 

# References 

Albergo, M. S., Goldstein, M., Boffi, N. M., Ranganath, R., and Vanden-Eijnden, E. Stochastic interpolants with data-dependent couplings. arXiv preprint arXiv:2310.03725 ,2023. Applegate, D. L., Bixby, R. E., Chv ´atal, V., Cook, W., Es-pinoza, D. G., Goycoolea, M., and Helsgaun, K. Cer-tification of an optimal tsp tour through 85,900 cities. 

Operations Research Letters , 37(1):11–15, 2009. Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940 , 2016. Bose, A. J., Akhound-Sadegh, T., Huguet, G., Fatras, K., Rector-Brooks, J., Liu, C.-H., Nica, A. C., Korablyov, M., Bronstein, M., and Tong, A. Se(3)-stochastic flow matching for protein backbone generation, 2024. URL 

https://arxiv.org/abs/2310.02391 .Bresson, X. and Laurent, T. The transformer network for the traveling salesman problem. arXiv preprint arXiv:2103.03012 , 2021. Christofides, N. Worst-case analysis of a new heuristic for the travelling salesman problem. Technical Report 388, Graduate School of Industrial Administration, Carnegie Mellon University, Pittsburgh, PA, 1976. Friedmann, B. and Werman, M. Canonnet: Canonical order-ing and curvature learning for point cloud analysis. arXiv preprint arXiv:2504.02763 , 2025. Graikos, A., Malkin, N., Jojic, N., and Samaras, D. Diffusion models as plug-and-play priors. In Thirty-Sixth Conference on Neural Information Processing Systems , 2022. URL https://arxiv.org/pdf/ 2206.09012.pdf .Helsgaun, K. An extension of the lin-kernighan-helsgaun tsp solver for constrained traveling salesman and vehicle routing problems. Roskilde: Roskilde University , 12: 966–980, 2017. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-bilistic models. Advances in neural information process-ing systems , 33:6840–6851, 2020. Joshi, C. K., Laurent, T., and Bresson, X. An efficient graph convolutional network technique for the travelling salesman problem. arXiv preprint arXiv:1906.01227 ,2019. Joshi, C. K., Cappart, Q., Rousseau, L.-M., and Lau-rent, T. Learning the travelling salesperson prob-lem requires rethinking generalization. arXiv preprint arXiv:2006.07054 , 2020. Kabsch, W. A discussion of the solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A , 34:827–828, 1978. URL https://api. semanticscholar.org/CorpusID:98766395 .Karp, R. M. Reducibility among combinatorial problems. In Miller, R. E. and Thatcher, J. W. (eds.), Complexity of Computer Computations , pp. 85–103. Plenum Press, New York, 1972. doi: 10.1007/978-1-4684-2001-2 9. Kim, M., Park, J., and Park, J. Sym-nco: Leveraging sym-metricity for neural combinatorial optimization. arXiv preprint arXiv:2205.13209 , 2022. Kool, W., van Hoof, H., and Welling, M. Attention, learn to solve routing problems! In ICLR , 2019. Kwon, Y.-D., Choo, J., Kim, B., Yoon, I., Gwon, Y., and Min, S. Pomo: Policy optimization with multiple optima for reinforcement learning. Advances in Neural Informa-tion Processing Systems , 33:21188–21198, 2020. Li, Y., Guo, J., Wang, R., and Yan, J. T2t: From distribution learning in training to gradient search in testing for com-binatorial optimization. Advances in Neural Information Processing Systems , 36:50020–50040, 2023. Li, Y., Guo, J., Wang, R., Zha, H., and Yan, J. Fast t2t: Opti-mization consistency speeds up diffusion-based training-to-testing solving for combinatorial optimization. Ad-vances in Neural Information Processing Systems , 37: 30179–30206, 2024. Lin, S. and Kernighan, B. W. An effective heuristic algo-rithm for the traveling-salesman problem. Operations research , 21(2):498–516, 1973. Lipman, Y. e. a. Flow matching for generative modeling. 

arXiv:2210.02747 , 2022. Liu, G.-H., Vahdat, A., Huang, D.-A., Theodorou, E. A., Nie, W., and Anandkumar, A. I2sb: Image-to-image schr ¨odinger bridge. arXiv preprint arXiv:2302.05872 ,2023. Qiu, R., Sun, Z., and Yang, Y. Dimes: A differentiable meta solver for combinatorial optimization problems. Ad-vances in Neural Information Processing Systems , 35: 25531–25546, 2022. Satorras, V. G., Hoogeboom, E., and Welling, M. E (n) equivariant graph neural networks. In International con-ference on machine learning , pp. 9323–9332. PMLR, 2021. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-mon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 , 2020. 9Transport, Don’t Generate 

Sun, Z. and Yang, Y. Difusco: Graph-based diffusion solvers for combinatorial optimization. In NeurIPS , 2023. Zhao, H., Yu, K., Huang, Y., Yi, R., Zhu, C., and Xu, K. Disco: Efficient diffusion solver for large-scale combinatorial optimization problems. arXiv preprint arXiv:2406.19705 , 2024. 10