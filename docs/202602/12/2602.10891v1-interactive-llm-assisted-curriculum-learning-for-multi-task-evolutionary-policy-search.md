# Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search
# 交互式大语言模型辅助的多任务演化策略搜索课程学习

**Authors**: Berfin Sakallioglu, Giorgia Nadizar, Eric Medvet \\
**Date**: 2026-02-11 \\
**PDF**: https://arxiv.org/pdf/2602.10891v1 \\
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 8.0 \\
**Evidence**: Evolutionary policy search with automated curriculum generation \\

---

## Abstract
Multi-task policy search is a challenging problem because policies are required to generalize beyond training cases. Curriculum learning has proven to be effective in this setting, as it introduces complexity progressively. However, designing effective curricula is labor-intensive and requires extensive domain expertise. LLM-based curriculum generation has only recently emerged as a potential solution, but was limited to operate in static, offline modes without leveraging real-time feedback from the optimizer. Here we propose an interactive LLM-assisted framework for online curriculum generation, where the LLM adaptively designs training cases based on real-time feedback from the evolutionary optimization process. We investigate how different feedback modalities, ranging from numeric metrics alone to combinations with plots and behavior visualizations, influence the LLM ability to generate meaningful curricula. Through a 2D robot navigation case study, tackled with genetic programming as optimizer, we evaluate our approach against static LLM-generated curricula and expert-designed baselines. We show that interactive curriculum generation outperforms static approaches, with multimodal feedback incorporating both progression plots and behavior visualizations yielding performance competitive with expert-designed curricula. This work contributes to understanding how LLMs can serve as interactive curriculum designers for embodied AI systems, with potential extensions to broader evolutionary robotics applications.

## 摘要
多任务策略搜索是一个具有挑战性的问题，因为策略需要具备泛化到训练案例之外的能力。课程学习已被证明在此类场景中非常有效，因为它能逐步引入复杂性。然而，设计有效的课程既费时费力，又需要大量的领域专业知识。基于大语言模型（LLM）的课程生成最近才作为一种潜在的解决方案出现，但其局限在于以静态、离线模式运行，未能利用来自优化器的实时反馈。在此，我们提出了一种用于在线课程生成的交互式 LLM 辅助框架，其中 LLM 根据演化优化过程的实时反馈自适应地设计训练案例。我们研究了不同的反馈模态（从单纯的数值指标到结合图

---

## 论文详细总结（自动生成）

这篇论文介绍了一种利用大语言模型（LLM）动态生成课程（Curriculum）来辅助演化算法进行多任务策略搜索的方法。以下是对该论文的结构化深入总结：

### 1. 论文的核心问题与整体含义
*   **研究动机**：在机器人多任务策略搜索中，让智能体具备泛化能力（即能解决未见过的任务）非常困难。**课程学习（Curriculum Learning）**通过由易到难地引入任务，能有效提升学习效率，但传统课程设计高度依赖领域专家的经验，且费时费力。
*   **核心问题**：现有的基于 LLM 的课程生成多为“静态/离线”模式（一次性生成所有任务），无法根据优化器的实时表现调整难度。本研究旨在探索一种**交互式、在线**的 LLM 辅助框架，让 LLM 根据演化过程的实时反馈动态设计训练案例。

### 2. 论文提出的方法论
核心思想是建立一个**“优化器-LLM”交互循环**，包含以下关键环节：
*   **核心流程**：
    1.  **初始化（Contextualization）**：优化器向 LLM 发送背景信息，说明机器人任务、反馈格式和输出要求。
    2.  **优化阶段（Optimization）**：优化器（采用 **MAP-Elites** 算法）在当前任务集上训练策略（采用**遗传编程 GP** 表示的符号树策略）。
    3.  **反馈阶段（Feedback）**：优化器将训练结果（如成功率、收敛曲线、行为轨迹）反馈给 LLM。
    4.  **生成阶段（Response）**：LLM 分析反馈，理解当前策略的瓶颈，生成一个新的、难度适中的训练案例并加入任务集。
*   **三种反馈模态（Feedback Modalities）**：
    *   **N (Numeric)**：仅提供数值指标（如到目标的距离）。
    *   **N+P (Progression)**：增加优化收敛曲线图，让 LLM 看到学习速度。
    *   **N+P+B (Behavioral)**：进一步增加机器人运动轨迹的可视化图，让 LLM 直观看到机器人是在哪里撞墙或绕路的。
*   **策略表示**：使用遗传编程（GP）生成的符号方程作为控制器，具有较强的可解释性。

### 3. 实验设计
*   **场景**：2D 机器人导航任务。机器人需在 15x15 的网格阵列中避开障碍物（墙壁）到达目标点。
*   **Benchmark**：使用 6 个未见过的测试地图来评估策略的泛化性能。
*   **对比方法**：
    *   **Expert**：人类专家设计的 8 阶段课程（基准上限）。
    *   **Static**：LLM 一次性生成 8 个任务，不进行交互。
    *   **Random**：随机生成的障碍物地图。
    *   **交互式变体**：N、N+P、N+P+B 三种不同反馈深度的实验。

### 4. 资源与算力
*   **硬件**：Intel Core i5 14500T CPU (14 核)，16GB RAM。
*   **LLM 型号**：**Claude 3.5 Sonnet**（具有思维链推理能力）。
*   **训练时长**：单次交互循环平均耗时约 20 分钟（10% 时间为 LLM 响应，90% 为演化优化）。
*   **自动化程度**：文中提到消息转发是**手动（Copy-paste）**完成的，尚未完全通过 API 闭环自动化。

### 5. 实验数量与充分性
*   **实验规模**：每种方法重复实验 **10 次**以消除随机性影响。
*   **课程阶段**：每个课程包含 **8 个阶段**，总计进行了大量的演化评估。
*   **充分性评价**：实验设计较为充分，包含了消融实验（对比不同反馈模态）和统计显著性检验（Mann-Whitney U test）。通过对比“渐进式训练”与“全量批处理训练”，验证了课程顺序的有效性。

### 6. 论文的主要结论与发现
*   **交互式优于静态**：在线交互生成的课程显著优于 LLM 一次性生成的静态课程。
*   **多模态反馈的关键性**：加入“收敛曲线图 (P)”和“行为轨迹图 (B)”后，LLM 生成的任务质量大幅提升，最终表现**接近人类专家设计**的水平。
*   **LLM 的理解力**：通过分析 LLM 的推理过程发现，它能准确识别出机器人是在哪个障碍物处失败，并据此调整下一个地图的布局（如增加或减少障碍物）。
*   **课程的真实性**：实验证明 LLM 生成的任务确实构成了“由易到难”的序列，按顺序学习的效果远好于直接在所有任务上同时训练。

### 7. 优点（亮点）
*   **全无监督