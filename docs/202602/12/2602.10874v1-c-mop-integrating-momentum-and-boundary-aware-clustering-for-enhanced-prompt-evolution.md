# C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution
# C-MOP：集成动量与边界感知聚类以增强提示词演化

**Authors**: Binwei Yan, Yifei Fu, Mingjian Zhu, Hanting Chen, Mingxuan Yuan, Yunhe Wang, Hailin Hu \\
**Date**: 2026-02-11 \\
**PDF**: https://arxiv.org/pdf/2602.10874v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 6.0 \\
**Evidence**: Automatic prompt evolution aligns with the evolution of heuristics and efficient automatic algorithms \\

---

## Abstract
Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based Momentum Optimized Prompting), a framework that stabilizes optimization via Boundary-Aware Contrastive Sampling (BACS) and Momentum-Guided Semantic Clustering (MGSC). Specifically, BACS utilizes batch-level information to mine tripartite features--Hard Negatives, Anchors, and Boundary Pairs--to precisely characterize the typical representation and decision boundaries of positive and negative prompt samples. To resolve semantic conflicts, MGSC introduces a textual momentum mechanism with temporal decay that distills persistent consensus from fluctuating gradients across iterations. Extensive experiments demonstrate that C-MOP consistently outperforms SOTA baselines like PromptWizard and ProTeGi, yielding average gains of 1.58% and 3.35%. Notably, C-MOP enables a general LLM with 3B activated parameters to surpass a 70B domain-specific dense LLM, highlighting its effectiveness in driving precise prompt evolution. The code is available at https://github.com/huawei-noah/noah-research/tree/master/C-MOP.

## 摘要
自动提示词优化是提升大

---

## 速览摘要（自动生成）

**问题**：现有自动提示词优化方法存在更新信号噪声大、语义冲突