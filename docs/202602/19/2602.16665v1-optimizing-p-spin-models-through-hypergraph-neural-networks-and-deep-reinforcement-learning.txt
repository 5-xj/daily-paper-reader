Title: Optimizing p-spin models through hypergraph neural networks and deep reinforcement learning

URL Source: https://arxiv.org/pdf/2602.16665v1

Published Time: Thu, 19 Feb 2026 02:03:08 GMT

Number of Pages: 13

Markdown Content:
## Optimizing p-spin models through hypergraph neural networks and deep reinforcement learning 

Li Zeng, 1, âˆ— Mutian Shen, 2, âˆ— Tianle Pu, 1 Zohar Nussinov, 2, 3 

Qing Feng, 1 Chao Chen, 1 Zhong Liu, 1 and Changjun Fan 1, â€ 

> 1

Laboratory for Big Data and Decision, College of Systems Engineering, National University of Defense Technology, Changsha 410073, China 

> 2

Department of Physics, Washington University in St. Louis, Campus Box 1105, 1 Brookings Drive, St. Louis, MO 63130, USA 

> 3

Rudolf Peierls Centre for Theoretical Physics, University of Oxford, Oxford OX1 3PU, United Kingdom 

p-spin glasses, characterized by frustrated many-body interactions beyond the conventional pair-wise case ( p > 2), are prototypical disordered systems whose ground-state search is NP-hard and computationally prohibitive for large instances. Solving this problem is not only fundamental for understanding high-order disorder, structural glasses, and topological phases, but also central to a wide spectrum of hard combinatorial optimization tasks. Despite decades of progress, there is still a lack of an efficient and scalable solver for generic large-scale p-spin models. Here we introduce PLANCK, a physics-inspired deep reinforcement learning framework built on hypergraph neural net-works. PLANCK directly optimizes arbitrary high-order interactions, and systematically exploits gauge symmetry throughout both training and inference. Trained exclusively on small synthetic instances, PLANCK exhibits strong zero-shot generalization to systems orders of magnitude larger, and consistently outperforms state-of-the-art thermal annealing methods across all tested structural topologies and coupling distributions. Moreover, without any modification, PLANCK achieves near-optimal solutions for a broad class of NP-hard combinatorial problems, including random k-XORSAT, hypergraph max-cut, and conventional max-cut. The presented framework provides a physics-inspired algorithmic paradigm that bridges statistical mechanics and reinforcement learn-ing. The symmetry-aware design not only advances the tractable frontiers of high-order disordered systems, but also opens a promising avenue for machine-learning-based solvers to tackle previously intractable combinatorial optimization challenges. 

HIGH-ORDER SPIN GLASSES AND NP-HARD OPTIMIZATION 

Over the past few decades, the spin glass model has become a cornerstone of statistical physics[1, 2]. A key breakthrough came with Parisiâ€™s replica symmetry break-ing (RSB) theory[3], which provides fundamental insights into the fully connected Sherringtonâ€“Kirkpatrick (SK) model[4] and extends naturally to more complex disor-dered systems[5â€“7]. Gardner further generalized the SK model by introducing p-spin interactions[8], where the SK model corresponds to the special case p = 2 and the random energy model (REM) arises in the limiting case 

p â†’ âˆ [9]. Optimizing p-spin glass models[10] with p > 2is difficult, particularly for the Edwardsâ€“Anderson (EA) model, which describes finite-dimensional lattices with nearest-neighbor interactions[11]. In this regime, mean-field approaches break down, sparking ongoing debates between the RSB framework and the competing droplet model[12]. At the heart of these challenges lies in mini-mizing the following p-spin Hamiltonian: 

Hp = âˆ’ X

> i1<i 2<...<i p

Ji1,i 2,...,i p

> p

Y

> k=1

Ïƒik . (1)   

> âˆ—These authors contributed equally to this work.
> â€ fanchangjun@nudt.edu.cn

In general, this Hamiltonian can be defined on arbitrary graphs. Here we will focus on the most heavily studied Edwardsâ€“Anderson (EA) model with Ising spins Ïƒi = Â±1placed on regular two-dimensional latticesâ€”triangular (p = 3), square ( p = 4), and hexagonal ( p = 6). The coupling Ji1,i 2,...,i p is generally drawn from bimodal or Gaussian distributions (Fig. 1 for a hexagonal lattice ex-ample). Optimizing the p-spin model with p > 2 is of fun-damental importance for three reasons. First, it is the key to understanding structural glasses[13], supercooled liquids[14], and topological quantum error correction[15] governed by high-order disorder. Second, optimizing the native high-order Hamiltonian, rather than a quadra-tized proxy, eliminates auxiliary variables and retains the original interaction geometry, both of which simplify the problem and improve computational efficiency[16]. Third, the p-spin formulation exactly encodes a vast fam-ily of NP-hard combinatorial tasks, from pure-p systems (random k-XORSAT[17], Modern Hopfield model[18]) to mixed-p (hypergraph MaxCut[19], MAXSAT[20]) and even conventional pairwise problems (MaxCut[21], ver-tex cover[22]). These characteristics position the p-spin model as a universal and physically grounded bench-mark for assessing optimization algorithms across diverse regimes. Unlike the well-studied pairwise case ( p = 2), instances with p â‰¥ 3 give rise to a rugged, fractal-like free-energy landscape[8, 23] (Fig. 2) and are notoriously resistant 

> arXiv:2602.16665v1 [cond-mat.dis-nn] 18 Feb 2026

2ğ‘ğ‘ = 2  ğ‘ğ‘ â†’ âˆ 2 < ğ‘ğ‘ < âˆ     

> Gardner Landscape Random Peaks
> Ising Spin Glass ğ‘ğ‘ -Spin Model REM
> Greedy SA PLANCK PT
> Applications ğ‘ğ‘ -Spin Instances

c Case Study         

> Droplet Picture
> FIG. 1. Illustrative ground-state optimization on a hexagonal lattice. Ground-state search on a L= 8 hexagonal ( p= 6) lattice with fixed boundary conditions and Gaussian couplings. Each hexagon represents a six-spin interaction (opacity scales with |J|;blue-shaded areas indicate unsatisfied bonds with âˆ’JÎ Ïƒi>0). Snapshots compare the steady-state configurations achieved by Greedy search, Simulated Annealing (SA), Parallel Tempering (PT), and PLANCK. As can be observed, PLANCK is the only method that reaches the exact ground state (verified by Gurobi, a branch-and-bound based exact solver), whereas other methods stall in higher-energy local minima.

to both exact and heuristic solvers. Exact algorithms (branch-and-bound[24], branch-and-cut[25]) are limited to tens of spins, while metaheuristics such as simulated annealing (SA)[26] and parallel tempering (PT)[27] suf-fer from slow mixing and frequent entrapment in local minima and often require an impractically large number of sweeps to locate low-energy configurations. Recently, machine learning has emerged as a pow-erful paradigm for combinatorial optimization[28â€“31], particularly through learning-based models. In phys-ical and energy-based settings, notable examples in-clude physics-inspired graph neural networks (PIGNN) [32], hypergraph-oriented optimization frameworks like hypOp [33], and the Free-Energy Machine (FEM) [34]. These approaches achieve strong results on spe-cific problem classes by embedding energy minimiza-tion or landscape navigation into gradient-based learn-ing. Yet these approaches are tightly coupled to particu-lar task formulations and lack a unified training-inference pipeline, limiting their cross-domain applicability. An alternative line of work frames optimization as se-quential decision-making and leverages reinforcement learning (RL) to construct solutions step by step. DIRAC[35], for instance, learns efficient spin-flip poli-cies for Edwardsâ€“Anderson spin glasses and achieves state-of-the-art performance for pairwise ( p = 2) inter-actions. However, extending such RL-based solvers to arbitrary higher-order couplings has remained an open challenge. This limitation is not merely incremental: in-teractions of order p â‰¥ 3 create many-body correlations that are exponentially harder to model and require fun-damentally different state representations and credit as-signment mechanisms. In this work, we introduce PLANCK ( P-spin-g LA ss model optimization leveraging deep rei Nfor Cement learning and hypergraph neural networ Ks), a rein-forcement learning framework that, for the first time, solves the ground-state problem for p-spin glasses with arbitrary interaction order p in a unified manner. PLANCK is built on three key innovations. First, it directly operates on the native hypergraph repre-sentation of p-spin Hamiltonians, eliminating the need for auxiliary variables or quadratization and bypassing artificial pairwise reductions. The specially designed hypergraph neural network (PHGNN) encodes spin states and many-body couplings into order-independent features, enabling PLANCK to seamlessly scale to higher orders. Second, PLANCK systematically exploits gauge symmetry[36]â€”a fundamental property of spin glassesâ€”throughout both training and inference. This symmetry-aware design drastically reduces the search space, accelerates training convergence, and enhances fi-nal solution quality. Third, PLANCK provides a unified solver that, once trained on small synthetic instances, transfers zero-shot to substantially larger systems and to a wide spectrum of NP-hard combinatorial problems, in-cluding random k-XORSAT, hypergraph max-cut, and conventional max-cutâ€”without any task-specific cus-tomization. 

> PLANCK ARCHITECTURE AND EMPIRICAL PERFORMANCE Reinforcement learning formulation

We formulate the p-spin ground-state search as aMarkov decision process (MDP), specified by the tu-ple ( S, A, R, P, Î³ ), on the problemâ€™s natural hypergraph 

G = (V, E) (Fig. 3). In this representation, nodes 

vi âˆˆ V (sites) correspond to spin variables Ïƒi, while hy-peredges e âˆˆ E (bonds) encode the p-body couplings 

Je = Ji1,i 2,...,i p . This setup allows the agent to learn a sequential spin-flip policy that directly minimizes the Hamiltonian, avoiding quadratic reformulations. A state st âˆˆ S encodes the spin configuration Ïƒ(t) =3ğ‘ğ‘ = 3   

> ğ‘ğ‘ = 4
> ğ‘ğ‘ = 6

a            

> ğ‘ğ‘ = 2 ğ‘ğ‘ â†’ âˆ 2 < ğ‘ğ‘ <âˆ
> Full RSB
> Gardner Landscape Random Peaks
> Ising Spin Glass ğ‘ğ‘ -Spin Model REM
> Pairwise High Order I.I.D
> Greedy SA PLANCK PT

b

> Supercooled Liquids
> Structural Glasses
> Topological Color Code
> Applications ğ‘ğ‘ -Spin Instances

c 

> ğ’‘ğ’‘ -Spin and Applications Different Landscapes
> Case Study
> Droplet Picture

# ï¼Ÿ

FIG. 2. Structural topologies and landscape complexity of p-spin glasses. a, Depiction of higher-order structural topologies and their broad physical applications. Left: Representative p-spin models on triangular ( p = 3), square ( p = 4), and hexagonal (p = 6) Edwardsâ€“Anderson lattices. Right: These models offer theoretical tools for probing complex systems such as structural glasses and supercooled liquids, and for tackling decoding problems in quantum topological color codes. b, Hierarchy of energy landscape topologies across p-spin models. For p = 2 (SK model), the landscape reflects the tension between the full replica symmetry breaking (RSB) scenario and the droplet picture. In the intermediate range 2 < p < âˆ, the system enters a Gardner phase, creating a rugged, fractal-like free-energy landscape. In the limit p â†’ âˆ , the landscape reduces to the uncorrelated energy levels of the Random Energy Model (REM). 

{Ïƒ(t)1 , . . . , Ïƒ (t) 

> N

}. An action at âˆˆ A corresponds to the specific spin selected for flipping, inducing a transition to st+1 with probability P (st+1 | st, a t). The imme-diate reward r(st, a t, s t+1 ) âˆˆ R measures the result-ing energy reduction and is given by r(st, a t, s t+1 ) = 2Ïƒat

P 

> eâˆˆE (at)

Je

Q 

> kâˆˆe\{ at}

Ïƒk, where E(at) = {e âˆˆ E | 

at âˆˆ e} is the set of interactions involving the flipped spin. As it is computed analytically from the current configuration and the coupling tensor, this reward is both computationally efficient and statistically unbiased. Fu-ture rewards are discounted by a factor Î³.To navigate the non-convex and rugged energy land-scape characteristic of p-spin glass systems, we deliber-ately restrict each episode to start at the all-spins-up configuration and terminate at the all-spins-down config-uration. This constraint drastically reduces the number of possible trajectories, which would compromise global exploration in a naive implementation. We resolve this tension by exploiting gauge symmetry (Eq. (2)), which maps any configuration to the all-up state while preserv-ing the energy. This insight leads to a dual use of gauge transformation: feature-level augmentation during train-ing and instance-level path resetting during inference. The resulting framework explores the configuration space efficiently while retaining the sample-efficiency benefits of a fixed start-end pair. 

PLANCK architecture 

Given the intricate challenge of optimizing p-spin mod-els, we develop PLANCK to learn the policy network 

Ï€Î˜. During training (Fig. 3), PLANCK interacts on-line with randomly initialized small-scale p-spin glass in-stances, producing experience tuples ( st, a t, r t, s t+1 ) that are stored in a replay buffer B and periodically sam-pled to update the parameters Î˜ via gradient descent on the temporal-difference error. During the testing stage (Fig. 4), PLANCK combines its trained Q network with simulated annealing for optimized p-spin glass solutions. The system alternates between neural-guided spin flips (based on Q values) and thermal sampling, dynamically adjusting this balance via a temperature-dependent se-lection probability. The success of PLANCK largely depends on two key elements: (1) How can the p-spin glass system be repre-sented, taking into account its physical characteristics? (2) How can these representations be used to compute a 

Q-value that predicts the long-term benefits of an action in a given state? We describe these challenges as the encoding and the decoding problem, respectively. We design an encoder based on hypergraph neural net-works [37â€“39] named PHGNN ( p-spin HyperGraph Neu-ral Network), a gauge symmetry-aware Message Passing Neural Network [40] to capture the complex high-order interactions in the p-spin glass system. PHGNN be-gins with carefully designed features. For each site, we design its feature as xs = (Ïƒi, h i) âˆˆ R2, where 

Ïƒi âˆˆ {âˆ’ 1, +1 } indicates the current state of the site and hi = P 

> eâˆˆE (i)

Je

Q 

> kâˆˆe\{ i}

Ïƒk represents the local field contribution from p-body interactions, with E(i) denot-ing the set of hyperedges incident to site i. For each bond, we design its feature as xb = Ji1...i p âˆˆ R1, where Ji1...i p

is the coupling strength from the Hamiltonian. These features naturally decouple feature complexity from the interaction order p, allowing the model to handle higher-order interactions without explosion of the feature space. 4Select PMH Select EMH Start Flip End Flip Start End 

> Sampling

d

ef hg

> Bond Site

a b

GT GT GT      

> Feature concat Feature augmentation Message Passing Node embedding Gauge Transformation Output Input Readout Encoder
> Bond to Site Site to Bond
> Aggregation Update
> K Iterations Repeat
> Decoder Start Step Encoder & Decoder Encoder & Decoder Training Pool End
> vanilla all-spins-up all-spins-down

## Application    

> Metropolis Hastings Metropolis Hastings
> Action Reward Experience Replay Buffer
> Update Update Update

c 

> State Sampling Sampling Next Step Next Step Sampling

FIG. 3. PLANCK framework (I): training stage and learning components. During training, we generate synthetic small 

p-spin glass instances and represent each instance as a hypergraph (nodes correspond to sites and hyperedges represent multi-spin interactions), which are added to the training pool. The PLANCK agent is optimized by sampling from this pool and interacting with instances in episodes that traverse from an all-spins-up to an all-spins-down configuration. At each step, gauge transformations produce energetically identical representations; the encoder Î˜ E performs message passing to produce node embeddings, and the decoder Î˜ D estimates Q-values for spin flips. Experience transitions are stored in a replay buffer and mini-batches are sampled to update {Î˜E , Î˜D } via gradient descent. 

We note that the Hamiltonian of the p-spin glass sys-tem is invariant between the instances ( Ji1,i 2,...i p , Ïƒ ik )and ( Jâ€²

i1,i 2,...i p , Ïƒ â€²

ik ) through the technique of gauge trans-formation (GT) [41â€“43]: 

Jâ€²

i1,i 2,...i p = Ji1,i 2,...i p

pY

k=1 

tik , Ïƒâ€²

ik = Ïƒik tik (2) where the gauge variables tik âˆˆ {âˆ’ 1, +1 } induce symme-try transformation which is capable of toggling the spin glass system between any two configurations while keep-ing the system energy invariant. We use a simple but ef-fective feature augmentation trick to convert the raw fea-tures ( xs, xb) into gauge-equivalent augmented features (x(aug) 

s , x (aug) 

b ): 

ï£±ï£²ï£³

x(aug) 

s = [ xs, x(up) 

s , x(down) 

s ] âˆˆ R6

x(aug) 

b = [ xb, x (up) 

b , x (down) 

b ] âˆˆ R3 (3) where ( x(up) 

s , x (up) 

b ) and ( x(down) 

s , x (down) 

b ) represent the features derived through a gauge transformation that sets the system to an all-spins-up configuration and an all-spins-down configuration, respectively. This feature aug-mentation integrates gauge invariance directly within the embedding process. The features ( x(aug) 

s , x (aug) 

b ) are then processed through a two-stage message passing process which alternates between sharing information from site-to-bond and bond-to-site aggregations (see Figs. 3 and 4). Once message passing is completed, we get the site em-beddings zi that encapsulate both individual conditions and their shared surroundings within K hops. These 5Select PMH Select EMH     

> Start Flip End
> Flip Start End
> GT GT GT
> Metropolis Hastings Metropolis Hastings
> Next Step Next Step

FIG. 4. PLANCK framework (II): application stage and hybrid inference. In the application stage, an instance is initialized at a high-temperature phase with a random configuration St. The system probabilistically selects between a traditional energy-based metaheuristic method (EMH) and a PLANCK metaheuristic (PMH). If PMH is chosen, a gauge transformation GT( Â·) maps the system to an equivalent representation; the trained PLANCK performs coordinated spin flips to produce an energy trajectory, and a conditional update mechanism is applied (updating to the lowest-energy state when âˆ† E â‰¤ 0, or to a perturbed state when âˆ†E > 0), followed by the inverse transformation GT âˆ’1(Â·) to yield the next state. If EMH is selected, a Metropolis-Hastings annealing process is executed, accepting flips with probability min[1 , e âˆ’Î²tâˆ†E ]. The global minimum-energy configuration found through this hybrid strategy is output as the predicted ground state in the low-temperature phase. 

are aggregated by summation to produce a global sys-tem representation zs, which is invariant to permutation and maintains gauge symmetry. Next, we use zi to rep-resent states and zs to denote the global context in the Markov Decision Process of PLANCK. For decoding, we design a deep parameterization of the score function (the Q-function) which operates on learned representations of states and actions, denoted as zs and 

za respectively, to compute the state-action value func-tion Q(s, a (i); Î˜). This function estimates the expected future cumulative rewards associated with action a(i) in state s, while following the policy Ï€Î˜(a(i) | s) throughout the entire episode. The Q-function takes the following form: 

Q(s, a (i); Î˜) = MLP([ zs, zi]; Î˜) (4) Here MLP denotes a two-layer multi-layer perceptron network with ReLU activation functions, and Î˜ =

{Î˜E , Î˜D }, where Î˜ E denotes the encoder parameters and Î˜ D represents the decoder parameters. The training pipeline of PLANCK uses n-step Q-learning to fine-tune the Q-function parameters Î˜ for optimizing the p-spin model. Experience transitions are stored in a replay buffer B, with each transition denoted by a tuple ( st, a t, r t,t +n, s t+n). The main objective of the training is to minimize the n-step Q-learning loss, which measures the difference between predicted Q-values and 

n-step bootstrapped targets. This loss function combines immediate rewards with discounted future rewards via n-step reward accumulation, with the discount factor Î³

adjusting the significance of future rewards. The formula for the n-step Q-learning loss function is given as: 

L = E(st,a t,r t,t +n,s t+n)âˆ¼B 

h rt,t +n + Î³ max 

> at+n

Q(st+n, a t+n; Ë†Î˜) 

âˆ’ Q(st, a t; Î˜) 2i

.

(5) We periodically draw mini-batches from the replay buffer B, where each sample consists of a 4-tuple (st, a t, r t,t +n, s t+n), and use gradient descent to refine the parameters Î˜. The target network parameters Ë†Î˜are maintained separately and periodically synchronized with Î˜ at fixed intervals to ensure training stability. PLANCK is trained on a large number of small-scale random p-spin glass instances. Once trained, it serves as a solver that can be applied to efficiently and repeatedly search for ground states of p-spin glass instances that are significantly larger than those seen during training. Sim-ilar to the training process, it begins from the all-spin-up configuration and proceeds to the all-spin-down configu-ration, flipping at each step the spin with the highest 

Q-value as predicted by the trained model. Notably, PLANCK always starts from the same initial configura-tion, which helps drastically reduce the number of poten-tial trajectories and reduces the amount of training data required. Ending at a fixed terminal configuration en-sures that the agent completes the MDP within a finite number of steps. This constraint enables the agent to learn an efficient, data-driven spin-flipping strategy with minimal regret. However, we emphasize that while the strategy adopted by PLANCK provides significant efficiency gains, it also introduces certain limitations. Specifically, 6starting from a fixed initial state and proceeding to a fixed terminal state, each intermediate configuration is explored exactly once. Although this single-pass explo-ration is highly likely to succeed, it severely restricts the agentâ€™s exploration capability, particularly in extremely large decision spaces. In contrast, heuristic methods such as simulated annealing owe much of their success to the ability to initiate from diverse states and revisit promising configurations multiple times through stochas-tic perturbations. Therefore, to further enhance the agentâ€™s ability to locate ground states, one straightfor-ward direction is to endow PLANCK with similar capa-bilities. Unfortunately, the current strategy adopted by PLANCK does not permit such flexible exploration. To address the aforementioned limitation, we adopt a simple yet effective technique known as gauge transfor-mation (GT), applied at the instance level, similar to its use at the feature level. After the agent reaches the final spin configuration in each trajectory, we apply a GT that resets the spin configuration back to the initial all-spin-up configuration. This transformation simultaneously mod-ifies the bond weights such that the total energy of the system remains invariant. In doing so, GT enables the agent to restart exploration from a consistent initial con-figuration, thereby extending the effective search horizon without violating energy conservation principles. Through this mechanism, we introduce a hybrid ap-proach for solving challenging spin systems by integrating the predictive capabilities of PLANCK with the stochas-tic exploration of simulated annealing (SA). At each temperature Î²t in the annealing schedule, the system probabilistically selects between traditional Metropolis-Hastings updates and PLANCK-guided spin flips: 

Pselect =

(

SA: min[1 , e âˆ’Î²tâˆ†E ] with probability p

PLANCK: Q-value optimization with probability 1 âˆ’ p

(6) If the spin flip suggested by PLANCK fails to lower the energy (i.e., âˆ† E â‰¥ 0), thermal perturbations are introduced with probability: 

Pperturb = q Ã— Î²max âˆ’ Î²t

Î²max âˆ’ Î²min 

, (7) where q = 0 .5 balances the trade-off between exploration and exploitation. This adaptive blending allows SA to dominate at high temperatures ( Î²t â†’ Î²min ) for global exploration, while favoring PLANCKâ€™s learned Q-value gradients at low temperatures ( Î²t â†’ Î²max ) for precise local optimization. During testing, this hybrid framework demonstrates superior performance compared to conventional SA. While traditional SA relies purely on local energy differ-ences âˆ† E, PLANCKâ€™s Q-network encodes global multi-spin correlations through learned parameters Î˜. The GT-based reset mechanism allows periodic non-local transitions via: (Ïƒ(s)up , J (gt )) = GT( Ïƒ(sâˆ’1) , J ) (8) preserving the systemâ€™s topology while enabling er-godic traversal of gauge-equivalent configurations. The temperature-adaptive perturbation probability Pperturb 

provides a smooth interpolation between thermal and learned behaviors, enhancing the ability to tunnel out of local minima. 

> Training details and baseline solvers

We selected PLANCK hyperparameters via ablation studies, focusing on hypergraph neural networks and re-inforcement learning aspects such as message-passing lay-ers k, RL discount Î³, and reward horizon n. While PLANCKâ€™s current performance is already robust, we note that additional improvements could potentially be achieved through more systematic hyper-parameter opti-mization. In particular, we found that the optimal num-ber of message-passing layers k depends significantly on both the system size and interaction order p, with K = k

performing best for most cases we tested. This finding partially confirms our initial hypothesis that deeper mes-sage passing would better capture complex correlations. For baselines, we implemented two classical optimiza-tion techniques. For SA, per Ref. [44], we utilized a linear temperature annealing from Î²min = 0 to Î²max = 5 with 

Nt = 100 steps and Ns = 50 sweeps each. For PT, as guided by Ref. [45], we used Nr = 20 replicas across 

T âˆˆ [0 .1, 1.6], swapping configurations every N flips. The greedy search algorithm optimizes energy locally by flipping individual spins. It stops when further flips cannot reduce energy, settling in a local minimum. Though efficient, it often stalls in suboptimal states due to the spin glassesâ€™ intricate energy landscape.  

> Optimizing the p-spin models

To systematically evaluate PLANCKâ€™s capability in optimizing p-spin glass systems with different sizes L,we conducted extensive numerical experiments on both Gaussian and bimodal bond weight distributions across multiple lattice geometries. We first trained PLANCK on small-scale synthetic systems: for triangular ( p = 3) and square ( p = 4) lattices we set L = 5, while for the hexag-onal lattices ( p = 6) we set L = 4. All training and testing datasets are generated by the lattice graph gen-eration functions from the Networkx package [46]. Sub-sequently, we tested the trained models on larger system sizes to evaluate their generalization capability. For base-line comparisons, we selected two classical approaches for spin glass problems: Simulated Annealing (SA) [47] and Parallel Tempering (PT) [27]. Since all methods (PLANCK, SA, and PT) share a similar inner loop struc-ture where spins are flipped from a special initial spin configuration, the total computational budget for each algorithm is naturally quantified by the number of dis-tinct initial configurations processed, which we denote as 7

Ninitial .To ensure fair comparison, we fixed Ninitial = 5000 for all methods. For SA, this budget is allocated across 

NT temperature steps with NS initial configurations pro-cessed at each step ( Ninitial = Ns Ã— Nt). The PT imple-mentation distributes the budget over Ne epochs of Nr

replicas, with each epoch-replica combination processing one initial configuration ( Ninitial = Ne Ã— Nr ). Fig. 5(a-c) present the comparative performance for bimodal cou-pling distributions across the three lattice types on 50 independent instances respectively, with results reported as mean Â±SEM. The results demonstrate PLANCKâ€™s su-perior solution quality, with particularly notable advan-tages emerging at larger system sizes. This scaling be-havior provides strong evidence for PLANCKâ€™s enhanced generalization capability compared to the baseline meth-ods. The performance advantage remains consistent for Gaussian coupling distributions as shown in Fig. 5(d-f), with PLANCK maintaining significantly lower final ener-gies across all tested system sizes and lattice geometries. For small p-spin models, exact solutions can be ob-tained using branch-and-bound solvers such as Gurobi [48]. However, as the system size L increases, finding the ground state with exact methods becomes computa-tionally intractable. Heuristic or approximate algorithms like SA, PT, and PLANCK are impacted by the param-eter Ninitial , which is the count of independent search attempts. Intuitively, larger values of Ninitial generally lead to better performance, although at the cost of in-creased computation time. To investigate how Ninitial af-fects algorithm performance, we conducted experiments on three EA-p spin models with Gaussian couplings: 

p = 3 ( L = 20 , 25 , 30), p = 4 ( L = 15 , 20 , 30) and p = 6 (L = 10 , 16 , 20). We produced 50 independent test cases for each scenario, altering Ninitial between 1 and 2 Ã—10 4 to investigate the variation in e0 (energy density per bond) as Ninitial rises. For PT, we recorded e0 at each incre-ment of Ninitial , while for SA and PLANCK, each data point represents results using 5000 initial configurations. Fig. 5 shows the mean energy density e0 with SEM (shaded areas) for all algorithms across the test sets. Sev-eral key observations emerge from these results. First, PLANCK consistently achieves the best performance at any given Ninitial value, demonstrating statistically sig-nificant advantages over the baseline methods. Notably, PLANCKâ€™s performance with just 5000 initial configura-tions surpasses that of both SA and PT even when they use 2 Ã— 10 4 initial configurations. Finally, the trends with increasing Ninitial reveal important differences be-tween the algorithms. PT (blue curves) shows the most pronounced performance improvement as Ninitial in-creases, with its energy density decreasing substantially across the tested range. SA exhibits more moderate im-provements with additional initial configurations (brown curves). In contrast, PLANCKâ€™s performance remains remarkably stable regardless of Ninitial (red curves), in-dicating that its carefully designed reinforcement learn-ing components and hypergraph representation learning modules enable highly efficient search without requiring extensive sampling of initial states. Beyond the effectiveness, PLANCK achieves excep-tional computational efficiency through carefully de-signed algorithmic strategies. The framework employs a dynamic hybrid approach during inference that strate-gically balances neural-guided spin flips with traditional Monte Carlo sweeps, which enables precise control over the speed-accuracy trade-off. All experiments were conducted on a 64-core computer with 512GB RAM and a NVIDIA A100 GPU with 32GB memory. Although PLANCK requires an initial offline training phase, this cost is amortized as the trained model can be applied indefinitely to systems of varying sizes within the same lattice type without retraining. The computational complexity of PLANCK during the test-ing stage scales favorably with system size. For sparse hypergraph systems, the time complexity remains linear 

O(N ) due to efficient message passing in the hypergraph neural network, while dense systems exhibit quadratic scaling O(N 2). The gauge-invariant feature engineering requires only O(N + E) operations, processing each spin and hyperedge in parallel. The encoding phase, imple-mented through K = 5 sparse message-passing layers, maintains O(KN d 2) complexity for typical systems by leveraging hypergraph sparsity. The lightweight decoder contributes minimal overhead with O(N h 2) complexity, where h = 32. Notably, the incorporation of gauge transformations provides significant training acceleration, achieving faster convergence compared to non-invariant baselines while simultaneously improving the final approximation ra-tio. This improvement stems from the constrained policy search space enabled by symmetry-aware representations. These advances establish PLANCK as an efficient and scalable framework for solving complex high-order opti-mization problems in both physics and computer science domains. 

> Applications on General NP-hard problems

The Ising formulation has emerged as a universal paradigm for representing combinatorial optimization problems, capable of encoding Karpâ€™s 21 NP-complete problems [21]. Although higher-order Ising models can be reduced to quadratic form [49], this typically requires auxiliary variables, which can inflate problem size and computational cost [16, 50, 51]. Consequently, there is growing interest in tackling higher-order ( p-spin) in-teractions directly in their native form [16, 17, 52, 53], thereby avoiding the overhead of variable expansion. To evaluate PLANCKâ€™s ability to solve high-order NP-hard problems, we design an end-to-end framework that first transforms original problems into p-spin models. The PLANCK is then trained to find their ground states, which are mapped back to solutions for the original opti-mization tasks. An overview of the end-to-end workflow 8SA PT FEM PLANCK                                 

> -1.0 p=3,Bimodal 5-0.70 -0.60 -0.62 -0.68 -0.72 -0.76 -0.79 -0.83 -0.66 -0.70 -0.75 -0.79 -0.62 -0.64 -0.66 -0.68 -0.70 -0.72 -0.83 -0.88 -0.94 -0.99 -1.04 -0.78 -0.84 -0.90 -0.96 -0.71 -0.72 -0.73 -0.74 10 15 20 30 510 15 20 30 10 20 30 40 50 510 15 20 30 510 15 20 30 10 20 30 40 50 p=4,Bimodal p=6,Bimodal p=3,Guassian p=4,Gaussian p=6,Gaussian
> 0

# e

> 0

# e

FIG. 5. Performance of different methods in minimizing the p-spin Hamiltonian. We compared the disorder averaged â€œground-stateâ€ energy per bond (predicted by each method), denoted as e0, to benchmark various methods. Each result is computed over 50 independent instances, with mean values (bars) and standard error of the mean (SEM; error bars) reported. Note that PLANCK tested here is trained exclusively on small synthetic instances ( L = 5 for triangular p = 3 and square p = 4; L = 4 for hexagonal p = 6). It is then tested, without any retraining or fine-tuning, on systems 4â€“6 times larger than seen during training. Both simulated annealing (SA) and parallel tempering (PT) use identical computational budget ( Ninit = 5000). It can be observed that across all lattice types and both Bimodal (aâ€“c) and Gaussian (dâ€“f) couplings, PLANCK (red) consistently finds the lowest-energy configurations among all other methods. The presented results demonstrate that the policy PLANCK learned on small instances transfers effectively to much larger, never-before-seen instances. 

is shown in Fig. 6, and representative case studies are summarized in Fig. 7. Specifically, we categorize p-spin models transformed from NP-hard problems into three types: pure p prob-lems with interactions of precisely p-spins (e.g., random 

k-XORSAT); mixture p problems with varied interac-tion orders (e.g., hypergraph MaxCut); and pairwise Ising problems with solely quadratic terms (e.g., stan-dard MaxCut). Random k-XORSAT (pure p) is a key variant of Boolean satisfiability with wide applications in cryptog-raphy [54], error-correcting codes [55, 56], and hardware verification [57]. This problem aims to assign values to N

Boolean variables such that M = cN parity constraints are satisfied. Each constraint is a p-tuple of random vari-able indices, associated with a binary value ym. This problem can be translated to a p-spin model as follows: 

Hrandom âˆ’kâˆ’XORSAT =

> M

X

> m=1

(1 âˆ’ JmÏƒim 

> 1

Â· Â· Â· Ïƒimp ) (9) Here (1 âˆ’ JmÏƒim 

> 1

Â· Â· Â· Ïƒimp ) denotes a parity condition, where Ïƒimk = ( âˆ’1) ximk reflects the original Boolean vari-able ximk for the k-th variable in the m-th constraint. The 

Jm = ( âˆ’1) ym captures the desired parity ym for the m-th condition. We conducted extensive experiments on both tough random 3-XORSAT and random 4-XORSAT problems. The results demonstrate that PLANCK achieves remark-able performance in solving these challenging instances. Hypergraph Max-Cut (mixed p) [19] extends the con-ventional Graph MaxCut problem and is of essential im-portance in combinatorial optimization with profound theoretical and practical impacts. Specifically, within a k-uniform hypergraph [58], where each hyperedge com-prises precisely k vertices, the problem is to partition the vertex set into two separate, non-empty groups to maxi-mize the number (or total weight) of hyperedges linking these groups. This problem can subsequently be formu-9ï¿½ï¿½ ï¿½ ï¿½ï¿½        

> PLANCK
> Oiriginal Problems Pspin Formulations Hypergraph Modeling Problem Solving Solution Output
> FIG. 6. PLANCK as a unified solving framework for NP-hard problems (workflow). The framework unifies the optimization workflow by recasting general NP-hard combinatorial optimization problems, originally expressed via objectives F, into a generalized p-spin Hamiltonian H. This Hamiltonian is encoded as a hypergraph, on which the PLANCK iteratively optimizes the spin configuration to find the ground state, subsequently translating it back into a solution of the initial problem.

lated as a p-spin model as follows [59]: 

HHypergraph MaxCut = X

> eâˆˆE

ï£®ï£° we

2|e|âˆ’ 1

X

> SâˆˆP +even (e)

Y

> iâˆˆS

Ïƒi

!ï£¹ï£»

(10) Here e denotes a hyperedge comprising k vertices, and we

is the hyperedgeâ€™s weight. The spin variables, Ïƒi (where 

Ïƒi âˆˆ {âˆ’ 1, +1 }), define the partitioning of the vertices. The set P+even (e) = {S âŠ† e | | S| is even , S Ì¸ = âˆ…} denotes the collection of all non-empty even-sized subsets of the hyperedge e.We evaluated PLANCKâ€™s performance on k-uniform hypergraphs ( k = 4 , 5) of different sizes. The results demonstrate PLANCKâ€™s proficiency in addressing hyper-graph MaxCut problems. For 4-uniform hypergraphs, PLANCK surpassed SA and PT, especially with larger graphs. This advantage is even greater for 5-uniform hy-pergraphs. Conventional MaxCut (pairwise p=2) is an NP-hard problem involving pairwise spin interactions and is cru-cial in fields such as operations research [60], computer vision [61], and VLSI circuit design [62]. For an undi-rected graph G = ( V, E ) with n = |V | vertices and edge weights wij , the MaxCut problem is to divide the vertex set V into two non-overlapping subsets in a way that the total weight of the edges between them is maximized. Following the standard Ising model conversion [63], we can transform this problem into a p = 2 spin glass for-mulation: 

HMaxCut = âˆ’ X

> âŸ¨i,j âŸ©âˆˆ E

Jij ÏƒiÏƒj (11) Here Ïƒi âˆˆ {âˆ’ 1, +1 } are Ising spins representing vertex partitions, Jij = wij  

> 2

are the couplings. We train our model on synthetically produced BarabÂ´ asi-Albert (BA) graphs [64], which replicate the scale-free characteristics typical of real-world networks. Then we evaluate PLANCKâ€™s performance using the real-world Gset benchmark dataset [65]. Our base-line includes heuristics methods such as DSDP [66] and KHLWG [67], as well as machine learning methods such as RUN-CSP [68], PI-GNN [32], and HypOp [33]. The results demonstrate PLANCKâ€™s remarkable effectiveness across all Gset benchmarks. All the experiments above demonstrate that PLANCK can serve as a unified solver for combinatorial optimization problems, exhibiting strong performance across diverse settings. PLANCK not only generalizes across different scales (Fig. 5) but also efficiently adapts to various parameters and transfers between problem types. This robust generalization ability stems from PLANCKâ€™s preservation of the underlying mathematical structure in p-spin glass representations, making it particularly suitable for real-world optimization tasks where problem specifications may vary dynamically. 

> Emergent human-like strategy in the Baxter-Wu model

To systematically examine the outstanding perfor-mance of PLANCK, we performed an in-depth inter-pretability analysis using the exactly solvable Baxter-Wu (BW) model [69] as our test system. This canonical p-spin model ( p = 3), first solved exactly by Baxter and Wu fifty years ago [69, 70], has become a fundamental benchmark in the study of frustrated systems due to its rich theoretical properties and experimental relevance. The BW model ( p = 3) belongs to the same universality class as the two-dimensional four-state Potts model and the directional three-spin Ising model [71], despite their apparent structural differences. This equivalence makes it particularly valuable for understanding universal be-havior in frustrated systems. The Hamiltonian for the BW model on a triangular lattice is given by: 

H = âˆ’J X

> âŸ¨i,j,k âŸ©

ÏƒiÏƒj Ïƒk (12) Here Ïƒi âˆˆ {âˆ’ 1, +1 } represents Ising spins, âŸ¨i, j, k âŸ© de-notes the elementary triangular plaquettes, and J is the uniform coupling constant that we set to 1 for our studies. 10 ï¿½ ï¿½ ï¿½ ï¿½ï¿½ ï¿½ï¿½ ï¿½ ï¿½ ï¿½ï¿½                                 

> 4.63 3.67 2.99 0.00 0.00 3.24 3.95 4.00 2.48 0.00 0.00 1.73 1.24 1.97 1.33 1.37 1.02 1.52 0.72 0.66 1.30 0.02 0.02 1.40 0.13 0.23 0.42 0.00 0.00 0.23
> G14 G15 G22 G49 G50 G55 DSDP RUN-CSP PI-GNN HypOp PLANCK 012345MaxCut (% Gap to Best Known) Random-XOR-SAT Hypergraph Maxcut Maxcut Oiriginal Problems Pspin Formulations

# ï¿½    

> SA PT PLANCK 400 500 600 700 800 900 Cut Value SA PT PLANCK Satisfaction Ratio
> ï¼ˆ100,93 ï¼‰ï¼ˆ200,185 ï¼‰ï¼ˆ300,277 ï¼‰
> 0.60 0.70 0.80 0.90 1.00 Greedy SA PT PLANCK 3-XOR-SAT

# ï¿½

FIG. 7. PLANCK as a unified solving framework for NP-hard problems (case studies). Empirical evaluations across three distinct representative problems demonstrate the PLANCKâ€™s versatility and performance. For each problem, the original objective function and its corresponding p-spin formulation are explicitly shown. In the Random 3-XOR-SAT benchmarks, comprising 50 synthetic instances per problem scale generated via the Python cnfgen library, PLANCK maintains a near-optimal satisfaction ratio across varying problem scales (e.g., N = 100 , 200 , 300), significantly outperforming Greedy, Simulated Annealing (SA), and Parallel Tempering (PT) baselines while remaining robust to system size growth. For the Hypergraph MaxCut problem, evaluations were conducted on a dataset of 50 complex instances, each comprising 50 nodes and 600 to 900 mixed-order couplings (including interactions such as p = 2 and p = 4). PLANCK achieves higher mean cut values and lower variance compared to SA and PT, demonstrating its capacity to effectively navigate highly rugged, multi-body energy landscapes. On the classical MaxCut problem using standard Gset benchmarks, PLANCK attains the smallest relative gap to the best-known results compared to other advanced heuristic and learning-based methods (Hypop, PI-GNN, RUN-CSP, DSDP). It exactly matches the optimal cuts for several instances (e.g., G49, G50), reflecting strong generalization capabilities to diverse, real-world graph structures. 

The ground state of this system exhibits a fourfold de-generacy that reveals its underlying symmetry structure, consisting of one completely ferromagnetic state with all spins aligned along with three ferrimagnetic states where spins in two of the three sublattices are anti-aligned with the third. This semi-global symmetry, in which the Hamiltonian remains invariant under reversal of all spins in any two sublattices, plays a crucial role in the systemâ€™s dynamics and provides an ideal testbed for understand-ing algorithmic behavior. For our interpretability analysis, we initialize a 94-spin BW system with all spins in an anti-ferromagnetic con-figuration and without periodic boundary conditions to maximize frustration. We compare PLANCK against three baseline approaches: a greedy search algorithm that always flips the spin giving the maximum imme-diate energy reduction, SA with optimized parameters (Ns = 50, Nt = 100), and PT with optimized pa-rameters ( Ne = 16, Nr = 20). To isolate the effects of the reinforcement learning components, we configure PLANCK to use only its RL decision-making module without the SA augmentation. Fig. 8 illustrates the entire evolution of the energy den-sity and spin configurations. Spins with Ïƒi = âˆ’1 and Ïƒi = +1 are depicted by empty and filled circles, respectively, and shaded triangles de-note frustrated plaquettes. This comparative analysis reveals several key insights. Firstly, the greedy search al-gorithm, while aligned with PLANCK in step efficiency by design, is inherently unable to overcome local optima 11 Step = 30794 Step = 30794 Step = 4712 Step = 4712 Step = 100 Step = 28 Step = 28 Step = 10 Step = 20 Step = 1 Step = 10 Step = 20 Step = 27 Step = 27 Step = 1000 Step = 100 Step = 10000 PLANCK Greedy Step SA PT Initial State Step = 1 Step = 1 Step = 1 1 10 100 1000 10000 -1.0 -0.5 0.0 0.5 1.0 SA PT Greedy PLANCK     

> Spins (state = âˆ’1) Spins (state = +1) Frustrated couplings Pattern learned by PLANCK Probability of next spin flipping to âˆ’1
> 01

0e

FIG. 8. PLANCK exhibits human-like intelligence by efficiently optimizing the ferromagnetic Baxter-Wu model with couplings 

J = 1. We evaluate various optimization methods in searching for the ground state of the Baxter-Wu model ( p = 3) with uniform ferromagnetic couplings ( J = 1) on a 94-spin triangular lattice without periodic boundary conditions. We compare PLANCK (using only its RL components) against Greedy search, SA, and PT, with all methods starting from the same maxi-mally frustrated ferromagnetic configuration. The figure illustrates energy density changes (y-axis, 1 to -1) against optimization steps (x-axis). Crucial configurations are depicted at representative trajectory points, where empty circles denote Ïƒi = âˆ’1, filled circles signify Ïƒi = +1, and shaded triangles mark frustrated plaquettes. The PT results correspond to the lowest-energy replica. Besides, we highlight the configurations at key steps of the PLANCK algorithm, ( a)â€“( g) to illustrate the evolution of its frustration-resolving strategy during optimization. The results reveal different optimization behaviors: while Greedy search (27 steps) and PLANCK (28 steps) show comparable step efficiency, only PLANCK successfully reaches the ground state by employing an emergent strategy of flipping core spins in hexagonal clusters to simultaneously resolve multiple frustrations. In contrast, SA (30,794 steps) and PT (4,712 steps, shown for the lowest-energy replica) eventually find the ground state but through lengthy trajectories exhibiting random-walk characteristics without discernible pattern. The energy density trajectories emphasize PLANCKâ€™s superior performance, achieving faster convergence and lower final energies than traditional methods, while keeping the optimization dynamics physically meaningful. 12 due to its localized optimization strategy. Secondly, an-alyzing the stochastic algorithms SA and PT, we notice that they eventually reach the ground state, albeit af-ter exceedingly lengthy trajectories, specifically 30,794 and 23,836 steps. These pathways extend several orders of magnitude beyond the system size and lack identifi-able patterns or physical interpretability. Although frus-trated plaquettes reduce over time, this reduction occurs via seemingly arbitrary spin flips without a systematic plan, rendering it difficult to derive generalizable insights into the systemâ€™s physics from their behavior. In con-trast, PLANCK not only reaches the ground state quickly but also employs an elegant and physically interpretable strategy. The algorithm autonomously devises a hierar-chical optimization approach by identifying and sequen-tially flipping the core spins of non-overlapping hexagonal clusters, each composed of six triangular plaquettes. This â€œsmartâ€ technique satisfies all six plaquettes simultane-ously, demonstrating PLANCKâ€™s ability to adopt a global optimization perspective inspired by human-like reason-ing in optimizing the p-spin glass model. The emergence of such problem-solving behavior, where the algorithm discovers higher-order spin update patterns rather than relying on single-spin flips, provides strong evidence that PLANCK develops physically meaningful strategies, sur-passing traditional methods in both efficiency and inter-pretability. 

> CONCLUSION AND OUTLOOK

This paper presents PLANCK, a unified machine learning framework that integrates hypergraph neural networks and deep reinforcement learning to efficiently optimize p-spin glass models. Through extensive numer-ical experiments, we demonstrate that PLANCK con-sistently outperforms conventional simulated annealing methods, achieving significantly lower energy densities and faster convergence across various lattice structures (triangular, square, and hexagonal) and interaction or-ders. Additionally, PLANCK exhibits robust perfor-mance under different bond distributions (bimodal vs. Gaussian), varying system scales, and mixed-p interac-tion models. A key strength of PLANCK lies in its ability to nat-urally handle high-order interactions without relying on auxiliary spins or quadratization techniques, thus pre-serving computational efficiency and reducing resource overhead. By incorporating gauge transformations di-rectly into the learning process, PLANCK achieves faster training convergence and superior optimization perfor-mance compared to baseline methods. Furthermore, PLANCKâ€™s versatility extends beyond spin glass models, achieving state-of-the-art results on NP-hard problems such as random p-XORSAT, hypergraph MaxCut, and traditional MaxCut. The integration of gauge invariance in PLANCKâ€™s ar-chitecture and its application to high-order optimization problems open new avenues for physics-guided AI re-search. Our framework not only advances the theoretical understanding of disordered systems but also provides a powerful tool for real-world combinatorial optimization challenges.                               

> [1] M. MÂ´ ezard, G. Parisi, and M. A. Virasoro, Spin glass the-ory and beyond: An Introduction to the Replica Method and Its Applications , Vol. 9 (World Scientific Publishing Company, 1987). [2] P. Charbonneau, E. Marinari, G. Parisi, F. Ricci-tersenghi, G. Sicuro, F. Zamponi, and M. Mezard, Spin Glass Theory and Far Beyond: Replica Symmetry Break-ing after 40 Years (World Scientific, 2023). [3] G. Parisi and F. Ricci-Tersenghi, Philosophical Magazine
> 92 , 341 (2012). [4] D. Sherrington and S. Kirkpatrick, Physical review let-ters 35 , 1792 (1975). [5] G. GyÂ¨ orgyi, Physics Reports 342 , 263 (2001). [6] N. Ghofraniha, I. Viola, F. Di Maria, G. Barbarella, G. Gigli, L. Leuzzi, and C. Conti, Nature communica-tions 6, 6058 (2015). [7] D. Pierangeli, A. Tavani, F. Di Mei, A. J. Agranat, C. Conti, and E. DelRe, Nature communications 8, 1501 (2017). [8] E. Gardner, Nuclear Physics B 257 , 747 (1985). [9] D. J. Gross and M. Mezard, Nuclear Physics B 240 , 431 (1984). [10] C. K. Thomas and H. G. Katzgraber, Physical Review E
> 83 , 046709 (2011), arXiv:1010.2524 [cond-mat]. [11] S. F. Edwards and P. W. Anderson, Journal of Physics F: Metal Physics 5, 965 (1975). [12] C. M. Newman and D. L. Stein, arXiv:2110.11229 [cond-mat, physics:math-ph] (2021), arXiv: 2110.11229. [13] T. Kirkpatrick and P. Wolynes, Physical Review A 35 ,3072 (1987). [14] D. Larson, H. G. Katzgraber, M. Moore, and A. Young, Physical Review Bâ€”Condensed Matter and Materials Physics 81 , 064415 (2010). [15] Y. Takada, Y. Takeuchi, and K. Fujii, Physical Review Research 6, 013092 (2024). [16] C. Bybee, D. Kleyko, D. E. Nikonov, A. Khosrowshahi, B. A. Olshausen, and F. T. Sommer, Nature Communi-cations 14 , 6033 (2023). [17] S. Nikhar, S. Kannan, N. A. Aadit, S. Chowdhury, and K. Y. Camsari, Nature Communications 15 , 8977 (2024). [18] A. Bovier and B. Niederhauser, arXiv preprint cond-mat/0108235 (2001). [19] N. Veldt, A. R. Benson, and J. Kleinberg, SIAM Review
> 64 , 650 (2022).

13 

[20] B. MolnÂ´ ar, F. MolnÂ´ ar, M. Varga, Z. Toroczkai, and M. Ercsey-Ravasz, Nature communications 9, 4864 (2018). [21] A. Lucas, Frontiers in physics 2, 5 (2014). [22] I. Dinur and S. Safra, Annals of mathematics , 439 (2005). [23] P. Charbonneau, J. Kurchan, G. Parisi, P. Urbani, and F. Zamponi, Nature Communications 5, 3725 (2014), publisher: Nature Publishing Group. [24] A. Hartwig, F. Daske, and S. Kobe, Computer Physics Communications 32 , 133 (1984). [25] C. De Simone, M. Diehl, M. JÂ¨ unger, P. Mutzel, G. Reinelt, and G. Rinaldi, Journal of Statistical Physics 

80 , 487 (1995). [26] G. S. Grest, C. Soukoulis, and K. Levin, Physical review letters 56 , 1148 (1986). [27] D. J. Earl and M. W. Deem, Physical Chemistry Chem-ical Physics 7, 3910 (2005). [28] E. Khalil, H. Dai, Y. Zhang, B. Dilkina, and L. Song, Advances in neural information processing systems 30 

(2017). [29] Y.-D. Kwon, J. Choo, B. Kim, I. Yoon, Y. Gwon, and S. Min, Advances in Neural Information Processing Sys-tems 33 , 21188 (2020). [30] Z. Li, Q. Chen, and V. Koltun, Advances in neural infor-mation processing systems 31 (2018). [31] C. Fan, L. Zeng, Y. Sun, and Y.-Y. Liu, Nature machine intelligence 2, 317 (2020). [32] M. J. Schuetz, J. K. Brubaker, and H. G. Katzgraber, Nature Machine Intelligence 4, 367 (2022). [33] N. Heydaribeni, X. Zhan, R. Zhang, T. Eliassi-Rad, and F. Koushanfar, Nature Machine Intelligence 6, 664 (2024). [34] Z.-S. Shen, F. Pan, Y. Wang, Y.-D. Men, W.-B. Xu, M.-H. Yung, and P. Zhang, Nature Computational Science , 1 (2025). [35] C. Fan, M. Shen, Z. Nussinov, Z. Liu, Y. Sun, and Y.-Y. Liu, Nature communications 14 , 725 (2023). [36] H. Nishimori and M. J. Stephen, Physical Review B 27 ,5644 (1983). [37] Y. Feng, H. You, Z. Zhang, R. Ji, and Y. Gao, in Pro-ceedings of the AAAI conference on artificial intelligence ,Vol. 33 (2019) pp. 3558â€“3565. [38] N. Yadati, M. Nimishakavi, P. Yadav, V. Nitin, A. Louis, and P. Talukdar, Advances in neural information process-ing systems 32 (2019). [39] Y. Dong, W. Sawin, and Y. Bengio, arXiv preprint arXiv:2006.12278 (2020). [40] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, in International conference on machine learn-ing (PMLR, 2017) pp. 1263â€“1272. [41] F. J. Wegner, Journal of Mathematical Physics 12 , 2259 (1971). [42] H. Nishimori, Progress of Theoretical Physics 66 , 1169 (1981). [43] Y. Ozeki, Journal of Physics A: Mathematical and Gen-eral 28 , 3645 (1995). [44] W. Wang, J. Machta, and H. G. Katzgraber, Physical Review E 92 , 013303 (2015). [45] F. RomÂ´ a, S. Risau-Gusman, A. J. Ramirez-Pastor, F. Ni-eto, and E. E. Vogel, Physica A: Statistical Mechanics and its Applications 388 , 2821 (2009). [46] A. Hagberg and D. Conway, URL: https://networkx. github. io , 1 (2020). [47] S. Kirkpatrick, C. D. Gelatt Jr, and M. P. Vecchi, science 

220 , 671 (1983). [48] L. Gurobi Optimization, Gurobi optimizer reference manual (2021). [49] I. G. Rosenberg, Cahiers du Centre dâ€™etudes de Recherche Operationnelle 17 , 71 (1975). [50] A. Mandal, A. Roy, S. Upadhyay, and H. Ushijima-Mwesigwa, in Proceedings of the 17th ACM International Conference on Computing Frontiers (2020) pp. 126â€“131. [51] A. Glos, A. Krawiec, and Z. ZimborÂ´ as, npj Quantum In-formation 8, 39 (2022). [52] T. Bhattacharya, G. H. Hutchinson, G. Pedretti, X. Sheng, J. Ignowski, T. Van Vaerenbergh, R. Beau-soleil, J. P. Strachan, and D. B. Strukov, Nature Com-munications 15 , 8211 (2024). [53] G. Pedretti, F. BÂ¨ ohm, T. Bhattacharya, A. Heittmann, X. Zhang, M. Hizzani, G. Hutchinson, D. Kwon, J. Moon, E. Valiante, et al. , npj Unconventional Computing 2, 7 (2025). [54] F. Massacci and L. Marraro, Journal of Automated Rea-soning 24 , 165 (2000). [55] A. Nandi, S. Chakrabartty, and C. S. Thakur, IEEE Transactions on Communications (2024). [56] N. Sourlas, Nature 339 , 693 (1989). [57] A. Gupta, M. K. Ganai, and C. Wang, in International School on Formal Methods for the Design of Computer, Communication and Software Systems (Springer, 2006) pp. 108â€“143. [58] D. Kogan and R. Krauthgamer, in Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science (2015) pp. 367â€“376. [59] J. Brakensiek, N. Huang, A. Potechin, and U. Zwick, in Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA) (SIAM, 2021) pp. 484â€“503. [60] B. Alidaee, G. A. Kochenberger, and A. Ahmadian, In-ternational Journal of Systems Science 25 , 401 (1994). [61] C. Arora, S. Banerjee, P. Kalra, and S. Mahesh-wari, in Computer Visionâ€“ECCV 2010: 11th Euro-pean Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part III 11 

(Springer, 2010) pp. 552â€“565. [62] F. Barahona, M. GrÂ¨ otschel, M. JÂ¨ unger, and G. Reinelt, Operations Research 36 , 493 (1988). [63] F. Barahona, Journal of Physics A: Mathematical and General 15 , 3241 (1982). [64] A.-L. BarabÂ´ asi and R. Albert, science 286 , 509 (1999). [65] Y. Ye, The Gset dataset, https://web.stanford.edu/ ~yyye/yyye/Gset/ (2003), stanford. [66] C. Choi and Y. Ye, Manuscript, Department of Manage-ment Sciences, University of Iowa, Iowa City, IA 52242 ,115 (2000). [67] G. A. Kochenberger, J.-K. Hao, Z. LÂ¨ u, H. Wang, and F. Glover, Journal of Heuristics 19 , 565 (2013). [68] J. TÂ¨ onshoff, B. Kisin, J. Lindner, and M. Grohe, arXiv preprint arXiv:2208.10227 (2022). [69] R. Baxter and F. Wu, Physical Review Letters 31 , 1294 (1973). [70] R. J. Baxter, Exactly solved models in statistical mechan-ics (Elsevier, 2016). [71] E. Arashiro and J. D. de FelÂ´ Ä±cio, Physical Review E 67 ,046123 (2003).