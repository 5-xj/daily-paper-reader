Title: Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning

URL Source: https://arxiv.org/pdf/2602.16435v1

Published Time: Thu, 19 Feb 2026 02:05:58 GMT

Number of Pages: 32

Markdown Content:
# CAFE: Causally-Guided Automated Feature Engineering with Multi-Agent Reinforcement Learning 

Arun Vignesh Malarkkan *, Wangyang Ying, and Yanjie Fu School of Computing, Arizona State University 

Abstract 

Automated feature engineering (AFE) enables AI systems to autonomously construct high-utility representations from raw tabular data. However, existing AFE methods rely on statistical heuristics, yielding brittle features that fail under distribution shift. We introduce CAFE , a framework that refor-mulates AFE as a causally-guided sequential decision process, bridging causal discovery with reinforce-ment learning-driven feature construction. Phase I learns a sparse directed acyclic graph over features and the target to obtain soft causal priors , grouping features as direct, indirect, or other based on their causal influence with respect to the target. Phase II uses a cascading multi-agent deep Q-learning archi-tecture to select causal groups and transformation operators, with hierarchical reward shaping and causal group-level exploration strategies that favor causally plausible transformations while controlling feature complexity. Across 15 public benchmarks (classification with macro-F1; regression with inverse relative absolute error), CAFE achieves up to 7% improvement over strong AFE baselines, reduces episodes-to-convergence, and delivers competitive time-to-target. Under controlled covariate shifts, CAFE reduces performance drop by ∼ 4× relative to a non-causal multi-agent baseline, and produces more compact feature sets with more stable post-hoc attributions. These findings underscore that causal structure, used as a soft inductive prior rather than a rigid constraint, can substantially improve the robustness and effi-ciency of automated feature engineering. 

## 1 Introduction 

In high-stakes domains such as chemical process optimization, industrial manufacturing, drug formulation, and energy systems, AI systems must operate under evolving conditions where input distributions shift. Automated feature engineering (AFE) promises to extract useful representations from raw tabular data with minimal manual effort. Yet, most AFE methods rely on statistical correlations, yielding brittle features that fail once distributions change, limiting use in safety-critical settings. For instance, in chemical formulation, a 2°C temperature variation can alter product yield by 15 –30% , but correlation-based AFE often attributes outcomes to spurious variables rather than causal drivers. As process conditions evolve, such features lose predictive power despite stable causal mechanisms. This highlights a fundamental limitation: conventional feature engineering overlooks causality, producing models vulnerable to interventions or operational shifts. We study causally-guided AFE : automatically transforming high-dimensional process data into features that preserve causal relations with target outcomes. This is key for generalization, transparency, and com-putational tractability.  

> *Corresponding author: arun.malarkkan@asu.edu

1

> arXiv:2602.16435v1 [cs.AI] 18 Feb 2026

Yet, causally-guided AFE introduces several core technical challenges: 1) Causal discovery under noise and confounding: Real-world observational data often includes latent confounders and noise, mak-ing reliable causal structure learning difficult without strong assumptions. 2) Combinatorial search explo-sion: The space of candidate feature transformations scales exponentially with dimensionality, rendering greedy exploration computationally challenging. 3) Structured exploration under uncertainty: Navigat-ing the exponential transformation space requires principled search strategies that favor causal plausibil-ity over spurious correlations. 4) Robustness: Features should preserve utility under covariate shifts and mechanism-preserving changes. The existing methods only partially address these challenges. Most traditional AFE methods rely on greedy exploration and utility-based pruning strategies and can be brittle under shifts [11, 9]. Reinforce-ment learning (RL) methods [4, 25, 15, 16] improve exploration but suffer from sparse rewards and ignore causal structure. Recent LLM-based methods [8, 32, 1] treat feature generation as sequence modeling but lack principled causal guidance. While causal feature selection methods [24, 30, 27, 28] identify causally relevant features, they do not synthesize transformed features. This gap motivates a framework that inte-grates causality with principled feature generation. 

Our Insights: A Causally-Guided RL Perspective for Feature Engineering. AFE must move beyond statistical correlations to leverage causal mechanisms that remain stable across operational changes. To meet this end, we integrate insights from causal discovery and reinforcement learning: First, causal graphs sum-marize structural dependencies, providing invariant principled guidance for which variables to transform and how to combine them. Second, RL methods enable adaptive navigation of large, discrete transformation spaces. We operationalize these foundations through three key insights: 1) Soft Causal Inductive Bias : In-stead of enforcing rigid causal constraints, the causal structure provides soft inductive priors to guide feature transformation decisions, maintaining flexibility under causal uncertainty; 2) Causal-aware RL Exploration :Clustering features by causal roles (direct, indirect, or unrelated to the target), enabling reduced search com-plexity while preserving causal interpretability; 3) Causally Shaped Rewards : Reward functions that balance predictive utility with causal consistency and transformation diversity, ensuring intervention stability. 

Summary of Proposed Solution: Building on these insights, we introduce CAFE , a two-phase Causally-guided Automated Feature Engineering framework. In Phase I , we apply sparsity-regularized causal discov-ery to construct a causal graph over the feature space, categorizing features as direct causes, indirect causes (via multi-hop paths), or non-causal with respect to the target, providing soft inductive guidance for feature transformation policy while maintaining flexibility under causal uncertainty. In Phase II , we deploy a cas-cading multi-agent deep Q-learning architecture where three specialized agents make sequential decisions: selecting causal feature clusters, choosing transformation operators, and constructing causally-informed feature interactions. The agents use hierarchical reward shaping and an adaptive exploration strategy to favor causally plausible transformations while controlling complexity. Together, these phases unify causal reasoning with reinforcement learning to improve robustness and compactness in high-stakes applications. 

Our Contributions: 1) We formulate AFE as a causally-guided sequential decision process, mov-ing beyond correlation-based heuristics to leverage stable causal mechanisms; 2) We introduce three core principles-soft causal inductive bias, causal structure-aware exploration, and causally shaped reward func-tion, integrating causal discovery with adaptive feature construction through novel multi-agent coordination; 3) We develop CAFE , a two-phase AFE framework combining causal graph discovery with cascading multi-agent reinforcement learning that strategically constructs causally-informed feature transformations. 4) We provide extensive empirical validation of CAFE across 15 benchmark datasets, demonstrating consistent improvements up to 7% in predictive performance, reduces episodes-to-convergence, and exhibits ∼ 4×

smaller degradation under controlled covariate shifts relative to a non-causal multi-agent baseline. 22 Problem Statement 

Let D = {(xi, y i)}Ni=1 be a dataset with N instances, where each xi ∈ RK is a K-dimensional feature vector and yi ∈ Y is the corresponding target. Let F denote the original feature set, and O a predefined set of transformation operators (e.g., arithmetic, interactions; unary or binary with standard domain guards). Applying transformations from O to subsets of F induces a set of candidate features Fg. The goal of AFE is to construct an optimal feature set F∗ ⊆ F ∪ F g that maximizes a task-specific performance metric VA

for a given ML task A (e.g., classification, regression). We formulate the objective as: 

F∗ = arg max  

> S⊆F∪F g

VA(S, y ) (1) where S denotes a candidate feature subset and VA quantifies model performance on task A. The cen-tral challenge lies in efficiently navigating the exponentially large transformation space to identify F∗ that achieves: (i) high predictive utility, (ii) robustness under distribution shift, and (iii) computational tractability in exploring the exponential feature space, guided by causality. 

## 3 Methodology 

Automated feature engineering under distribution shift must move beyond correlation-based heuristics to exploit stable causal mechanisms. CAFE addresses this via a two-phase framework that couples causal dis-covery with multi-agent reinforcement learning (Fig. 1). Phase I learns a causal graph to group features by relation to the target, providing soft inductive guidance rather than rigid constraints. Phase II uses a fac-torized multi-agent policy to explore the exponential transformation space, steering by causal priors while remaining flexible under causal uncertainty. Causal guidance enters through probabilistic selection and re-ward shaping; only safety/validity checks (e.g., guarded division) are enforced as hard constraints. Together, these phases yield compact, transparent feature sets that retain predictive performance under mechanism-preserving covariate shifts. By using the learned causal structure as a soft inductive prior, we transform the AFE problem from a broad, undirected search into a more strategic and efficient exploration, justify-ing the initial computational overhead of causal discovery with faster convergence to high-quality feature transformations. 

3.1 Theoretical Foundation: Why Causal Structure Guides Robust Feature Engineering 

Core principle. In a structural causal model (SCM) with assignments Xj := fj (PA j , ϵ j ), the conditional mechanisms P (Xj | PA j ) remain invariant across environments even when marginals P (Xj ) shift [20]. This invariance suggests that features constructed from variables that are causally relevant to Y are more likely to preserve their predictive relation to Y under mechanism-preserving shifts than features relying on spurious correlations. 

Formal setting. Consider a structural causal model (SCM) with variables X = ( X1, . . . , X d) and target Y , where each variable follows Xj := fj (PA j , ϵ j ) with parents PA j and noise ϵj . A mechanism-preserving shift transforms the distribution from P (X) to P ′(X) while preserving conditional distributions 

P (Xj | PA j ) = P ′(Xj | PA j ).

Proposition 1 (Informal invariance for causally sufficient summaries) . Let S ⊆ F contain a subset S⋆ that suffices for predicting Y across environments (e.g., S⋆ ⊇ PA Y and the shift is mechanism-preserving). If a transformation ϕ preserves the information in S⋆ (e.g., is injective on S⋆ or is a sufficient statistic for S⋆), then 

P (Y | ϕ(S)) = P ′(Y | ϕ(S)) .

3Original 

Features 

> Feature Feature Feature Feature Feature

⋯

> Target Attribute
> yt f3 f2 f₁
> f4 f5 f7 f6

Causal Inductive-Prior 

Construction          

> Feature Group Agent 1 Operator Agent Feature Group Agent 2
> Causal Graph Representation Causal Feature Grouping
> yt sqrt f ₁f₁x f 2
> ... f₁/f 2fn-₁f₁
> f2f₁+f 2f₁

Optimal 

Feature Set       

> log f 2f2
> ... fn-₁f₁
> f2f₁+f 2f₁

Multi-agent Reinforcement Learning Module          

> ... ... ... ... ... ... ...
> f7 f3 f6
> Direct Causal Antecedents
> f1 f4
> Indirect/ Mediated Causal Antecedents
> f2 f5
> Non-Causal Feature Nodes Target Attribute Causal Feature Groups Operation Set
> O*tC*direct C*direct C*indirect O*tC*non-causal O*t
> Policy Network
> Downstream Task Evaluation Causal-Guided Feature Group Interaction Causal-Exploration +Mutual Information Top-K Exploration Adaptive Epsilon Decay Feature Diversity Penalty Performance Feedback Causal-Alignment Reward
> Reward Function R *

Figure 1: CAFE Framework Overview. Phase I learns a causal graph and derives soft causal priors that group features by their relation to the target: direct causes, indirect causes, and non-causal features. Phase II employs cascaded multi-agent reinforcement learning with causally shaped rewards and adaptive exploration strategies, balancing causal coherence with statistical discovery. 

If S excludes any such invariant set, no general invariance guarantee is available. Discussion. Proposition 1 restates a standard invariance intuition [20]: conditioning (directly or via suffi-cient transforms) on observed causal parents of Y stabilizes the conditional across mechanism-preserving environments. We give precise assumptions and proofs for idealized settings in Appendix A, and enumerate violations (latent confounding, selection bias, mechanism changes) that break invariance. 

Empirical regularity. When causal discovery yields a reasonable approximation to the true relations, features built from variables with stronger (estimated) causal connection to Y empirically exhibit greater sta-bility under mechanism-preserving shifts than features built from purely correlational signals. We quantify this trend via controlled robustness experiments (Sec. 4.2.3) and sensitivity analyses under graph misspeci-fication (Appendix B). This perspective motivates organizing features by approximate causal relationships: direct causes ( Xj →

Y ), indirect causes ( Xj ⇝ Y via paths of length ≥ 2), and other variables. This causal hierarchy acts as a soft inductive bias that guides but does not constrain feature construction decisions. 

3.2 Phase I: Causal Graph Discovery as a Soft Inductive Prior for Feature Grouping 

In Phase I, we construct a causal graph to organize the feature space by underlying causal relationships rather than statistical correlations. The central insight is that features with direct or indirect causal influence on the target provide more reliable guidance for feature construction, offering stable inductive signals under mechanism-preserving shifts, whereas correlations may be spurious. 

Causal Discovery Backend. Given an observational dataset D = {(xi, y i)}Ni=1 , we construct a Directed Acyclic Graph (DAG) G = ( V, E) over the variable set V = F ∪ { y}, where F represents the original features. Each directed edge (u, v ) ∈ E represents a causal relationship from variable u to variable v.We employ NOTEARS with Lasso regularization [33] as our primary causal discovery method, though the framework accommodates alternative algorithms (Appendix B). Given dataset D = {(xi, y i)}Ni=1 with features F and target y, we learn a DAG G = ( V, E) over V = F ∪ { y} by optimizing: 

min 

W ∈Rd×d

12N ∥X − XW ∥2

F + λ∥W ∥1 s.t. h(W ) = tr (eW ◦W ) − d = 0 (2) 4where W is the weighted adjacency matrix, λ > 0 controls sparsity, ◦ denotes Hadamard product, and 

h(W ) = 0 enforces acyclicity via a differentiable constraint. This formulation enables gradient-based op-timization while maintaining theoretical guarantees about DAG structure. The optimized adjacency matrix 

W defines the edge weights of the causal graph G = ( V, E), where larger weights indicate stronger causal influence. This produces a sparse DAG that captures the most significant causal pathways while filtering out weak or spurious connections. 

Assumptions and Scope. Our main analysis assumes: (1) causal sufficiency (no unobserved con-founders), (2) linear additive noise model with sparse structure, and (3) mechanism-preserving shifts. We evaluate robustness to assumptions violation and compare alternative discovery methods like PC [10], GES [6], LiNGAM [21] in Appendix B. 

Causal Role Assignment. From learned DAG G, each feature f ∈ F receives a causal role: 

M(f ) = 



direct if f → y

indirect if f ⇝ y (path length ≥ 2)other otherwise (3) This induces causal-semantic groups {C direct , Cindirect , Cother }. To balance signal preservation with com-putational tractability, we apply within-group screening: 

C∗ 

> g

=

(Cg if |C g| ≤ kg

top-kg(Cg, MI (·, y )) otherwise (4) where top-kg selects features with highest mutual information with target y. Robust fallbacks (Pearson correlation, χ2 test, stratified sampling) handle cases where MI estimation is unreliable. These causal roles serve as soft inductive priors and guide feature transformation decisions without rigid constraints. This design maintains flexibility when causal discovery is imperfect while providing principled structure that statistical methods lack. 

3.3 Phase II: Multi-Agent Reinforcement Learning with Causal Guidance 

Phase I establishes causal-semantic feature groups through theoretically grounded structure discovery. Phase II addresses the exponentially large transformation space ( O(|O| · |F| 2)) via multi-agent reinforcement learning with a Decision Factorization Rationale, where agents select entire causal feature groups rather than individual features. 

Decision Factorization Rationale. The AFE action space is combinatorially large: O(|O| × |F| 2) for operator set O and feature set F. Direct optimization suffers from sparse rewards and high variance. We factorize decisions into three stages: (1) select primary feature group, (2) choose transformation operator, (3) optionally select secondary group for binary operations. This factorization substantially reduces the effective action space each agent explores; from joint choices over (group , operator , partner ) to three smaller, structured decisions, while preserving semantic coherence through causal organization. Empirically, this reduces value-estimate variance and improves sample efficiency (learning-curve dispersion in Appendix A). We also include a stylized analysis clarifying conditions where factorization reduces estimation variance; we treat it as a modeling advantage rather than a general guarantee. 

Multi-Agent Architecture. Three specialized DQN agents operate in cascade: Primary Group Agent 

(π1): Selects from {C ∗

> direct

, C∗

> indirect

, C∗

> other

}. Operator Agent (πo): Chooses transformation from operator library O. Secondary Group Agent (π2): Selects partner group for binary operations 5State Representation Design. Agents receive context through statistical feature summaries and group-specific information: 

s(1)  

> t

= NestedStats (Ft−1) ⊕ GroupStats (C∗

> direct

, C∗

> indirect

, C∗

> other

) (5) 

s(o) 

> t

= s(1)  

> t

⊕ NestedStats (C(1)  

> t

) (6) 

s(2)  

> t

= s(o) 

> t

⊕ OneHot (ot) (7) where NestedStats (X) computes a hierarchical statistical summary: first computing standard descriptive statistics (mean, std, min, max, quartiles) across features, then applying the same descriptive analysis to this summary vector, yielding a compact yet informative representation that captures both individual feature properties and their collective distributional characteristics. This approach provides consistent, interpretable state encodings without requiring learned representations, while maintaining sufficient information density for effective RL decision-making. 

Causally-Guided Exploration. We design three principled feature interaction strategies that integrate causal theory, statistical relevance, and diversity discovery with adaptive scheduling: 

Causal-Hierarchical Feature Interaction : Prioritizes transformations respecting causal structure. For unary operations, favors Cdirect ∪ C other over indirect causes (which mediate relationships). For binary operations, emphasizes Cdirect × C indirect and Cdirect × C direct combinations. 

Top-k Mutual Information Selection : Hedges against causal discovery errors by incorporating mutual infor-mation rankings, selecting features with strongest empirical associations with target. 

Diversity Sampling : Prevents premature convergence through controlled randomization, ensuring adequate exploration of the transformation space. Strategy weights adapt based on validation performance using exponential moving averages with decay 

α (Appendix). 

Reward Function Design. We align optimization with predictive utility and causal principles: 

Rt = Rperf ,t · (1 + α · Ψcausal ,t ) + λdiv H(πt) − λcomp C(Fgt ), (8) where Rperf ,t = ∆ ValidationScore t, α ∈ (0 , 1) controls causal bonus intensity, and 

Ψcausal ,t = 1

|F gt |

X 

> f∈F gt

wM(f ) · Usage (f ), with wdirect > w indirect > w other ≥ 0.

The modulated structure (1 + α · Ψcausal ,t ) ensures causal bonuses amplify positive improvements while pro-portionally reducing penalties for causally-relevant features, preventing reward hacking while maintaining causal preferences. The algorithm is given in Appendix A 1. 

Robust Feature Generation. Transformation operators include domain validation and numerical safe-guards addressing common failure modes: 

Logarithmic operations : Apply to log( |x| + ϵ) where ϵ = 10 −8 for numerical stability. 

Division operations : Use protected division x · sign( y)max( |y|,ϵ ) with ϵ = 10 −8.

Square root : Apply p|x| for negative inputs. 

Range clipping : Bound generated features to [−10 6, 10 6] preventing overflow. For binary operations, we limit candidate pairs to min(50 , |C 1|×|C 2|) through relevance-based sampling, maintaining computational tractability while preserving quality. 

Learning and Optimization. Agents train via temporal-difference learning with experience replay and 6target networks: 

L(θ) = E(s,a,r,s ′)∼B 

"

Qθ(s, a ) −



r + γ max  

> a′

Qθ− (s′, a ′)

 2#

(9) where B is the replay buffer, θ− denotes target network parameters, and γ is the discount factor. 

Scalability and Termination. We implement intelligent pruning and adaptive termination to ensure computational efficiency while maintaining feature quality. Two-stage filtering first applies variance thresh-olding to remove near-constant features, then uses mutual information ranking to retain top-k features while ensuring minimum representation per causal group. Multiple termination criteria trigger when: (1) valida-tion improvement falls below δ for N consecutive episodes; (2) feature count exceeds computational limit; or (3) maximum episodes are reached. This design choice makes the group-choice action constant-size (|G| = 3 ), improving exploration stability. With a fixed operator library and capped within-group sampling (kg) and pairing budget B = min(50 , |C1| · | C2|), the per-step decision cost is O(1) for group selection and O(|O| + B) for operator and pairing, independent of raw dimensionality d. Overall runtime remains dominated by candidate evaluation, pruning, and downstream model training. Upon termination, we return the feature set F∗, achieving the highest validation performance across all episodes. Complete state repre-sentations, network architectures, and hyperparameters are detailed in Appendix C-D for reproducibility. 

## 4 Experimental Study 

We designed our experiments to meticulously address the following research questions: RQ1: Predic-tive Utility. Does CAFE consistently outperform recent AFE baselines in terms of predictive accuracy? 

RQ2: Ablation Study. How do causal priors, exploration strategies, and reward components individually contribute to the performance and efficiency of CAFE? RQ3: Distributional Robustness. Can CAFE gen-erate features that are resilient under covariate shifts and maintain predictive accuracy on out-of-distribution data? RQ4: Interpretability and Compactness. Are the synthesized features semantically meaningful and causally aligned with the underlying data-generating process? 

4.1 Experimental Protocol 

Data Description and Evaluation Framework. We evaluate our framework on 15 diverse public datasets collected from Kaggle, LibSVM, UCIrvine, and OpenML repositories, across two core tasks: (1) classifi-cation and (2) regression. The data statistics are given in Table 1. All experiments employ 5-fold stratified cross-validation with fixed random seeds for reproducibility. We report macro-averaged F1-scores for clas-sification and inverse relative absolute error (1-RAE) for regression, both scale-invariant metrics suitable for cross-dataset comparison. 

Implementation Details. We use XGBoost with default hyperparameters as the downstream model, fol-lowing established automated feature engineering evaluation protocols. This choice enables fair comparison across all methods while avoiding confounding effects from model-specific tuning. An extended evaluation across multiple model families (Random Forest, Neural Networks, Linear Models) is provided in the Ap-pendix. 

Baseline Methods. We compare against 10 representative methods spanning three categories: 

Statistical Baselines : Original (ORG): Raw features without transformation. Random (RDG): Random transformation application for lower-bound comparison. Exhaustive (ERG): Systematic transformation followed by statistical selection. 

Traditional Automated Feature Engineering : AFT [9]: AutoFeat - Multi-stage expansion with statistical 7Table 1: Overall performance comparison. ‘C’ for classification and ‘R’ for regression. Bold indicates the best result. Dataset Source C/R Samples Features RDG ERG LDA AFT NFS TTG GRFG ELLM-FT CAFE                                                                                                                                                                                                    

> Amazon Employee Kaggle C32769 90.744 0.740 0.920 0.943 0.935 0.806 0.946 0.946 0.947
> SVMGuide3 LibSVM C1243 21 0.703 0.747 0.683 0.829 0.831 0.766 0.831 0.836 0.846
> German Credit UCIrvine C1001 24 0.695 0.661 0.627 0.751 0.765 0.731 0.772 0.775 0.793
> Messidor features UCIrvine C1150 19 0.673 0.635 0.580 0.678 0.746 0.726 0.757 0.757 0.774
> Ionosphere UCIrvine C351 34 0.919 0.926 0.730 0.827 0.949 0.938 0.960 0.963 0.976
> Wine Quality Red UCIrvine C999 12 0.599 0.611 0.600 0.658 0.666 0.647 0.686 0.685 0.707
> Wine Quality White UCIrvine C4900 12 0.552 0.587 0.571 0.673 0.679 0.638 0.685 0.689 0.752
> Housing Boston UCIrvine R506 13 0.605 0.617 0.374 0.641 0.665 0.658 0.684 0.671 0.681 Airfoil UCIrvine R1503 50.737 0.732 0.463 0.774 0.771 0.783 0.797 0.786 0.799
> Openml 586 OpenML R1000 25 0.595 0.546 0.472 0.687 0.748 0.704 0.783 0.801 0.810
> Openml 589 OpenML R1000 25 0.638 0.560 0.331 0.672 0.711 0.682 0.753 0.781 0.783
> Openml 607 OpenML R1000 50 0.579 0.406 0.376 0.658 0.675 0.639 0.680 0.793 0.793
> Openml 616 OpenML R500 50 0.448 0.472 0.385 0.585 0.593 0.559 0.603 0.739 0.751
> Openml 618 OpenML R1000 50 0.415 0.427 0.372 0.665 0.640 0.587 0.672 0.778 0.790
> Openml 620 OpenML R1000 25 0.575 0.584 0.425 0.663 0.698 0.656 0.714 0.725 0.725

significance testing. LDA [3]: Latent factor extraction via matrix factorization. 

Modern Learning-Based Methods : NFS [5]: Sequential RL-based feature construction on individual fea-tures. TTG [12]: Graph-based RL transformation path learning. GRFG [26]: Multi-agent RL without causal guidance (key comparison). ELLM-FT [8]: LLM-guided evolutionary feature transformation. 

4.2 Experimental Results 

4.2.1 Overall Performance Analysis (RQ1) 

To answer RQ1, we evaluate CAFE against nine competitive baselines across 15 benchmark datasets span-ning classification and regression tasks with diverse modalities, sample sizes, and dimensionalities (Ta-ble 1). CAFE achieves superior performance on 13 of 15 datasets, demonstrating consistent advantages from causally-guided feature engineering. Notable improvements include high-dimensional regression tasks (OpenML 616 : 1.6%, OpenML 618 : 1.5%) and small-sample classification problems ( Ionosphere : 1.4%, 

SVMGuide3 : 1.2%), indicating effective causal structure leverage across varied domains. Established bench-marks show meaningful gains: German Credit (2.3%) and Messidor Features (1.7%), underscoring the reliability of its inductive bias under feature sparsity and class imbalance. CAFE does not uniformly dom-inate—GRFG wins on Housing Boston (0.684 vs 0.681) and CAFE ties with ELLM-FT on OpenML 620 

(0.725). These mixed results validate experimental rigor while demonstrating that causal guidance provides the greatest advantages where interpretable causal relationships exist. Results confirm our hypothesis that integrating causal discovery as soft inductive bias with multi-agent reinforcement learning enables more robust feature transformations than purely statistical approaches. 

4.2.2 Ablation Study and Component Analysis (RQ2) 

We conduct systematic ablations across four representative datasets to isolate each component’s contribu-tion (Fig. 2). Causal priors prove essential . Replacing causal discovery with correlation-based grouping (CAFE ¬G ) consistently degrades performance, with substantial drops on Wine Quality White (8.0% reduc-tion) and Housing Boston (3.0% reduction), confirming that causal structure provides superior inductive 8bias over statistical clustering. Exploration strategies show clear hierarchy . Pure causal-hierarchical exploration (CAFE Ec ) consistently outperforms mutual information-based exploration (CAFE Et ) across all datasets, with the advantage most pronounced on Wine Quality datasets where causal relationships are interpretable. Reward components contribute synergistically . Removing causal reward amplification (CAFE ¬R c ) reduces performance by 1.5-2.8% across datasets. Excluding diversity rewards (CAFE ¬R d )causes performance drops, particularly in Wine Quality White (7.2% reduction), while omitting complex-ity penalties (CAFE ¬R p ) shows mixed effects, suggesting this component primarily prevents overfitting rather than improving optimization. The full CAFE framework achieves optimal performance on all ablated datasets, demonstrating that causal priors, hierarchical exploration, and multi-component rewards work syn-ergistically, validating our architectural design. 0.641 

> 0.682
> 0.723
> 0.764
> 0.707
> 0.682
> 0.697
> 0.692 0.687
> 0.699
> 0.692
> CAFE
> CAFE ¬
> CAFE c
> CAFE t
> CAFE ¬c
> CAFE ¬d
> CAFE ¬p

(a) Wine red 0.650  

> 0.704
> 0.758
> 0.812
> 0.752
> 0.692
> 0.721
> 0.699 0.698
> 0.698
> 0.701
> CAFE
> CAFE ¬
> CAFE c
> CAFE t
> CAFE ¬c
> CAFE ¬d
> CAFE ¬p

(b) Wine white 0.614  

> 0.660
> 0.706
> 0.752
> 0.695
> 0.674
> 0.696
> 0.654
> 0.671
> 0.653
> 0.681
> CAFE
> CAFE ¬
> CAFE c
> CAFE t
> CAFE ¬c
> CAFE ¬d
> CAFE ¬p

(c) Housing Boston 0.731  

> 0.775
> 0.819
> 0.863
> 0.799
> 0.785
> 0.792
> 0.778 0.781
> 0.791
> 0.795
> CAFE
> CAFE ¬
> CAFE c
> CAFE t
> CAFE ¬c
> CAFE ¬d
> CAFE ¬p

(d) Airfoil 

Figure 2: Comparison of different CAFE variants in terms of F1 or 1-RAE. 

4.2.3 Robustness Analysis (RQ3) 

To assess generalization under distribution shifts, we systematically evaluate robustness by applying multi-plicative and additive transformations to feature distributions while preserving underlying causal relation-ships. We test across low, medium, and high shift intensities, comparing CAFE against GRFG, a statistical multi-agent RL baseline on four diverse datasets (Fig. 3). CAFE consistently shows stronger robustness, with average degradation of only 7.1% versus 28 .1% for GRFG, a roughly fourfold improvement support-ing the claim that causal-guided features withstand distributional changes better than correlation-based ones. The advantage is most evident under severe shifts: CAFE remains stable (e.g., wine red drops just 3.4% ), while the statistical baseline suffers failures up to 74% . Even when in-distribution performance is matched (e.g., wine white ), CAFE’s degradation is far smaller ( 22 .2% vs. 86 .7% ), indicating robustness derives from causal invariance rather than merely higher base accuracy. Overall, causal-guided features generalize more reliably across distributions, whereas purely statistical models lean on spurious correlations that collapse under covariate shift. This makes CAFE well-suited for dynamic, real-world deployments and addresses a key shortcoming of existing AFE methods. low medium high                   

> Distribution Shift Intensity
> openml_586
> openml_589
> wine_red
> wine_white
> Datasets
> -0.3 -0.1 6.6
> 0.4 30.3 17.6
> 3.2 0.3 3.4
> 22.2 0.1 1.2
> Causal Model - CAFE
> low medium high
> Distribution Shift Intensity
> 14.7 49.1 38.6
> 10.7 37.1 25.1
> 11.2 11.2 74.0
> 86.7 10.3 10.4
> Statistical Model - GRFG
> 0
> 20
> 40
> 60
> 80
> Performance Drop (%)

Figure 3: Robustness Study of CAFE. 94.2.4 Interpretability Analysis (RQ4) 

To evaluate interpretability, we assess explanation stability via SHAP value variance under controlled input perturbations. We add proportional Gaussian noise ( σ ∈ 0.1, 0.3, 0.5, 0.7) to test instances and, for each noise level, compute SHAP variance over 100 perturbations across multiple test points, comparing CAFE 

to GRFG using TreeSHAP (Fig. 4). CAFE shows superior explanation stability across all noise settings and datasets. On Amazon Employee , CAFE maintains low SHAP variance while the statistical baseline grows rapidly, yielding up to 90 .8% stability improvement. OpenML datasets display the same trend: CAFE stays near-constant as statistical models become volatile. The advantage increases with noise intensity: at 

σ = 0 .1, CAFE improves stability by 64–90% across datasets, rising to > 85% under high noise. This indicates that causal-guided features yield more reliable explanations by capturing stable mechanisms rather than spurious correlations whose attributions drift under perturbations. Across all experiments, CAFE re-duces explanation variance by an average of 58 .6% , supporting that causal frameworks produce interpretable representations suitable for safety-critical deployments where explanation reliability is essential. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 

> Noise Level (σ)
> 0.000
> 0.002
> 0.004
> 0.006
> 0.008
> 0.010
> 0.012
> 0.014
> Mean SHAP Variation
> (Lower is Better)
> 35.9%
> 40.2%
> 39.6%
> 39.3%
> 0.0038
> 0.0060
> 0.0064
> 0.0069
> 0.0059
> 0.0100
> 0.0107
> 0.0114
> Causal Model
> Statistical Model

(a) Wine White 0.1 0.2 0.3 0.4 0.5 0.6 0.7     

> Noise Level (σ)
> 0.000
> 0.005
> 0.010
> 0.015
> 0.020
> 0.025
> 0.030
> Mean SHAP Variation
> (Lower is Better)
> 64.2%
> 78.2%
> 86.8%
> 90.8%
> 0.0013 0.0015 0.0019 0.0021
> 0.0037
> 0.0071
> 0.0148
> 0.0235
> Causal Model
> Statistical Model

(b) Amazon Employee 0.1 0.2 0.3 0.4 0.5 0.6 0.7    

> Noise Level (σ)
> 0.000
> 0.002
> 0.004
> 0.006
> 0.008
> 0.010
> 0.012
> 0.014
> Mean SHAP Variation
> (Lower is Better)
> 15.8%
> 62.4%
> 30.0%
> 52.2%
> 0.0016 0.0020
> 0.0057 0.0058
> 0.0019
> 0.0055
> 0.0082
> 0.0121
> Causal Model
> Statistical Model

(c) Openml 586 0.1 0.2 0.3 0.4 0.5 0.6 0.7     

> Noise Level (σ)
> 0.000
> 0.002
> 0.004
> 0.006
> 0.008
> 0.010
> Mean SHAP Variation
> (Lower is Better)
> 89.7% 70.5%
> 64.0%
> 85.3%
> 0.0002
> 0.0005
> 0.0016 0.0012
> 0.0019 0.0020
> 0.0044
> 0.0086
> Causal Model
> Statistical Model

(d) Openml 616 

Figure 4: SHAP Explanation Stability. Lower variation values indicate more stable model explanations under noise. 

## 5 Related Works 

Automated Feature Engineering (AFE). Early AFE relies on hand-crafted heuristics [13, 2], which are domain-specific and hard to scale. Modern approaches include: (1) Expansion–Pruning (e.g., Deep Fea-ture Synthesis [11], Autofeat [9]), which apply operator libraries then prune by relevance; (2) Search-Based 

methods using genetic programming [23] or RL with multi-agent policies [25, 15, 29]; (3) Neural Con-trollers that adapt NAS/RNN controllers for feature generation [4]; and (4) LLM-based AFE where language models propose transformations, often with RL/evolutionary refinement [8, 32, 1]. 

Reinforcement Learning for Feature Engineering. NFS [5] treats feature creation as sequential de-cisions on single features; TTG [12] encodes transformations as graph traversals; GRFG [26] introduces multi-agent cooperation. These methods largely optimize statistical correlation, offering limited feature organization or robustness to distribution shift, and weaker interpretability for deployment. 

Causal Discovery and Feature Selection. Classical filter/wrapper/embedded schemes degrade under high dimensionality, confounding, and shifts [14]. Causal approaches include (1) Constraint-based Markov-Blanket discovery via conditional independence tests [24, 18, 31, 30] and (2) Score-based graph learning (e.g., BIC) with strong scalability [27, 7]. Deep causal discovery broadens robustness and explainabil-ity [28, 19, 22, 17]. Yet most causal feature selection is post-hoc filtering; it rarely guides automated feature generation. Our work addresses this gap by using learned causal structure as a soft inductive prior to steer multi-agent RL feature construction. 10 6 Conclusion 

We introduced CAFE, a principled causally-guided automated feature engineering framework that leverages causal discovery as inductive bias within a multi-agent reinforcement learning paradigm. CAFE consistently improves predictive accuracy, robustness, and interpretability across datasets, demonstrating up to 7% per-formance gains, four-fold robustness improvement under distribution shifts ( 7.1% vs 28 .1% degradation), and 58 .6% enhancement in explanation stability. These results validate the necessity of causal reasoning in automated feature engineering and establish a new paradigm for robust, interpretable AI systems. Our framework faces inherent challenges rooted in causal discovery limitations. The reliability of causal struc-ture learning is constrained by data quality, sample size, and validity of causal assumptions. Although our design mitigates these risks by treating causal maps as flexible guidance rather than rigid constraints, unobserved confounders may introduce bias into feature transformations. Additionally, our approach as-sumes static causal graph structure and does not accommodate temporal dynamics or feedback loops in real-world systems. Future work includes developing robust causal discovery under partial observability, adaptive mechanisms in streaming environments, and extending CAFE to temporal settings. Incorporating human-in-the-loop feedback and domain expertise may enhance interpretability, unlocking the potential of causal-aware AI in safety-critical applications. 

## Ethics Statement 

This work uses only publicly available tabular datasets under their licenses; no personally identifiable infor-mation (PII) or protected health information (PHI) is collected or created, and no human-subjects research was conducted. While CAFE’s causal priors aim to reduce reliance on spurious correlations, automated feature engineering can still propagate or create proxies for sensitive attributes; practitioners should (when legally and ethically permissible) audit subgroup performance, avoid or carefully control sensitive features, and conduct domain reviews before deployment. Our claims are predictive, not prescriptive. CAFE does not estimate treatment effects or confer intervention-level causal guarantees; “causal” refers to soft inductive priors, not immutable truths. Given potential high-stakes use, deployments should include human oversight, post-deployment monitoring, and compliance with applicable regulations and institutional governance. We document assumptions (causal sufficiency, linear discovery, mechanism-preserving shifts), limitations, and robustness diagnostics so users can assess suitability in their domains. 

## Reproducibility Statement 

To ensure reproducibility, detailed implementation instructions, hyperparameters, and evaluation protocols for CAFE are provided in the Appendix. The source code and configurations will be provided upon request. 11 References 

[1] A BHYANKAR , N., S HOJAEE , P., AND REDDY , C. K. Llm-fe: Automated feature engineering for tabular data with llms as evolutionary optimizers. arXiv preprint arXiv:2503.14434 (2025). [2] B ANERJEE , S., C HATTOPADHYAY , T., P AL , A., AND GARAIN , U. Automation of feature engineering for iot analytics. SIGBED Rev. 15 , 2 (June 2018), 24–30. [3] B LEI , D. M., N G, A. Y., AND JORDAN , M. I. Latent dirichlet allocation. J. Mach. Learn. Res. 3 ,null (Mar. 2003), 993–1022. [4] C HEN , X., L IN , Q., L UO , C., L I, X., Z HANG , H., X U, Y., D ANG , Y., S UI , K., Z HANG , X., Q IAO ,B., ET AL . Neural feature search: A neural architecture for automated feature engineering. In 2019 IEEE International Conference on Data Mining (ICDM) (2019), IEEE, pp. 71–80. [5] C HEN , X., Q IAO , B., Z HANG , W., W U, W., C HINTALAPATI , M., Z HANG , D., L IN , Q., L UO ,C., L I, X., Z HANG , H., X U, Y., D ANG , Y., S UI , K., AND ZHANG , X. Neural feature search: A neural architecture for automated feature engineering. In 2019 IEEE International Conference on Data Mining, ICDM 2019, Beijing, China, November 8-11, 2019 (2019), J. Wang, K. Shim, and X. Wu, Eds., IEEE, pp. 71–80. [6] C HICKERING , D. M. Optimal structure identification with greedy search. Journal of machine learning research 3 , Nov (2002), 507–554. [7] G AO , T., AND JI, Q. Efficient score-based markov blanket discovery. International Journal of Ap-proximate Reasoning 80 (2017), 277–293. [8] G ONG , N., R EDDY , C. K., Y ING , W., C HEN , H., AND FU, Y. Evolutionary large language model for automated feature transformation. In Proceedings of the AAAI conference on artificial intelligence 

(2025), vol. 39, pp. 16844–16852. [9] H ORN , F., P ACK , R., AND RIEGER , M. The autofeat python library for automated feature engineering and selection. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (2019), Springer, pp. 111–120. [10] K ALISCH , M., AND B ¨ UHLMAN , P. Estimating high-dimensional directed acyclic graphs with the pc-algorithm. Journal of Machine Learning Research 8 , 3 (2007). [11] K ANTER , J. M., AND VEERAMACHANENI , K. Deep feature synthesis: Towards automating data science endeavors. In 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA) (2015), pp. 1–10. [12] K HURANA , U., S AMULOWITZ , H., AND TURAGA , D. S. Feature engineering for predictive modeling using reinforcement learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018 (2018), S. A. McIlraith and K. Q. Weinberger, Eds., AAAI Press, pp. 3407–3414. 12 [13] K HURANA , U., T URAGA , D., S AMULOWITZ , H., AND PARTHASRATHY , S. Cognito: Automated feature engineering for supervised learning. In 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW) (2016), pp. 1304–1307. [14] L AMSAF , A., C ARRILHO , R., N EVES , J. C., AND PROENC ¸ A, H. Causality, machine learning, and feature selection: a survey. Sensors 25 , 8 (2025), 2373. [15] L IU , K., F U, Y., W U, L., L I, X., A GGARWAL , C., AND XIONG , H. Automated feature selection: A reinforcement learning perspective. IEEE Transactions on Knowledge and Data Engineering 35 , 3 (2021), 2272–2284. [16] M ALARKKAN , A. V., B AI , H., K AUSHIK , A., AND FU, Y. Delta: Variational disentangled learning for privacy-preserving data reprogramming. arXiv preprint arXiv:2509.00693 (2025). [17] M ALARKKAN , A. V., W ANG , D., B AI , H., AND FU, Y. Incremental causal graph learning for online cyberattack detection in cyber-physical infrastructures. IEEE Transactions on Big Data (2025), 1–12. [18] M ARGARITIS , D., AND THRUN , S. Bayesian network induction via local neighborhoods. Advances in neural information processing systems 12 (1999). [19] M ORAFFAH , R., S HETH , P., V ISHNUBHATLA , S., AND LIU , H. Causal feature selection for respon-sible machine learning. arXiv preprint arXiv:2402.02696 (2024). [20] P ETERS , J., B ¨ UHLMANN , P., AND MEINSHAUSEN , N. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society Series B: Statistical Methodology 78 , 5 (2016), 947–1012. [21] S HIMIZU , S. Lingam: Non-gaussian methods for estimating causal structures. Behaviormetrika 41 , 1 (2014), 65–98. [22] S UN , Y.-N., Q IN , W., H U, J.-H., X U, H.-W., AND SUN , P. Z. A causal model-inspired automatic feature-selection method for developing data-driven soft sensors in complex industrial processes. En-gineering 22 (2023), 82–93. [23] T RAN , B., X UE , B., AND ZHANG , M. Genetic programming for feature construction and selection in classification on high-dimensional data. Memetic Computing 8 , 1 (2016), 3–15. [24] T SAMARDINOS , I., A LIFERIS , C. F., S TATNIKOV , A. R., AND STATNIKOV , E. Algorithms for large scale markov blanket discovery. In FLAIRS (2003), vol. 2, pp. 376–81. [25] W ANG , D., F U, Y., L IU , K., L I, X., AND SOLIHIN , Y. Group-wise reinforcement feature generation for optimal and explainable representation space reconstruction. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (2022), pp. 1826–1834. [26] W ANG , D., F U, Y., L IU , K., L I, X., AND SOLIHIN , Y. Group-wise reinforcement feature generation for optimal and explainable representation space reconstruction, 2022. [27] W ANG , H., L I, J., AND ZHU , G. A data feature extraction method based on the notears causal inference algorithm. Applied Sciences 13 , 14 (2023), 8438. [28] Y AO , L., AND GE, Z. Causal variable selection for industrial process quality prediction via attention-based gru network. Engineering Applications of Artificial Intelligence 118 (2023), 105658. 13 [29] Y ING , W., W EI , C., G ONG , N., W ANG , X., B AI , H., M ALARKKAN , A. V., D ONG , S., W ANG , D., ZHANG , D., AND FU, Y. A survey on data-centric ai: Tabular learning from reinforcement learning and generative ai perspective. arXiv preprint arXiv:2502.08828 (2025). [30] Y U, K., G UO , X., L IU , L., L I, J., W ANG , H., L ING , Z., AND WU, X. Causality-based feature selection: Methods and evaluations. ACM Computing Surveys (CSUR) 53 , 5 (2020), 1–36. [31] Y U, K., L IU , L., AND LI, J. A unified view of causal and non-causal feature selection. ACM Transactions on Knowledge Discovery from Data (TKDD) 15 , 4 (2021), 1–46. [32] Z HANG , X., Z HANG , J., R EKABDAR , B., Z HOU , Y., W ANG , P., AND LIU , K. Dynamic and adaptive feature generation with llm. arXiv preprint arXiv:2406.03505 (2024). [33] Z HENG , X., A RAGAM , B., R AVIKUMAR , P., AND XING , E. P. Dags with no tears: continuous optimization for structure learning. In Proceedings of the 32nd International Conference on Neu-ral Information Processing Systems (Red Hook, NY, USA, 2018), NIPS’18, Curran Associates Inc., p. 9492–9503. 14 Appendix A Algorithm, Theoretical Analysis and Formal Foundations 

A.1 Causal Assumptions and Scope 

Assumption 1 (Structural Causal Model) . The data generating process follows a Structural Causal Model (SCM) with variables X = ( X1, . . . , X d) and target Y , where each variable follows: 

Xj := fj (PA j , ε j ), Y := fY (PA Y , ε Y )

with parents PA j ⊂ X \ { Xj }, noise terms εj , and functions fj .

Assumption 2 (Causal Sufficiency) . All common causes of the observed variables are included in the vari-able set, i.e., no unobserved confounders exist between any pair of variables. 

Assumption 3 (Mechanism Stability) . Under distribution shift from P (X, Y ) to P ′(X, Y ), the conditional distributions remain invariant: P (Xj |PA j ) = P ′(Xj |PA j ) for all j, and P (Y |PA Y ) = P ′(Y |PA Y ).

Definition 1 (Causal Ancestry) . For variables Xi, X j in DAG G, we define: • Xi is a direct cause of Xj if (Xi, X j ) ∈ E(G)

• Xi is an indirect cause of Xj if there exists a directed path Xi ⇝ Xj in G of length ≥ 2

• Xi is causally irrelevant to Xj if no directed path exists from Xi to Xj

A.2 Theoretical Justification for Causal Feature Engineering 

Proposition 2 (Causal Feature Invariance) . Under Assumptions 1-3, let ϕ : R|S| → R be a measurable transformation applied to feature set S ⊆ X. If S ∩ An( Y )̸ = ∅ where An( Y ) denotes the ancestors of Y

in the true DAG, then: 

E[Y |ϕ(S)] is more stable under mechanism-preserving shifts than E[Y |ϕ(S′)] 

where S′ ∩ An( Y ) = ∅.Proof Sketch. By the causal Markov property and invariance of conditional mechanisms (Assumption 3), transformations of causal ancestors preserve the structural relationship to Y . Non-causal variables may exhibit spurious correlations that break under distributional changes, while causal relationships remain stable by construction. 

Corollary 1 (Causal Hierarchy for Feature Engineering) . The stability ranking for feature transformations follows: Direct causes > Indirect causes > Non-causal variables, providing theoretical justification for our causal grouping strategy. 

We emphasize that the theoretical statements in this appendix provide motivational grounding for the use of causal structure in feature engineering; CAFE’s empirical effectiveness relies on soft causal guidance and reward-driven refinement rather than on strict satisfaction of causal discovery assumptions. 15 Algorithm 1 CAFE: Causally-Guided Automated Feature Engineering 

Require: Dataset D, operator library O, hyperparameters (Appendix) 

Ensure: Optimal feature set F∗ 

> 1:

Phase I: Learn causal DAG G via NOTEARS-Lasso (Eq. 2)  

> 2:

Form causal groups C∗

> direct

, C∗

> indirect

, C∗ 

> other

(Eq. 4)  

> 3:

Phase II: Initialize DQN agents π1, π o, π 2 with experience replay buffers  

> 4:

Fbest ← F original , best score ← baseline performance  

> 5:

for episode = 1 to Emax do  

> 6:

Fcurrent ← F original  

> 7:

for step = 1 to Smax do  

> 8:

C1 ∼ π1(s(1)  

> t

), o ∼ πo(s(o) 

> t

), C2 ∼ π2(s(2)  

> t

) # Agent cascade  

> 9:

Apply causal-guided sampling within selected groups  

> 10:

Fg ← GenerateFeatures (C1, o, C2) with safety guards  

> 11:

Fcurrent ← TwoStagePruning (Fcurrent ∪ F g) 

> 12:

Rt ← ComputeReward (Fcurrent ) via Eq. 8  

> 13:

Update agents π1, π o, π 2 using TD-learning and replay buffers  

> 14:

Adapt exploration strategy weights based on performance trend  

> 15:

if validation score > best score then  

> 16:

Fbest ← F current , best score ← validation score  

> 17:

end if  

> 18:

if early stopping criteria met then  

> 19:

break  

> 20:

end if  

> 21:

end for  

> 22:

end for  

> 23:

return F∗ = Fbest 

A.3 Multi-Agent Decision Factorization Analysis 

Proposition 3 (Multi-Agent Factorization Benefits) . Consider the joint action space A = A1 × A o × A 2

with |A 1| = 3 , |A o| = O, |A 2| = 3 . While both the joint and factorized approaches have the same action space complexity O(3 O) = O(O), the factorized approach offers computational advantages through: 1. Parameter efficiency : Learning three Q-functions with parameter counts O(3 s), O(Os ), and O(3 s)

respectively (where s is state dimension) versus one joint Q-function with O(3 Os ) parameters. 2. Sample efficiency : Each agent learns a simpler state-action mapping, potentially requiring fewer samples to converge. 3. Estimation variance : Under independence assumptions, the factorized estimator can achieve lower variance: Var [ ˆQ1 + ˆQo + ˆQ2] ≤ Var [ ˆQ1] + Var [ ˆQo] + Var [ ˆQ2].

16 B Causal Discovery Implementation Details 

B.1 NOTEARS-Lasso Formulation 

The NOTEARS-Lasso optimization problem is: 

min  

> W∈Rd×d

12n ∥X − XW ∥2 

> F

+ λ∥W ∥1 (10) subject to h(W ) = tr (eW ◦W ) − d = 0 (11) where X ∈ Rn×d is the data matrix, W is the weighted adjacency matrix, λ > 0 controls sparsity, and 

h(W ) = 0 enforces acyclicity via the matrix exponential constraint. 

Algorithm 2 NOTEARS-Lasso for CAFE 

Require: Dataset X ∈ Rn×d, regularization path Λ = {λ1, . . . , λ K }

Ensure: Adjacency matrix W ∗, causal groups {Cdirect , C indirect , C other } 

> 1:

for λ ∈ Λ do  

> 2:

Initialize W (0) ← 0, ρ ← 1.0, α ← 0.0 

> 3:

repeat  

> 4:

Solve: W (k+1) ← arg min W Lρ(W, α (k)) via L-BFGS  

> 5:

Update: α(k+1) ← α(k) + ρh (W (k+1) ) 

> 6:

Update: ρ ← min(1 .25 ρ, 10 12 ) 

> 7:

until |h(W (k+1) )| < 10 −8 and ∥∇ L∥2 < 10 −6 

> 8:

Store solution: (Wλ, BIC λ) 

> 9:

end for  

> 10:

Select: W ∗ ← Wλ∗ where λ∗ = arg min λ BIC λ 

> 11:

Construct DAG: G = ( V, E ) with E = {(i, j ) : |W ∗ 

> ij

| > τ }, τ = 0 .1 

> 12:

Assign causal roles via transitive closure of G 

> 13:

return W ∗, causal groups 

B.2 Alternative Causal Discovery Methods 

To assess robustness to the causal discovery backend, we evaluate four algorithms: Table 2: Causal discovery algorithm comparison on synthetic data (10 datasets, d = 20 , n = 1000 ). 

Algorithm SHD ↓ F1-Score ↑ Runtime (s) CAFE Performance 

PC Algorithm 8.3 ± 2.1 0.64 ± 0.08 12 .4 ± 3.2 0.742 ± 0.034 

GES 6.7 ± 1.8 0.71 ± 0.07 45 .7 ± 8.9 0.756 ± 0.028 

LiNGAM 7.2 ± 2.0 0.68 ± 0.09 8.9 ± 2.1 0.748 ± 0.031 

NOTEARS-Lasso 4.9 ± 1.3 0.78 ± 0.06 23 .1 ± 5.4 0.773 ± 0.025 

Key Advantages of NOTEARS-Lasso: 

1. Continuous Optimization Framework: Unlike constraint-based methods (PC, FCI) that rely on conditional independence tests, NOTEARS formulates causal discovery as a smooth optimization problem, enabling gradient-based solvers and better scalability. 17 2. Explicit Sparsity Control: The Lasso regularization provides fine-grained control over graph spar-sity through the beta parameter, crucial for feature engineering where we need interpretable causal relationships without spurious edges. 3. Acyclicity as Smooth Constraint: The innovative acyclicity constraint h(W ) = tr (eW ◦W ) − d = 0 

is differentiable, avoiding the combinatorial complexity of traditional DAG constraints. 4. Computational Efficiency: Scales to hundreds of variables with O(p2) complexity per iteration, making it suitable for high-dimensional feature spaces in automated feature engineering. 5. Robustness to Noise: Lasso regularization provides natural robustness to measurement noise and irrelevant variables, common in real-world datasets. 

Limitations of Alternative Methods: 

• PC Algorithm: Exponential complexity in maximum degree, poor performance on continuous vari-ables, requires discretization that loses information. • GES (Greedy Equivalence Search): Limited sparsity control, can get trapped in local optima, less effective for dense feature spaces. • LiNGAM: Assumes linear non-Gaussian additive noise model, restrictive for diverse real-world data distributions. • Constraint-based methods: Rely on statistical tests that can be unreliable with finite samples and high dimensions. 

Empirical Validation: NOTEARS-Lasso has demonstrated superior performance on benchmark datasets and real-world applications, making it the preferred choice for production causal discovery systems. 

B.3 Non-Linear Causal Discovery with NOTEARS-MLP 

To evaluate CAFE under non-linear mechanisms, we conducted experiments replacing NOTEARS-Lasso with NOTEARS-MLP. 

Synthetic Non-Linear SCMs: We generated 3 datasets ( n = 1000 , d = 20 ) with known non-linear structures: • Quadratic: Y = X21 + X2X3 + ε

• Exponential: Y = exp(0 .5X1) + log( |X2| + 1) + ε

• Mixed: Y = X21 + sin( πX 2) + X3X4 + ε

Results: Table 3 shows NOTEARS-MLP improves accuracy by 3.4–5.1% on non-linear SCMs. 

Trade-offs: NOTEARS-MLP requires more samples ( n ≥ 500 ) and 3× longer Phase I runtime (68 sec vs. 23 sec for d = 20 ). However, improved causal structure leads to faster RL convergence, partially offsetting the discovery cost. 18 Table 3: CAFE with linear vs. non-linear causal discovery on synthetic SCMs.                 

> Dataset CAFE-Linear CAFE-MLP MLP Gain Sample Efficiency Quadratic 0.768 0.794 +2.6% -7% episodes Exponential 0.724 0.761 +3.7% -12% episodes Mixed 0.747 0.779 +3.2% -11% episodes

B.4 Robustness to Causal Discovery Errors 

CAFE is explicitly designed to use causal structure as a soft inductive prior rather than as a hard constraint. In Phase II, causal groups guide exploration probabilistically, but all feature groups and operators remain accessible to the RL agents, and empirical validation performance ultimately governs feature selection. To assess robustness under imperfect causal discovery, we analyze downstream performance as a func-tion of graph quality. Across synthetic and real-data experiments, we observe graceful degradation: as the structural Hamming distance (SHD) between the learned and oracle DAG increases, CAFE’s perfor-mance decreases smoothly rather than catastrophically. Even under severely misspecified graphs, CAFE consistently outperforms correlation-based and random-group baselines, indicating that approximate causal ordering is sufficient to provide useful inductive bias. This behavior arises from three design choices: (i) causal groups act as exploration hints rather than filters; (ii) reward-driven learning can override causal suggestions when they conflict with validation per-formance; and (iii) adaptive exploration increases MI-based and random sampling when causal guidance becomes unreliable (Appendix D). These results confirm that CAFE does not require correct causal identification to be effective, and that causal discovery errors do not propagate unchecked into feature construction. Table 4: CAFE performance degradation vs. graph quality (SHD quartiles). 

Graph Quality SHD Range CAFE Performance vs. Oracle vs. Random 

Excellent [0 .0, 2.0] 0.798 ± 0.021 −0.8% +12 .4% 

Good (2 .0, 5.0] 0.773 ± 0.025 −3.9% +8 .8% 

Fair (5 .0, 8.0] 0.751 ± 0.031 −6.7% +5 .7% 

Poor (8 .0, ∞) 0.728 ± 0.038 −9.5% +2 .4% 

The soft causal prior approach enables graceful degradation under imperfect causal discovery, maintain-ing benefits even with moderate graph errors. 

B.5 Ensemble Causal Discovery for Robustness 

To assess robustness to graph misspecification, we implemented an ensemble DAG approach combining bootstrap NOTEARS with GES. 

Methodology: 

1. Bootstrap NOTEARS: Run NOTEARS-Lasso 10 times with bootstrap resampling 2. Edge Probability: Compute eij = fraction of DAGs containing i → j

3. Soft Grouping: Assign features to groups based on highest aggregate edge probability 4. Uncertainty Weighting: Scale causal bonus α by edge confidence 19 Results: Table 5 shows ensemble approach improves mean accuracy by 0.2–1.0%. Table 5: Ensemble DAG results. 

Dataset CAFE (single) CAFE (ensemble) Improvement Wine Red 0.707 0.714 +0.7% German Credit 0.793 0.804 +1.0% OpenML 586 0.810 0.819 +0.9% Ionosphere 0.976 0.978 +0.2% 

Insights: Ensemble reduces variance by aggregating multiple graph estimates, providing more reliable causal priors. The modest accuracy gains suggest that even approximate single-graph estimates provide substantial value when used as soft priors rather than hard constraints. 

## C Multi-Agent Architecture Specifications 

This appendix provides full RL implementation details corresponding to the summarized definitions in Sec-tion 3.3, enabling exact reproduction of all experiments. 

C.1 State Representation Details 

Dataset-Level Features: sdata ∈ R8

• Sample size: log( n), dimensionality: log( d)

• Target type: binary encoding (classification=1, regression=0) • Missing rate: # missing values 

> n×d

• Class imbalance: min c p(y = c)/ max c p(y = c) for classification • Feature correlation: mean pairwise |ρij |

• Target correlation: mean |ρ(xi, y )|

• Noise estimate: median R2 from univariate regressions 

Performance Context: sperf ∈ R6

• Current train/validation scores (normalized to [0 , 1] )• Score improvements over last 3 steps: ∆1, ∆2, ∆3

• Episode progress: t/T max 

• Best score achieved so far 

Feature Context: sfeat ∈ R7+3 

• Original feature count: |F 0|

20 • Generated feature count: |F gt |

• Selected feature count: |F st |

• Feature density: |F st ||F gt |

• Mean feature importance (from XGBoost) • Feature variance statistics: mean, std of Var (f )

• Group sizes: |Cdirect |, |Cindirect |, |Cother |

Auxiliary context saux ∈ R3 includes: • Normalized episode index • Rolling validation variance • Binary indicator for binary vs multi-class classification 

Agent-Specific States: 

s(1)  

> t

= [ sdata , s perf , s feat , s aux ] ∈ R24 (12) 

s(o) 

> t

= [ s(1)  

> t

, OneHot (gselected )] ∈ R27 (13) 

s(2)  

> t

= [ s(o) 

> t

, OneHot (oselected ), arity ] ∈ R43 (14) 

C.2 Network Architectures 

Each DQN agent uses the following architecture: • Input layer: state dimension (varies by agent) • Hidden layers: [512, 256, 128] with ReLU activation • Dropout: 0.1 after each hidden layer during training • Output layer: linear, dimension = action space size • Weight initialization: Xavier uniform • Batch normalization: applied to first hidden layer 

Training Configuration: 

• Optimizer: Adam with α = 10 −4 (default), β1 = 0 .9, β2 = 0 .999 

• Experience replay: buffer size = 50,000, batch size = 256 • Target network: hard update every 1000 steps (soft update variant yields similar performance) • Exploration: ε-greedy with εstart = 0 .95 , εend = 0 .1, decay over 1000 steps • Loss function: Huber loss with δ = 1 .0

21 D Reward Function and Exploration Strategy 

D.1 Complete Reward Specification 

The reward function combines four components: 

Rt = Rperf (t) ·



1 + α ˜Ψcausal (t)



+ λdiv H(πt) − λcomp C(Fgt ) (15) 

Performance Reward: 

Rperf (t) = 



100 · Score t−Score t−1 

> max( Score baseline ,10 −6)

if improvement 

−10 · Score t−Score t−1 

> Score baseline

if degradation 

Causal Amplification: 

˜Ψcausal (t) = X 

> f∈F gt

wM(f ) · Importance (f )

P  

> f′∈F gt

Importance (f ′)

with weights: wdirect = 1 .0, windirect = 0 .6, wother = 0 .2, and α = 0 .5.

Exploration Diversity: 

H(πt) = − X

> a∈A

πt(a) log πt(a)

estimated from action frequencies over a sliding window of size 20. 

Complexity Penalty: 

C(Fgt ) = α1|F gt | + α2

X 

> f∈F gt

OpDepth (f )

with α1 = 0 .001 and α2 = 0 .01 .

Hyperparameters: λdiv = 0 .05 , λcomp = 0 .001 

D.2 Adaptive Exploration Strategy 

The exploration strategy combines three approaches with adaptive weights: 

Causal-Hierarchical Strategy: 

• Unary operations: Sample from Cdirect ∪ Cother with probabilities [0 .7, 0.3] 

• Binary operations: Prioritize (Cdirect , C indirect ) and (Cdirect , C direct ) pairs • Operator selection: Weighted by estimated causal relevance 

D.3 Baseline Method Configurations 

Statistical Baselines: 

• Original (ORG): Raw features, no transformation • Random (RDG): Random transformation selection with fixed budget • Exhaustive (ERG): Systematic enumeration with statistical pruning ( p < 0.05 )22 Algorithm 3 Adaptive Exploration Strategy 

Require: Current episode e ≥ 6, performance history {Score i}e−1

> i=1

Ensure: Selected exploration strategy  

> 1:

if e ≤ 5 then  

> 2:

Use default weights: wcausal = 0 .5, wMI = 0 .3, wrandom = 0 .2 

> 3:

else  

> 4:

Compute trend: trend = 15

P5

> j=1

(Score e−j − Score e−j−1) 

> 5:

if trend > 0.01 then  

> 6:

/* Performance improving */  

> 7:

wcausal = 0 .7, wMI = 0 .2, wrandom = 0 .1 

> 8:

else  

> 9:

if trend < −0.01 then  

> 10:

/* Performance degrading */  

> 11:

wcausal = 0 .4, wMI = 0 .3, wrandom = 0 .3 

> 12:

else  

> 13:

/* Performance stagnating */  

> 14:

wcausal = 0 .5, wMI = 0 .3, wrandom = 0 .2 

> 15:

end if  

> 16:

end if  

> 17:

end if  

> 18:

Sample s ∼ Categorical ([ wcausal , w MI , w random ])  

> 19:

Apply exploration strategy s

Traditional AFE: 

• AutoFeat: Multi-stage expansion with significance testing, max depth=2 • Deep Feature Synthesis: Primitive-based construction, max depth=3 • LDA: Latent factor extraction, 10 components 

Modern RL-based: 

• NFS: Sequential DQN on individual features, replay buffer=5000 • TTG: Graph-based RL, ε-greedy exploration • GRFG: Multi-agent RL based Feature Generation 

LLM-based: 

• ELLM-FT: GPT-4 based feature generation with evolutionary refinement 

D.4 Statistical Testing Protocol 

Test Selection: Wilcoxon signed-rank test (paired, non-parametric). 23 Table 6: Statistical significance analysis with multiple testing correction. 

Comparison Wilcoxon p Effect Size ( r) Interpretation 

CAFE vs GRFG < 0.001 0.68 Large effect CAFE vs ELLM-FT 0.003 0.51 Medium effect CAFE vs AutoFeat < 0.001 0.62 Large effect CAFE vs NFS < 0.001 0.59 Large effect CAFE vs Original < 0.001 0.84 Large effect 

## E Robustness Analysis 

E.1 Distribution Shift Simulation Protocol 

Covariate Shift Generation: For intensity level γ ∈ { 0.1, 0.3, 0.5} (low, medium, high): 

X′ 

> ij

= Xij · (1 + γϵ ij ) (multiplicative) (16) 

X′ 

> ij

= Xij + γσ j ϵij (additive) (17) where ϵij ∼ N (0 , 1) and σj = std (X·j ).

Mechanism Preservation: We verify that P (Y |PA Y ) remains approximately unchanged by computing: KL-divergence (P (Y |PA Y )∥P ′(Y |PA Y )) < 0.1

Table 7: Robustness under mechanism-preserving distribution shifts (performance degradation %). Lower magnitude indicates stronger robustness. Results are averaged across datasets and seeds. These shifts modify feature or label distributions while preserving underlying causal mechanisms. 

Shift Type Intensity CAFE GRFG ELLM-FT AutoFeat 

Covariate Low −4.1 ± 1.8 −12 .3 ± 3.2 −11 .7 ± 2.9 −15 .2 ± 4.1

Medium −12 .8 ± 3.4 −31 .8 ± 5.7 −29 .4 ± 5.1 −34 .7 ± 6.8

High −22 .1 ± 4.9 −58 .7 ± 8.2 −54 .3 ± 7.6 −61 .2 ± 9.1

Label Low −7.3 ± 2.1 −15 .2 ± 3.8 −14 .8 ± 3.5 −18 .4 ± 4.7

Medium −18 .2 ± 4.2 −34 .1 ± 6.3 −32 .6 ± 5.9 −37 .8 ± 7.2

High −31 .4 ± 6.1 −52 .8 ± 7.9 −49 .7 ± 7.3 −55 .3 ± 8.6

Scope and limitations. The covariate and label shifts in Table 7 are mechanism-preserving by construction, following standard causal robustness evaluations. While these tests isolate the effect of spurious correlations, they do not capture all real-world deployment shifts (e.g., feature deletion or temporal population drift). See Appendix E.2 for a decomposition of in-distribution vs. out-of-distribution performance, showing that CAFE’s robustness is not achieved at the expense of in-distribution accuracy. 

E.2 In-distribution vs. out-of-distribution performance decomposition 

To clarify whether robustness gains arise from sacrificing in-distribution (ID) performance, we explicitly decompose ID and OOD performance across methods using the same evaluation protocol as Section 4.2.3. Table 8 reports average ID performance, OOD performance under covariate shift, and relative degradation. 24 While GRFG and ELLM-FT exhibit substantial drops (28-32%), CAFE maintains strong ID performance while degrading by only 7.1%, indicating that its robustness is not achieved by underfitting or reduced expressivity. Table 8: In-distribution (ID) vs. OOD performance and relative degradation (mean ± std). Method ID Performance OOD Performance Drop (%) GRFG 0.734 ± 0.038 0.528 ± 0.067 −28 .1

ELLM-FT 0.741 ± 0.035 0.503 ± 0.072 −32 .1

CAFE (ours) 0.773 ± 0.032 0.718 ± 0.044 −7.1

These findings indicate that causal guidance in CAFE improves both in-distribution generalization and robustness, rather than inducing a trade-off between the two. 

## F Interpretability Analysis 

F.1 SHAP Stability Methodology 

For each test instance xi, we generate perturbed versions {x(j) 

> i

}100  

> j=1

by adding Gaussian noise: 

x(j) 

> i

= xi + σϵ (j), ϵ(j) ∼ N (0 , I )

SHAP values are computed using TreeSHAP for each perturbed instance, and stability is measured as: SHAP-Stability σ = 1

N

> N

X

> i=1

1100 

> 100

X

> j=1

∥ϕi − ϕ(j) 

> i

∥22

where ϕi are the SHAP values for the original instance and ϕ(j) 

> i

for the perturbed versions. Table 9: Comprehensive interpretability metrics comparison. 

Method SHAP Stab. ↓ Feat. Stab. ↑ Causal Cons. ↑ Compactness ↑ Overall 

GRFG 0.089 ± 0.012 0.42 ± 0.08 0.31 ± 0.06 0.23 ± 0.05 2.8/5

ELLM-FT 0.078 ± 0.011 0.38 ± 0.07 0.28 ± 0.05 0.31 ± 0.06 3.0/5

AutoFeat 0.094 ± 0.013 0.51 ± 0.09 0.25 ± 0.04 0.19 ± 0.04 2.7/5

CAFE 0.043 ± 0.008 0.68 ± 0.11 0.73 ± 0.09 0.47 ± 0.08 4.2/5

Metric Definitions: 

• Feature Stability: Jaccard similarity of selected features across CV folds • Causal Consistency: Weighted usage of causally relevant features • Compactness: Ratio of selected to generated features 25 G Computational Analysis and Scalability 

G.1 Detailed Complexity Analysis 

Phase I Complexity: 

TPhase I = TNOTEARS + Tgrouping + Tscreening (18) 

= O(T d 3) + O(d2) + O(nd log d) (19) 

= O(T d 3) (20) 

Phase II Complexity per Episode: 

Tepisode = S · (Tdecision + Tgeneration + Tevaluation ) (21) 

= S · (O(1) + O(k2max |O| ) + O(n|F current |)) (22) 

= O(S · n|F current |) (23) where S is steps per episode and |F current | is capped at 5d.

G.2 Memory Optimization Strategies 

• Intelligent Pruning: Remove features with variance < 10 −8 immediately • Batch Processing: Process feature generation in batches of 100 • Memory Monitoring: Stop generation if memory usage > 80% of available • Feature Capping: Maximum 800 features per episode with priority-based retention 

## H The Time-Convergence Paradox in Causal Feature Engineering 

In this section, we address what appears to be a paradoxical aspect of our experimental results: despite CAFE’s higher computational cost per episode, it often requires significantly fewer episodes to reach optimal performance. This trade-off represents a fundamental property of causal-aware feature engineering that warrants deeper examination. 

H.1 Empirical Evidence 

Our experiments across 15 benchmark datasets reveal a consistent pattern: • Higher Per-Episode Computational Cost: CAFE episodes require approximately 30-50% more computation time than standard GRFG episodes, depending on the dataset complexity and causal graph structure. • Fewer Episodes to Convergence: CAFE consistently reaches its optimal performance in 40-70% fewer episodes than GRFG across all tested datasets. • Total Time to Optimal Performance: When measuring the total time required to reach optimal performance (episodes × time-per-episode), CAFE is often more efficient overall, particularly for complex datasets with intricate causal relationships. 26 Figure 5: Convergence Comparison: CAFE vs GRFG. • Feature Efficiency: At the point of performance convergence, CAFE typically generates more fo-cused feature sets with stronger predictive power and better interpretability. 

H.2 Explanation of the Training Time – Feature Convergence Trade-off 

The apparent paradox can be explained through the lens of exploration-exploitation balance and computa-tional complexity theory: 1. Informed Search vs. Random Exploration: CAFE performs a more computationally intensive but informed search of the feature space, targeting areas likely to yield causal relationships. This contrasts with GRFG’s broader but less directed exploration, which follows a uniform sampling strategy across the feature space. 2. Early Pruning of Unproductive Paths: By incorporating causal knowledge, CAFE avoids many unproductive paths in the feature generation process. The causal-hierarchical exploration strategy eliminates approximately 60-80% of potentially spurious feature combinations, resulting in faster convergence despite higher per-step computational costs. 3. Front-loaded Computation: CAFE’s computational investment is front-loaded—spending more time per episode but requiring fewer episodes overall—versus GRFG’s approach of many faster but less efficient iterations. This amortizes the causal analysis cost across fewer, more productive episodes. 4. Hierarchical Reward Shaping: The causally-shaped reward function provides stronger gradient signals for learning, enabling faster policy convergence in the multi-agent reinforcement learning framework. 

H.3 Mathematical Analysis 

Let T CAFE episode and T GRFG episode denote the per-episode computation time, and N CAFE episodes and N GRFG episodes denote the number of episodes to convergence. The total time to convergence is: 27 T CAFE total = T CAFE episode × N CAFE episodes (24) 

T GRFG total = T GRFG episode × N GRFG episodes (25) Our empirical results show that despite T CAFE episode ≈ 1.3 − 3.0 × T GRFG episode , we observe N CAFE episodes ≈ 0.3 −

0.6 × N GRFG episodes , leading to: 

T CAFE total 

T GRFG total 

≈ 0.6 − 1.2 (26) This indicates that CAFE achieves comparable or superior total efficiency while providing better feature quality and interpretability. 

## I Complete Operator Library and Safety Guards 

I.1 Transformation Operators 

Based on the actual implementation, CAFE uses the following operators: 

Unary Operators (11): 

1. √x - Square root 2. x2 - Square 3. x3 - Cube 4. sin( x) - Sine function 5. cos( x) - Cosine function 6. tanh( x) - Hyperbolic tangent 7. 1/x - Reciprocal 8. exp( x) - Exponential 9. log( x) - Natural logarithm 10. σ(x) = 1 /(1 + e−x) - Sigmoid function 11. Preprocessing transformations: StandardScaler, MinMaxScaler, QuantileTransformer 

Binary Operators (4): 

1. x + y - Addition 2. x − y - Subtraction 3. x × y - Multiplication 4. x/y - Division 28 I.2 Safety Mechanisms 

The implementation relies on NumPy’s built-in handling for most edge cases: 

Input Validation: 

• NumPy functions handle domain restrictions (e.g., √x for negative x returns NaN) • Reciprocal operations use NumPy’s protected division behavior • Logarithm operations rely on NumPy’s domain handling 

Output Validation: 

• Generated features are validated during the pruning process • Features with excessive missing values or constant values are filtered • Downstream model evaluation handles any remaining numerical issues 

Operator Selection: The operator set is deliberately conservative, using only well-established mathe-matical functions that are: • Differentiable (important for gradient-based downstream models) • Numerically stable under typical data ranges • Interpretable for causal reasoning • Computationally efficient for large-scale feature generation 

I.3 Implementation Notes 

• All transformations use vectorized NumPy operations for efficiency • Scaling operations (StandardScaler, MinMaxScaler, QuantileTransformer) are applied via scikit-learn transformers • The relatively small operator set (15 total) ensures computational tractability while providing suffi-cient expressiveness for feature construction 

## J Hyperparameter Sensitivity and Selection 

J.1 Grid Search Results 

We perform a comprehensive grid search over key hyperparameters: 

J.2 Adaptive Hyperparameter Rules 

• NOTEARS regularization: λ = 0 .03 with dataset-specific CV selection from {0.01 , 0.03 , 0.05 }

• Group size: kg = min( ⌊√d⌋, 15) (balances coverage and computational cost) • Early stopping: Stop if validation improvement < 0.001 for 3 consecutive episodes • Experience replay: Buffer size scales with complexity: min(10000 , 100 × d)

29 Table 10: Hyperparameter sensitivity analysis (mean performance across datasets). 

Parameter Range Tested Optimal Performance Range Sensitivity Selection Rule 

λ (NOTEARS) [0.001, 0.1] 0.03 [0.734, 0.773] Medium 3-fold CV 

α (causal bonus) [0.0, 1.0] 0.5 [0.742, 0.773] Low Fixed at 0.5 Episodes E [10, 50] 30 [0.751, 0.775] Low Early stopping Steps per episode S [5, 25] 15 [0.743, 0.771] Low Fixed 

kg (group size) [5, 20] 10 [0.758, 0.773] Very Low min( √d, 15) 

Learning rate [1e-4, 1e-2] 1e-3 [0.745, 0.773] Medium Fixed 

ε decay steps [500, 2000] 1000 [0.762, 0.773] Very Low Fixed 

## K Failure Mode Analysis and Limitations 

K.1 Systematic Failure Analysis 

Table 11: Performance under challenging conditions where CAFE shows limitations. 

Challenge CAFE Best Method Best Score Gap Root Cause 

High-dim, low-n ( d > n ) 0.612 ± 0.045 GRFG 0.635 ± 0.041 −3.6% Unreliable CD Sparse graphs 0.687 ± 0.031 ELLM-FT 0.694 ± 0.028 −1.0% Weak causal signal Non-linearities 0.723 ± 0.027 AutoFeat 0.734 ± 0.025 −1.5% Linear CD assumption Hidden confounders 0.641 ± 0.044 GRFG 0.668 ± 0.039 −4.0% Sufficiency violation Temporal dynamics 0.658 ± 0.038 ELLM-FT 0.671 ± 0.035 −1.9% Static DAG 

K.2 Theoretical Limitations 

The framework faces several fundamental limitations: 1. Causal Discovery Dependence: CAFE’s effectiveness directly depends on causal discovery quality, creating a single point of failure 2. Linear Mechanism Assumption: NOTEARS-Lasso assumes linear additive noise models, missing complex non-linear relationships 3. Computational Scalability: O(d3) complexity in Phase I becomes prohibitive for d > 200 

4. Sample Size Requirements: Reliable causal structure learning typically requires n ≫ d, limiting applicability to high-dimensional, small-sample problems 5. Static Assumption: Cannot handle time-varying causal structures or feedback loops 

K.3 Mitigation Strategies and Future Directions 

Short-term mitigations: 

• Graceful degradation: When causal discovery confidence is low, increase weight on mutual infor-mation exploration 30 • Regularization adaptation: Use cross-validation to select λ rather than fixed scaling rules • Early detection: Monitor causal graph quality metrics to identify when CAFE may underperform 

Long-term extensions: 

• Non-linear causal discovery: Integration with neural causal discovery methods (NOTEARS-MLP, DAG-GNN) • Robust causal inference: Methods that handle hidden confounding and model uncertainty • Temporal extensions: Dynamic causal structure learning for time-series data • Hybrid approaches: Combining multiple causal discovery backends with uncertainty quantification 

## L Extended Experimental Results 

L.1 Cross-Model Validation 

Table 12: Performance across different downstream models (mean F1/1-RAE over 5-fold CV). 

Method XGBoost Random Forest Linear SVM (RBF) Average 

Original 0.682 ± 0.047 0.671 ± 0.052 0.598 ± 0.078 0.648 ± 0.055 0.650 

GRFG 0.734 ± 0.038 0.718 ± 0.043 0.627 ± 0.071 0.691 ± 0.041 0.693 

ELLM-FT 0.741 ± 0.035 0.725 ± 0.041 0.634 ± 0.068 0.698 ± 0.038 0.700 

AutoFeat 0.728 ± 0.039 0.712 ± 0.044 0.618 ± 0.074 0.681 ± 0.042 0.685 

CAFE 0.773 ± 0.032 0.756 ± 0.037 0.662 ± 0.063 0.724 ± 0.035 0.729 

Model Configurations: - XGBoost: Default parameters, 100 estimators - Random Forest: 100 estimators, max depth=10 - Linear: Ridge regression with α = 1 .0 (classification: Logistic with L2) - SVM: RBF kernel, C=1.0, γ=’scale’ 

L.2 Learning Curves and Convergence Analysis 

Table 13: Convergence characteristics (episodes to reach 95% of final performance). 

Method Episodes to Converge Final Performance Efficiency Ratio 

GRFG 28 .4 ± 4.7 0.734 ± 0.038 1.00 

CAFE 18.3 ± 3.1 0.773 ± 0.032 1.55 

## M Reproducibility Checklist 

M.1 Implementation Details 

Software Environment: 

31 • Python 3.9.18 • PyTorch 1.13.1 • scikit-learn 1.2.0 • XGBoost 1.7.3 • NumPy 1.24.2 • SHAP 0.41.0 

Hardware Requirements: 

• Minimum: 16 GB RAM, 4-core CPU • Recommended: 64 GB RAM, 16-core CPU • GPU: Not required but can accelerate neural network components 

Random Seeds: 

• NumPy: 42 • PyTorch: 42 • Scikit-learn: 42 • Cross-validation splits: Fixed with seed 42 

## N Future Research Directions 

The integration of causal discovery with reinforcement learning opens several research avenues addressing current limitations. The most immediate priority involves extending beyond linear assumptions through neural causal discovery methods (NOTEARS-MLP, DAG-GNN) and developing robust approaches for hid-den confounders with uncertainty quantification. Time-varying causal structures would enable applications in dynamic environments where relationships evolve. Methodologically, ensemble approaches combining multiple discovery algorithms could address the vulnerability of relying on single methods, while distributed computing could overcome the O(d³) complexity bottleneck. Meta-learning techniques could enable rapid domain adaptation, and active learning could guide experimental design for improved causal structure learn-ing. Applications span domains requiring robust, interpretable features. Scientific discovery could integrate domain knowledge in physics and biology, healthcare applications could maintain clinical interpretability while handling medical confounding, and financial modeling could leverage regulatory changes as natural experiments. Climate science represents a compelling application where robustness to distribution shift ad-dresses the critical need for models that remain valid under unprecedented environmental conditions. Future work will address identified limitations, particularly around non-linear causal relationships, temporal dy-namics, and scalability to very high-dimensional settings. The framework provides a solid foundation for advancing the integration of causal reasoning with automated machine learning. 32