Title: Learning Adaptive Parallel Execution for Efficient Code Localization

URL Source: https://arxiv.org/pdf/2601.19568v1

Published Time: Wed, 28 Jan 2026 01:59:17 GMT

Number of Pages: 13

Markdown Content:
# Learning Adaptive Parallel Execution for Efficient Code Localization 

Ke Xu 1,2,* , Siyang Xiao 1,* , Ming Liang 1, Yichen Yu 1,Zhixiang Wang 1,2 , Jingxuan Xu 1,3 , Dajun Chen 1, Wei Jiang 1, Yong Li 1,† 

> 1

Ant Group, 2Peking University, 3Beijing Jiaotong University  

> {siyang.xsy, liangming.liang, yuyichen.yyc, chendajun.cdj, jonny.jw, liyong.liy}@antgroup.com {xuke59, ekko}@stu.pku.edu.cn, 23120315@bjtu.edu.cn *

Abstract 

Code localization constitutes a key bottleneck in automated software development pipelines. While concurrent tool execution can enhance discovery speed, current agents demonstrate a 34.9% redundant invocation rate, which negates parallelism benefits. We propose Fus-eSearch , reformulating parallel code localiza-tion as a joint quality–efficiency optimiza-tion task. Through defining tool efficiency —the ratio of unique information gain to invoca-tion count—we utilize a two-phase SFT and RL training approach for learning adaptive par-allel strategies. Different from fixed-breadth approaches, FuseSearch dynamically modu-lates search breadth according to task context, evolving from exploration phases to refine-ment stages. Evaluated on SWE-bench Ver-ified, FuseSearch-4B achieves SOTA-level per-formance (84.7% file-level and 56.4% function-level F1 scores) with 93.6% speedup, utilizing 67.7% fewer turns and 68.9% fewer tokens. Results indicate that efficiency-aware training naturally improves quality through eliminat-ing noisy redundant signals, enabling high-performance cost-effective localization agents. 

1 Introduction 

Code localization—identifying the relevant code entities needed to resolve a given issue—is a crit-ical bottleneck in automated software develop-ment (Jimenez et al., 2024; Xia et al., 2024). Re-cent studies show that state-of-the-art agents devote more than 50% of their computational resources to this task, highlighting the need for more efficient strategies (Pan et al., 2025). In response, recent work has proposed specialized localization agents that operate as dedicated search components, de-coupling localization from downstream repair or generation steps (Chen et al., 2025; Jiang et al., 2025). These agents typically rely on multi-turn  

> *Equal contribution. Work done during the internship at Ant Group. †Corresponding author

Figure 1: Parallel execution solves sequential search’s information starvation under limited turns. However, 34.9% of enforced parallel tools are redundant, exhibit-ing redundancy. 

interactions with sequential tool execution (e.g., code retrieval and analysis), iteratively refining queries, inspecting intermediate results, and nar-rowing down candidate files or functions to achieve high localization accuracy. However, this iterative paradigm introduces a fundamental trade-off: ag-gressive constraints on the number of allowed tool interactions (i.e., tight turn budgets) are increas-ingly necessary to meet real-world computational cost requirements, as emphasized in recent bench-marks for production-grade agent systems (Gao and Peng, 2025). Under such tight budgets, agents often fail to gather sufficient contextual evidence before exhausting their interaction quota—a phe-nomenon we refer to as information starvation. Consequently, further reductions in computational cost come at the cost of sharp accuracy degradation, limiting the deployability of current localization ap-proaches in time-sensitive settings. 

Parallel tool execution presents a promising av-enue for addressing the cost–accuracy trade-off by enabling the simultaneous invocation of multiple tools within a single interaction turn, thereby in-creasing the information density per turn. As illus-trated in Figure 1(a), under tight turn budgets, par-allel execution significantly outperforms sequential search in terms of localization accuracy. Despite this potential, most existing agents only provide 

> arXiv:2601.19568v1 [cs.AI] 27 Jan 2026

technical support for parallelism—allowing con-current tool calls—without consistently harnessing its benefits in practice (Pan et al., 2025). Moreover, naive parallelization schemes that en-force a fixed number of tool calls per turn can be highly inefficient. As shown in Figure 1(b), such approaches incur more than 34.9% redundant tool invocations. These unnecessary calls not only waste computational resources but also introduce noisy or irrelevant signals that can degrade local-ization performance. This raises a key question: how can parallelization be made both comprehen-sive and non-redundant —maximizing information coverage to avoid starvation under tight turn bud-gets while eliminating redundant exploration of previously examined code? To address this challenge, we introduce Fus-eSearch , a code localization agent that achieves su-perior quality–efficiency trade-offs through learned adaptive parallel execution . Instead of prescribing a fixed degree of parallelism, FuseSearch dynam-ically adjusts the breadth of parallel tool invoca-tions by explicitly optimizing tool efficiency —the ratio of tool calls that yield novel, relevant infor-mation to the total number of invocations. We in-stantiate tool efficiency as a reward that credits ex-ploration of distinct code regions while penalizing redundancy and failed queries, enabling RL-based joint optimization of localization accuracy (mea-sured by F1) and search efficiency. FuseSearch adopts a minimalist design, relying only on three language-agnostic, read-only tools— grep , glob ,and read_file —and requires no auxiliary infras-tructure such as code graphs or language-specific parsers. Building on this formulation, we employ a two-stage training pipeline that combines SFT with RL to train compact models (4B and 30B parameters) to decide which tools to invoke and how many to run in parallel at each turn, balancing exploration breadth with precision. Notably, optimizing tool efficiency not only reduces search cost but also im-proves localization quality: by discouraging waste-ful calls, the efficiency-driven reward guides the agent toward more targeted and accurate search strategies. Experimental results on SWE-bench Veri-fied (Jimenez et al., 2024) show that Fus-eSearch (train) substantially outperforms Re-poSearcher (Ma et al., 2025) under the Qwen3-4B backbone (Team, 2025). In terms of localization quality, FuseSearch delivers substantial gains, im-proving file-level F1 from 38.1% to 84.7% and function-level F1 from 21.7% to 56.4%, indicating markedly stronger precision in pinpointing both relevant files and target functions. Meanwhile, Fus-eSearch is significantly more efficient: it reduces overall interaction turns by 67.7%, cuts time by 93.6%, and lowers token consumption by 68.9%. These results suggest that the learned adaptive par-allel execution not only boosts localization accu-racy but also streamlines the search process, en-abling faster and more targeted exploration with substantially less redundant tool usage. Our main contributions are: • We propose tool efficiency to quantify infor-mation novelty in code search and integrate it into an efficiency-aware training frame-work (via SFT and RL), enabling the joint optimization of search effectiveness and com-putational efficiency. • We introduce FuseSearch, a minimalist local-ization agent that uses only three read-only tools ( grep , glob , read_file ) yet matches or exceeds the performance of far more com-plex systems. • We demonstrate that high-quality, low-latency localization significantly accelerates down-stream agent workflows, cutting interaction turns by 23.1% and end-to-end task time by 28.5% without sacrificing success rates. 

2 Preliminary 

2.1 Task Formulation 

Code localization aims to identify the specific code entities—such as files, functions, or code snip-pets—that require modification to resolve a given issue. We formulate this problem as a repository-level information-seeking task . Unlike static re-trieval approaches, this process involves an agent that actively interacts with the repository to pro-gressively accumulate relevant context. Through iterative tool calls, the agent retrieves and analyzes various code entities, narrowing down the search space to produce the final localization result. Formally, an agent operates over discrete turns 

t = 1 , . . . , T . A search trajectory is defined as: 

τ = ( q, a 1, o 1, . . . , a T , o T , A) (1) where q is the issue description, at is the set of tool calls at turn t, ot is the aggregated observation containing the retrieved code content, and A is the final localization result identifying the target entities for modification. 

2.2 Parallel Tool Execution 

Traditional sequential agents invoke one tool per turn, leading to prolonged search duration when comprehensive exploration is needed. Parallel tool execution enables simultaneous invocation of mul-tiple tools within a single turn, increasing infor-mation density per interaction. In this paradigm, agents generate multiple tool calls in one response, each formatted as a JSON object. All tools within a turn execute concurrently—their read-only na-ture eliminating synchronization concerns—and their results are aggregated before the next agent response. This design reduces the total number of interaction turns required and shortens overall search time. 

3 FuseSearch 

We present FuseSearch, a minimalist framework for efficient code localization through learned par-allel execution. This section is organized as fol-lows: Section 3.1 introduces our minimalist tool set. Section 3.2 defines efficiency metrics and dual-objective optimization framework—the key innova-tion enabling joint quality-efficiency optimization. Section 3.3 details the training approach imple-menting these objectives. Figure 2 illustrates the overall architecture. 

3.1 Minimalist Tool Set 

We employ a minimalist architecture comprising three read-only tools that enable effective code lo-calization without infrastructure dependencies: • grep : Regex-based pattern search in file contents • glob : File path pattern matching • read_file : Reading file contents with optional line range specification This minimalist design offers several practical advantages. First, the language-agnostic nature requires no parsers or runtime environments, en-abling immediate deployment across diverse code-bases. Second, the simplicity reduces learning com-plexity for models, allowing training resources to focus on strategic tool orchestration rather than intricate tool semantics. Third, it eliminates de-pendency on pre-computed structures like ASTs or dependency graphs, avoiding the overhead of building and maintaining auxiliary infrastructure. 

3.2 Dual-Objective Metrics for Code Localization 

While parallel tool execution increases informa-tion throughput, naive parallelization often results in substantial redundancy. As shown in Figure 1, over 34.9% of tools in enforced parallel execution provide no incremental value. To optimize parallel search strategies, we must quantify search effective-ness along two complementary dimensions: output quality and process efficiency. We define metrics for both dimensions and establish their joint opti-mization framework. 

Localization Quality Following standard prac-tice (Xia et al., 2024; Chen et al., 2025), we mea-sure localization quality using precision P, recall 

R, and their harmonic mean F1 at file-level and function-level granularities. Let ˆA denote the pre-dicted entity set and A the ground truth: 

P = | ˆA ∩ A| | ˆA| , R = | ˆA ∩ A| |A| , F1 = 2PR P + R

(2) The F1 score balances precision and recall, re-warding models that identify relevant code com-prehensively (high recall) without excessive over-prediction (high precision). 

Tool Efficiency To quantify search efficiency, we measure the information gain of each tool call rela-tive to search progress. During execution, we main-tain a history of discovered code entities, including files accessed and content regions examined. For each tool call, we compute its information gain gi

by comparing returned entities against this cumula-tive history: 

gi =

( |E i\H| |E i| if |E i| > 00 otherwise (3) where Ei denotes the set of entities returned by tool 

i, and H represents the union of all entities discov-ered in preceding turns. The term Ei \ H thus quan-tifies the incremental knowledge gain provided by the tool. Tool calls exhibit diverse effectiveness patterns. Some discover entirely new content ( gi = 1 .0), oth-ers retrieve only previously-seen entities ( gi = 0 ), and many return mixed results with partial overlap (0 < g i < 1). We define tool efficiency e as the mean information gain across all tool invocations: Figure 2: FuseSearch framework overview. (a) Inference : Agent executes three minimalist tools in parallel, with each tool’s information gain tracked to compute trajectory efficiency e. (b) Training : Dual-metric filtering selects high-quality trajectories for SFT, followed by RL optimization with joint F1-efficiency reward. 

e = 1

k

> k

X

> i=1

gi (4) where k is the total number of tool calls. This met-ric credits tools proportionally to their novel contri-butions: higher e indicates exploration discovering distinct code regions, while lower e reveals waste-ful redundancy. 

Joint Optimization Objectives We formulate the agent learning objective as jointly maximizing localization quality F1 and tool efficiency e.This joint objective induces mutually reinforcing optimization dynamics. High F1 requires collect-ing sufficient yet focused information: insufficient exploration yields incomplete coverage, while ex-cessive unfocused exploration creates context over-load that degrades answer precision. High e re-quires each tool to explore distinct regions of the search space, avoiding redundant queries. These objectives are complementary rather than conflicting. An agent cannot achieve high e

through low-information tools—empty results or duplicated observations yield zero information gain. However, high e alone is insufficient: brute-force strategies like sequentially reading distinct but ir-relevant files achieve high efficiency (each file is novel) yet produce low F1, as context accumulation with off-target exploration degrades answer preci-sion. The joint objective thus enforces focused, non-redundant exploration : agents must efficiently locate relevant code without wasteful detours. 

3.3 Joint Quality-Efficiency Training 

To optimize the dual objectives defined in Sec-tion 3.2—achieving high localization quality F1

and search efficiency e—we employ a two-stage training approach. SFT provides initial capabilities for parallel tool execution and establishes a strong baseline for both metrics. RL then refines the pol-icy to jointly maximize F1 and e through strategic exploration. 

Training Data Construction We construct a repository-level code localization dataset from 233 high-quality GitHub repositories, ensuring no over-lap with our evaluation benchmarks. To ensure data quality, we exclude samples where (1) patches introduce entirely new files or functions, (2) issue descriptions are incomplete or overly brief, or (3) no code changes occur. From ∼21K filtered sam-ples, we extract ground truth localization targets as the modified files, functions/methods, and line ranges from each patch. 

Quality-Efficiency Guided Fine-Tuning Base language models exhibit weak parallel tool usage capabilities, occasionally generating at most a few tool calls per turn. To bootstrap reliable parallel execution while ensuring high initial quality and efficiency, we perform SFT on trajectories filtered by both F1 and e metrics. We use a capable teacher model (Kimi-K2-Instruct) to synthesize training trajectories. Since even advanced models exhibit inconsistent paral-lel behavior, we employ system-level guidance to increase parallel execution frequency: for 6K ran-domly sampled training queries, we generate multi-ple trajectories per query, each explicitly guided to use 2-8 tools per turn, yielding approximately 24K candidate trajectories. We then apply dual-metric filtering, retaining only trajectories satisfying: 

F1 ≥ ρF and e ≥ ρe (5) This filtering ensures demonstration data exhibits both accurate localization and high tool efficiency. The resulting ∼6K high-quality trajectories are used for fine-tuning Qwen3 base models (4B and 30B-A3B). The resulting SFT models serve dual purposes: (1) they reliably generate parallel tool calls (2-8 tools per turn), addressing the base model’s limited parallel execution capability, and (2) they provide high-quality initialization with reasonable F1 and e

values, enabling effective trajectory sampling dur-ing subsequent RL training. 

RL with Joint Reward Building on the SFT initialization, we apply group relative policy op-timization (GRPO) (Shao et al., 2024) to further optimize both localization quality and search effi-ciency. GRPO samples multiple outputs per query, computes advantages based on reward signals, and updates the policy to favor high-reward behaviors while maintaining proximity to a reference policy through KL regularization. To jointly optimize localization quality ( F1) and search efficiency ( e), we consider a general reward function encompassing both linear and interactive contributions: 

R(τ ) = α·F1(τ )+ β ·e(τ )+ γ ·(F1(τ )·e(τ )) (6) where α, β, γ ≥ 0 are weighting coefficients. For code localization, we impose a strict bound-ary condition: a trajectory that fails to identify rele-vant code ( F1 = 0 ) provides zero utility, regardless of how "efficiently" it executed. This constraint necessitates setting β = 0 , yielding: 

R(τ ) = α · F1(τ )

| {z }

> Base Guarantee

+ γ · (F1(τ ) · e(τ )) 

| {z }

> Efficiency Bonus

(7) The linear term guarantees a baseline reward for correct localization, preventing the vanishing re-ward problem when efficiency is low. The interac-tion term acts as a soft gate, amplifying the reward only when high quality is achieved with high effi-ciency. In practice, F1 is computed as a weighted combi-nation of file-level and function-level localization accuracy: 

F1 = λfile · F file  

> 1

+ λfunc · F func  

> 1

(8) where F file  

> 1

and F func  

> 1

measure precision and recall at their respective granularities. The efficiency met-ric e is computed as defined in Section 3.2. By explicitly coupling search efficiency with localiza-tion quality, this objective aligns the RL signal with our dual goals, encouraging the model to maximize information gain per action without compromising the validity of the final result. 

4 Experiments 

Datasets We evaluate on SWE-bench Veri-fied (Jimenez et al., 2024), a curated benchmark for repository-level issue resolution. Following Suresh et al. (2025), we exclude examples where patches introduce entirely new files or functions, retaining 386 of 500 examples. 

Metrics We evaluate localization quality using precision, recall, and F1 scores at both file-level and function-level granularities. We measure search cost through wall-clock time ( T (s)), inter-action turns (#Turn), and total tokens consumed (Tok.(k)) per instance, capturing both latency and computational overhead (averaged over three runs). 

Baselines We compare against three categories: (1) Workflow-based : Agentless (Xia et al., 2024); (2) Agent-based : LocAgent (Chen et al., 2025), CoSIL (Jiang et al., 2025), and RepoSearcher (Ma et al., 2025). Implementation details are provided in Appendix C. 

4.1 Overall Performance 

Table 1 presents our main results on SWE-bench Verified. We highlight three key findings: 

Parallel vs. Sequential Execution Comparing sequential and parallel execution modes with iden-tical toolsets, parallel invocation achieves compa-rable or superior localization quality while signifi-cantly reducing search time (e.g., 60% on Haiku Model Method File (%) Func (%) #Turn T(s) Tok.(k) Mode 

P R F1 P R F1

Proprietary Models 

Haiku 4.5 Agentless 38.82 91.71 54.55 21.48 61.37 31.83 2.00 7.32 10.6 Seq CoSIL 19.62 96.63 32.62 18.37 69.41 29.05 7.22 38.7 53.0 Seq LocAgent 61.57 87.56 72.30 41.29 65.49 50.64 17.3 318 567 Seq RepoSearcher 19.55 97.41 32.57 20.33 67.65 31.26 19.7 114 193 Seq FuseSearch* 86.38 62.44 72.48 66.30 47.45 55.31 23.9 90.3 270 Seq FuseSearch 73.54 94.50 82.71 48.58 70.61 57.56 6.24 36.2 110 Par 

Open-Source Models 

Kimi-K2 Agentless 34.53 92.23 50.25 28.22 56.27 37.59 2.00 11.8 8.33 Seq CoSIL 21.33 95.60 34.88 23.04 70.20 34.69 6.8 58.8 94.0 Seq LocAgent 55.39 95.85 70.21 33.10 73.33 45.61 14.9 261 447 Seq RepoSearcher 20.61 96.11 33.94 25.20 72.35 37.39 15.6 94.5 108 Seq FuseSearch* 77.33 79.53 78.42 51.87 49.02 50.40 15.0 71.3 216 Seq FuseSearch 75.11 89.31 81.60 51.00 54.90 52.88 7.92 43.6 62.1 Par Qwen3-4B Agentless 28.11 76.68 41.14 11.62 34.12 17.33 2.00 4.24 8.33 Seq CoSIL 21.21 94.82 34.66 18.17 65.49 28.45 10.8 50.8 63.9 Seq LocAgent 39.10 56.22 46.12 27.37 31.18 26.97 6.09 109 135 Seq RepoSearcher 31.99 47.15 38.12 16.92 30.39 21.74 14.8 85.3 99.2 Seq FuseSearch (base) 64.75 64.25 64.50 43.95 34.90 38.91 4.24 6.12 47.9 Par FuseSearch (train) 83.59 85.75 84.65 59.91 53.33 56.43 4.78 5.43 30.9 Par Qwen3-30B Agentless 24.22 87.56 38.19 15.34 55.10 24.00 2.00 8.04 32.0 Seq CoSIL 21.22 96.11 34.77 18.40 66.86 28.86 11.1 49.2 66.8 Seq LocAgent 45.16 62.95 52.59 29.96 32.75 31.32 11.4 112 136 Seq RepoSearcher 40.25 50.78 44.90 20.07 34.71 25.43 16.7 92.4 113 Seq FuseSearch (base) 70.41 79.53 74.70 53.27 46.27 45.65 7.50 14.9 80.1 Par FuseSearch (train) 83.12 82.90 83.01 66.58 52.35 58.62 5.77 10.6 43.2 Par 

Table 1: Localization performance and efficiency comparison on SWE-bench Verified. For agent-based and workflow-based methods, we evaluate using Kimi-K2-Instruct (abbr. as Kimi-K2) and Claude Haiku 4.5 (abbr. as Haiku 4.5). For FuseSearch, we compare base Qwen3 models (Qwen3-4B-Instruct and Qwen3-30B-A3B-Instruct) with their trained counterparts. FuseSearch* denotes using the FuseSearch framework with sequential prompts to contrast the two execution modes. 

4.5) and requiring substantially fewer interaction turns. This demonstrates that parallelization pro-vides efficiency gains beyond mere latency reduc-tion—simultaneous information gathering enables better-informed decisions at each search step. 

Minimalist Toolset Effectiveness Even in se-quential mode, our minimalist toolset achieves competitive performance compared to specialized agent-based methods with graph navigation or AST parsing. This indicates that language-agnostic primitives suffice for effective code localization, while being simpler to deploy and maintain. 

Training Effects Targeted training with joint F1

and efficiency optimization substantially improves both precision and recall. Our trained Qwen3 mod-els (4B and 30B) achieve 83-84% file F 1 and 56-58% function F1, matching Claude Haiku 4.5’s performance while being significantly faster and more token-efficient. Additional evaluation on LocBench (Chen et al., 2025) further confirms the 

Stage File F 1 Func F 1 e #Turn #Tool T(s) Tok.(k) Qwen3-4B 

Base 64.50 38.91 59.50 4.24 1.63 6.12 47.9 RL 70.11 40.18 54.01 3.16 3.44 7.10 31.7 SFT 78.86 47.94 68.46 4.96 3.59 9.17 54.8 SFT+RL 84.65 56.43 69.00 4.78 2.15 5.43 30.9 Qwen3-30B-A3B 

Base 74.70 45.65 54.92 7.50 1.65 14.9 80.1 RL 79.17 47.67 49.21 4.23 1.24 6.63 51.0 SFT 81.13 51.17 59.80 5.49 3.40 11.7 65.2 SFT+RL 83.01 58.62 64.53 5.77 3.44 10.6 43.2 

Table 2: Progressive training effects. SFT establishes parallel tool usage from high-quality trajectories, while RL refines search strategies through F 1 optimization. 

superior performance of our trained models (Ap-pendix D). 

4.2 Training Analysis 

Table 2 reveals key insights about our training framework. Figure 3: Evolution of average tools per turn across training stages. RL learns adaptive parallelism: high initial exploration transitioning to focused refinement. 

Complementary Training Stages The two-stage training exhibits clear complementarity. SFT es-tablishes parallel tool usage capability and substan-tially improves F1, but introduces redundancy that degrades efficiency and increases search cost. RL then resolves this trade-off through joint optimiza-tion: it further improves F1 while simultaneously recovering efficiency and reducing time cost. This validates our design—neither stage alone achieves optimal quality-efficiency balance, but their com-bination enables effective parallelization without sacrificing precision. 

Evolving Parallel Strategies Figure 3 reveals how parallel search strategies evolve across train-ing. Base models use minimal parallelism; SFT shifts to uniformly aggressive parallelism, explain-ing both recall gains and efficiency drops. RL produces qualitatively different behavior: adap-tive parallelism that begins with broad exploration and rapidly transitions to focused refinement. This breadth-first-to-depth-first pattern emerges from joint optimization, demonstrating that models learn not just to parallelize, but when and how much. 

4.3 Ablation Studies 

We validate key design choices through systematic ablations on Qwen3-4B. 

Parallel vs. Sequential Execution To isolate execution mode impact, we train sequential vari-ants (1 tool/turn) with identical data and configu-rations. Table 3 shows parallel execution consis-tently outperforms sequential. Sequential requires nearly twice as many turns yet achieves lower F 1

and consumes more tokens. This confirms parallel execution offers fundamental advantages beyond latency—simultaneous information gathering en-                                                          

> Mode Stage File F 1Func F 1#Turn T(s) Tok.(k)
> Seq SFT 74.02 47.15 9.82 10.42 95.9 SFT+RL 78.82 50.21 7.52 8.03 59.4 Par SFT 78.86 47.94 4.96 9.17 54.8 SFT+RL 84.65 56.45 5.60 5.43 30.9
> Table 3: Parallel vs sequential execution with identical training configurations. Parallel consistently outper-forms sequential across both training stages.
> Filtering File F 1Func F 1eT(s) Tok.(k)
> No filtering 75.44 43.52 55.77 9.84 60.7 Filter F178.55 45.43 56.72 10.53 73.2 Filter e76.74 42.63 60.14 12.94 61.8 Joint filtering 78.86 47.94 62.03 9.17 54.8
> Table 4: SFT performance under different filtering strategies. Joint filtering achieves optimal F 1and ef-ficiency simultaneously.

ables better decisions at each step. 

SFT Data Filtering We evaluate four filtering strategies, each yielding 6K trajectories for SFT: (A) no filtering, (B) F 1-only filtering, (C) efficiency-only filtering, (D) joint filtering (ours). Table 4 shows joint filtering produces SFT models with superior F 1 and efficiency simultaneously. Single-metric filtering improves its target metric but de-grades the other, while unfiltered data yields sub-optimal initialization on both. This validates that dual-metric filtering provides high-quality starting points for subsequent RL optimization. 

Reward Design We compare three RL reward formulations: (A) F1 only, (B) additive F1 + e, (C) multiplicative F1 + F1 · e (ours). Table 5 reveals distinct trade-offs. F1-only optimization improves over SFT with reduced token usage but moderate time reduction. Additive F1 + e achieves highest efficiency, but reduced search quality necessitates more tool invocations, increasing both time and token costs. Our multiplicative reward delivers the best F1, highest efficiency, and lowest time and to-ken costs simultaneously. This validates the multi-plicative reward structure for practical deployment scenarios where both quality and efficiency matter. 

4.4 Downstream Task Applications 

To evaluate FuseSearch’s practical impact, we test its effectiveness in assisting Kimi-K2-Instruct on SWE-bench Verified issue resolution. We compare three configurations: (A) No Localization , where the main agent performs full-stack exploration; (B) 

Pre-Search , where FuseSearch-4B conducts ini-Reward Type File F 1 Func F 1 e T(s) Tok.(k)                                              

> SFT 78.86 47.94 62.03 9.17 54.8
> F1only 81.84 54.90 59.66 7.28 39.4
> F1+e79.22 51.98 66.62 9.40 45.7
> F1+F1·e(ours) 84.65 56.45 69.00 5.43 30.9
> Table 5: Reward design ablation. The composite reward balances quality and efficiency optimally.
> Method Pass Rate (%) #Turn T(s) Tok.(k)
> No Localization 68.4 41.1 312 1053 Pre-Search 68.1 31.6 223 562 Sub-Agent 68.7 31.9 290 713
> Table 6: Impact of FuseSearch-4B on Kimi-K2’s issue resolution performance. Localization reduces token consumption and time while maintaining quality.

tial localization before the main agent begins; and (C) Sub-Agent , where the main agent dynamically invokes FuseSearch-4B during task execution. Table 6 shows that both localization modes main-tain comparable pass rates while substantially re-ducing the main agent’s token consumption and to-tal inference time. Pre-search mode offers the most efficient configuration, demonstrating that fast, ac-curate localization models can significantly acceler-ate downstream task completion without sacrificing solution quality. 

5 Related Work 

5.1 Code Localization Methods 

Recent LLM-based approaches divide into two paradigms. Workflow methods like Agent-less (Xia et al., 2024) employ fixed hierarchical strategies that progressively narrow search scope from files to functions, but lack adaptability to vary-ing task complexity. Agent-based methods enable flexible multi-step exploration: graph-guided ap-proaches like LocAgent (Chen et al., 2025) and CoSIL (Jiang et al., 2025) represent code as static or dynamic graphs for navigation, while general agents like OpenHands (Wang et al., 2025) and SWE-agent (Yang et al., 2024) use bash-like inter-faces for repository traversal. However, existing agents execute tool calls se-quentially, leading to prolonged search duration when comprehensive exploration is needed. Graph-based methods further require language-specific preprocessing infrastructure that limits generaliza-tion across programming languages. We address these limitations through a minimalist parallel exe-cution framework that accelerates search via con-current tool invocation while eliminating infrastruc-ture dependencies. 

5.2 Parallel Tool Use and Agent Training 

Parallel tool execution has emerged as a strategy to accelerate multi-step search processes. Commer-cial systems like SWE-grep (Pan et al., 2025) train specialized retrieval models with basic weighted 

F1 rewards to issue fixed parallel calls, achiev-ing speedups but requiring expensive inference in-frastructure (Cerebras at 2800+ tokens/s). In web search, recent work trains models to distinguish parallelizable from sequential queries: Hybrid-DeepSearcher (Ko et al., 2025) fine-tunes models on synthetic hybrid-hop QA data to reduce search turns, while ParallelSearch (Zhao et al., 2025) and RAG-R1 (Tan et al., 2025) apply RL with query decomposition for parallel execution. However, existing work optimizes primarily for accuracy, leading to wasteful tool invocations that contribute nothing to search progress, or incom-plete utilization of discovered information. While some approaches penalize trajectory length (Zelik-man et al., 2024), this does not directly measure tool usage quality or distinguish between effective and redundant exploration. We address this gap by introducing tool effi-ciency—the ratio of tools that discover new code entities to total tools invoked—as a metric that directly penalizes redundant exploration while en-couraging focused information gathering. Our com-pact models achieve strong performance without specialized hardware by co-optimizing localization quality and search efficiency through efficiency-based RL rewards. 

6 Conclusion 

We introduced FuseSearch , a code localization agent that achieves superior accuracy-efficiency trade-offs through learned adaptive parallel execu-tion. By optimizing tool efficiency —rewarding information novelty while penalizing redun-dancy—via SFT and RL, FuseSearch-4B achieves 84.7% file-level F1 while completing searches 93.6% faster with 67.7% fewer turns. As pre-processing for downstream repair tasks, it re-duces interaction turns by 23.1% and comple-tion time by 28.5%. Our results demonstrate that efficiency-aware training enables accurate yet com-putationally practical agents—a crucial step to-ward production-grade automated software devel-opment. 7 Limitations 

Our work is subject to limitations in current evalu-ation frameworks. First, ground truth derived from golden patches represents only one valid solution, potentially missing alternative correct localizations. Second, available benchmarks predominantly cover Python repositories (SWE-bench Verified); while our toolset is language-agnostic, assessing effec-tiveness on statically-typed languages like Java or C++ requires more diverse benchmarks and train-ing data. Third, existing benchmarks focus exclu-sively on issue-driven localization, whereas code search is fundamental to broader scenarios such as repository question answering, code compre-hension, and documentation generation—contexts our approach has not been evaluated on. These constraints highlight the need for comprehensive localization benchmarks spanning diverse tasks and languages. 

References 

Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, and Xingyao Wang. 2025. Locagent: Graph-guided llm agents for code localization. Preprint ,arXiv:2503.09089. Pengfei Gao and Chao Peng. 2025. More with less: An empirical study of turn-control strategies for efficient coding agents. arXiv preprint arXiv:2510.16786 .Zhonghao Jiang, Xiaoxue Ren, Meng Yan, Wei Jiang, Yong Li, and Zhongxin Liu. 2025. Issue localiza-tion via llm-driven iterative code graph searching. 

Preprint , arXiv:2503.22424. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. 2024. SWE-bench: Can language mod-els resolve real-world github issues? In The Twelfth International Conference on Learning Representa-tions .Dayoon Ko, Jihyuk Kim, Haeju Park, Sohyeon Kim, Dahyun Lee, Yongrae Jo, Gunhee Kim, Moontae Lee, and Kyungjae Lee. 2025. Hybrid deep searcher: Integrating parallel and sequential search reasoning. 

Preprint , arXiv:2508.19113. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-cient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles .Weichen Li, Xiaotong Huang, Jianwu Zheng, Zheng Wang, Chaokun Wang, Li Pan, and Jianhua Li. 2025. rllm: Relational table learning with llms. Preprint ,arXiv:2407.20157. Zexiong Ma, Chao Peng, Qunhong Zeng, Pengfei Gao, Yanzhen Zou, and Bing Xie. 2025. Tool-integrated re-inforcement learning for repo deep search. Preprint ,arXiv:2508.03012. Ben Pan, Carlo Baronio, Albert Tam, Pietro Marsella, Mokshit Jain, Daniel Chiu, Swyx, and Silas Alberti. 2025. Introducing swe-grep and swe-grep-mini: Rl for multi-turn, fast context retrieval. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathemati-cal reasoning in open language models. Preprint ,arXiv:2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 .Tarun Suresh, Revanth Gangi Reddy, Yifei Xu, Zach Nussbaum, Andriy Mulyar, Brandon Duderstadt, and Heng Ji. 2025. Cornstack: High-quality contrastive data for better code retrieval and reranking. Preprint ,arXiv:2412.01007. Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, and Jinjie Gu. 2025. Rag-r1: Incentivizing the search and reasoning capabilities of llms through multi-query parallelism. Preprint ,arXiv:2507.02962. Qwen Team. 2025. Qwen3 technical report. Preprint ,arXiv:2505.09388. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, and 5 others. 2025. Openhands: An open platform for ai software developers as generalist agents. Preprint , arXiv:2407.16741. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agentless: Demystifying llm-based software engineering agents. Preprint ,arXiv:2407.01489. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. Swe-agent: Agent-computer interfaces enable automated software engineering. Preprint ,arXiv:2405.15793. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. 2024. Quiet-star: Language models can teach themselves to think before speaking. Preprint , arXiv:2403.09629. Shu Zhao, Tan Yu, Anbang Xu, Japinder Singh, Aaditya Shukla, and Rama Akkiraju. 2025. Parallelsearch: Train your llms to decompose query and search sub-queries in parallel with reinforcement learning. 

> Preprint , arXiv:2508.09303.

A Parallel Execution Framework 

A.1 Inference Algorithm 

Algorithm 1 describes the inference process of Fus-eSearch. At each turn, the model generates an action a that may contain multiple tool calls for-matted as JSON objects. The Parse function ex-tracts these tool calls {c1, . . . , c n}, which are then executed concurrently by the environment. The ag-gregated observations {o1, . . . , o n} are appended to the trajectory before the next model invocation. This cycle continues until the model produces a final answer without tool calls. 

Algorithm 1 Inference Process of FuseSearch 

Require: Query q, model M, environment E

Ensure: Localized files F 

> 1:

Initialize trajectory τ ← ∅  

> 2:

while True do  

> 3:

a ← M (q, τ ) // Generate action  

> 4:

τ ← τ ⊕ a // Append action to trajectory  

> 5:

if no <tool_call> in a then  

> 6:

return answer ← Extract( a) 

> 7:

end if  

> 8:

{c1, c 2, . . . , c n} ← Parse( a) 

> 9:

{o1, o 2, . . . , o n} ← E .step ({c1, . . . , c n}) 

> 10:

τ ← τ ⊕ { o1, o 2, . . . , o n} 

> 11:

end while A.2 Tool Specifications 

Table 7 summarizes the core parameters of our three tools. Each tool is implemented as a func-tion with a JSON-formatted parameter schema, en-abling models to invoke them through structured tool calls.            

> Tool Required Optional
> read_file path start_line ,end_line grep pattern path ,glob ,output_mode glob pattern path
> Table 7: Core parameters for the three minimalist tools. All paths are absolute.

read_file Reads file contents with optional line range specification. The path parameter speci-fies the absolute file path. When start_line and 

end_line are provided, only the specified range is returned; otherwise, the entire file is read (up to a default limit of 1000 lines). 

grep Performs regex-based content search built on ripgrep. The pattern parameter accepts full regex syntax. The optional output_mode controls result format: files_with_matches (default) re-turns only file paths, content returns matching lines with context, and count returns match counts per file. The glob parameter filters files by pat-tern (e.g., *.py ), while path restricts search to a specific directory. 

glob Matches files by name patterns. The 

pattern parameter accepts standard glob syntax (e.g., **/*.js for recursive search, test_*.py for prefix matching). Results are limited to 100 file paths to prevent overwhelming context windows. 

B Training Configuration 

B.1 Data Split 

From the collected repository-level localization dataset of approximately 21K issue-patch pairs, we allocate 6K samples for SFT and the remaining 15K samples for RL. The SFT subset undergoes trajectory synthesis and dual-metric filtering as de-scribed in Section 3.3, yielding approximately 6K high-quality demonstration trajectories. 

B.2 SFT 

We fine-tune Qwen3-4B-Instruct and Qwen3-30B-A3B-Instruct on the filtered trajectories for 1 epoch with a batch size of 32. Both models are trained on 8×NVIDIA H20 GPUs(96GB). We use AdamW optimizer with a learning rate of 2e-5 and linear warmup for the first 10% of training steps. The maximum sequence length is set to 32768 tokens to accommodate long repository contexts and multi-tool trajectories. 

B.3 RL Infrastructure We employ vLLM (Kwon et al., 2023) as the inference engine to accelerate trajec-tory sampling during policy rollouts. The train-ing framework is built on RLLM (Li et al., 2025), which leverages veRL (Sheng et al., 2024) for dis-tributed RL. 

Sampling Configuration To ensure diversity in trajectory exploration, we set the sampling tem-perature to 0.7 during rollouts. For each training instance, we sample 8 trajectories (rollout=8) to compute advantage estimates for GRPO updates. 

Training Hyperparameters Table 8 summa-rizes the key hyperparameters for GRPO training. We use a per-GPU batch size of 32, yielding a global batch size of 256 across 32 NVIDIA H20 GPUs(96GB) with rollout factor 8. The prompt length is capped at 49152 tokens to accommodate extensive repository context, while response length is limited to 32768 tokens for tool call sequences and reasoning. We train for 1 epoch over the 15K RL training samples.            

> Hyperparameter Value
> Training batch size 32 Rollout per instance 8Global batch size 256 Sampling temperature 0.7 Max prompt length 49152 Max response length 32768 Training epochs 1Training samples 15K Learning rate 1e-6 KL coefficient ( β)0.01
> Table 8: Hyperparameters for GRPO-based RL.

Reward Coefficients For the joint reward func-tion R(τ ) = α · F1(τ ) + γ · (F1(τ ) · e(τ )) , we set 

α = 0 .8 and γ = 0 .2. The F1 score is computed as F1 = 0 .7 · F file  

> 1

+ 0 .3 · F func  

> 1

, placing higher weight on file-level accuracy while still incentiviz-ing function-level precision. 

C Baseline Implementation Details 

We compare FuseSearch against four representa-tive code localization frameworks, each employing distinct strategies and infrastructure requirements. 

Agentless (Xia et al., 2024) adopts a hierarchi-cal pipeline approach without maintaining agent state across turns. It performs localization in three sequential stages: (1) file-level filtering us-ing keyword matching and LLM ranking, (2) class/function identification within selected files, and (3) fine-grained line-level localization. This workflow-based design eliminates the need for complex reasoning chains but requires careful tun-ing of each pipeline stage. 

CoSIL (Jiang et al., 2025) implements an it-erative agent that dynamically constructs module call graphs during exploration. Starting from en-try points identified by keyword search, it progres-sively expands the graph by analyzing function invocations and dependencies. The agent employs context pruning to manage token limits, discarding less relevant code paths based on semantic similar-ity to the issue description. This dynamic graph construction enables adaptive exploration without requiring pre-built static analysis infrastructure. 

LocAgent (Chen et al., 2025) pre-processes repositories into directed heterogeneous graphs encoding file, class, and function nodes along with their structural relationships (imports, inheri-tance, invocations). The agent navigates this graph through node-visiting actions, leveraging graph connectivity to perform multi-hop reasoning. This approach requires upfront graph construction but enables efficient traversal of complex dependency chains. 

RepoSearcher (Ma et al., 2025) provides a lightweight tool suite (GetRepoStructure, Search-Class, SearchFunction, SearchClassMethod) for direct code retrieval without graph preprocessing. The agent iteratively invokes tools to gather rele-vant context, terminating via an explicit Exit action. This minimalist design reduces infrastructure over-head while maintaining competitive localization performance. 

C.1 Model Deployment 

For proprietary models (Claude-3.5-Sonnet, Kimi-K2-Instruct), we invoke their official APIs with temperature set to 0 for deterministic outputs. For open-source models (Qwen3-4B-Instruct, Qwen3-30B-A3B-Instruct), we deploy local inference servers using vLLM (Kwon et al., 2023) on 8×NVIDIA H20 GPUs. We set tensor parallelism to 8 for distributed inference and enable continu-ous batching to maximize throughput. All models use their default system prompts and tool-calling formats as specified in their official documentation. 

D Additional Experiments 

D.1 Evaluation on LocBench 

To further validate the generalization of FuseSearch across different benchmarks, we conduct additional evaluation on LocBench (Chen et al., 2025), a localization-focused benchmark designed to assess code localization capabilities. Following the same filtering criteria as SWE-bench Verified (excluding patches that introduce entirely new files or func-tions), we retain 456 out of 560 examples for eval-uation. Table 9 presents the performance comparison between FuseSearch-4B (trained model) and Kimi-K2-Instruct on LocBench. Both models use the FuseSearch framework with parallel tool execution. The results demonstrate that our trained 4B model achieves superior localization quality while being significantly more efficient: it improves file-level F1 by 5.4 points and function-level F 1 by 5.8 points compared to Kimi-K2, while reducing search time by 77% (6.24s vs 27.8s) and token consumption by 35% (37.5k vs 57.9k). The efficiency metric 

e is also higher (74.06 vs 68.97), indicating that the trained model generates fewer redundant tool calls. These results confirm that our efficiency-aware training approach generalizes well beyond SWE-bench Verified, consistently producing mod-els that achieve better quality-efficiency trade-offs across diverse localization tasks.                 

> Model File F 1Func F 1e#Turn T(s) Tok.(k)
> Kimi-K2 70.33 45.65 68.97 6.60 27.8 57.9 FuseSearch-4B 75.74 51.47 74.06 4.61 6.24 37.5
> Table 9: Performance comparison on LocBench (456 examples). Both models use the FuseSearch framework with parallel execution.

E Prompt Design 

E.1 System Prompt and Output Format 

FuseSearch employs a structured prompt design that guides the model to produce localization re-sults in two distinct sections: Locations to Modify and Related Context. Figure 4 illustrates the com-plete system prompt used during inference. 

Locations to Modify contains the core lo-calization results—the specific files and functions that require modification to resolve the issue. The model outputs a ranked list of code entities, where higher-ranked items are deemed more likely to be the root cause. All precision, recall, and F 1 scores reported in our experiments are computed based on this section, treating it as the model’s primary prediction. 

Related Context allows the model to include additional code entities that are semantically re-lated to the issue but do not necessarily require direct modification. For example, when localizing a bug in a data validation function, the model might include related utility functions or constants in Re-lated Context even though the fix only requires modifying the validation logic itself. While not used for localization metric calculation, this sup-plementary information proves valuable for down-stream issue resolution tasks: by providing repair agents with a broader context of relevant code, it enables faster and more accurate patch generation without requiring additional exploration turns. This two-part output design reflects a key insight: effective localization should distinguish between "must-fix" locations (high precision for metrics) and "helpful context" (high utility for downstream tasks). By separating these concerns, FuseSearch simultaneously optimizes for localization accuracy and downstream task efficiency. Figure 4: System prompt for FuseSearch. The prompt instructs the model to output localization results in two sections: Locations to Modify (required) and Related Context (optional).