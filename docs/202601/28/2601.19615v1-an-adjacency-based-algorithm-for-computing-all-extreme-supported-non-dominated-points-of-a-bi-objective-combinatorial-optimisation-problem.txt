Title: An adjacency-based algorithm for computing all extreme-supported non-dominated points of a bi-objective combinatorial optimisation problem

URL Source: https://arxiv.org/pdf/2601.19615v1

Published Time: Wed, 28 Jan 2026 02:02:02 GMT

Number of Pages: 22

Markdown Content:
# An adjacency-based algorithm for computing all extreme-supported non-dominated points of a bi-objective combinatorial optimisation problem 

## Oliver Bachtler ∗,1 , Felix Fritz 1, and Stefan Ruzika 11RPTU University Kaiserslautern-Landau, Kaiserslautern, Germany 

Abstract 

Generally, multi-objective optimisation problems are solved exactly or approximated by solving a series of scalarisations, for example by dichotomic search. In this paper, we take a different approach and attempt to compute the set of all extreme-supported non-dominated points of a bi-objective combinatorial optimisation problem by using a neighbourhood-based approach. Whether or not this works depends on the definition of adjacency and we provide sufficient conditions that guarantee its success. The resulting generic algorithm is an alternative to dichotomic search in our setting. We then apply our generic algorithm to a specific example: the bi-objective minimum weight basis problem, in which we are given a matroid and want to find bases of minimum weight. We use the natural definition of adjacency, in which two bases are adjacent if they differ in exactly one element. Since this satisfies our sufficient condition on the adjacency relation, our generic algorithm works in this case and we analyse its running time, showing that it is polynomial. By tailoring this algorithm specifically to matroids, we obtain one that is faster but no longer transitions between adjacent solutions, instead swapping directly from one extreme-supported point to the next in a combinatorial fashion. 

Keywords: multi-objective optimization, matroids, adjacency 

# 1 Introduction 

In multi-objective optimisation, problems have multiple conflicting objectives and, thus, they rarely have a single optimal solution. Instead they have several, potentially very many, solutions whose objective values are not naturally comparable. Since the corresponding single-objective problems are usually far better studied, scalarisation is the typical approach for solving such problems. Scalarisation methods take a multi-objective problem and systematically turn it into a single-objective one, which potentially has some parameters or additional constraints. For surveys of scalarisation in multi-objective optimisation, we refer to [Jah85, MM02, EW05]. The simplest and most common scalarisation method, and the one we use here, is the weighted-sum scalarisation method, in which each objective is given a weight and these are added up while the feasible set remains unchanged [Ehr05, Chapter 3]. This method typically cannot compute all non-dominated points YN , but those on the boundary of the convex hull of the outcome set: the supported non-dominated points YSN . Of these, the extreme points YESN are of special interest since 

• they provide an optimal point for every weighted-sum scalarisation problem [Bac26, PGE10]. 

> ∗

Corresponding author, o.bachtler@math.rptu.de 

1

> arXiv:2601.19615v1 [math.OC] 27 Jan 2026

• They form a {(2 , 1) , (1 , 2) }-approximation of the set of all non-dominated points [Bac26, Baz+21], that is, for every non-dominated point, there is an extreme-supported non-dominated point that is at least as good in one objective and off by at most a factor of 2 in the other. 

• Based on an empirical study [Say24], they provide a good representation of the entire non-dominated set, based on their coverage error and hypervolume ratio. Plenty of algorithms exist that compute the set YESN , which we call extremely-supportive algorithms ,most of which concern multi-objective (integer) linear programs. The most common and general one for two objectives is dichotomic search [AN79], which proceeds by solving weighted sum scalarisation problems for certain weights that depend on the previously found images. An extension of this to more than two objectives can be found in [PGE10], which solves the higher-dimensional cases by recursively reducing them to the bi-objective case. Note that our algorithm (and any other one for the bi-objective case) can be inserted into this recursive procedure. For further examples, see [Ben98, BS02, ÖK10, ELS11, Hal+20, Bök+24]. All of these algorithms operate in the objective space to obtain their solutions. On the other hand, methods that operate in the decision space are more rare and often less efficient due to the fact that the decision space usually has a higher dimension than the objective space. One prominent example of a decision space method is the parametric Simplex algorithm [SG54, Ehr05], which transitions between adjacent basic solutions to find YESN .We want to use definitions of adjacency in this paper to develop an adjacency-based extremely-supportive algorithm. While this has not been done generically, concepts of adjacency have been studied before. Note that we focus on discrete problems, so we are not concerned with topological connectivity, for which results exist as well, see [Nac78] as an example, which is applicable to multi-objective linear programs. In combinatorial problems, connectivity is usually defined with respect to an adjacency graph. The vertices of this graph are the feasible solutions and two vertices are connected by an edge if the two solutions are adjacent, for whatever the definition of adjacency is. These graphs are still typically connected, but for algorithmic purposes, it would be desirable for the efficient solutions (so the subgraph induced by them) to be connected, which is rarely the case. To be precise, it is not the case for the minimum spanning tree problem and the shortest path problem by [EK97], nor for the bi-objective integer minimum cost flow problem [PGE06]. The efficient solutions of the multi-objective knapsack, travelling salesman, and linear assignment problems are not connected either, except for certain special cases [SCF04, Gor10, GKR11]. After all these negative results, there is some good news: the supported efficient solutions are often connected. This is true for basic solutions in linear programming (where they are equal to the basic efficient solutions) [Ise77] but also for minimum bases matroids [Ehr96]. There is no reason to assume that the extreme-supported efficient solutions are connected however and, indeed, they are not for the minimum spanning tree problem, as we shall see. But, since the supported efficient solutions typically are, we shall develop an algorithm for bi-objective combinatorial problems that generically computes YESN while only visiting points that are supported. The basic idea is similar to the combinatorial argument used in [Ehr96] to show that the supported efficient bases of a matroid are efficient for the natural definition of adjacency, where two bases are adjacent if they differ in exactly one element. We start with a lexicographically optimal solution, which is optimal for the second objective and, among those, for the first. Then, we look at the solutions ‘to the left’, so ones with better first objective and transition to one with the flattest slope. In this way, we traverse the Pareto front and do not skip the extreme-supported non-dominated points. This is computationally possible, since our feasible set is finite, but very expensive. To improve this, we do not check all solutions that are ‘to the left’ of our current one, but just those that are also adjacent. This limits the solutions we need to check to those in the neighbourhood, but may also cause the result to be incorrect. We provide sufficient conditions on the adjacency relation that ensure that we do not 2make mistakes by not considering some solutions. We also strengthen the criterion slightly to obtain a sufficient criterion for the connectedness of the supported efficient solutions, which can be used to obtain the corresponding result in [Ehr96], for example. Consequently, we obtain a framework which takes an adjacency relation and, if this relation satisfies our sufficient criterion, yields an extremely-supportive algorithm for the problem. In particular, if we use adjacent feasible solutions to define the adjacency relation, our framework yields the parametric Simplex algorithm. Afterwards, we look at the bi-objective problem of computing a minimum weight basis of a matroid, which is a large problem class and contains, for example, the minimum spanning tree problem. We use the adjacency definition mentioned earlier and verify that it satisfies our sufficient condition, yielding a working algorithm. We prove that it runs in polynomial time, despite the fact that there may be exponentially many supported efficient solutions. Finally, we use ideas from parametric optimisation to obtain an algorithm tailored to this problem class. We give up on transitioning between adjacent solutions and instead show how we can directly traverse the extreme-supported non-dominated points in the order ‘from bottom right to top left’. We compare this theoretically to dichotomic search, for which our algorithms are substitutes. 

Contributions and outline. After covering preliminary definitions and results in Section 2, we develop a generic extremely-supportive algorithm for bi-objective combinatorial problems in Section 3, which can be seen as an alternative to dichotomic search for this problem class. We show how to incorporate a notion of adjacency in order to make it computationally feasible and obtain the parametric Simplex algorithm as a special case. Additionally, we provide sufficient conditions on the adjacency relation that guarantee that the algorithm works and that the supported efficient solutions are connected. In Section 4, we look at the bi-objective minimum weight basis problem for matroids and apply our framework to it. We prove that the resulting extremely-supportive algorithm runs in polynomial time before we improve it in Section 5, where we also compare it to dichotomic search. 

# 2 Preliminaries 

In this section, we introduce the basic concepts we need for the remainder of this paper. We start by defining notions of multi-objective optimisation, then we introduce matroids and some of their properties, and finally, we look at what adjacency means in the context of multi-objective optimisation in general and for matroids in particular. 

## 2.1 Multi-objective optimisation 

A combinatorial bi-objective optimisation problem Π2 is of the following form: 

min f (x) = ( f1 (x) , f 2 (x)) T (Π2)

s.t. x ∈ X

where f1, f 2 : X ! R are the objective functions and X is finite. Let Y = f (X) be the outcome set. In this paper, relations like ≤ and < are always interpreted component-wise and we write y ⪇ y′ if 

y ≤ y′ but y̸ = y′. We use the Pareto-concept of optimality, in which a point y is called dominated by y′

if and only if y′ ⪇ y. A point y ∈ Y which is not dominated by any other point is called a non-dominated 

point and the set of non-dominated points is denoted by YN . We define XE := {x ∈ X : f (x) ∈ YN }

as the set of efficient solutions. 3v1

v2

v3

v4

v5

(−14

)

(00

)

(00

)

(04

)

(40

)

(22

)

> (a) An example graph for the bi-objective minimum spanning tree problem.

2 4 6

2

4

6

8

10 

T1

T3

T4

T2

f1

f2

YESN 

YSN 

Y \ YN   

> (b) The costs of all spanning trees. The grey area is conv (Y) + R≥0.

Figure 1: An example bi-objective minimum spanning tree problem. A typical task in a multi-objective optimisation problem is to find all non-dominated points in the outcome set Y and to every non-dominated point y at least one efficient solution x ∈ XE with f (x) = y.This is typically done by the ε-constraint approach or by Tchebycheff scalarisation, which transfer the multi-objective optimisation problem to a single objective one. Another typical scalarisation, that cannot yield all non-dominated points in general, is weighted-sum problems , which minimises wTf (x).By normalising w, we can always assume that w1 + w2 = 1 and it suffices to specify one component, say w1 = λ (and w2 = 1 − λ). Thus, for λ ∈ [0 , 1] , we define ΠWS  

> 2

(λ) by 

min λf 1 (x) + (1 − λ)f2 (x) (ΠWS  

> 2

(λ))

s.t. x ∈ X. 

We write Xλ for all optimal solutions of ΠWS  

> 2

(λ).A solution x ∈ Xλ, for λ ∈ (0 , 1) , is a supported efficient solution and its image y is a supported non-dominated point . If y is an extreme point of conv (Y ) + R≥0, then y is an extreme-supported non-dominated point and x an extreme-supported efficient solution . We write XSE , XESE , YSN , and 

YESN for the sets of such solutions and points. The solutions x ∈ XSE are efficient and y ∈ YSN are non-dominated [Ehr05, Theorem 3.6]. We note that several different definitions of supported efficient solutions and supported non-dominated points are used in the literature. For an overview we refer to [Chl25, KS25], which show that they all coincide in our setting of bi-objective combinatorial problems. Let us look at a small but illustrative example. 

2.1 Example. Let G be the graph on five vertices shown in Figure 1a. We want to find a minimum spanning tree with respect to the costs depicted therein. This graph has nine spanning trees and their corresponding costs are shown in Figure 1b. The efficient spanning trees correspond to the red and blue dots. The trees T1, T2, and T3 all contain the edges v1v3

and v2v3 as well as the edges v3v5 and v4v5, v3v4 and v3v5, or v3v4 and v4v5. The tree T4 uses the edges v1v2, v1v3, v3v4, and v4v5, though the same point is attained by the tree T ′ 

> 4

:= T4 − v1v3 + v2v3.The remaining trees T5, T ′ 

> 5

and T6, T ′ 

> 6

have objective values (5, 6)T and (3, 8)T, which are dominated by the images of T2 or T3. ◁

We also need the definition of the weight set decomposition (with respect to the weighted sum scalarisation), which we shall briefly recall. We denote the weight set [0 , 1] by Λ and write Λ( x) for the weight set component of x, that is, Λ( x) = {λ ∈ Λ : x ∈ Xλ}. We regularly use the following result concerning the weight set, for which we refer to [Bac26, Theorem 3.14] (which is based on [PGE10]). 4Algorithm 2.1: The Greedy algorithm for a matroid M .Sort the elements of E = {e1, e 2, . . . , e m} such that c(e1) ≤ c(e2) ≤ . . . ≤ c(em)

Initialize I ∅

for i = 1 , . . . , m do if I ∪ { ei} ∈ I then 

I I ∪ { ei}

return I

2.2 Lemma. The weight set component Λ( x) of x ∈ X consists of more than a single weight if and only if x ∈ XESE . ◁

## 2.2 Matroids and the Greedy algorithm 

Matroids are a generalisation of spanning trees and are the structure in which the Greedy algorithm is optimal. For the basics of matroids, we refer to [Oxl11], but we briefly state the definitions and properties we need here. 

2.3 Definition. A matroid M is an ordered pair (E, I), where E is a finite set and I ⊆ 2E satisfies the following three properties: (a) ∅ ∈ I .(b) If I ∈ I and I′ ⊆ I, then I′ ∈ I .(c) If I1, I 2 ∈ I and |I1| < |I2|, then there exists an element e ∈ I2 \ I1 such that I1 ∪ { e} ∈ I . ◁

The elements of I are called independent sets of M , while all other subsets of E are called dependent . A 

basis of M is a maximal independent set of M and a circuit is a minimal dependent set. We denote the set of all bases of M by B(M ) and the set of all circuits by C(M ). All bases have the same cardinality which is called the rank rank( M ) of M .

2.4 Example. Let E be the edge set of a connected graph G and let F be the set of all acyclic subsets 

F ⊆ E. Then M = ( E, F) is called a graphic matroid . The bases of M are the spanning trees, the circuits are cycles, and the rank of M is n − 1 where n is the order of the graph G. ◁

For some single-objective cost function c : E ! R, we can find a basis with minimal cost by applying the Greedy algorithm (Algorithm 2.1). It performs m independence tests, the complexity of which may differ from matroid to matroid. Thus, we denote the time to perform an independence check in M by 

Tindep (M ) and obtain a running time of O (m · (log m + Tindep (M ))) .That this algorithm is correct is a key property of matroids. In fact, the Greedy algorithm is correct for every choice of costs c if and only if the M is a matroid [Oxl11, Theorem 1.8.5]. The following strong basis exchange property will be helpful for us later. 

2.5 Lemma ([Bru69, Theorem 2]). Let B and B′ be bases of M . For every e ∈ B there exists an 

e′ ∈ B′ such that (B − e) + e′ and (B′ − e′) + e are bases of M . ◁

Our main application in this paper will be the bi-objective minimum weight basis problem (bmwb ), which is defined as follows: 

min f (B) = ( f1 (B) , f 2 (B)) T (bmwb )

s.t. B ∈ B (M )

where, for i ∈ { 1, 2}, fi (B) = ∑ 

> e∈B

ci(e) with c : E ! R2. Note that we can solve the weighted-sum problem ΠWS  

> 2

(λ) in this case by applying the Greedy algorithm with weights cλ, where cλ(e) :=

λc 1(e) + (1 − λ)c2(e) for every λ ∈ Λ.5T1

T2T3

T4

T ′

> 4

T5

T ′

> 5

T6

T ′

> 6

Figure 2: The adjacency graph of the MST problem from Example 2.1. 

## 2.3 Definitions and results concerning adjacency 

Finally, we want to define what it means for certain solutions of our combinatorial optimisation problem 

Π2 to be connected. To do so, we need a notion of adjacency between different solutions, which is just a binary relation. 

2.6 Definition. Let R be a binary relation on X. A feasible solution x ∈ X is adjacent to a feasible solution x′ ∈ X if (x, x ′) ∈ R .The adjacency graph of R is the digraph DR with vertex set X and arc set R. We treat DR as an undirected graph if R is symmetric. ◁

Now, a set of solutions X′ ⊆ X is (weakly or strongly) connected if DR[X′] is. To shorten notation, we write DE , DSE , and DESE for DR[X′] if X′ is XE , XSE , and XESE .As an example, for matroids, we use the natural definition of adjacency where two bases are adjacent if they differ in exactly one element. 

2.7 Definition. Let M be a matroid and RM be the relation on B(M ) containing all pairs of bases 

B, B ′ such that |B \ B′| = 1 . ◁

This is a symmetric relation, so we can regard the adjacency graph D := DRM as undirected. For graphic matroids, this just means that adjacent spanning trees differ in exactly one edge. If we recall Example 2.1, the adjacency graph for that problem is shown in Figure 2. Several results concerning the connectivity of certain subgraphs of D are known, though not many of them are positive. By the exchange property (Property (c) in Definition 2.3), we can transform every basis B into every other basis B′ by iteratively exchanging single elements, which yields that D

is connected. But these exchanges could lead to intermediate bases that are not efficient for Π2, indeed, 

DE need not be [EK97]. Ehrgott [Ehr96] showed that DSE is connected and we shall generalise this proof and obtain it again as a special case of our result. In addition, we shall develop an adjacency-based algorithm that computes 

YESN that works in the case of matroids. However, the following example shows that DESE is not connected. 

2.8 Example. Regard the graph from Figure 1a, where we replace the costs of the left triangle by the same costs as on the right. By doing so, we obtain the objective values shown in Figure 3a. Note that the missing trees have the following values: f (T ′

> 4

) = f (T5) = f (T2), f (T ′

> 6

) = f (T1), and f (T6) = f (T3).Here we see that all these points lie on one line and the extreme-supported efficient solutions are exactly the trees T4 and T ′

> 5

, which are not adjacent, see Figure 2. ◁

As the example shows, an algorithm that computes YESN while moving between adjacent solutions in 

XESE cannot exist. Thus, we shall need to allow our algorithm to visit intermediate solutions that are 64 6 8 10 12 

4

6

8

10 

12 

T4

T ′

> 5

T2

T3

T1

f1

f2

YESN 

YSN 

> (a) The costs of all spanning trees.

T1

T2T3

T4

T ′

> 4

T5

T ′

> 5

T6

T ′

> 6
> (b) The adjacency graph.

Figure 3: The costs and adjacency graph of the spanning trees in Example 2.8. 

2 4 6

2

4

6

8

10 

T1

T3

T4

T2

f1

f2

Figure 4: Possible steps for an extremely-supportive algorithm. not in XESE , but we shall make sure that it does not leave XSE . Therefore, we call an algorithm that computes a set S ⊆ XESE such that f (S) = YESN an extremely-supportive algorithm .Do note, however, that XSE can contain exponentially many elements: if we look at the graphic matroid for the complete graph whose edges all have the same cost, then all nn−2 spanning trees [AZ18] are optimal bases with respect to every weight vector. This result can be extended to an example where 

YN is exponentially large as well, see [HR94, Theorem 4.2]. 

# 3 A generic extremely-supportive algorithm 

The goal of this section is to develop a generic adjacency-based extremely-supportive algorithm for a combinatorial bi-objective problem Π2. Our algorithm starts with a lexicographically optimal solution 

x0 with respect to the ordering (f2, f 1), which is extreme-supported efficient [Bac26, Lemma 2.2]. If we recall Example 2.1, the solution x0 is the spanning tree T1. For the sake of convenience, Figure 4 shows the outcome set of the example again. Here, the weight set Λ( T1) = [0 , 12 ]. The algorithm will now increase λ from 0 until 12 , at which points there are multiple optimal solutions to ΠWS  

> 2

( 12 ), namely 

T1, T2, and T3. At this point, as indicated in the figure, the algorithm would want to transition to T3

(or to T2 and then T3 since we allow intermediate points in XSE to be visited). The way this is realised is by noting that T2 and T3 are exactly those spanning trees, for which the line between their objective value and f (T1) is least steep. Thus, the algorithm looks at the points that 7have lower f1-value and transition to the one that leads to the least steep slope and then continues onward from that point. We formally prove that this approach works in the first subsection, which gives us a global algorithm .The downside of this algorithm is that it needs to look at lots of points in every step. We remedy this by not looking at all points that have lower f1-value, but only at those that are adjacent to the current point. This will not work in general, since we might miss all the solutions with the least steep slope in this manner. Thus, the algorithm only works for specific notions of adjacency. In the second subsection, we provide a sufficient condition for the adjacency relation that guarantees the algorithm’s correctness. There, we will also see that adjacent basic solutions of linear programs satisfy this condition and that our algorithm in this case is exactly the parametric Simplex algorithm. 

## 3.1 A global algorithm 

As we noted above, we want to transition from a solution x to a solution x′ of lower f1-value for which the slope α between f (x) and f (x′) is least steep. Such a slope α is directly related to the value λ for which both x and x′ are optimal: this is the case when w := ( λ, 1 − λ)T is a normal vector of the line of slope α. A simple computation shows that this is equivalent to λ = αα−1 . Thus, wTf (x) is lower than 

wTf (x′) before this λ and higher after, making λ an upper bound on the weight set component of x.Consequently, to find the upper bound of Λ( x) (at which point we get another optimal solution), we need to find the maximal slope, which leads to the smallest bound. Analogously, solutions x′ with higher f1-value provide us with lower bounds on the weight set component in the same way, where the highest, so the one corresponding to the minimal slope, is the lower bound of 

Λ( x). That this works is the result of Lemma 3.2, for which the following definition gives us convenient access to slopes and their corresponding parameters in the weight set. 

3.1 Definition. Let x ∈ X. For x′ ∈ X, we define 

(∆1(x′, x )∆2(x′, x )

)

:= f (x′) − f (x) .

If ∆1(x′, x )̸ = 0 , we define s(x′, x ) := ∆2(x′,x )∆1(x′,x ) . In addition, we set X<(x) := {x′ ∈ X : ∆ 1(x′, x ) < 0}

and X>(x) := {x′ ∈ X : ∆ 1(x′, x ) > 0}.Finally, for λ ∈ [0 , 1) we define α(λ) as −λ 

> 1−λ

∈ (−∞ , 0] . Conversely, for α ∈ (−∞ , 0] , we define λ(α) as  

> αα−1

∈ [0 , 1) . We extend the notation by setting α(1) := −∞ and λ(−∞ ) := 1 . ◁

Before we do the computation, we note the following two properties of α and λ. First, they are inverse functions and, second, they invert inequalities, for example, if λ ≤ λ′, then α(λ) ≥ α(λ′). We shall make use of these properties regularly. 

3.2 Lemma. Let x ∈ XSE . Then Λ( x) = [ λ(αx), λ (αx)] , where 

αx := max {s(x′, x ) : x′ ∈ X<(x)} ∪ {−∞} ,αx := min {s(x′, x ) : x′ ∈ X>(x)} ∪ { 0} .

Additionally, if s(x′, x ) = αx for x′ ∈ X<(x), then x′ ∈ Xλ(αx) and if s(x′, x ) = αx for x′ ∈ X>(x),then x′ ∈ Xλ(αx). In both cases, x′ is supported efficient. ◁

Proof. Let x, x ′ ∈ X with ∆1(x′, x ) < 0 and λ ∈ (0 , 1) , w := ( λ, 1 − λ)T. Then 

wTf (x) ≤ wTf (x′) ⇐⇒ (1 − λ)∆ 2(x′, x ) ≥ − λ∆1(x′, x )

⇐⇒ (1 − λ)s(x′, x ) ≤ − λ

⇐⇒ s(x′, x ) ≤ −λ

1 − λ = α(λ)

⇐⇒ λ(s(x′, x )) ≥ λ. (1) 8Algorithm 3.1: A generic extremely-supportive algorithm. Let x0 be a lexicographically optimal solution with respect to (f2, f 1)

S {x0}, k 1

while X<(xk−1)̸ = ∅ do 

Let xk ∈ arg max {s(x′, x k−1) : x′ ∈ X<(xk−1)}

S S ∪ {xk}, k k + 1 

return S

The same computation works with ‘ ≥’ and ‘ =’. In particular, if ∆1(x′, x ) > 0, then 

wTf (x) ≤ wTf (x′) ⇐⇒ λ(s(x′, x )) ≤ α(λ).

Since x ∈ Xλ if and only if wTf (x) ≤ wTf (x′) for all x′ ∈ X, we can use the computation above to reformulate the right hand side to λ ≤ λ(s(x′, x )) for x′ ∈ X<(x), λ ≥ λ(s(x′, x )) for x′ ∈ X>(x), and 

f2 (x) ≤ f2 (x′) if ∆1(x′, x ) = 0 .If x is supported efficient, then the third condition is satisfied and thus, by the definition of αx and 

αx, we get that x ∈ Xλ if and only if λ ∈ [λ(αx), λ (αx)] . As a result, Λ( x) has the desired form. For the ‘additionally’ part, we regard the case that s(x′, x ) = αx = α(λ(αx)) . Using the equality version of (1) , we see that this implies that x and x′ have the same objective value for λ(αx). Since 

x ∈ Xλ(αx), so is x′.To see that x′ is supported efficient, we note that ∆1(x′, x ) < 0 and, since x was efficient, ∆2(x′, x ) > 0.Hence, αx ∈ (−∞ , 0) and λ(αx) ∈ (0 , 1) . The case for αx is analogous. □

Before we formulate our algorithm, let us note two simple consequences of this lemma. The first tells us that a transition from x to a solution x′ to the left of maximal slope is next in the weight set decomposition in the sense that the weight set component of x′ starts where the component of x ended. The second tells us that the slope between f (x) and f (x′) must be equal to α(λ) if both x and x′ are optimal for ΠWS  

> 2

(λ).

3.3 Corollary. Let x ∈ XSE , x′ ∈ X<(x), and αx = s(x, x ′). Then, αx′

= αx. ◁

Proof. Since x′ ∈ X<(x), we get x ∈ X>(x′) and αx′

≤ s(x, x ′) = αx. But since both x and x′ are in Xλ(αx) by Lemma 3.2, we have that λ(αx) ∈ Λ( x′). Hence, λ(αx) ≥ λ(αx′

), which yields αx ≤ αx′

,giving us the desired equality. □

3.4 Corollary. Let λ ∈ Λ, x ∈ Xλ, and x′ ∈ X. Then s(x′, x ) ≤ α(λ) if x′ ∈ X<(x) and s(x′, x ) ≥ α(λ)

is x′ ∈ X>(x). In particular, if x′ ∈ Xλ and f (x)̸ = f (x′), then α(λ) = s(x′, x ). ◁

Proof. If x′ ∈ X<(x), we get s(x′, x ) ≤ αx ≤ α(λ) and if x′ ∈ X>(x), then s(x′, x ) ≥ αx ≥ α(λ) by Lemma 3.2. For the ‘in particular’ part, note that we can exchange the roles of x and x′ in this case. We may assume that x′ ∈ X<(x), which yields α(λ) ≤ s(x′, x ) = s(x, x ′) ≤ α(λ). □

We can now formulate the very simple Algorithm 3.1: it starts with the lexicographically optimal solution x0 as described and, in iteration k, determines the solution xk in X<(xk−1) that maximises the slope s(xk, x k−1) and continues looking for points from xk.Based on the previous lemma and its corollaries, we can make a few simple observations. 

3.5 Observation. For points x0, . . . , x l computed by Algorithm 3.1 the following properties hold: (a) x0 ∈ XSE , because it is a lexicographic solution. (b) For k ∈ { 1, . . . , l }, s(xk, x k−1) = αxk−1 = αxk

=: αk, by Corollary 3.3. (c) For k ∈ { 1, . . . , l }, both xk−1 and xk are supported efficient and in Xλk for λk := λ(αk),inductively by Lemma 3.2. 9Algorithm 3.2: A generic adjacency-based extremely-supportive algorithm. Let x0 be a lexicographically optimal solution with respect to (f2, f 1)

S {x0}, k 1

while N <(xk−1)̸ = ∅ do 

Let xk ∈ arg max {s(x′, x k−1) : x′ ∈ N <(xk−1)}

S S ∪ {xk}, k k + 1 

return S

(d) For k ∈ { 2, . . . , l }, αk−1 = αxk−1

≥ αxk−1 = αk and thus λk−1 ≤ λk. ◁

We now have everything we need to prove correctness. 

3.6 Theorem. Algorithm 3.1 is correct, that is, it returns a set S ⊆ XSE such that YESN ⊆ f (S). ◁

Proof. By Observation 3.5, we know that all the points x0, . . . , x l computed are in XSE . Thus, we only need to show that we compute at least one preimage for every point in YESN .To this end, let y ∈ YESN , then y = f (x) for an x ∈ XESE . We know that f2

(x0) ≤ f2 (x) and, since 

x is efficient, f1 (x) ≤ f1

(x0). Furthermore, f1

(xl) ≤ f1 (x) since the algorithm terminated at xl.If f1 (x) = f1

(xk) for some k, then f2 (x) = f2

(xk) since both x and xk are efficient solutions and 

y = f (xk) ∈ f (S). Otherwise, f1 (x) ∈ (f1

(xk) , f 1

(xk−1)) for some 1 ≤ k ≤ l. In this case, we can apply Observation 3.5: since x ∈ X<(xk−1), αk = αxk−1 ≥ s(x, x k−1) = s(xk−1, x ). Analogously, 

x ∈ X>(xk) implies αk = αxk

≤ s(x, x k) as well. Combining this with the fact that xk ∈ X<(x) and xk−1 ∈ X>(x), we conclude that αx ≥ s(xk, x ) ≥

αk ≥ s(xk−1, x ) ≥ αx, which yields Λ( x) ⊆ {λk}, a contradiction to Lemma 2.2. □

Also note that we could run the algorithm starting at an arbitrary solution x0 ∈ XSE and we would still obtain all extreme-supported efficient solutions with a higher f2-value. This is true since the only properties of x0 that we used is that x0 is supported efficient and that f2

(x0) ≤ y2 for y ∈ YESN . In summary, we get the following corollary. 

3.7 Corollary. Algorithm 3.1, when initialised with an arbitrary solution x0 ∈ XSE , computes a set 

S ⊆ XSE with YESN ∩ {y ∈ R2 : y2 ≥ f2

(x0)} ⊆ f (S). ◁

Algorithm 3.1 can easily be modified to return a set S ⊆ XESE by simply not adding the non-extreme-supported efficient solutions we find. These can be determined on the fly since we have implicitly computed the weight set of each of the points we added: by Lemma 3.2 and Observation 3.5, 

Λ( xk) = 

[

λ(αxk

), λ (αxk )

]

= [λk, λ k+1 ] ,

for k ∈ { 1, . . . , l − 1}. For the first and last solution, we get Λ( x0) = [0, λ 1] and Λ( xl) = [λl, 1]. This makes it easy to check whether the weight set contains at most one element and the point can be discarded by Lemma 2.2. Additionally, the algorithm also computes a weight set decomposition. 

## 3.2 Obtaining an adjacency-based version 

Now that we have our base outline, we want to use adjacency. The basic idea is to not determine the next solution by finding the maximum slope among all other solutions, but just amongst those in the neighbourhood (so amongst the successors in the adjacency graph). To this end, we let D be the adjacency graph and denote the set X<(x) ∩ N out  

> D

(x) by N <(x). The new algorithm, which is illustrated in Algorithm 3.2, now simply replaces X<(x) by N <(x).To make this work, we need to ensure that our notion of adjacency does not make us miss essential points when looking in this restricted set. This is done by the following theorem. 10 3.8 Theorem. Algorithm 3.2 is correct if, for all λ ∈ (0 , 1) and all x ∈ Xλ, the set X<(x) ∩ Xλ is either empty or contains a successor of x in D. ◁

Proof. We show inductively that the algorithm is a possible execution of Algorithm 3.1, letting us transfer the correctness from Theorem 3.6. More precisely, we show that all solutions chosen up to iteration k could also have been candidates in Algorithm 3.1 and that both terminate at the same time. The first part is clearly true before the first iteration. Hence, we may assume the claim holds until iteration k − 1 and want to show that the chosen element xk is also a possible choice for Algorithm 3.1, that is, s(xk, x k−1) = αk. Since N <(x)̸ = ∅, X<(x) is as well and Algorithm 3.1 chooses a solution 

˜xk with s( ˜xk, x k−1) = αx. Using Observation 3.5, we recall that xk−1, ˜xk ∈ Xλk . Hence, ˜xk ∈

X<(x) ∩ Xλk , meaning this set contains a successor ˜x of x. By Corollary 3.4 and the choice of xk,

s(xk, x k−1) ≥ s(˜ x, x k−1) = αk = αxk−1 ≥ s(xk, x k−1),

proving the desired equality. □

Again, this proof works for the modified algorithm which starts with an arbitrary supported efficient point x0.We now take a look at an example, which shows that the parametric simplex algorithm is a special case of our result, at least if the bi-objective linear program it solves is non-degenerate. 

3.9 Example (The parametric Simplex algorithm). The Simplex method [NW88] is a method for solving linear programs. Its extension to the bi-objective setting is the parametric simplex al-gorithm [Ehr05], which solves the problem 

min 

(cT 

> 1

xcT 

> 2

x

)

s.t. Ax = b, x ≥ 0

where A ∈ Rm×n with rank A = m, b ∈ Rm, and c1, c 2 ∈ Rn. For our presentation, we adhere closely to [Ehr05, Section 6.2], with the exception that the algorithm there goes from λ = 1 to λ = 0 , so we swap the objectives and thus λ with (1 − λ).A basis B of A is a selection of m linearly independent columns of A, that is, the submatrix AB

containing these columns is invertible. We denote A−1 

> B

A by ˜A and A−1 

> B

b by ˜b. If ˜b ≥ 0, the basis is feasible and the vector of reduced costs is given by cT = cT − cT 

> B

˜A. We need to assume that the problem is non-degenerate, that is, ˜b > 0 for all bases B.The Simplex algorithm then starts with a feasible basis B and then iteratively chooses a variable xi to enter the basis that has negative reduced cost ci. It then determines how far xi can be increased before a variable xj with j ∈ B reaches 0 and one such variable then leaves the basis. If xi can be increased arbitrarily, the problem is infeasible and if no variable with negative reduced cost remains, the solution is optimal. The parametric Simplex now uses the Simplex algorithm to find optimal bases for weighted sum scalarisations for increasing λ, see Algorithm 3.3. It starts by computing a lexicographically optimal basis with respect to (f2, f 1). We obtain reduced costs c1 and c2, of which c2 ≥ 0. Note that the reduced costs for ΠWS  

> 2

(λ) are exactly λc 1 + (1 − λ)c2. Thus, if c1 ≥ 0 holds as well, x is also optimal for all larger λ and the algorithm terminates. Otherwise it finds the first λ after which a new entry of negative reduced cost is created, which becomes the entering variable and the leaving variable is computed as in the Simplex algorithm. To see that this is exactly Algorithm 3.2, we have already formulated it in terms of feasible bases (instead of basic feasible solutions), but every basis B comes with its basic solution xB. We say two 11 Algorithm 3.3: The parametric Simplex algorithm. Let B0 be a lexicographically optimal basis with respect to (f2, f 1)

S = {B0}, k 1

I {i / ∈ B 0 : c1 

> i

< 0}

while I̸ = ∅ do 

λ min 

{ −c1 

> i
> c2
> i−c1
> i

: i ∈ I 

}

and s ∈ arg min 

{ −c1 

> i
> c2
> i−c1
> i

: i ∈ I 

}

r ∈ arg min 

{

j ∈ B k−1 : ˜bj

> ˜Ajs

: ˜Ajs > 0

}

Bk Bk−1 ∪ { s} \ { r}, I {i / ∈ B k : c1 

> i

< 0}

S S ∪ {Bk}, k k + 1 

return S

feasible bases B and B′ are adjacent if they differ in exactly one element. Thus, Algorithm 3.3 transitions between adjacent bases. To see that Bk ∈ N <(Bk−1), we need to take a closer look at this set. The set N <(Bk−1) contains those bases B that differ from Bk−1 by one element and that satisfy (c1)Tx < (c1)Txk−1 where x, x k−1

are the basic solutions corresponding to B and Bk−1. Like in the Simplex algorithm, 

(c1)Tx = ( c1)Txk−1 + ( c1 

> N

)TxN (2) where N := {1, . . . , n } \ B and c are the reduced costs for xk−1.If we assume that B = Bk−1 ∪ { i} \ { j}, then (2) becomes (c1)Tx = ( c1)Txk−1 + c1 

> i

xi, since the entering variable i is the only variable in N where x has a positive value. Consequently, x ∈ N <(xk−1) if and only if c1 

> i

< 0, by our non-degeneracy assumption. Thus, the parametric Simplex chooses an element 

Bk in N <(xk−1) in each step and it is a special case of Algorithm 3.2. To obtain a proof of correctness from our framework, we need to verify that the condition of Theorem 3.8 is satisfied. Hence, let λ ∈ (0 , 1) and B be an optimal basis for ΠWS  

> 2

(λ). We may assume there exists a basis B′ ∈ X<(B) that is also optimal for this weighted sum scalarisation and now need to show that such a basis exists which is also a neighbour. Let x and x′ be the corresponding basic solutions. Since (c1)Tx′ < (c1)Tx, we can use (2) to obtain 

(c1)Tx > (c1)Tx′ = ( c1)Tx + ( c1 

> N

)Tx′N .

Thus, for some i ∈ N , cix′ 

> i

< 0, that is, ci < 0 and x′ 

> i

> 0. The latter gives us i ∈ B ′ \ B . Let B∗

be a feasible basis obtained by letting i enter the basis B and x∗ be the corresponding basic solution. Then, by (2) , ci < 0, and the non-degeneracy assumption, (c1)Tx∗ = ( c1)Tx + c1 

> i

x∗ 

> i

< (c1)Tx. Thus, 

B∗ ∈ N <(B).To complete the proof, we need to show that B∗ is optimal for ΠWS  

> 2

(λ). Let cλ := λc 1 + (1 − λ)c2 and 

cλ := λc 1 + (1 − λ)c2. Since both x and x′ are optimal for cλ, we can conclude that 

(cλ)Tx = ( cλ)Tx′ (2) 

= ( cλ)Tx + ( cλ 

> N

)Tx′N = ( cλ)Tx + ( cλ

> B′\B

)Tx′B′\B .

By the optimality of x for cλ and the non-degeneracy assumption, we get that cλ 

> N

≥ 0, so cλ 

> B′\B

= 0 . In particular, cλi = 0 and cλx∗ (2) 

= ( cλ)Tx + cλi x∗ 

> i

= ( cλ)Tx. Thus, B∗ is optimal. ◁

We end this section on a connectivity result for DSE , which we can obtain by slightly strengthening the condition in Theorem 3.8. 

3.10 Theorem. If the condition from Theorem 3.8 is met and the lexicographically optimal solutions for the ordering (f1, f 2) are weakly connected, then so is DSE . ◁

12 Proof. Let x ∈ XSE be an arbitrary point. By Theorem 3.8 and Corollary 3.7, Algorithm 3.2 computes a sequence of adjacent solutions x0, . . . , x l such that YESN ∩{y ∈ R2 : y2 ≥ f2

(x0)} ⊆ f ({ x0, . . . , x l}) .In particular, since for xl satisfies that X<(xl) = ∅, it must be a lexicographically optimal solution for the ordering (f1, f 2). Since these solutions are weakly connected by assumption and every supported efficient solution is connected to one of them, DSE is connected. □

An alternative (and more natural) criterion requires the optimal solutions to each weighted sum scalarisation to be connected. 

3.11 Theorem. Then the graph DSE is (weakly or strongly) connected if D[Xλ] is (weakly or strongly) connected for all λ ∈ (0 , 1) . ◁

Proof. By [Bac26, Corollary 3.6] or [PGE10, Proposition 6], we know that Λ is given by the union of intervals [0 , λ 1], [λ1, λ 2], . . . , [λk, 1] with 0 < λ 1 < . . . < λ k < 1 that originate from extreme-supported efficient solutions, say Λ( xi) = [ λi, λ i+1 ] for i ∈ { 1, . . . , k − 1} and xi ∈ XESE (by Lemma 2.2). Note that xi ∈ Xλi ∩ Xλi+1 . Since D[Xλi ] is connected for all i, so is ⋃ki=1 D[Xλi ] ⊆ D[⋃ki=1 Xλi ]. But 

⋃ki=1 Xλi = XSE by [Bac26, Theorem 3.9], from which the result follows. □

Note that it would have been sufficient to require D[Xλ] to be connected for λ ∈ {λ1, . . . , λ k}, we did not use connectivity for the remaining λ-values. 

# 4 A polynomial time adjacency-based extremely-supportive algorithm for bmwb 

In this section we want to make use of the generic Algorithm 3.2 to compute the extreme-supported non-dominated points for the minimum weight basis in a matroid in polynomial time. Thus, for the remainder of this section, let M be a matroid, where M = ( E, I), and let D be the adjacency graph for the notion of adjacency defined in Definition 2.7. We first show that for this notion of adjacency the requirements of Theorem 3.8 are met and then discuss how to implement the algorithm in this special case to determine its running time. 

## 4.1 Applying Algorithm 3.2 

Using Lemma 2.5, we can prove that the requirements of Theorem 3.8 are met, giving us an extremely-supportive algorithm. 

4.1 Theorem. Let B ∈ Xλ such that X<(B) ∩ Xλ̸ = ∅. Then there exists a basis B′ ∈ N <(B) ∩ Xλ.In particular, Algorithm 3.2 is correct in this case. ◁

Proof. Let λ and B be as in the claim and let B′ be a basis in X<(B) ∩ Xλ that minimises |B \ B′|.We show that |B \ B′| = 1 , which yields B′ ∈ N <(B) ∩ Xλ.Let e ∈ B \ B′. By Lemma 2.5 we can find an element e′ ∈ B′ such that B1 := ( B − e) + e′ and 

B2 := ( B′ − e′) + e are bases. Since B and B′ are optimal with respect to the costs cλ(e), we can conclude that cλ(e) = cλ(e′): otherwise, either B1 or B2 would have a better objective value than B

or B′. Hence, B1 and B2 are in Xλ.We now note that B2 differs from B is one fewer element than B′ does. So by minimality of |B \ B′|,

B2 /∈ X<(B) and c1(e) > c 1(e′). Hence, c1(B1) < c 1(B) and B1 ∈ X<(B) ∩ Xλ, making it a possible choice for B′, so |B \ B′| = 1 . □

Before we implement the algorithm for matroids, let us briefly remark that we can also prove connectivity of the supported efficient solutions easily in this case, giving us an alternative proof of [Ehr96, Theorem 4]. 13 4.2 Theorem. The graph DSE is connected. ◁

Proof. By Theorems 3.10 and 4.1 it suffices to show that the lexicographically optimal bases are connected. But since any such basis can be obtained by the Greedy algorithm (that sorts elements of identical weight such that the basis elements come first), these are connected. Simply exchange pairs of elements of identical weight to transform one sorting into the other. □

Note that the argument suffices to use Theorem 3.11 as well, when applied to all orderings cλ.

## 4.2 Implementing the algorithm 

But what is the running time of the algorithm? For the analysis, recall that we denote the time to perform an independence check in M by Tindep (M ). Additionally, let r := rank( M ) and m := |E|.To obtain a solution that is lexicographically optimal, we need to run the Greedy algorithm on the lexicographic sorting, resulting in a running time of O (m · (log m + Tindep (M ))) . The remaining time of the algorithm is spent on finding an element B′ ∈ N <(B) that maximises the slope s(B′, B ) in each iteration. Note that a neighbour B′ in N (B) is of the form (B − e) + e′. Since c1(B) > c 1(B′), we must have c1(e) > c 1(e′) and we can test all O (( m − r)r) such pairs. This yields a running time of 

O (mr · Tindep (M )) per iteration. We remark that this is an approach that works generically, but it might be suboptimal. An alternative approach is to determine the circuit in B + e′, for e′ /∈ B. Then the potential elements in B we can swap e′ with are exactly the elements e ∈ C \ { e′} with c1(e) > c 1(e′). We can find a circuit in 

O (r · Tindep (M )) by removing the elements of B from B + e′ if the remaining set is still dependent. This implementation directly yields our original running time, but we benefit if this can be done faster than O (r · Tindep (M )) .One example where this is the case is the graphic matroid. Here, Tindep (M ) ∈ O (n), but this is also the time required to find the cycle in T + e′. In this way, we could reduce the time needed for enumerating the neighbourhood to O (mn ) from O (mn 2). But, for simplicity, we shall restrict the analysis to just the generic running time of Tindep (M ).It remains to bound the number of iterations. The basic idea is that, when we transition from a basis 

Bk−1 to a neighbour Bk = ( Bk−1 − e) + e′, then cλk (e) = cλk (e′), since both bases are in Xλk by Observation 3.5. So such swaps can only occur for a specific value of λk, which only increases during the course of our algorithm. A certain swap may occur multiple times, but since we exchange an element 

e of larger c1-value with an element e′ of smaller value, we can still strictly decrease the number of potential swaps that remain in each iteration. Since these are initially bounded by all possible swaps, the iterations are bounded by O (m2). The following notation and theorem formalises this idea. 

4.3 Notation. Let P := {(e, f ) ∈ E2 : c1(e) > c 1(f ), c 2(e) < c 2(f )}. Let e, f ∈ E be two fixed elements with c1(e) ≥ c1(f ). Then, cλ(e) and cλ(f ) are linear functions in λ, which intersect in exactly one point λ(e, f ) ∈ (0 , 1) if and only if (e, f ) ∈ P . We write E for the set {λ(e, f ) : ( e, f ) ∈ P} . ◁

4.4 Theorem. Algorithm 3.2 applied to a matroid M terminates after at most O (m2) iterations of the while loop. ◁

Proof. Let B0, . . . , B l be the bases computed by Algorithm 3.2 applied to M . Let (e, f ) ∈ P . We call the pair (e, f ) k-feasible if λk < λ (e, f ) or if λk = λ(e, f ), e ∈ Bk, and f / ∈ Bk. Initially, for k = 0 ,there are at most O (m2) k-feasible pairs and we now prove that each iteration strictly reduces this number. To do so, let k ∈ { 1, . . . , l } and Bk = Bk−1 − e + f , where c1(e) > c 1(f ) since B′ ∈ X<(B) and, since 

B′ is efficient, c2(f ) > c 2(e). By Observation 3.5, cλk (e) = cλk (f ), since both bases are in Xλk , so 

λ(e, f ) = λk ≥ λk−1 and {e, f } is a (k − 1) -feasible pair but not k-feasible. We now show that we can assign a distinct (k − 1) -feasible pair to every k-feasible pair, without using {e, f }.

14 Algorithm 5.1: An algorithm that computes a representation of YESN for matroids. Let λ1 < . . . < λ s be the values in E

Let B0 be a lexicographically optimal solution with respect to (f2, f 1)

S {B0}

for k = 1 , . . . , s do 

Bk Bk−1 \ Eλk

Let e1, . . . , e t be the elements in Eλk , sorted lexicographically by (cλk , c 1) in non-descending order 

for i = 1 , . . . , t do if Bk + ei is independent then 

Bk Bk + ei

S S ∪ {Bk}, k k + 1 

return S

Let (e′, f ′) be a k-feasible pair. If λ(e′, f ′) > λ k, then this pair is also (k − 1) -feasible. Thus, we may assume that λk = λ(e′, f ′), e′ ∈ Bk and f ′ /∈ Bk. If e′ ∈ Bk−1 and f ′ /∈ Bk−1, then again (e′, f ′) is 

(k − 1) -feasible. This leaves the case that e′ ∈ Bk \ Bk−1 = {f } or f ′ ∈ Bk−1 \ Bk = {e}. In either case, the elements 

e, f, e ′, f ′ all have the same cλk -value. If e′ = f , then c1(e) > c 1(f ) = c1(e′) > c 1(f ′), so f ′̸ = e and 

f ′ /∈ Bk−1. In particular, the pair (e, f ′) satisfies λ(e, f ′) = λk and e ∈ Bk−1 \ Bk, f ′ /∈ Bk−1. Thus, the pair (e, f ′) is (k − 1) -feasible but not k-feasible. The case f ′ = e is analogous. Here, c1(f ) < c 1(e) = c1(f ′) < c 1(e′), so e′̸ = f and e′ ∈ Bk−1. This time, the pair (e′, f ) satisfies λ(e′, f ) = λk and e′ ∈ Bk−1, f ∈ Bk \ Bk−1 and the pair is (k − 1) -feasible but not k-feasible. □

With this bound on the number of iterations, we obtain the following result. 

4.5 Corollary. Algorithm 3.2 can be implemented to run in O (m3r · Tindep (M )) time on matroids. ◁

# 5 Tailoring an algorithm to bmwb 

Again, let M be a matroid with M = ( E, I). In this section, we design a faster extremely-supportive algorithm for the bi-objective minimum weight basis problem. To this end, we shall make use of the Greedy algorithm: the original lexicographic solution is obtained by running Greedy on the lexicographically sorted edges. We then saw that edge exchanges happen when edges have the same weight. By sorting the points λ where two edges have the same weight, the adjacency-based algorithm traversed solutions that were in Xλ until it reached the next extreme point and λ increased. We shall prove here that the extreme points correspond to two specific orderings of the edges: the first extreme point our algorithm found corresponds to the ordering where edges of equal cλ-value are ordered by non-increasing c1-value, while the second extreme point orders by non-decreasing c1-value. In this way, we can jump directly to the next extreme point. The following definition provides us with convenient notation to formulate the resulting Algorithm 5.1. 

5.1 Definition. For λ ∈ E , we define 

Eλ := ⋃  

> (e,f )∈P :
> λ(e,f )= λ

{e, f } .◁

## 5.1 Proving correctness of Algorithm 5.1 

To show that this algorithm works, some more notation is needed. 15 5.2 Definition. Let S be an ordering of the elements of a matroid M . We write BM (S) = B(S) for the bases obtained by running the Greedy algorithm for this ordering. For λ ∈ [0 , 1] , we write Sλ for a non-descending ordering with respect to cλ. The two special cases, where the ordering is lexicographic with respect to (cλ, c 1) and (cλ, −c1), are denoted by S" 

> λ

and S#

> λ

. ◁

Note that elements of equal cost can always be ordered arbitrarily, but we assume the orderings we regard from now on do this consistently (as does the algorithm). As a result, orderings Sλ can only differ in their ordering of the elements in Eλ. We start by showing that S" 

> λk

= S# 

> λk+1

.

5.3 Observation. Let e, f ∈ E with c1(e) ≥ c1(f ). Additionally, let Sλ and Sλ′ be two orderings with 0 ≤ λ < λ ′ < 1.If (e, f ) ∈ P , then 

cλ(e) < c λ(f ) for all λ < λ (e, f ),cλ(e) > c λ(f ) for all λ > λ (e, f ),cλ(e) = cλ(f ) at λ = λ(e, f ).

The ordering of e and f is consistent in Sλ and Sλ′ unless the intersection point λ(e, f ) lies between λ

and λ′, that is, unless λ ≤ λ(e, f ) ≤ λ′.If (e, f ) /∈ P , then c1(e) = c1(f ) or c1(e) > c 1(f ) and c2(e) ≥ c2(f ). In the first case, cμ(e) and cμ(f )

intersect at 1. Thus, c(e) = c(f ) or cμ(e) and cμ(f ) do not intersect on [0 , 1) , meaning their ordering in Sλ and Sλ′ coincides, by our assumption on the consistent ordering of elements with identical cost. This is true in the last missing case as well, where c1(e) > c 1(f ) and c2(e) ≥ c2(f ), since here the cost functions cμ(e) and cμ(f ) do not intersect at all. In summary, elements e and f are ordered consistently by Sλ and Sλ′ unless (e, f ) ∈ P and λ ≤

λ(e, f ) ≤ λ′. ◁

5.4 Lemma. Let λ1 < . . . < λ s be the values in E, and λ0 := 0 . Then, for k ∈ { 1, . . . , s }, S" 

> λk−1

= S# 

> λk

.◁

Proof. Let e, f ∈ E with c1(e) ≥ c1(f ). By Observation 5.3, since 0 ≤ λk−1 < λ k < 1, we know that 

S" 

> λk−1

and S# 

> λk

order e and f consistently unless (e, f ) ∈ P and λk−1 ≤ λ(e, f ) ≤ λk. But, since these are two consecutive elements of E, this is the case if and only if λ(e, f ) ∈ {λk−1, λ k}.So assume that λ(e, f ) = λk > λ k−1. In this case, e occurs before f in S# 

> λk

. By Observation 5.3, 

cλk−1 (e) < c λk−1 (f ), so e occurs before f in S" 

> λk−1

as well. Analogously, if λ(e, f ) = λk−1 < λ k, then e occurs after f in S" 

> λk−1

and cλk (e) > c λk (f ), so this is the case in S" 

> λk−1

, too. □

As a next step, we show that the bases Bk that Algorithm 5.1 computes satisfy Bk = B(S" 

> λk

). For this, we need one more definition concerning matroids. 

5.5 Definition. Let M be a matroid, M = ( E, I), and E′ ⊆ E. The restriction of M to E′, denoted by M |E′, is the matroid (E′, I′) with I′ := {I ∈ I : I ⊆ E′}. If E′ is independent, then the contraction of M by E′ is the matroid (E \ E′, I′) with I′ := {I ⊆ E \ E′ : I ∪ E′ ∈ I} . We denote it by M/E ′. ◁

5.6 Lemma. Let S be an ordering of the elements of a matroid M , e ∈ E, and ˆS be the ordering S

without the element e. If e / ∈ B(S), then B ˆM ( ˆS) = B(S), where ˆM = M |(E − e). If e ∈ B(S), then 

{e} ∪ B ˆM ( ˆS) = B(S), where ˆM = M/ {e}. ◁

Proof. We show that the Greedy algorithm on M includes or rejects an element in E − e if and only if the Greedy algorithm on ˆM does. In the case that ˆM = M |(E − e), this is easy to see: since e is rejected, both algorithms ask for independence on the same sets S + e′, yielding the same answers. 

16 If e ∈ B(S), then {e} is independent and a set I is independent in ˆM = M/ {e} if and only if I + e is independent in M . Let Sk and ˆSk be the sets after Greedy regards the kth element ek of ˆS. Initially, 

S0 = ˆS0 and let k > 0. The element ek is added ˆSk−1 if and only if ˆSk−1 + e + ek is independent, in which case it is also added to Sk. Conversely, if ek is added to Sk, then Sk + ek is independent. Since 

e is included in B(S), it is added by the algorithm at some point and Sk + ek + e is independent as well. Thus, the Greedy algorithms behave the same. □

5.7 Lemma. Let λ1 < . . . < λ s be the values in E and λ0 := 0 . For k ∈ { 0, . . . , s }, the basis Bk

computed by Algorithm 5.1 satisfies Bk = B(S" 

> λk

). ◁

Proof. For B0 this is the case, since S" 

> 0

is just a lexicographic ordering by (f2, f 1). Assume it holds up to Bk−1, for some k ∈ { 1, . . . , s }, and regard Bk.The operations performed by Algorithm 5.1 in iteration k are equivalent to running Greedy on the matroid ˆM in which it all the elements in E \(Bk−1 ∪Eλk ) are removed and the elements in Bk−1 \Eλk

are contracted, that is, in ˆM = ( M |(Bk−1 ∪ Eλk )) /(Bk−1 \ Eλk ). This is a matroid on ˆE = Eλk and we use the lexicographic ordering with respect to (cλk , c 1).By Lemma 5.6, if we show that B(S" 

> λk

) does not contain the elements in E \ (Bk−1 ∪ Eλk ), then we can run Greedy on M |(Bk−1 ∪ Eλk ) instead. If we then show that B(S" 

> λk

) contains the elements in Bk−1 \ Eλk , then we may contract these elements. With these two properties we obtain that 

Bk = B(S" 

> λk

).Thus, we need to show that for e ∈ E \ (Bk−1 ∪ Eλk ), e / ∈ B(S" 

> λk

) and for e ∈ Bk−1 \ Eλk , e ∈ B(S" 

> λk

).This is equivalent to Bk−1 \ Eλk ⊆ B(S" 

> λk

) ⊆ Bk−1 ∪ Eλk , which is, in turn, equivalent to 

B(S" 

> λk

) \ Eλk = Bk−1 \ Eλk = B(S" 

> λk−1

) \ Eλk = B(S# 

> λk

) \ Eλk

where the last two equalities use the induction hypothesis and Lemma 5.4. The orderings S" 

> λk

and S#

> λk

only differ by their ordering of the sets Eλk . But exchanging two adjacent elements e and e′ in the ordering either has no effect on the result of the Greedy algorithm or it removes e and adds e′ instead. Thus, the only elements, for which a different result can be obtained, are those in Eλk . □

Now we know that we compute solutions Bk = B(S" 

> λk

) and we just need to show that these suffice. 

5.8 Lemma. Let y ∈ YESN , then y = f

(

B(S"

> λ

)

)

for some λ ∈ E ∪ { 0}. ◁

Proof. Let y ∈ YESN , then y = f (B) for B ∈ Xλ and λ ∈ (0 , 1) . We can obtain B by the Greedy algorithm using an ordering that prefers elements of B if they have equal weight cλ. Since exchanging elements of identical weight in the ordering has not effect on the objective, we may assume that 

B = B(Sλ).Let λ1 < . . . < λ s be the points in E, λ0 := 0 , λs+1 := 1 , and assume that λk−1 < λ < λ k for some 

k ∈ { 1, . . . , s + 1 }. Then cλ(e) = cλ(f ) if and only if c(e) = c(f ). Hence, Sλ is unique and, by an analogous argument to Lemma 5.4, Sλ = S" 

> λk−1

. Hence, B(Sλ) = B(S" 

> λk−1

).Now let λ = λk for some k ∈ { 1, . . . , s }. We show that f (B(Sλ)) is a convex combination of f

(

B(S#

> λ

)

)

and f

(

B(S"

> λ

)

)

. As a result, since y ∈ YESN , y ∈

{

f

(

B(S#

> λ

)

)

, f 

(

B(S"

> λ

)

)} 

. As S# 

> λ

= S" 

> λk−1

by Lemma 5.4, the claim follows. Since B(Sλ), B (S#

> λ

), B (S"

> λ

) ∈ Xλ, their images y, y #, y " all lie on the line {y′ : wTy′ = wTy}, where 

w = ( λ, 1 − λ)T. Thus, we can write y = μy # + (1 − μ)y".Since we can transform Sλ into S# 

> λ

by exchanging adjacent elements e, f with cλ(e) = cλ(f ) and 

c1(e) ≤ c1(f ), we see that c1(B(S#

> λ

)) ≥ c1(B(Sλ)) . Analogously, c1(B(S"

> λ

)) ≤ c1(B(Sλ)) . This yields 

μ ∈ [0 , 1] as required. □

17 This shows correctness. 

5.9 Theorem. Algorithm 5.1 is correct, that is, it computes a set S ⊆ XSE with f (S) = YESN . More precisely, it actually computes a set S ⊆ XESE . ◁

Proof. By Lemma 5.8, any y ∈ YESN can be written as f

(

B(S"

> λ

)

)

for λ ∈ E ∪ { 0} and by Lemma 5.7, we compute all these values. To see that each Bk computed by the algorithm is extreme-supported, we simply realise that, by Lemmas 5.4 and 5.7, Bk = B(S" 

> λk

) = B(S# 

> λk+1

), so Λ( Bk) contains at least the two elements λk < λ k+1 ,where λ0 := 0 and λs+1 := 1 . Using Lemma 2.2, we get the desired result. □

## 5.2 Implementing Algorithm 5.1 

How do we implement Algorithm 5.1 efficiently? Initially, we compute the intersection points λ(e, f )

for the pairs (e, f ) ∈ P , adding both to the set Eλ(e,f ). Combined with the sorting of the intersection points obtained, this takes O (m2 log m) time. Computing B0 is again done by the Greedy algorithm and takes time O (m · (log m + Tindep (M ))) .In the iteration for λ ∈ E , we sort a set Eλ and then test independence for each of its elements. Hence, this takes O (|Eλ| · (log |Eλ| + Tindep (M ))) time. Since each pair in P results in at most two elements being added to a set Eλ, we can conclude that ∑ 

> λ∈E

|Eλ| ∈ O (m2). Thus, these iterations take 

O (m2 · (log m + Tindep (M )) ) time, dominating our prior value. 

5.10 Theorem. Algorithm 5.1 can be implemented to run in O (m2 · (log m + Tindep (M )) ) time. ◁

We want to complete this section by providing a brief comparison to dichotomic search, which has a running time of Θ( |YESN | · m · ( log m + Tindep (M ))) [AN79]. From computational geometry [Dey98, Epp98], we know that |YESN | ∈ Θ( mr 13 ). We would thus obtain Θ( m2r 13 · ( log m + Tindep (M ))) as the running time for dichotomic search, which is worse than the running time of our algorithm. This is to be taken with a grain of salt however, since the independence tests we run differ: dichotomic search runs the Greedy algorithm while Algorithm 5.1 removes and readds elements, which can make a real difference. In the case of spanning trees, for example, doing the independence tests in the Greedy algorithm takes O (log n) time [AMO93], so it is dominated by the sorting, where the independence tests for Algorithm 5.1 require us to check the existence of a cycle, so they need O (n) time, putting dichotomic search in the lead again. 

# 6 Conclusion and Future Research 

In this paper, we presented a generic way to obtain an alternative to dichotomic search for combinatorial problems, provided a sufficiently strong notion of adjacency. Unlike dichotomic search, it operates in the decision space and, despite our use of weight set arguments, it does not need to solve any weighted sum scalarisation problem. Instead, it just needs to evaluate adjacent solutions and transition to the best one. We used this generic algorithm to solve the bi-objective minimum weight basis problem using the natural definition of adjacency. We analysed its running time and showed that it is polynomial before developing an optimised algorithm for this special case. Our algorithm can be extended to more objectives in the same way that dichotomic search is extended in [PGE10], which would yield an algorithm that is a hybrid between a decision and an image space method. An interesting question is whether it can be extended to a purely decision space method, for which multi-objective Simplex algorithms could provide an orientation. 18 Finally, it is interesting to apply the method to more combinatorial optimisation problems and compare it computationally to dichotomic search. In particular, it is interesting to see whether the optimised algorithm for the minimum weight basis problem is competitive with dichotomic search for a specific class of matroids. 

# References 

[AMO93] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin. Network Flows: Theory, Algorithms, and Applications . Englewood Cliffs, NJ: Prentice Hall, 1993. isbn : 978-0-13-617549-0. [AN79] Y. P. Aneja and K. P. K. Nair. ‘Bicriteria Transportation Problem’. Management Science 

25.1 (Jan. 1979), pages 73–78. issn : 1526-5501. doi : 10.1287/mnsc.25.1.73 .[AZ18] M. Aigner and G. M. Ziegler. Proofs from THE BOOK . Springer Berlin Heidelberg, 2018. 

isbn : 9783662572658. doi : 10.1007/978-3-662-57265-8 .[Bac26] O. Bachtler. ‘Folklore in Multi-Objective Optimisation’. Version 1 (Jan. 2026). arXiv: 

2601.15499 [math.OC] .[Baz+21] C. Bazgan et al. ‘The Power of the Weighted Sum Scalarization for Approximating Multiob-jective Optimization Problems’. Theory of Computing Systems 66.1 (Nov. 2021), pages 395– 415. issn : 1433-0490. doi : 10.1007/s00224-021-10066-5 .[Ben98] H. P. Benson. ‘An Outer Approximation Algorithm for Generating All Efficient Extreme Points in the Outcome Set of a Multiple Objective Linear Programming Problem’. Journal of Global Optimization 13.1 (Jan. 1998), pages 1–24. issn : 1573-2916. doi : 10.1023/a: 1008215702611 .[Bök+24] F. Bökler et al. ‘An outer approximation algorithm for generating the Edgeworth–Pareto hull of multi-objective mixed-integer linear programming problems’. Mathematical Methods of Operations Research 100.1 (Jan. 2024), pages 263–290. issn : 1432-5217. doi : 10.1007/ s00186-023-00847-8 .[Bru69] R. A. Brualdi. ‘Comments on bases in dependence structures’. Bulletin of the Australian Mathematical Society 1.2 (Aug. 1969), pages 161–167. issn : 1755-1633. doi : 10.1017/ s000497270004140x .[BS02] H. P. Benson and E. Sun. ‘A Weight Set Decomposition Algorithm for Finding All Efficient Extreme Points in the Outcome Set of a Multiple Objective Linear Program’. European Journal of Operational Research 139.1 (May 2002), pages 26–41. issn : 0377-2217. doi :

10.1016/s0377-2217(01)00153-9 .[Chl25] F. Chlumsky-Harttmann. ‘Robust Multi-Objective Optimization: Analysis and Algorithmic Approaches’. en. PhD thesis. 2025. doi : 10.26204/KLUEDO/8859 .[Dey98] T. K. Dey. ‘Improved Bounds for Planar k-Sets and Related Problems’. Discrete & Com-putational Geometry 19.3 (Mar. 1998), pages 373–382. issn : 0179-5376. doi : 10.1007/ pl00009354 .[Ehr05] M. Ehrgott. Multicriteria Optimization . Second edition. Lecture Notes in Economics and Mathematical Systems 491. Berlin: Springer, 2005. isbn : 978-3-540-21398-7. [Ehr96] M. Ehrgott. ‘On matroids with multiple objectives’. Optimization 38.1 (Jan. 1996), pages 73– 84. issn : 1029-4945. doi : 10.1080/02331939608844238 .[EK97] M. Ehrgott and K. Klamroth. ‘Connectedness of efficient solutions in multiple criteria combinatorial optimization’. European Journal of Operational Research 97.1 (Feb. 1997), pages 159–166. issn : 0377-2217. doi : 10.1016/s0377-2217(96)00116-6 .[ELS11] M. Ehrgott, A. Löhne, and L. Shao. ‘A Dual Variant of Benson’s Outer Approximation Algorithm for Multiple Objective Linear Programming’. Journal of Global Optimization 

52.4 (Apr. 2011), pages 757–778. issn : 1573-2916. doi : 10.1007/s10898-011-9709-y .19 [Epp98] D. Eppstein. ‘Geometric Lower Bounds for Parametric Matroid Optimization’. Discrete & Computational Geometry 20.4 (Dec. 1998), pages 463–476. issn : 0179-5376. doi : 10.1007/ pl00009396 .[EW05] M. Ehrgott and M. M. Wiecek. ‘Mutiobjective Programming’. Multiple Criteria Decision Analysis: State of the Art Surveys . Springer New York, 2005, pages 667–708. isbn : 978-0-387-23081-8. doi : 10.1007/0-387-23081-5_17 .[GKR11] J. Gorski, K. Klamroth, and S. Ruzika. ‘Connectedness of Efficient Solutions in Multiple Objective Combinatorial Optimization’. Journal of Optimization Theory and Applications 

150.3 (Apr. 2011), pages 475–497. issn : 1573-2878. doi : 10.1007/s10957-011-9849-8 .[Gor10] J. Gorski. Multiple objective optimization and implications for single objective optimization .Berichte aus der Mathematik. Aachen: Shaker, 2010. 283 pages. isbn : 9783832295899. [Hal+20] P. Halffmann et al. ‘An Inner Approximation Method to Compute the Weight Set Decom-position of a Triobjective Mixed-Integer Problem’. Journal of Global Optimization 77.4 (Mar. 2020), pages 715–742. issn : 1573-2916. doi : 10.1007/s10898-020-00898-9 .[HR94] H. W. Hamacher and G. Ruhe. ‘On spanning tree problems with multiple objectives’. 

Annals of Operations Research 52.4 (Dec. 1994), pages 209–230. issn : 1572-9338. doi :

10.1007/bf02032304 .[Ise77] H. Isermann. ‘The Enumeration of the Set of All Efficient Solutions for a Linear Multiple Objective Program’. Journal of the Operational Research Society 28.3 (Oct. 1977), pages 711– 725. issn : 1476-9360. doi : 10.1057/jors.1977.147 .[Jah85] J. Jahn. ‘Scalarization in Multi Objective Optimization’. Mathematics of Multi Objective Optimization . Springer Vienna, 1985, pages 45–88. isbn : 978-3-7091-2822-0. doi : 10.1007/ 978-3-7091-2822-0_3 .[KS25] D. Könen and M. Stiglmayr. ‘An output-polynomial time algorithm to determine all supported efficient solutions for multi-objective integer network flow problems’. Discrete Applied Mathematics 376 (Dec. 2025), pages 1–14. issn : 0166-218X. doi : 10.1016/j.dam. 2025.06.001 .[MM02] K. Miettinen and M. M. Mäkelä. ‘On scalarizing functions in multiobjective optimization’. 

OR Spectrum 24.2 (May 2002), pages 193–213. issn : 1436-6304. doi : 10.1007/s00291-001-0092-9 .[Nac78] P. H. Naccache. ‘Connectedness of the set of nondominated outcomes in multicriteria optimization’. Journal of Optimization Theory and Applications 25.3 (July 1978), pages 459– 467. issn : 1573-2878. doi : 10.1007/bf00932907 .[NW88] G. Nemhauser and L. Wolsey. Integer and Combinatorial Optimization . Wiley, June 1988. 

isbn : 9781118627372. doi : 10.1002/9781118627372 .[ÖK10] Ö. Özpeynirci and M. Köksalan. ‘An Exact Algorithm for Finding Extreme Supported Nondominated Points of Multiobjective Mixed Integer Programs’. Management Science 

56.12 (Dec. 2010), pages 2302–2315. issn : 1526-5501. doi : 10.1287/mnsc.1100.1248 .[Oxl11] J. Oxley. Matroid Theory . Second edition. Oxford University Press, Feb. 2011. isbn : 978-0-19-856694-6. doi : 10.1093/acprof:oso/9780198566946.001.0001 .[PGE06] A. Przybylski, X. Gandibleux, and M. Ehrgott. ‘The biobjective integer minimum cost flow problem—incorrectness of Sedeño-Noda and Gonzàlez-Martin’s algorithm’. Computers & Operations Research 33.5 (May 2006), pages 1459–1463. issn : 0305-0548. doi : 10.1016/j. cor.2004.11.001 .[PGE10] A. Przybylski, X. Gandibleux, and M. Ehrgott. ‘A Recursive Algorithm for Finding All Nondominated Extreme Points in the Outcome Set of a Multiobjective Integer Programme’. 

INFORMS Journal on Computing 22.3 (Aug. 2010), pages 371–386. issn : 1526-5528. doi :

10.1287/ijoc.1090.0342 .20 [Say24] S. Sayın. ‘Supported nondominated points as a representation of the nondominated set: An empirical analysis’. Journal of Multi-Criteria Decision Analysis 31.1–2 (Jan. 2024). issn :1099-1360. doi : 10.1002/mcda.1829 .[SCF04] C. G. da Silva, J. Clímaco, and J. Figueira. Geometrical configuration of the Pareto frontier of bi-criteria {0, 1}-knapsack problems . Technical report. INESC, Coimbra, Portugal, 2004. [SG54] T. Saaty and S. Gass. ‘Parametric Objective Function (Part 1)’. Journal of the Operations Research Society of America 2.3 (Aug. 1954), pages 316–319. issn : 2326-3229. doi : 10. 1287/opre.2.3.316 .21 Funding statement 

Oliver Bachtler was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Founda-tion) - GRK 2982, 516090167 ‘Mathematics of Interdisciplinary Multiobjective Optimization’ 

# Disclosure statement 

The authors report there are no competing interests to declare. 22