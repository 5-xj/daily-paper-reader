Title: Tournament Informed Adversarial Quality Diversity

URL Source: https://arxiv.org/pdf/2601.19562v1

Published Time: Wed, 28 Jan 2026 01:58:44 GMT

Number of Pages: 10

Markdown Content:
# Tournament Informed Adversarial Quality Diversity 

# TimothÃ©e Anne 

timl@itu.dk IT University of Copenhagen Copenhagen, Denmark 

# Noah Syrkis 

nobr@itu.dk IT University of Copenhagen Copenhagen, Denmark 

# Meriem Elhosni 

meriem.elhosni@ar.admin.ch armasuisse Science+Technology Thun, Switzerland 

# Florian Turati 

florian.turati@ar.admin.ch armasuisse Science+Technology Thun, Switzerland 

# Alexandre Manai 

alexandre.manai@ar.admin.ch armasuisse Science+Technology Thun, Switzerland 

# Franck Legendre 

franck.legendre@ar.admin.ch armasuisse Science+Technology Thun, Switzerland 

# Alain Jaquier 

alain.jaquier@ar.admin.ch armasuisse Science+Technology Thun, Switzerland 

# Sebastian Risi 

sebr@itu.dk IT University of Copenhagen Copenhagen, Denmark 

## Abstract 

Quality diversity (QD) is a branch of evolutionary computation that seeks high-quality and behaviorally diverse solutions to a problem. While adversarial problems are common, classical QD cannot be easily applied to them, as both the fitness and the behavior de-pend on the opposing solutions. Recently, Generational Adversarial MAP-Elites (GAME) has been proposed to coevolve both sides of an adversarial problem by alternating the execution of a multi-task QD algorithm against previous elites, called tasks. The original algorithm selects new tasks based on a behavioral criterion, which may lead to undesired dynamics due to inter-side dependencies. In addition, comparing sets of solutions cannot be done directly using classical QD measures due to side dependencies. In this paper, we (1) use an inter-variants tournament to compare the sets of solu-tions, ensuring a fair comparison, with 6 measures of quality and diversity, and (2) propose two tournament-informed task selection methods to promote higher quality and diversity at each generation. We evaluate the variants across three adversarial problems: Pong, a Cat-and-mouse game, and a Pursuers-and-evaders game. We show that the tournament-informed task selection method leads to higher adversarial quality and diversity. We hope that this work will help further advance adversarial quality diversity. Code, videos, and sup-plementary material are available at https://github.com/Timothee-ANNE/GAME_tournament_informed. 

## Keywords 

Quality-Diversity, Adversarial coevolution 

## 1 Introduction 

Quality Diversity (QD) [ 28 ] is an evolutionary computation subfield that finds a set of diverse, high-quality solutions, in a process called illumination. It has been applied to domains such as robotics [ 9], video games [20], chemical synthesis [22], or aeronautics [7]. QD can be used to illuminate adversarial problems, for which it can be critical to identify all possible attack strategies, for exam-ple, to evaluate a systemâ€™s current safety and defend accordingly. Example of applications include: video games for automatic bal-ancing of competitive games [ 17 , 18 ], generalization evaluation of Select the next              

> Blue tasks GAME
> MTMB ME
> MTMB ME
> AK
> WB
> AK
> WB
> EG
> SU
> Z
> E
> R
> T
> Y
> U
> I
> O
> P
> SD
> G
> EG
> SU
> W
> L
> M
> B
> NXKH
> J
> F
> A
> C
> Select the next
> Red tasks

Figure 1: GAME is a coevolutionary QD algorithm that illu-minates adversarial problems by alternating the execution of MTMB-ME [ 1] on a set of tasks (i.e., fixed solutions from the opposing side) to encourage arms race dynamics. For example, in this illustration, the blue letters can represent strategies for a mouse to avoid a cat, and the red letters rep-resent strategies for a cat to catch a mouse. 

machine learning models [ 30 , 33 ], and red teaming [ 19 ], i.e., finding adversarial prompts that generate harmful content [10, 27, 31, 37]. Those works illuminate only one side of the adversarial problem while fixing the other, thereby providing only partial illumination against the set of opposing problems picked by the experimenter. Generational Adversarial MAP-Elites (GAME) [ 2] has recently been proposed as a general coevolutionary adversarial QD algorithm that illuminated both sides of an adversarial problem. GAME alter-nates the illumination of each side in a sequence of generations, using Multi-Task Multi-Behavior MAP-Elites (MTMB-ME) [ 1] to il-luminate one side against a fixed set of opposing solutions selected from the previous generation, called tasks (Fig. 1). For example, in a game of Cat-and-mouse, it corresponds to finding different strategies for the cat to catch the mouse while also searching for different strategies for the mouse to avoid the cat. The task selection mechanism drives the coevolutionary process and should select tasks that present a higher diversity of challenges at each generation to expand the illumination. The original GAME uses a behavioral criterion to select tasks; we argue that, by disre-garding the adversarial aspect of the problem, it fails to yield the  

> arXiv:2601.19562v1 [cs.NE] 27 Jan 2026 Preprint, submitted to Gecco â€™26, in January 2026 Anne et al.

greatest illumination. In addition, we argue that classic quality and diversity measures are not suited to adversarial problems. In this paper, we propose (1) two new task selection methods, 

Ranking and Pareto , and compare them against the original method and a random baseline; (2) six measures of adversarial quality and diversity; and (3) the evaluation on three adversarial problems: Pong, Cat-and-mouse, and Pursuers-and-evaders. The main contributions of this paper are: 

â€¢ comparison of six measures to evaluate the quality and diver-sity of a set of solutions to an adversarial problem on three different adversarial problems; 

â€¢ comparison of two new task selection mechanisms informed by a tournament and based on ranking and Pareto front optimality for GAME, with the conclusion that getting ad-versarial information from a tournament results in higher quality and diversity than the original method. 

## 2 Problem 

We define an adversarial QD problem as a tuple ( Sğ‘…ğ‘’ğ‘‘ , Sğµğ‘™ğ‘¢ğ‘’ , F , B)with Sğ‘…ğ‘’ğ‘‘ and Sğµğ‘™ğ‘¢ğ‘’ the two opposing search spaces (they can be identical or different spaces), F is a fitness function defined by: 

F : ğ‘† Red Ã— ğ‘† Blue âˆ’â†’ [0, 1]2

(ğ‘  red , ğ‘  blue ) â†¦ â†’ (ğ‘“ red , ğ‘“ blue ) s.t. ğ‘“ red + ğ‘“ blue = 1

and B is a behavior descriptor function defined by: 

B : ğ‘† Red Ã— ğ‘† Blue âˆ’â†’ Rğ‘š 

(ğ‘  red , ğ‘  blue ) â†¦ â†’ ğ‘ .

The problem is adversarial because the fitness of one side cannot increase without the other decreasing, and because the behavior descriptor depends on both sides. This raises multiple challenges: 

â€¢ optimizing both search spaces simultaneously is tricky be-cause an increase in fitness can result from either high-quality on one side or low-quality on the other; 

â€¢ there is no intrinsic behavior descriptor for a solution. This paper proposes different measures of adversarial quality diversity corresponding to different end goals of the illumination. 

## 3 Related Work 3.1 Quality Diversity 

A QD problem is defined by a fitness function to maximize and a behavior space to cover. Novelty Search with Local Competition (NSLC) [ 24 ] and Multidimensional Archive of Phenotypic Elites (MAP-Elites) [25] are two of the main QD algorithms. MAP-Elites discretizes the behavior space into cells and keeps updated the solution with maximal fitness for each cell, called the elite, forming an archive of high-quality diverse solutions. At each iteration, MAP-Elites generates a new solution candidate from the archiveâ€™s elites using a variation operator, evaluates it, and adds it to the archive if the new solution belongs to an empty cell (i.e., new behavior) or has a greater fitness than the current elite of its cell. Multi-Task MAP-Elites (MT-ME) [ 26 ] is a variant of MAP-Elites that tackles multi-task problems by simultaneously searching for the optimal solution for each task, where a task is defined by a specific behavior and fitness function. The intuition is that simi-lar tasks should have similar solutions, enabling greater sample efficiency than optimizing each task individually. MTMB-ME [ 1] is the QD extension of MT-ME and simultane-ously searches for a set of diverse, high-quality solutions for each task in a multi-task problem. GAME [ 2] leverages MTMB-MEâ€™s abil-ity to solve multi-task QD problems to search for a set of diverse solutions that perform well against each fixed opposing solution of the current generations, i.e., the tasks. 

## 3.2 Quality Diversity for Adversarial Problems 

QD algorithms are increasingly applied to adversarial problems, for which illuminating the set of high-performing solutions is impor-tant. It has been applied in video games to illuminate possible decks in a card game [ 17 ] or policies for playing those decks [ 18 ], which helps estimate the gameâ€™s balance. It can also be used to assess procedural content generation; for example, [ 33 ] use MAP-Elites to examine the levels generated by different GANs in the Lone Runner game. MADRID [ 30 ] applies MAP-Elites to find adversarial scenar-ios and evaluate the generalization capacities of a reinforcement learning (RL) agent in the Google Research Football environment. In recent years, QD has been highlighted for its efficiency in red teaming with numerous algorithms, such as Rainbow Teaming [ 31 ], Quality-Diversity Red-Teaming [ 37 ], and RAINBOWPLUS [ 10 ]. [ 27 ]also uses MAP-Elites to illuminate the space of possible adversarial attacks on a text-to-image generative model. Those methods address sample-efficiency challenges specific to LLMs, but compared to GAME, they only optimize one side of the adversarial problem. Due to the difficulty of maintaining an adver-sarial coevolutionary process [ 16 ], few methods attempt to search both sides of the adversarial problem. We can identify two types of such methods: self-play, which comes from RL, and adversarial coevolution, which comes from evolutionary computation. 

## 3.3 Self-Play 

Self-play [ 38 ] is an RL method that consists of training the agent against itself or previous versions of itself. It has enabled surpass-ing human performance in multiple adversarial domains, such as Chess [ 32 ], GO [ 32 ], Dota 2 [ 5 ], and StarCraft [ 34 ]. [ 3] uses adver-sarial self-play to train RL agents in a 3D multi-agent hide-and-seek game. They show that it allows the successive emergence of mul-tiple strategies on each side, showcasing an artificial arms race dynamic. Digital Red Queen (DRQ) [ 23 ] is a self-play algorithm that leverages LLMs to evolve assembly scripts that compete for survival in a memory-bound virtual machine. DRQ sequentially evolves a solution that competes against all previous elite solutions, leveraging MAP-Elites at each generation to preserve diversity. DRQ is interested in an adversarial environment where a solution competes against multiple others to survive, while GAME is, in a sense, more general, as it can be applied to any adversarial problem, but is currently restricted to only two-opponent problems. While all those agents benefit from training against a diversity of opponents, self-play seeks to find the most robust solutions rather than all possible ways to beat an opponent. Tournament Informed Adversarial Quality Diversity Preprint, submitted to Gecco â€™26, in January 2026 

## 3.4 Adversarial Coevolution 

Adversarial coevolution is one of the main challenges of artificial life [ 4 , 14 ]. One of its goal is to create an open-ended artificial process by following a virtuous circle of arms race dynamics [11]. Some adversarial coevolution algorithms focus on coevolving RL agents on one side and the environments and tasks they must solve on the other [ 8, 15 , 35 , 36 ]. The idea is to generate a curriculum as an open-ended sequence of environments that are neither too easy nor too hard, which enables the discovery of necessary stepping stones. Enhanced-POET [ 36 ] introduces the Performance of All Trans-ferred Agents â€“ Environment Comparison (PATA-EC) measure of adversarial diversity to select interesting environments. It runs a round-robin tournament to collect fitness scores for all agents across all environments and uses the resulting ranking vector to compare environments. The idea is that diverse environments should pose di-verse challenges, i.e., yield different rankings of the current agents. In this paper, we draw inspiration from PATA-EC to propose a task selection mechanism for GAME and a general measure of adversar-ial diversity that is not specific to agent-environment problems. In comparison to GAME, those works search for a robust agent by generating diverse environments to train on, but do not explicitly aim to illuminate all possible agent behaviors or environments. 

## 3.5 Adversarial Coevolutionary QD 

Few works tackle the adversarial coevolutionary QD problem. [ 13 ]uses a self-play QD algorithm to coevolve Python scripts using LLMs in a pursuer-and-evader game (similar to Cat-and-mouse presented in this paper). An LLM estimates the genotypic diversity of scripts by comparing their embedding vectors, and uses an NSLC-type unstructured archive to store the different solutions, requiring a hard-to-set novelty threshold. GAME [2] is a recent adversarial coevolutionary QD algorithm that leverages MTMB-ME to coevolve both sides of an adversarial problem. It is a general QD algorithm applicable to any two-sided adversarial problem (symmetric or asymmetric). GAME sequen-tially iterates the illumination of each side by selecting a set of elites from the previous generation and fixing them as tasks that the current generation competes against. In its original version, those tasks are selected based on a behavioral diversity and quality criterion. We argue that while being computationally cheap (as it directly reuses the evaluation performed by MT-ME), it disregards the adversarial aspect of the problem. In this paper, we propose and compare two new task selection mechanisms that make in-formed decisions based on a tournament, thereby improving the final quality and diversity of the solutions. Another limitation of current adversarial QD is the difficulty of using classic QD measures of quality and diversity, such as Behavior Coverage, max Fitness, and QD-Score, in adversarial problems, as both behaviors and fitness depend on the opposing side. Taking the maximum fitness of one side can easily correspond to selecting the worst solution on the opposing side, providing little information about the quality of the solutions. Behavior Coverage can still be used to compare two runs on the same problem, but it doesnâ€™t really answer the question: "Do I have many different solutions for both sides?" because the diversity could arise from one side rather than both. There is a need for specific measures of adversarial quality diversity. This paper proposes six of them. 

## 4 Method 4.1 Generational Adversarial MAP-Elites 

Algorithm 1 GAME 

Inputs : Red search space Sğ‘…ğ‘’ğ‘‘ , Blue search space Sğµğ‘™ğ‘¢ğ‘’ 

Parameters : ğ‘ ğ‘”ğ‘’ğ‘› , ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ , Tasks _Selection                                         

> 1: ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  â†Sample ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ random Blue solutions âŠ²(a)
> 2: B=âˆ…âŠ²For storing bootstrapping evaluations
> 3: for ğ‘”ğ‘’ğ‘› _ğ‘–ğ‘‘ =1in ğ‘ ğ‘”ğ‘’ğ‘› do
> 4: Sâ† S ğ‘…ğ‘’ğ‘‘ if ğ‘”ğ‘’ğ‘› _ğ‘–ğ‘‘ is odd else Sğµğ‘™ğ‘¢ğ‘’
> 5: Aâ†MTMB-ME (ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘ , ğ‘†, B)âŠ²(b-f) - Alg. 2
> 6: ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘ , Bâ†Tasks _Selection (A,ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  )âŠ²(g) - Alg. 4 and 5
> 7: return ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘ 

Algorithm 2 MTMB-ME with growing archive 

Inputs : ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  , search space ğ‘† , bootstrap set B

Parameters : Number of evaluations ğ‘ ğ‘ğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ , Number of initial ran-dom search ğ‘ ğ‘–ğ‘›ğ‘–ğ‘¡ , evaluation function Evaluate , variation operator 

Variation , VEM                                                                            

> 1: Aâ†Initialize ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ growing archives âŠ²(b)
> 2: for (ğ‘¡ğ‘ğ‘ ğ‘˜, ğ‘ , ğ‘“ , ğ‘ )in Bdo âŠ²(g) - Bootstrapping
> 3: Aâ†Update (A[ğ‘¡ğ‘ğ‘ ğ‘˜ ], ğ‘ , ğ‘“ , ğ‘ )âŠ²Alg. 3
> 4: for ğ‘– =1to ğ‘ ğ‘ğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ do âŠ²Main loop
> 5: ğ‘¡ğ‘ğ‘ ğ‘˜ â†Select a task at random from ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  âŠ²(c)
> 6: if Ahas fewer than ğ‘ ğ‘–ğ‘›ğ‘–ğ‘¡ elites then
> 7: ğ‘  â†Sample a random solution from ğ‘† âŠ²(d)
> 8: else
> 9: ğ‘  â†Variation (A)âŠ²(d)
> 10: ğ‘“ , ğ‘ â†Evaluate (ğ‘ , ğ‘¡ğ‘ğ‘ ğ‘˜ )âŠ²where Evaluate uses Fand B(e)
> 11: Aâ†Update (A[ğ‘¡ğ‘ğ‘ ğ‘˜ ], ğ‘ , ğ‘“ , ğ‘ )âŠ²(f) - Alg. 3
> 12: return Archives

GAME proceeds as follows: 

(a) randomly sample ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ Blue solutions as tasks; 

For ğ‘ ğ‘”ğ‘’ğ‘› generations: (b) initialize a multi-task multi-behavior growing archive with 

ğ‘ ğ‘ğ‘’ğ‘™ğ‘™ for each task; 

For ğ‘ ğ‘ğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ evaluations: (c) select a task at random 

(d) randomly picked elites from the whole archive to generate a candidate solution ğ‘  using a variation operator; 

(e) evaluate ğ‘  against the ğ‘¡ğ‘ğ‘ ğ‘˜ to collect the fitness ğ‘“ and be-havior ğ‘ ;

(f) update the taskâ€™s archive (Alg. 3): (1) if the new behavior 

ğ‘ is farther from all the current cellsâ€™ centroids than the closest pair of centroids, it is added to the archive and one of the two centroids from the pair is removed; (2) else if the new fitness ğ‘“ is greater than the corresponding cellâ€™s fitness, then the new solution becomes the elite of its cell; 

(g) select ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ elites from the whole archive (of size ğ‘ ğ‘ğ‘’ğ‘™ğ‘™ Â· ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ )to be the next generation of tasks (comparing different such Preprint, submitted to Gecco â€™26, in January 2026 Anne et al. (a.3) Select the Elite of each cluster                                                                  

> How to select the Red Tasks for the next generation?
> (a.5) Bootstrap the next Blue generation
> MT-MB MAP-Elites
> AK
> WB
> Z
> E
> R
> T
> Y
> U
> I
> O
> P
> SD
> G
> ED
> GU
> (a.4) Perform a tournament between the former Blue
> Tasks and the new Red Tasks
> EDGU
> AKWB
> ED
> GU
> K
> A
> W
> B
> A
> B
> A
> K
> B
> WK
> W
> (a.1) Aggregate the
> Red elites
> (a.2) Cluster with K-means given their Behavior
> E
> U
> D
> T
> O
> S
> R
> P
> G
> Z
> Y
> I
> (b.1) Perform a tournament between the Blue Tasks and all the Red Elites
> AKWBZ[0.8, 0.6, 0.2, 0.5]
> I[0.5, 0.8, 0.6, 0.7]
> â ‡â ‡
> O[0.5, 0.3, 0.7, 0.4]
> A K W B Z[4, 3, 1, 2]
> I[1, 4, 2, 3]
> â ‡â ‡
> O[3, 1, 4, 2]
> AKWBZ[ 1, 0.33, -1, -0.33]
> I[-1, 1, -0.33, 0.33]
> â ‡â ‡
> O[0.33, -1, 1, -0.33]
> (c.4) Select the Elite of each cluster and bootstrap the next Blue generation
> (c.3) Cluster with K-means given their Ranking vector
> (b.2) Compute the Tournamentâ€™s fitness matrix
> (c.1) Compute the ranking vector of each Red Elites
> (c.2) Normalize between -1 and 1
> E
> U
> D
> T
> O
> S
> R
> P
> G
> Z
> Y
> I
> IR
> TS
> B
> A
> B
> A
> K
> B
> WK
> W
> (d.1) Compute the Pareto front
> by modeling each Blue Tasks as a multi-objective problem
> E
> U
> D
> T
> O
> S
> R
> P
> G
> Z
> Y
> I
> E
> U
> D
> T
> O
> S
> R
> P
> G
> Z
> Y
> I
> (d.2) Bootstrap the next
> Blue generation
> IE
> ZS
> B
> A
> B
> A
> K
> B
> W
> W
> K
> (a) Behavior Task Selection
> (c) Ranking Task Selection
> (d) Pareto Task Selection
> (b) Tournament Informed Task Selection
> Previous Blue Tasks:
> {A, K, W, B}
> Current Red Elites :
> {Z, I, Y, P, G, R, E, D, U, S, T, O}
> ZIYPGREDUSTO
> AKWB
> Fitness against A
> Fitness against K
> K
> K
> A
> W
> K
> A
> W

Figure 2: To illuminate an adversarial problem, GAME selects elites from the previous generation and sets them as opposing tasks for the next generation, which should represent a diversity of challenges. (a) In its original version, GAME selects tasks using a behavioral criterion. (b) In this paper, we propose two new methods that use an adversarial criterion by being informed by a tournament between the previous tasks and all elites: (c) Ranking, which selects a set of solutions that present a diversity of challenges, the idea being that different challenges should creates different rankings for the opposing side, and (d) Pareto, which selects a Pareto Front of solutions by considering the current tasks as a multi-objective problem. 

methods is one of the contributions of this paper and is pre-sented in the next section); 

(h) bootstrap the next generation with the evaluations resulting from a tournament between the new and previous tasks. Algorithms 1-3 detail GAMEâ€™s implementation. We follow the original implementation, which uses an unstructured archive (i.e., growing a CVT discretization of the behavior space over the itera-tions), providing a general way to build an archive while specifying only the number of cells and not boundaries or a distance threshold (which can be tricky to set in high-dimensional spaces). 

## 4.2 How to select the generation of tasks? 

The task selection mechanism guides coevolution in a virtuous circle by proposing diverse and challenging opponents from the previous generationâ€™s elites to evolve the next generation. We first present the original method, Behavior , and a random baseline, 

Random , before the two new methods, Ranking and Pareto .Fig. 2 illustrates: (a) Behavior , (b) the task selection tournament, (c) Ranking , and (d) Pareto .

4.2.1 Behavior. The original GAME selects tasks based on a behav-ioral quality and diversity criterion (Alg. 4). (a.1) It aggregates all elites using behavior collected from MTMB-MEâ€™s evaluations, (a.2) then recomputes an archive as if they were from the same behavior space using ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ cells, and (a.3) then selects the elite of each cell, ignoring that they were evaluated on different tasks. This raises multiple issues: (1) the behavior aggregation does not take into account that the behavior is task dependent, and (2) taking the elite of each new cell may favor elites originating from â€œeasierâ€ tasks. One benefit of this method is that the task selection does not require additional computation, and the bootstrap tournament is of minimal size ğ‘ 2 

> ğ‘¡ğ‘ğ‘ ğ‘˜

<< ğ‘ ğ‘ğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ .

4.2.2 Random. As a baseline, we compare with selecting elites at random, which also requires no additional evaluation. 

4.2.3 Ranking. We propose a selection mechanism inspired by PATA-EC that uses the previous tasks to estimate, via a tournament, the elites that will propose diverse challenges (Alg. 5). It (b.1) performs a tournament between all the elites and the previous tasks (size ğ‘ 2 

> ğ‘¡ğ‘ğ‘ ğ‘˜

Â· ğ‘ ğ‘ğ‘’ğ‘™ğ‘™ ) to (b.2) collect the fitness vector, (c.1) compute the ranking vector of the different tasks for each elite, (c.2) normalize this ranking, (c.3) uses this as an adversarial behavior descriptor to cluster all the elites in ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ cells, and (c.4) select one elite for each cells, using the average fitness over all tasks as quality criteria. Note that all the elites have been evaluated against all tasks, so the comparison is fairer than with Behavior . Finally, for the bootstrapping, only the evaluations from the selected elites are Tournament Informed Adversarial Quality Diversity Preprint, submitted to Gecco â€™26, in January 2026 

Algorithm 3 Growing unstructured archive update 

Inputs : archive ğ´ , solution ğ‘  , fitness ğ‘“ , behavior ğ‘ 

Parameters : max archive size ğ‘ ğ‘ğ‘’ğ‘™ğ‘™ , distance function dist  

> 1:

(ğ¶, ğ¸, ğ¸ ğ‘ğ‘ğ‘ğ‘˜ğ‘¢ğ‘ ) = ğ´ âŠ² Centroids, Elites, and Backup Elites  

> 2:

if size (ğ¶ ) < ğ‘ ğ‘ğ‘’ğ‘™ğ‘™ then âŠ² Add a new cell  

> 3:

ğ‘– = size (ğ¸ ) 

> 4:

ğ¶ [ğ‘– ] = ğ‘  

> 5:

ğ¸ [ğ‘– ] â† ( ğ‘ , ğ‘“ , ğ‘ ) 

> 6:

ğ¸ ğ‘ğ‘ğ‘ğ‘˜ğ‘¢ğ‘ [ğ‘– ] â† [ ( ğ‘ , ğ‘“ , ğ‘ ) ]  

> 7:

else âŠ² Check behavior and fitness  

> 8:

ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘  = {dist (ğ¶ [ğ‘– ], ğ¶ [ ğ‘— ] ) } 0â‰¤ğ‘– < ğ‘— <ğ‘ ğ‘ğ‘’ğ‘™ğ‘™  

> 9:

ğ‘‘ ğ‘šğ‘–ğ‘› = min (ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘  ) 

> 10:

ğ‘‘ = min {dist (ğ‘, ğ¶ [ğ‘– ] ) } 0â‰¤ğ‘– <ğ‘ ğ‘ğ‘’ğ‘™ğ‘™  

> 11:

ğ‘–ğ‘‘ = find _cell (ğ¶, ğ‘ ) âŠ² Closest centroidâ€™s index  

> 12:

if ğ‘‘ > ğ‘‘ ğ‘šğ‘–ğ‘› then âŠ² New enough behavior = growth  

> 13:

ğ‘—, ğ‘˜ â† argmin (ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘  ) 

> 14:

ğ‘‘ ğ‘— â† min {dist (ğ¶ [ ğ‘— ], ğ¶ [ğ‘– ] ) } 0â‰¤ğ‘– â‰ ğ‘— <ğ‘ ğ‘ğ‘’ğ‘™ğ‘™  

> 15:

ğ‘‘ ğ‘˜ â† min {dist (ğ¶ [ğ‘˜ ], ğ¶ [ğ‘– ] ) } 0â‰¤ğ‘– â‰ ğ‘˜ <ğ‘ ğ‘ğ‘’ğ‘™ğ‘™  

> 16:

ğ‘˜ â† ğ‘— if ğ‘‘ ğ‘— < ğ‘‘ ğ‘˜ else ğ‘˜  

> 17:

ğ¶ [ğ‘˜ ] â† ğ‘  

> 18:

ğ¸ [ğ‘˜ ] â† ( ğ‘ , ğ‘“ , ğ‘ ) 

> 19:

ğ¸ ğ‘ğ‘ğ‘ğ‘˜ğ‘¢ğ‘ [ğ‘˜ ] â† [ ( ğ‘ , ğ‘“ , ğ‘ ) ]  

> 20:

for ğ‘– = 0 to ğ‘ ğ‘ğ‘’ğ‘™ğ‘™ âˆ’ 1 do âŠ² Check and repair holes  

> 21:

if find _cell (ğ¶, ğ¸ [ğ‘– ].ğ‘ ) â‰  ğ‘– then  

> 22:

ğ¸ [ğ‘– ] â† ğ¸ ğ‘ğ‘ğ‘ğ‘˜ğ‘¢ğ‘ [ğ‘– ] 

> 23:

else if f > E[ ğ‘–ğ‘‘ ].f then âŠ² Better fitness  

> 24:

ğ¸ [ğ‘–ğ‘‘ ] â† ( ğ‘ , ğ‘“ , ğ‘ ) 

> 25:

ğ¸ ğ‘ğ‘ğ‘ğ‘˜ğ‘¢ğ‘ [ğ‘–ğ‘‘ ]. append ( ( ğ‘ , ğ‘“ , ğ‘ ) )  

> 26:

return (ğ¶, ğ¸, ğ¸ ğ‘ğ‘ğ‘ğ‘˜ğ‘¢ğ‘ )

Algorithm 4 Behavior Task Selection 

Inputs : multi-task archive A, previous tasks ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  ğ‘œğ‘™ğ‘‘ 

Parameters : ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜  

> 1:

ğµğ‘’â„ğ‘ğ‘£ğ‘–ğ‘œğ‘Ÿğ‘  â† Aggregate Aâ€™s elitesâ€™ behaviors  

> 2:

Clusters â† K-means (ğµğ‘’â„ğ‘ğ‘£ğ‘–ğ‘œğ‘Ÿğ‘ , ğ‘˜ = ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ ) âŠ² Cluster the elites  

> 3:

ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  â† { Elite (ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ ) } ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ âˆˆğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘  âŠ² Select the elite of each  

> 4:

B â† ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  versus ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  ğ‘œğ‘™ğ‘‘ âŠ² Tournament  

> 5:

return ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  , B

used, meaning that most of the evaluations of the tournament (i.e., 

ğ‘ 2 

> ğ‘¡ğ‘ğ‘ ğ‘˜

Â· ( ğ‘ ğ‘ğ‘’ğ‘™ğ‘™ âˆ’ 1)) are not repurposed. 

Algorithm 5 Ranking Task Selection 

Inputs : archive A, previous ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  ğ‘œğ‘™ğ‘‘ 

Parameters : ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜  

> 1:

ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’ğ‘  â† Aggregate Aâ€™s elites âŠ² |ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’ğ‘  | = ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ Â· ğ‘ ğ‘ğ‘’ğ‘™ğ‘™  

> 2:

T â† ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’ğ‘  versus ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  ğ‘œğ‘™ğ‘‘ âŠ² (b.1) Tournament  

> 3:

ğ‘“ ğ‘’ = ( T [ ğ‘’ vs t ].ğ‘“ )ğ‘¡ âˆˆğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  ğ‘œğ‘™ğ‘‘ âˆˆ Rğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ âŠ² (b.2) Fitness vector  

> 4:

ğ‘Ÿ ğ‘’ = argsort (argsort (fe ) ) âŠ² (c.1) Ranking vector âˆ— 

> 5:

ğ‘Ÿ ğ‘’ = 2Â·ğ‘Ÿ ğ‘’   

> ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ âˆ’1

âˆ’ 1 âŠ² (c.2) Normalize between -1 and 1  

> 6:

ğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘  â† K-means ( { ğ‘Ÿ ğ‘’ }ğ‘’ âˆˆğ¸ğ‘™ğ‘–ğ‘¡ğ‘’ğ‘  , ğ‘˜ = ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ ) âŠ² (c.3) Cluster  

> 7:

ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  â† { Elite (ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ )ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ âˆˆğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘  } âŠ² (c.4) Select elites  

> 8:

B â† evaluations from T that includes elites from ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘   

> 9:

return ğ‘‡ ğ‘ğ‘ ğ‘˜ğ‘  , B 

> âˆ—

argsort 2 = inverse permutation of the sorted indices = the ranking 

4.2.4 Pareto. We can view the fitness vector from the tournament as a set of ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ objectives to optimize. Searching for a diverse set of challenges corresponds to selecting the elites from the Pareto front of the multi-objective optimization. To do so, we use NSGA-III [ 12 ] to select ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ elites from the successive Pareto fronts of the tournamentâ€™s fitness vectors. 

## 4.3 Measures of adversarial QD 

We propose six measures of quality and diversity for adversarial QD. They are all computed from an inter-variant tournament (which differs from the bootstrapping or the task selection tournaments), in which the last generation of tasks for each variant is selected and evaluated against one another. This has the advantage of eval-uating all solutions against the same set of opponents at the cost of additional evaluations (i.e., (ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ Â· ğ‘ ğ‘Ÿğ‘’ğ‘ Â· ğ‘ ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘¡ )2). Without loss of generality, in the following formulation, we measure the quality and diversity of a set ğ‘† ğ‘Ÿğ‘’ğ‘‘ of solutions from one variant against the aggregate set ğ‘† ğ‘ğ‘™ğ‘¢ğ‘’ of solutions from all variants. And use ğ‘“ ğ‘  ğ‘Ÿğ‘’ğ‘‘ ,ğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ as the fitness of the solution ğ‘  ğ‘Ÿğ‘’ğ‘‘ âˆˆ ğ‘† ğ‘Ÿğ‘’ğ‘‘ against the solution ğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ âˆˆ ğ‘† ğ‘ğ‘™ğ‘¢ğ‘’ .

4.3.1 Win rate. â€œDid it find a solution that wins against most oppo-nents?â€ As the adversarial fitness as the propriety ğ‘“ red + ğ‘“ blue = 1, we can set a winning threshold at 0.5 and compute the win rate of each solution ğ‘  ğ‘Ÿğ‘’ğ‘‘ as ğ‘¤ğ‘–ğ‘› ğ‘Ÿğ‘ğ‘¡ğ‘’ (ğ‘  ğ‘Ÿğ‘’ğ‘‘ ) = 1 

> |ğ‘† ğ‘ğ‘™ğ‘¢ğ‘’ |

Ã 

> ğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ âˆˆğ‘† ğ‘ğ‘™ğ‘¢ğ‘’

1ğ‘“ ğ‘  ğ‘Ÿğ‘’ğ‘‘ ,ğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ >0.5,and define 

Win rate (ğ‘† ğ‘Ÿğ‘’ğ‘‘ ) = max  

> ğ‘  ğ‘Ÿğ‘’ğ‘‘ âˆˆğ‘† ğ‘Ÿğ‘’ğ‘‘

ğ‘¤ğ‘–ğ‘› ğ‘Ÿğ‘ğ‘¡ğ‘’ (ğ‘  ğ‘Ÿğ‘’ğ‘‘ ).

This is a measure of quality that can also be interpreted as an evaluation of the sidesâ€™ balance. 

4.3.2 ELO Score. â€œDid it find a solution that wins against strong opponents?â€ Similarly, to Win rate , we compute from the tour-namentâ€™s winnings the ELO score of each solution. To ease the comparison, we compute the ranking resulting from this ELO score among all solutions of the different compared variants, normalize between 0% (lowest ELO score rank) and 100% (highest ELO score rank), and pick the highest rank: 

ELO Score (ğ‘† ğ‘Ÿğ‘’ğ‘‘ ) = max  

> ğ‘  ğ‘Ÿğ‘’ğ‘‘ âˆˆğ‘† ğ‘Ÿğ‘’ğ‘‘

ğ¸ğ¿ğ‘‚ _ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ _ğ‘Ÿğ‘ğ‘›ğ‘˜ (ğ‘  ğ‘Ÿğ‘’ğ‘‘ ).

Compared to Win rate , ELO Score corrects potential imbalance of the opposing side, by giving more value to winning against a few strong opponents than winning against numerous weak opponents. It can be seen as a robust estimation of quality for comparing variants, but cannot be directly interpreted. 

4.3.3 Robustness. â€œDid it find a solution with no weakness?â€ We de-fine the robustness of a solution as its worst fitness: ğ‘Ÿğ‘œğ‘ğ‘¢ğ‘ ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘  (ğ‘  ğ‘Ÿğ‘’ğ‘‘ ) =

min  

> ğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ âˆˆğ‘† ğ‘ğ‘™ğ‘¢ğ‘’

ğ‘“ ğ‘  ğ‘Ÿğ‘’ğ‘‘ ,ğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ and: 

Robustness (ğ‘† ğ‘Ÿğ‘’ğ‘‘ ) = max  

> ğ‘  ğ‘Ÿğ‘’ğ‘‘ âˆˆğ‘† ğ‘Ÿğ‘’ğ‘‘

ğ‘Ÿğ‘œğ‘ğ‘¢ğ‘ ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘  (ğ‘  ğ‘Ÿğ‘’ğ‘‘ ).

This is a different measure of quality that emphasizes having no weaknesses rather than being good against good solutions ( ELO Score ) or being good against most solutions ( Win rate ). Preprint, submitted to Gecco â€™26, in January 2026 Anne et al. 

4.3.4 Coverage. â€œDoes the set propose a diverse set of challenges?â€ 

Inspired by Ranking , we propose a measure of adversarial diversity that uses the normalized ranking vector of each solution as an adversarial behavioral descriptor of dimension the number of total solutions of the opposing sides ( ğ‘ ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘¡ Â· ğ‘ ğ‘Ÿğ‘’ğ‘ğ‘™ğ‘–ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› Â· ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ ). We cluster all the solutions ğ‘† ğ‘Ÿğ‘’ğ‘‘ with K-means ( ğ‘˜ = ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ ), and then count the percentage of cells filled by solutions from ğ‘† ğ‘Ÿğ‘’ğ‘‘ as a measure of adversarial coverage: 

Coverage (ğ‘† ğ‘Ÿğ‘’ğ‘‘ ) = |{ ğ¶ ğ‘  ğ‘Ÿğ‘’ğ‘‘ }ğ‘  ğ‘Ÿğ‘’ğ‘‘ âˆˆğ‘† ğ‘Ÿğ‘’ğ‘‘ ||ğ‘† ğ‘Ÿğ‘’ğ‘‘ | where ğ¶ ğ‘  ğ‘Ÿğ‘’ğ‘‘ is ğ‘  ğ‘Ÿğ‘’ğ‘‘ â€™s cluster .

This is a measure of adversarial diversity that should be maximal if the set of solutions covers all possible challenges. 

4.3.5 Expertise. â€œDoes the set contain counter-solutions?â€ A diverse set of solutions should be diverse enough that for any opponent, there is at least one solution of the set that is strong against it. In other words, the set should contain a counter-solution for any opposing solution. To measure that, we compute the highest fitness obtained against each opposing solution and then pick the lowest of those, which corresponds to the worst counter-solution: 

Expertise (ğ‘† ğ‘Ÿğ‘’ğ‘‘ ) = min  

> ğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ âˆˆğ‘† ğ‘ğ‘™ğ‘¢ğ‘’

max  

> ğ‘  ğ‘Ÿğ‘’ğ‘‘ âˆˆğ‘† ğ‘Ÿğ‘’ğ‘‘

ğ‘“ ğ‘  ğ‘Ÿğ‘’ğ‘‘ ,ğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ .

This is a measure of adversarial quality diversity as it requires both having covered all relevant strategies with high-quality solutions. 

4.3.6 Adversarial QD-Score. â€œDoes the set propose strictly different challenges?â€ In the opposite of the Expertise , a diverse and high-quality set of solutions should require a large number of different opposing solutions for each of its solutions to lose. We thus compute the size of the smallest set of counter-solutions needed to make each solution lose at least once: 

AQD-Score (ğ‘† ğ‘Ÿğ‘’ğ‘‘ ) = min    

> ğ‘† ğ‘ğ‘™ğ‘¢ğ‘’ âŠ†ğ‘† ğ‘ğ‘™ğ‘¢ğ‘’ ,

|ğ‘† ğ‘ğ‘™ğ‘¢ğ‘’ |

subject to âˆ€ğ‘  ğ‘Ÿğ‘’ğ‘‘ âˆˆ ğ‘† ğ‘Ÿğ‘’ğ‘‘ , âˆƒğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ âˆˆ ğ‘† ğ‘ğ‘™ğ‘¢ğ‘’ such that ğ‘“ ğ‘  ğ‘Ÿğ‘’ğ‘‘ ,ğ‘  ğ‘ğ‘™ğ‘¢ğ‘’ < 0.5.

## 5 Experiments 5.1 Environments 

To evaluate both the task selection methods and the measures of adversarial quality diversity, we designed three increasingly complex adversarial problems. For all of them, the search space is the set of weights for a small MLP with hidden layers of 32 and 16 neurons. The variation operator used is a mutation that applies Gaussian noise with standard deviation 0.1 to 30% of the weights. For the behavior space, we follow the original GAME and use a VEM (CLIP [ 29 ]) that embeds a visualization of the adversarial problem (described in the following sections). All problems are implemented in JAX [6] to facilitate parallel evaluation on a GPU. 

5.1.1 Pong. We implemented the two-player Pong game (Fig. 3.a), in which the ballâ€™s speed increases by 5% after each successful rebound to increase difficulty . Each side, left and right, scores one point at each successful ball that the opponent does not rebound, after which the ball resets randomly at the center of the arena. The MLPsâ€™ input is the concatenation of the ballâ€™s position and velocity, and the vertical positions of the paddles ( ğ· = 6), each normalized between -1 and 1. The MLPsâ€™ output is the vertical velocity of the paddle (output of tanh multiplied by the paddle (a) Pong (c) Cat-and-mouse (e) Pursuers-and-evaders   

> One-Frame Visualization
> (b) (d) (f)

Figure 3: Pretty (top) and one-frame (bottom) visualization of the adversarial problems. (a-b) Pong, (c-d) Cat-and-mouse, and (e-f) Pursuers-and-evaders. The one-frame visualizations are from duels between the two solutions with the highest ELO score from the inter-variant tournament. Videos of those duels are available in the supplementary material. 

velocity of 2% of the arena height per time step). The fitness function is the ratio of points scored to the total points from both sides after 1000 steps. It is bounded by 0 and 1 (we set 0.5 if no points have been scored to represent a tie). The one-frame visualization shows the ball position at each time step, with colors changing after a point is scored (Fig. 3.b). This adversarial problem is symmetrical. Ties can easily occur because either side scores no points (often) or both sides score the same number of points (rarely). 

5.1.2 Cat-and-mouse. Cat-and-mouse (Fig. 3.c) is an implementa-tion of the Homicidal Chauffeur problem [ 21 ] with neural-network controllers. In this adversarial problem, the cat, in red, is fast ( ğ‘£ ğ‘ğ‘ğ‘¡ =

2 ms âˆ’1 but cannot turn easily (i.e., its angular velocity is bounded so that the radius of the smallest full turn it can do is ğ‘Ÿ ğ‘šğ‘–ğ‘› = 1 m )and is chasing a mouse, in blue, which is slower ( ğ‘£ ğ‘ğ‘ğ‘¡ = 1 ms âˆ’1) but is agile (i.e., it can change direction instantly). The MLPsâ€™ input is the concatenation of the catâ€™s position and angular direction, and the mouseâ€™s position ( ğ· = 5). The MLPsâ€™ output is the new angular direction (tanh output) multiplied by  

> ğ‘£ ğ‘ğ‘ğ‘¡
> ğ‘Ÿ ğ‘šğ‘–ğ‘›

= 2 rad s âˆ’1 for the cat and ğœ‹ for the mouse. The duel lasts until the cat catches the mouse (i.e., the distance between the two is lower than a threshold of 0.2 m ) or a maximum number of timesteps of fitness ğ‘‡ ğ‘šğ‘ğ‘¥ = 5 s (i.e., 500 timesteps). If the cat catches the mouse, the fitness is 1 minus the ratio of the time to catch the mouse, scaled between 0.5 and 1, so that a value of 1 means catching the mouse at ğ‘¡ = 0 and a value of 0.5 at ğ‘¡ = ğ‘‡ ğ‘šğ‘ğ‘¥ . If the cat doesnâ€™t catch the mouse, the fitness is the closest distance between the two, normalized by the initial distance and the catching threshold to be between 0 and 0.5, 0 meaning the cat was the closest at the start, and 0.5 that the cat was exactly at the distance threshold, so that the fitness function is continuous between the two mode. The one-frame visualization shows the positions of the cat (red) and Tournament Informed Adversarial Quality Diversity Preprint, submitted to Gecco â€™26, in January 2026 

Table 1: Variants comparison of the different measures in Pong 

Ranking Pareto Behavior Random 

Left Right Left Right Left Right Left Right 

Win rate 59.8% 

> [58.4%, 60.8%]

64.1% 

> [62.5%, 65.9%]

59.9% 

> [58.0%, 61.3%]

61.7% 

> [60.8%, 63.1%]

53.8% 

> [52.7%, 54.8%]

57.5% 

> [55.9%, 60.2%]

54.0% 

> [49.8%, 55.1%]

58.3% 

> [57.0%, 59.7%]

ELO Score 99.3% 

> [98.5%, 99.6%]

98.5% 

> [97.3%, 99.7%]

99.3% 

> [98.2%, 99.8%]

99.3% 

> [98.4%, 99.6%]

92.9% 

> [90.7%, 94.9%]

87.9% 

> [82.7%, 95.1%]

93.3% 

> [83.1%, 95.4%]

82.2% 

> [72.4%, 87.1%]

Robustness 0.00 

> [0.00, 0.00]

0.00 

> [0.00, 0.00]

0.00 

> [0.00, 0.00]

0.00 

> [0.00, 0.00]

0.00 

> [0.00, 0.00]

0.00 

> [0.00, 0.00]

0.00 

> [0.00, 0.00]

0.00 

> [0.00, 0.00]

Coverage 40.0% 

> [34.0%, 42.0%]

49.0% 

> [44.0%, 54.0%]

46.0% 

> [42.0%, 48.0%]

46.0% 

> [42.0%, 48.0%]

54.0% 

> [50.0%, 56.0%]

44.0% 

> [43.5%, 51.0%]

52.0% 

> [49.5%, 54.5%]

46.0% 

> [44.0%, 50.0%]

Expertise 0.50 

> [0.50, 0.50]

0.50 

> [0.50, 0.50]

0.50 

> [0.50, 0.50]

0.50 

> [0.50, 0.50]

0.50 

> [0.50, 0.50]

0.50 

> [0.50, 0.50]

0.50 

> [0.50, 0.50]

0.50 

> [0.50, 0.50]

AQD-Score 3

> [3, 3]

3

> [3, 3.2]

3

> [3, 3]

3

> [3, 4]

2

> [2, 2]

2

> [2, 2]

2

> [2, 2]

2

> [2, 2]

the mouse (blue) throughout the duel (Fig. 3.d). For vectorization reasons, the duel continues for the maximal number of timesteps, but the fitness is computed early if the cat catches the mouse. This adversarial problem is not symmetrical. 

5.1.3 Pursuers-and-evaders. In Pursuers-and-evaders (Fig. 3.e), two pursuers, in red, must catch two evaders, in blue, i.e., getting closer than the catching threshold of 0.15 m before the end ğ‘‡ ğ‘šğ‘ğ‘¥ = 5ğ‘  ). The pursuers and evaders have the same velocity ( ğ‘£ = 1 m s âˆ’1), but the arena is bounded, and a central disc blocks the motion. The pursuers share the same MLP, which takes as input the concatenation of its absolute position, the relative position of the two evaders, the relative position of the other pursuer, the id (1 for the first pursuer or -1 for the second), and the truth value if the first evader was caught and if the second evader was caught ( ğ· = 11 ). Similarly, the evaders share the same MLP with a corresponding input. The output of each MLP is the direction (tanh output scaled by ğœ‹ ). Similarly to Cat-and-mouse, the fitness is split into different modes: (a) if the two evaders are caught at time ğ‘‡ 1 and ğ‘‡ 2, it is the ratio 1 âˆ’ ğ‘‡ 1+ğ‘‡ 2 

> 2ğ‘‡ ğ‘šğ‘ğ‘¥

normalized between 0.5 and 1 so that, 1 means that the pursuers caught them at ğ‘¡ = 0 s and 0.5 that they caught both of them at ğ‘¡ = ğ‘‡ ğ‘šğ‘ğ‘¥ ; (b) if only one evader is caught, it is the normalized closest distance to the remaining evader (between the initial distance and the capture threshold) between 0.25 and 0.5; and (c) if no evaders has been caught, it is the sum of the closest distance to the two evaders scaled between 0 and 0.25 by the initial position and the capture threshold. The one-frame visualization shows the positions of the pursuers (in red and brown) and the evaders (in light blue and dark blue) throughout the duel (Fig. 3.f). This adversarial problem is also not symmetrical, and even with identical velocities, it should favor the pursuers due to the enclosed arena, a factor balanced by the limited size of the MLP and the short duration. 

## 5.2 Results 

We performed 20 replications of the four variants for each ad-versarial problem with ğ‘ ğ‘”ğ‘’ğ‘› = 10 , ğ‘ ğ‘ğ‘’ğ‘™ğ‘™ = 20 , ğ‘ ğ‘¡ğ‘ğ‘ ğ‘˜ = 50 , and 

ğ‘ ğ‘ğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ = 100 000 . As Ranking and Pareto require additional evaluations to select the tasks, we increased ğ‘ ğ‘ğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ for Random 

and Behavior so that each variant has the same total number of evaluations, ğ‘ ğ‘”ğ‘’ğ‘› Â· ( ğ‘ ğ‘ğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ + ğ‘ 2 

> ğ‘¡ğ‘ğ‘ ğ‘˜

Â· ğ‘ ğ‘ğ‘’ğ‘™ğ‘™ ) = 1.5 M. We present the median result (and first and third quartiles in brackets) over the 20 replications in Tab. 1 for Pong, Tab. 2 for Cat-and-mouse, and Tab. 3 for Pursuers-and-evaders. Bold highlights values that are not significantly different from the best-performing variant on each side, using the Holm-Bonferroni correction applied separately to each adversarial problem, with a p-value of 0.05. The supplementary material presents the box plots and swarm plots. 

5.2.1 Variants Comparison. Ranking has the highest Win rate ,

ELO Score , Expertise , and AQD-Score for each side across all three adversarial problems, the highest Robustness for one side across the two adversarial problems where it is not null, and the highest Coverage for 4 out of 6 comparisons. Pareto consistently shows lower or similar performance across all metrics compared to 

Ranking . Behavior and Random have very similar performances across all measures, with lower Win rate and ELO-Score on each side in all three adversarial problems, but the highest Coverage 

for Pong and for Pursuers-and-evaders. 

5.2.2 Environments takeaways. Pong is not an open-ended adver-sarial problem. The Expertise of 0.5 and Robustness of 0 for all replications of all variants shows that it is easy to find a counter-solution that creates a tie, but hard (or impossible) to do better. This may be caused by the current parameters, and a longer duration or faster ball acceleration could make it harder. In Cat-and-mouse, the highest mouseâ€™s Win rate is much higher than the catâ€™s, indicating that, with the current parameters, it is easier for the mouse to escape the cat. However, Expertise being strictly above 0.5 for both sides indicates that it is always possible to win (given that we know the opposing solution). Pursuers-and-evaders has lower Robustness and Expertise 

than Cat-and-mouse, and an increased difference between the two sides, as the best pursuers have a Win rate below 50% while the evaders have a Win rate above 90%. 

5.2.3 Measures comparison. Win rate seems to be an effective quality measure that also provides insight into the balance between the two sides. However, one should keep in mind that the absolute value is sensitive to the dataset and may not provide a good estimate of the average win rate against any opponent. 

ELO Score appears to be correlated with Win rate , further validating its use as a quality measure for comparison. However, its Preprint, submitted to Gecco â€™26, in January 2026 Anne et al. 

Table 2: Variants comparison of the different measures in Cat-and-mouse 

Ranking Pareto Behavior Random 

Cat Mouse Cat Mouse Cat Mouse Cat Mouse 

Win rate 56.7% 

> [51.0%, 60.6%]

87.4% 

> [86.5%, 88.4%]

52.6% 

> [48.4%, 59.3%]

87.7% 

> [87.3%, 88.3%]

42.0% 

> [36.4%, 46.8%]

85.2% 

> [84.9%, 85.7%]

39.5% 

> [35.1%, 44.7%]

85.3% 

> [84.7%, 85.7%]

ELO Score 99.3% 

> [98.3%, 99.7%]

98.8% 

> [97.3%, 99.6%]

98.7% 

> [97.5%, 99.6%]

99.1% 

> [98.7%, 99.5%]

94.1% 

> [89.1%, 96.8%]

91.7% 

> [89.8%, 94.5%]

92.1% 

> [87.1%, 95.7%]

92.3% 

> [88.6%, 94.2%]

Robustness 0.40 

> [0.39, 0.42]

0.17 

> [0.17, 0.18]

0.38 

> [0.36, 0.40]

0.18 

> [0.17, 0.18]

0.35 

> [0.33, 0.36]

0.18 

> [0.18, 0.18]

0.35 

> [0.33, 0.36]

0.18 

> [0.18, 0.18]

Coverage 58.0% 

> [56.0%, 62.5%]

62.0% 

> [58.0%, 64.0%]

56.0% 

> [50.0%, 58.5%]

52.0% 

> [48.0%, 56.5%]

48.0% 

> [46.0%, 52.0%]

47.0% 

> [40.0%, 48.5%]

55.0% 

> [51.5%, 58.0%]

53.0% 

> [50.0%, 58.0%]

Expertise 0.81 

> [0.75, 0.82]

0.55 

> [0.55, 0.56]

0.60 

> [0.50, 0.64]

0.55 

> [0.54, 0.55]

0.47 

> [0.45, 0.48]

0.53 

> [0.52, 0.54]

0.48 

> [0.48, 0.49]

0.54 

> [0.54, 0.54]

AQD-Score 2

> [2, 2]

3

> [3, 3]

2

> [1, 2]

3

> [2.8, 3]

1

> [1, 1]

2

> [2, 2]

1

> [1, 1]

2

> [2, 2]

Table 3: Variants comparison of the different measures in Pursuers-and-evaders 

Ranking Pareto Behavior Random 

Pursuers Evaders Pursuers Evaders Pursuers Evaders Pursuers Evaders 

Win rate 48.0% 

> [46.0%, 50.1%]

91.9% 

> [91.0%, 92.4%]

45.2% 

> [42.1%, 46.0%]

90.9% 

> [89.7%, 92.0%]

37.5% 

> [36.2%, 41.2%]

90.6% 

> [89.0%, 91.1%]

41.5% 

> [37.7%, 43.1%]

89.1% 

> [88.3%, 90.2%]

ELO Score 99.5% 

> [98.8%, 99.8%]

99.3% 

> [98.5%, 99.7%]

98.5% 

> [96.5%, 98.8%]

98.2% 

> [95.9%, 99.4%]

89.1% 

> [85.8%, 95.4%]

97.7% 

> [94.3%, 98.5%]

95.7% 

> [89.5%, 97.4%]

94.7% 

> [92.4%, 97.0%]

Robustness 0.14 

> [0.13, 0.15]

0.12 

> [0.12, 0.12]

0.13 

> [0.12, 0.13]

0.12 

> [0.12, 0.13]

0.13 

> [0.12, 0.13]

0.13 

> [0.12, 0.13]

0.13 

> [0.12, 0.14]

0.12 

> [0.12, 0.12]

Coverage 52.0% 

> [47.5%, 56.0%]

55.0% 

> [50.0%, 58.0%]

44.0% 

> [40.0%, 48.5%]

42.0% 

> [37.5%, 46.0%]

54.0% 

> [50.0%, 58.0%]

49.0% 

> [47.5%, 56.0%]

55.0% 

> [54.0%, 60.0%]

56.0% 

> [52.0%, 60.0%]

Expertise 0.58 

> [0.49, 0.76]

0.76 

> [0.59, 0.77]

0.48 

> [0.47, 0.49]

0.58 

> [0.56, 0.61]

0.49 

> [0.48, 0.49]

0.56 

> [0.55, 0.62]

0.49 

> [0.48, 0.49]

0.60 

> [0.55, 0.76]

AQD-Score 1.5 

> [1, 2]

3

> [3, 3]

1

> [1, 1]

3

> [3, 3]

1

> [1, 1]

3

> [3, 3]

1

> [1, 1]

3

> [3, 3]

normalized nature makes it relative to the current set of solutions and does not provide any illumination information. 

Robustness , which is always less than 0.5, indicates that the three adversarial problems are fair because there is no solution that can always win (or such a solution has not been found). However, as shown in Pong, it is not usable if fitness is too sparse. 

Coverage is always tricky to compute in high-dimensional space. The K-means discretization allows comparison of the variantsâ€™ di-versity but does not provide meaningful insight into the value, which is between 40% and 60% for each on all problems. In addi-tion, its higher values donâ€™t correlate with greater ability to find counter-solutions, as shown in Pursuers-and-evaders. 

Expertise not only provides a useful quality diversity measure but also useful insights, as it shows the possible quality if one were able to predict or quickly recognize the opposing solution. 

AQD-Score low values (with a maximum of 3) indicate that the three adversarial problems are not open-ended. Still, the tournament-informed task selection methods found a set of solutions that require an additional counter-solution to be defeated, showing their higher performance for adversarial quality diversity. 

Validating Rankingâ€™s performance. To check that the superior performance of Ranking does not come only from selecting the last set of tasks competing in the inter-variant tournament, but mainly from the sequential selection through the generation, we performed an alternative inter-variant tournament using the same executions of GAME as the main experiment, but selecting the competing tasks with Ranking from the final archives of each variant. The results, presented in the supplementary material, show similar trends, validating Ranking â€™s success to its ability to guide coevolution across generations. 

## 6 Discussion and Future Work 

The result confirms that a tournament-informed task selection method leads to higher adversarial quality and diversity than the behavioral and random task selection. Ranking , being significantly better than Pareto , which may be slightly too quality-focused, fur-ther validates the intuition from PATA-EC. Behavior not being significantly better than Random also shows that behavioral qual-ity diversity from one evaluation does not generalize. The tournament-informed methods require a large tournament and thus a large evaluation budget. Future work should focus on improving their sample efficiency, as it seems unlikely that the full tournament is necessary to select the next generation of tasks with the highest adversarial quality diversity. Similarly, the proposed measures require an expensive inter-variant tournament. Future work should also focus on proposing sample-efficient versions of them. For example, ELO Score , which was created for such a purpose, should be easy to estimate with significantly fewer evaluations. The three adversarial problems lack open-endedness, either be-cause of the problems themselves or because of the small neural Tournament Informed Adversarial Quality Diversity Preprint, submitted to Gecco â€™26, in January 2026 

networks used as solutions. Future work should validate that GAME with Ranking as task selection allows for open-ended discovery of solutions in more complex adversarial problems. The choice of measure depends on the end goal of the adversarial illumination, finding: a solution likely to win in general ( Win rate )or against strong opponents ( ELO Score ); a solution with no weak-nesses ( Robustness ); all possible challenges, including the bad ones ( Coverage ); a set of counter-solutions ( Expertise ); a set of solutions that require diversity for the opposing side ( AQD-Score ). 

## 7 Conclusion 

GAME is a coevolutionary adversarial quality diversity algorithm that alternates the illumination of each side of an adversarial prob-lem by selecting elites from the previous generation to serve as fixed opposing tasks, promoting arms-race dynamics. In this paper, we show that using a tournament-informed task selection mechanism based on the ranking vector results in higher adversarial quality and diversity. In addition, we present six measures of adversarial quality and diversity that allow us to evaluate different adversar-ial illumination characteristics. Overall, our results suggest that a tournament-informed task selection is a promising direction for applying GAME to richer, more open-ended adversarial domains. 

## Acknowledgments 

Funded by the armasuisse S+T project F00-007. 

## References 

[1] TimothÃ©e Anne and Jean-Baptiste Mouret. 2023. Multi-task multi-behavior map-elites. In Proceedings of the companion conference on genetic and evolutionary computation . 111â€“114. [2] TimothÃ©e Anne, Noah Syrkis, Meriem Elhosni, Florian Turati, Franck Legendre, Alain Jaquier, and Sebastian Risi. 2025. Generational Adversarial MAP-Elites for Multi-Agent Game Illumination. In Artificial Life Conference Proceedings 37 , Vol. 2025. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info . . . , 31. [3] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. 2019. Emergent tool use from multi-agent autocur-ricula. In International conference on learning representations .[4] Mark A Bedau, John S McCaskill, Norman H Packard, Steen Rasmussen, Chris Adami, David G Green, Takashi Ikegami, Kunihiko Kaneko, and Thomas S Ray. 2000. Open problems in artificial life. Artificial life 6, 4 (2000), 363â€“376. [5] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemysÅ‚aw DÄ™biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al . 2019. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680 (2019). [6] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. [7] LoÃ¯c Brevault and Mathieu Balesdent. 2024. Bayesian Quality-Diversity ap-proaches for constrained optimization problems with mixed continuous, discrete and categorical variables. Engineering Applications of Artificial Intelligence 133 (2024), 108118. [8] Estelle Chigot and Dennis G Wilson. 2022. Coevolution of neural networks for agents and environments. In Proceedings of the Genetic and Evolutionary Computation Conference Companion . 2306â€“2309. [9] Antoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. 2015. Robots that can adapt like animals. Nature 521, 7553 (2015), 503â€“507. [10] Quy-Anh Dang, Chris Ngo, and Truong-Son Hy. 2025. RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search. arXiv preprint arXiv:2504.15047 (2025). [11] Richard Dawkins and John Richard Krebs. 1979. Arms races between and within species. Proceedings of the Royal Society of London. Series B. Biological Sciences 

205, 1161 (1979), 489â€“511. [12] Kalyanmoy Deb and Himanshu Jain. 2013. An evolutionary many-objective opti-mization algorithm using reference-point-based nondominated sorting approach, part I: solving problems with box constraints. IEEE transactions on evolutionary computation 18, 4 (2013), 577â€“601. [13] Aaron Dharna, Cong Lu, and Jeff Clune. 2024. Quality-Diversity Self-Play: Open-Ended Strategy Innovation via Foundation Models. In NeurIPS 2024 Workshop on Open-World Agents .[14] Alan Dorin and Susan Stepney. 2024. What Is Artificial Life Today, and Where Should It Go? 15 pages. [15] Maxence Faldor, Jenny Zhang, Antoine Cully, and Jeff Clune. 2024. OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code. In The Thirteenth International Conference on Learning Representations .[16] Sevan G Ficici and Jordan B Pollack. 1998. Challenges in coevolutionary learning: Arms-race dynamics, open-endedness, and mediocre stable states. In Proceedings of the sixth international conference on Artificial life . MIT Press Cambridge, MA, 238â€“247. [17] Matthew C Fontaine, Scott Lee, Lisa B Soros, Fernando de Mesentier Silva, Julian Togelius, and Amy K Hoover. 2019. Mapping hearthstone deck spaces through map-elites with sliding boundaries. In Proceedings of The Genetic and Evolutionary Computation Conference . 161â€“169. [18] Matthew C Fontaine, Julian Togelius, Stefanos Nikolaidis, and Amy K Hoover. 2020. Covariance matrix adaptation for the rapid illumination of behavior space. In Proceedings of the 2020 genetic and evolutionary computation conference . 94â€“102. [19] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al . 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 (2022). [20] Daniele Gravina, Ahmed Khalifa, Antonios Liapis, Julian Togelius, and Georgios N Yannakakis. 2019. Procedural content generation through quality diversity. In 

2019 IEEE Conference on Games (CoG) . IEEE, 1â€“8. [21] Rufus Isaacs. 1999. Differential games: a mathematical theory with applications to warfare and pursuit, control and optimization . Courier Corporation. [22] Yibin Jiang, Daniel Salley, Abhishek Sharma, Graham Keenan, Margaret Mullin, and Leroy Cronin. 2022. An artificial intelligence enabled chemical synthesis robot for exploration and optimization of nanomaterials. Science advances 8, 40 (2022), eabo2626. [23] Akarsh Kumar, Ryan Bahlous-Boldi, Prafull Sharma, Phillip Isola, Sebastian Risi, Yujin Tang, and David Ha. 2026. Digital Red Queen: Adversarial Program Evolution in Core War with LLMs. arXiv preprint arXiv:2601.03335 (2026). [24] Joel Lehman and Kenneth O Stanley. 2011. Evolving a diversity of virtual creatures through novelty search and local competition. In Proceedings of the 13th annual conference on Genetic and evolutionary computation . 211â€“218. [25] Jean-Baptiste Mouret and Jeff Clune. 2015. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909 (2015). [26] Jean-Baptiste Mouret and Glenn Maguire. 2020. Quality diversity for multi-task optimization. In Proceedings of the 2020 Genetic and Evolutionary Computation Conference . 121â€“129. [27] Thai Huy Nguyen and Ngoc Hoang Luong. 2025. Diversifying Adversarial Attacks on Text-to-image Generation. In Proceedings of the Genetic and Evolutionary Computation Conference Companion . 315â€“318. [28] Justin K Pugh, Lisa B Soros, and Kenneth O Stanley. 2016. Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI 3 (2016), 40. [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al . 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning . PmLR, 8748â€“8763. [30] Mikayel Samvelyan, Davide Paglieri, Minqi Jiang, Jack Parker-Holder, and Tim RocktÃ¤schel. 2024. Multi-agent diagnostics for robustness via illuminated diver-sity. arXiv preprint arXiv:2401.13460 (2024). [31] Mikayel Samvelyan, S. Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktaschel, and Roberta Raileanu. 2024. Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts. ArXiv abs/2402.16822 (2024). https: //api.semanticscholar.org/CorpusId:268031888 [32] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al . 2017. Mastering the game of go without human knowledge. nature 550, 7676 (2017), 354â€“359. [33] Kirby Steckel and Jacob Schrum. 2021. Illuminating the space of beatable lode runner levels produced by various generative adversarial networks. In Proceedings of the genetic and evolutionary computation conference companion . 111â€“112. [34] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, MichaÃ«l Mathieu, An-drew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. 2019. Grandmaster level in StarCraft II using multi-agent reinforcement learning. nature 575, 7782 (2019), 350â€“354. [35] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. 2019. Poet: open-ended coevolution of environments and their optimized solutions. In Proceedings of the Genetic and Evolutionary Computation Conference . 142â€“151. Preprint, submitted to Gecco â€™26, in January 2026 Anne et al. 

[36] Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth Stanley. 2020. Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. In 

International conference on machine learning . PMLR, 9940â€“9951. [37] Ren-Jian Wang, Ke Xue, Zeyu Qin, Ziniu Li, Sheng Tang, Hao-Tian Li, Shengcai Liu, and Chao Qian. 2025. Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models. arXiv preprint arXiv:2506.07121 (2025). [38] Ruize Zhang, Zelai Xu, Chengdong Ma, Chao Yu, Wei-Wei Tu, Wenhao Tang, Shiyu Huang, Deheng Ye, Wenbo Ding, Yaodong Yang, et al . 2024. A survey on self-play methods in reinforcement learning. arXiv preprint arXiv:2408.01072 

(2024).