# APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition
# APC-RL：通过自适应策略组合超越数据驱动的行为先验

**Authors**: Finn Rietz, Pedro Zuidberg dos Martires, Johannes Andreas Stork \\
**Date**: 2026-01-27 \\
**PDF**: https://arxiv.org/pdf/2601.19452v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 6.0 \\
**Evidence**: Adaptive policy composition for RL to refine priors aligns with the evolution of heuristics and efficient automated strategies. \\

---

## Abstract
Incorporating demonstration data into reinforcement learning (RL) can greatly accelerate learning, but existing approaches often assume demonstrations are optimal and fully aligned with the target task. In practice, demonstrations are frequently sparse, suboptimal, or misaligned, which can degrade performance when these demonstrations are integrated into RL. We propose Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple data-driven Normalizing Flow (NF) priors. Instead of enforcing strict adherence to the priors, APC estimates each prior's applicability to the target task while leveraging them for exploration. Moreover, APC either refines useful priors, or sidesteps misaligned ones when necessary to optimize downstream reward. Across diverse benchmarks, APC accelerates learning when demonstrations are aligned, remains robust under severe misalignment, and leverages suboptimal demonstrations to bootstrap exploration while avoiding performance degradation caused by overly strict adherence to suboptimal demonstrations.

## 摘要
将演示数据整合到强化学习（RL

---

## 速览摘要（自动生成）

**问题**：RL利用演示数据时，常因数据次优或不匹配导致性能受