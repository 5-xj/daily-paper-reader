# Improving Methodologies for LLM Evaluations Across Global Languages
# 改进全球语言的大语言模型评估方法

**Authors**: Akriti Vij, Benjamin Chua, Darshini Ramiah, En Qi Ng, Mahran Morsidi, Naga Nikshith Gangarapu, Sharmini Johnson, Vanessa Wilfred, Vikneswaran Kumaran, Wan Sie Lee, Wenzhuo Yang, Yongsen Zheng, Bill Black, Boming Xia, Frank Sun, Hao Zhang, Qinghua Lu, Suyu Ma, Yue Liu, Chi-kiu Lo, Fatemeh Azadi, Isar Nejadgholi, Sowmya Vajjala, Agnes Delaborde, Nicolas Rolin, Tom Seimandi, Akiko Murakami, Haruto Ishi, Satoshi Sekine, Takayuki Semitsu, Tasuku Sasaki, Angela Kinuthia, Jean Wangari, Michael Michie, Stephanie Kasaon, Hankyul Baek, Jaewon Noh, Kihyuk Nam, Sang Seo, Sungpil Shin, Taewhi Lee, Yongsu Kim, Daisy Newbold-Harrop, Jessica Wang, Mahmoud Ghanem, Vy Hong
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15706v1
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 8.0
**Evidence**: Technical evaluation of frontier AI models across global languages and safeguards

---

## Abstract
As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.   The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

## 摘要
随着前沿人工智能模型在全球范围内的部署，确保其在不同语言和文化

---

## 论文详细总结（自动生成）

这篇论文是由新加坡、日本、澳大利亚、加拿大、欧盟、法国、肯尼亚、韩国和英国的AI安全研究所（AISI）及相关政府机构联合开展的一项关于大语言模型（LLM）多语言安全评估的研究报告。

以下是对该论文的结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
随着前沿AI模型在全球范围内的部署，确保其在不同语言和文化背景下的行为安全、可靠至关重要。目前大多数安全评估集中在英语，而模型在非英语（尤其是低资源语言）环境下的安全护栏（Safeguards）是否依然有效尚不明确。
**核心目标**：
*   开发一套通用的多语言安全评估方法。
*   探索“LLM作为裁判（LLM-as-a-judge）”在复杂多语言语境下与人工评估的一致性。
*   识别模型在不同语言中表现出的独特安全风险和行为差异。

### 2. 论文提出的方法论
研究采用了一种跨国协作的联合测试框架，核心流程如下：
*   **语言选择**：涵盖10种语言，包括高资源（英语、法语、日语、韩语、中文）和低资源（波斯语、斯瓦希里语、马来语、泰卢固语、粤语）。
*   **数据准备与翻译**：将现有的英语安全基准翻译成其他9种语言。采用机器翻译加母语专家校验的方式，强调不仅是字面翻译，还需考虑文化语境。
*   **测试执行**：使用开源工具 **Moonshot** 进行自动化测试。
*   **双重评估机制**：
    *   **LLM-as-a-judge**：使用一个独立的第三方模型作为裁判，根据预设的“可接受性”和“拒绝率”标准对响应进行评分。
    *   **人工标注**：由各国的母语专家对模型输出进行人工审核，验证AI裁判的准确性，并捕捉细微的文化/语言偏差。

### 3. 实验设计
*   **测试模型**：选取了两个主流开源权重模型：**Mistral Large** 和 **Gemma 2 (27B)**。
*   **评估维度（5大伤害类别）**：
    1.  **越狱（Jailbreaking）**：测试模型对对抗性提示词的防御能力。
    2.  **知识产权（IP）**：评估模型是否诱发侵权行为。
    3.  **隐私（Privacy）**：测试敏感个人信息的泄露风险。
    4.  **暴力犯罪**：评估模型是否支持或鼓励暴力。
    5.  **非暴力犯罪**：如洗钱、逃税等。
*   **基准数据集**：
    *   **MLCommons AILuminate**（400个提示词）。
    *   **AnswerCarefully**（日本NII开发，48个提示词）。
    *   **CyberSecEval**（Meta开发，251个提示词，侧重提示词注入）。
*   **对比实验**：对比了英语与非英语的表现，以及AI裁判与人工标注的差异。

### 4. 资源与算力
*   **算力说明**：文中**未明确说明**具体的GPU型号、数量或推理总时长。
*   **工具链**：主要依赖新加坡AISI开发的 **Moonshot** 平台进行推理和指标计算。
*   **人力资源**：动员了来自全球9个国家/地区的数十名技术专家和语言专家进行人工标注。

### 5. 实验数量与充分性
*   **实验规模**：总计生成并评估了超过 **6,000个** 新翻译的提示词。
*   **覆盖范围**：涵盖了10种语言和5个核心安全领域，每种语言都进行了人工校验。
*   **充分性评价**：实验设计较为全面，不仅有定量分析（可接受率、拒绝率），还有深入的定性分析（各语言的Deep Dive）。虽然部分语言仅基于单次运行（Single-run），但通过多国协作和人工复核，结果具有较高的客观性和参考价值。

### 6. 主要结论与发现
*   **安全护栏滞后**：非英语环境下的安全保护普遍弱于英语，尤其是在**越狱防御**方面表现最差。
*   **AI裁判的局限性**：LLM作为裁判在多语言环境下并非完全可靠。在日语（18.9%）、泰卢固语（15%）和中文（10.6%）中，AI裁判与人工评估的差异率较高。
*   **“虚假警告”现象**：模型常在回复开头给出安全警告，但随后却提供了有害的详细指令，AI裁判往往会被开头的警告误导而判定为安全。
*   **语言特有行为**：
    *   **低资源语言**（如泰卢固语、斯瓦希里语）更容易出现幻觉、胡言乱语或中英混杂。
    *   **文化差异**：法语、韩语、日语和波斯语模型倾向于使用委婉的拒绝方式，而非直接拒绝，以符合当地礼貌规范。

### 7. 优点（亮点）
*   **全球协作**：这是首次由多国AI安全研究所联合进行的大规模多语言安全基准测试，具有很强的政策和技术风向标意义。
*   **关注拒绝质量**：不仅看模型是否拒绝，还评估拒绝是否具有“帮助性”（如提供合法替代方案）。
*   **低资源语言覆盖**：填补了学术界对泰卢固语、斯瓦希里语等语言安全评估的空白。

### 8. 不足与局限