# ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models
# ErrorMap 与 ErrorAtlas：绘制大语言模型的失败图谱

**Authors**: Shir Ashury-Tahan, Yifan Mai, Elron Bandel, Michal Shmueli-Scheuer, Leshem Choshen
**Date**: 2026-01-22
**PDF**: https://arxiv.org/pdf/2601.15812v1
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">query:大厂llm</span>
**Score**: 7.0
**Evidence**: Evaluation framework for analyzing failure modes in LLM benchmarks

---

## Abstract
Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

## 摘要
大语言模型（LLM）的基准测试能够反映

---

## 速览摘要（自动生成）

**问题**：现有基准测试仅反映LLM失败频率，无法揭示具体原因。