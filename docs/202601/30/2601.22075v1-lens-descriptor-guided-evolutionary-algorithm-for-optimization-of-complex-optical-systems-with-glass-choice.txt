Title: Lens-descriptor guided evolutionary algorithm for optimization of complex optical systems with glass choice

URL Source: https://arxiv.org/pdf/2601.22075v1

Published Time: Fri, 30 Jan 2026 02:36:12 GMT

Number of Pages: 15

Markdown Content:
# LENS -DESCRIPTOR GUIDED EVOLUTIONARY ALGORITHM FOR OPTIMIZATION OF COMPLEX OPTICAL SYSTEMS WITH GLASS CHOICE 

Kirill Antonov 1 Teus Tukker 2 Tiago Botari 2

Thomas H. W. B¨ ack 1 Anna V. Kononova 1 Niki van Stein 11Leiden University, The Netherlands 

> 2

ASML, Veldhoven, The Netherlands 

# ABSTRACT 

Designing high-performance optical lenses entails exploring a high-dimensional, tightly constrained space of surface curvatures, glass choices, element thicknesses, and spacings. In practice, stan-dard optimizers (e.g., gradient-based local search and evolutionary strategies) often converge to a single local optimum, overlooking many comparably good alternatives that matter for downstream engineering decisions. We propose the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage framework for multimodal lens optimization. LDG-EA first partitions the design space into behavior descriptors defined by curvature-sign patterns and material indices, then learns a probabilistic model over descriptors to allocate evaluations toward promising regions. Within each descriptor, LDG-EA applies the Hill-Valley Evolutionary Algorithm with covariance-matrix self-adaptation to recover multiple distinct local minima, optionally followed by gradient-based re-finement. On a 24-variable (18 continuous and 6 integer), six-element Double-Gauss topology, LDG-EA generates on average 14 .5 × 10 3 candidate minima spanning 636 unique descriptors, an order of magnitude more than a CMA-ES baseline, while keeping wall-clock time at one hour scale. Although the best LDG-EA design is slightly worse than a fine-tuned reference lens, it remains in the same performance range. Overall, the proposed LDG-EA produces a diverse set of solutions while maintaining competitive quality within practical computational budgets and wall-clock time. 

Keywords Optical Design, Evolutionary Algorithms, Multi-modal optimization, Multi-modal Search Landscapes, Quality-Diversity 

# 1 Introduction 

Optical lens systems enable a broad spectrum of advanced technologies, from smartphone and machine-vision cameras to the ultra-precise optics used in lithography and metrology equipment [1]. Designing these and similar systems involves selecting optical surface curvatures, glass types, element thicknesses, and inter-element spacings to meet key performance metrics, such as root mean square (RMS) spot size, focal length, and field number while respecting manufacturing constraints on element size, weight, and material availability [2–4]. The resulting design space is both high-dimensional and highly multimodal [5], with numerous local optima in the objective function [6]. Figure 1 illustrates this phenomenon on a two-dimensional slice through a real lens-design landscape: multiple separated peaks represent different high-performing families of designs rather than minor perturbations of a single solution. In industrial practice, optical engineers typically work in a designer-in-the-loop manner: they propose a starting layout (e.g., from patents or their own expertise) and then apply local numerical optimization using commercial software (e.g., OpticStudio [8] or CODE V [9]). This workflow is highly effective for refinement, but it is biased toward only several attraction basins and therefore explores only a tiny fraction of the design space. CODE V’s Global Synthesis (GS) module [10] is a widely used alternative intended to broaden exploration. However, GS provides limited control over the diversity of the returned solutions and offers only partial support for systematic glass optimization, making it difficult to deliberately generate a varied portfolio of high-quality designs. 

> arXiv:2601.22075v1 [cs.NE] 29 Jan 2026

Figure 1: Two views of the optimization landscape for a four-element camera lens, where variables are curvatures, distances, and materials of glasses, visualized using the method from work [7]. This method selects points between three local minima, identified through a local-search algorithm. The horizontal axes correspond to a plane in the high-dimensional design space, and the vertical axis shows the reversed merit function (the higher, the better). Multiple optima indicate distinct families of high-quality designs. The ability to produce multiple, diverse high-quality designs is practically valuable. A diverse solution set supports downstream engineering choices that cannot be reliably incorporated into the optimization itself, including validation of previously unknown structural constraints, consideration of glass availability, and selection of designs with advanta-geous cost, manufacturability, and tolerance characteristics. Motivated by these needs, a broad range of learning-based methods [11] and metaheuristics [12,13] have been proposed. Neural networks have been used to extrapolate reference designs taken from the literature [14–16], and surrogate models combined with Differential Evolution [17] can reduce expensive ray-trace evaluations (e.g., by around 10%) [18]. Deep models have also been explored for tolerancing and uncertainty quantification [19, 20] and for modeling specialized aberration behavior [21]. From the evolutionary computation perspective, classical global optimization methods have been successfully applied to lens design [22–24]. Despite these advances, these methods are still limited in glass selection and exploration since they do not exploit the specific structure of lens-design landscapes and therefore do not aim to systematically cover diverse high-quality designs. Multimodal optimization methods offer a natural framework for addressing this problem, as they are explicitly de-signed to identify multiple optima within a single run [25]. Recent studies have indeed investigated multimodal optimization for optical design [26–29]. However, the existing applications of multimodal methods optimize only surface curvatures, which limits practical relevance because glass choice is a crucial element of real designs. More-over, reported wall-clock times span multiple days to weeks even on moderate lens systems (three to four elements), whereas industrial tools such as GS can often deliver solutions within about an hour on such systems on the same machine [29]. In this paper, we close this gap by introducing a multimodal evolutionary algorithm that optimizes glasses, curvatures, element thicknesses, and inter-element spacings for a practical six-element Double-Gauss lens system under an hour-scale budget. We propose the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage framework that explicitly exploits domain structure by partitioning designs into interpretable behavior descriptors and learning an adaptive sampling distribution over them. At each iteration, LDG-EA samples promising descriptors, applies the Hill-Valley Evolutionary Algorithm [30] to locate local optima within the subspace induced by each descriptor, and updates descriptor probabilities based on the observed performance, balancing exploration of new regions with exploitation of high-quality design regions. Several leading multimodal optimizers also allocate search effort across landscape regions, for example BIPOP-CMA-ES [31], DPI-CMA-ES [32], DPI-MIES [33], LADE [34] and NetCDE [35]. In contrast to these largely domain-agnostic partitions, LDG-EA defines lens-specific niches a priori through behavior descriptors that capture curvature and glass patterns. This descriptor-driven partition explicitly enforces diversity across design patterns. As a result, LDG-EA balances between solution quality and diversity to enable robust multimodal search tailored to lens design. 

Contributions. 

• We formalize lens-design behavior descriptors and use them to structure and navigate the lens-design multi-modal optimization landscape. • We develop LDG-EA, which integrates descriptor learning, Hill-Valley Evolutionary Algorithm, and optional gradient-based refinement into an adaptive search strategy. 2Figure 2: (a) Visualization of behavior descriptor mapping: singlet lens θ shown in the figure is mapped to its behavior descriptor x = (1 , 0, 3) by the defined mapping x = D(θ). The glass material is represented by the color. (b) Visualization of the space of all behavior descriptors X . Two boxes on the right show examples of lenses with the same behavior descriptors x(1) and x(2) respectively. Note that singlet lenses in the same box have different thicknesses and different curvature values, but still have the same behavior descriptor due to the definition of our mapping D.

• We validate LDG-EA on a six-element Double-Gauss lens, demonstrating an order-of-magnitude increase in distinct high-quality designs over a baseline algorithm and competitive RMS performance under equal computational budgets. 

# 2 Formalization of the optimization problem 

Let Θ0 ⊆ Rnc × Rnd × Znm be the full design space for an optical lens, and let H ⊂ Θ0 denote a forbidden region of restricted parameter combinations. The possible values of curvatures and thicknesses are usually restricted by manufacturing constraints. We define the feasible domain 

Θ = Θ 0 \ H .

On Θ, the objective function 

F : Θ → R

assigns to each design θ = ( κ, d, m) its optical-quality metric. We introduce a descriptor mapping 

D : Θ −→ X , θ 7 → x,

which retains only the qualitative features of a design. Namely, the signs of its surface curvatures and its discrete material indices. Note that distance is not part of the behavior descriptor. Each element of set X is called a behavior descriptors , so the set X is the collection of all behavior descriptors . An example of the defined behavior descriptors for singlet lenses is shown in Figure 2. 

Definition 1 (Lens equivalence with respect to the descriptor and performance) . Two designs θ(1) , θ(2) ∈ Θ are said to be descriptor-equivalent , written θ(1) ≡ θ(2) , if 

D θ(1)  = D θ(2)  and F  θ(1)  = F  θ(2) .

In this case, D(θ) is called the behavior descriptor of θ. Definition 1 follows the method for optical systems com-parison from the book [36]. Note that, following this method, the thicknesses are not considered when comparing the similarities of optical systems. Our goal is to identify a set of k distinct (non-descriptor-equivalent) local optima in Θ, each achieving objective value below a threshold c and each having a unique behavior descriptor. We define the objective function in such a way that 3it penalizes imperfections in diffraction-limited imaging. The optimal value (although it is not always achievable in practice) of our objective function is 0, and the larger the value, the worse the performance of the system. Formally, find θ(1) , θ(2) , . . . , θ(k) ⊆ Θ

such that for every i = 1 , . . . , k :1. θ(i) is a local minimizer of F :

∇F  θ(i) = 0, ∇2F  θ(i) ⪰ 0; 

2. F  θ(i) < c ;3. the descriptors are pairwise different: 

D θ(i)̸ = D θ(j) ∀ i < j. 

Note that we tacitly assume that the objective function F is twice continuously differentiable. Although we do not ex-plicitly use this assumption in the following paper, we maintain this assumption because employed Evolution Strategies rely on it. We will assume that the gradient of the function F is available, for example, using automatic differentia-tion [37]. 

# 3 Methodology 

We introduce the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA) , an iterative two-stage optimization framework that dynamically focuses search effort on regions of the lens descriptor space most likely to yield high-quality designs. 

3.1 Notation and Overview 

Let X denote the finite set of behavior descriptors, let p(t)(x) be a probability mass function over X at iteration t,and let B be a fixed computational budget for evaluations of F inside a given behavior descriptor. We say that the designs x(1) , x(2) are evaluated inside the same behaviour descriptor if D x(1)  = D x(2) . Our algorithm uses parameters λ and μ to denote the number of samples and the selection size (defined in Stage 2 below), respectively, with 1 ≤ μ ≤ λ ≪ |X | .At each iteration t = 1 , 2, 3, . . . , the algorithm performs the following two stages: 1. Stage 1 – Descriptor Sampling and Evaluation. 

(a) Sampling: draw λ descriptors 

x(t, 1) , . . . , x(t,λ ) ∼ p(t)(x).

(b) Per-Descriptor Search Instances: for each sampled descriptor x(t,i ), instantiate an internal optimization algorithm 

constrained to generate solutions only inside the descriptor x(t,i ) within allocated budget B. We denote by 

A(t,i ) = θ(t,i, 1) , . . . , θ(t,i,n i)

the resulting archive of all found candidate solutions, that approximate local minima at the subspace 

D−1 

x(t,i )

⊂ Θ,

meaning such subspace that 

∀θ ∈ D −1 

x(t,i )

=⇒ D (θ) = x(t,i ).

(c) Performance Feedback: compute the best objective value per descriptor 

f (t,i ) = min 

> θ∈A(t,i )

F (θ),

and record the pair  x(t,i ), f (t,i ) for descriptor-level feedback. 2. Stage 2 – Descriptor Selection and Distribution Update. 

(a) Selection: sort the evaluated descriptors by ascending f (t,i ) and select the top μ:

x(1) , . . . , x(μ) = arg min  

> i∈[1 ,λ ]
> μ

f (t,i ).

4(b) Update Rule: update the sampling distribution p(t)(x) → p(t+1) (x) by increasing probability mass on the selected descriptors and decreasing it elsewhere. We use the following update rule, as proposed in the classical Univariate Marginal Distribution Algorithm (UMDA) [38] 

p(t+1) (x) = (1 − α) p(t)(x) + α

> nc+nm

Y

> i=1

 1

μ

> μ

X

> j=1

1

n

xi = xi(j)o ,

where α ∈ (0 , 1] is a learning rate. After the given number of iterations is finished, we can optionally apply gradient-based local optimization to each element in the set θ(t,i,j ) 

> i,j

of the approximated local minima. 

3.2 Learning Descriptor Models 

After each update of p(t+1) , we refine the internal models that generate descriptor components (signs of curvature and material indices, the thicknesses are ommited due to the resons discussed in Sec. 2) by fitting probability distributions conditioned on the selected descriptors. Concretely, if a descriptor x decomposes as 

x = (sign( κ1), . . . , sign( κnc ), m 1, . . . , m nm ),

we update: • A Bernoulli model for each curvature sign sign( κj ) based on its empirical frequency in the selected set of μ

best performing descriptors. • A categorical model for each material index mj based on its empirical frequency in the selected set of μ

best-performing descriptors. These learned models are then used to sample better descriptors in subsequent iterations, therefore, biasing the search toward promising structural patterns. However, the total number of descriptors is too large to sample them all, so the best learned descriptors are just an approximation of the true best descriptors. 

3.3 Internal optimization algorithm 

We employ the Hill-Valley Evolutionary Algorithm (HV-EA) [30] to discover a diverse set of high-quality solutions sharing the same descriptor x. This algorithm is used on Stage 1 (b) in the proposed method LDG-EA (see Sec. 3.1). By construction, each HV-EA run holds the material indices fixed and varies only the surface curvatures κ and thick-nesses d within the sign-constraints implied by x. Specifically, for each curvature component κi:

 κi ≤ − 4 mm , if sign( xi) = −1,κi ≥ +4 mm , if sign( xi) = +1 ,

where the threshold of 4 mm reflects our manufacturing limits [7, 28]. 

HV-EA Overview. HV-EA partitions the descriptor-constrained search space into niches via hill-valley clustering, then runs an another optimization method within each niche to approximate local minima over a given budget. This two-phase strategy balances exploration, namely identifying distinct attraction basins within the considered descriptor, with exploitation, namely refining within each basin. 

Internal Optimizer of HV-EA: CMSA-ES. We adopt the Covariance Matrix Self-Adaptation Evolution Strategy (CMSA-ES) [39] as the internal single-mode optimizer within HV-EA, instead of the AMaLGaM-Univariate algo-rithm [40] suggested by the HV-EA authors [30, 41]. In our preliminary experiments, CMSA-ES generated candidate solutions that lie slightly further from the exact local minima than those of AMaLGaM. Yet solutions by CMSA-ES remain sufficiently close to reveal the structure of the lens and enable rapid convergence of subsequent gradient-based optimization. By contrast, AMaLGaM’s finer-scale focus on approximating Gaussian-mutation iso-contours can place solutions much closer to the minimum, but this extra precision is redundant once precise gradients are available and only serves to consume additional computational budget. Moreover, the lightweight self-adaptation mechanism of CMSA-ES yields faster initial progress on the highly rugged landscapes of optical design problems. 5Figure 3: The Lens-Descriptor-Guided Evolutionary Algorithm (LDG-EA) applied to a two-dimensional test function. Panels (1. a), (1. b), etc. match the corresponding stages described in Sec. 3.1. In panel (1. a), the descriptors are indicated by black squares over the domain of the test function. Green ticks in Stage 1 denote sampled descriptors, while in Stage 2 green ticks denote descriptors selected after evaluation of the descriptor-level objectives f (t,i ); a black cross marks a descriptor that is not selected. The two plots at the bottom visualize the update of the probabilistic distribution over descriptors, p(t)(x), according to the formula displayed in panel (2. b). 

Archive Maintenance. The algorithm enforces a quality window of width w on each niche archive A(t,i ). Con-cretely, let 

A(t,i ) = θ(t,i, 1) , . . . , θ(t,i,n ) , F  θ(t,i, 1)  ≤ · · · ≤ F  θ(t,i,n ),

6and define 

Fmin = F  θ(t,i, 1) .

We require for all k that 

F  θ(t,i,k ) ≤ Fmin + w. 

When a new candidate θ′ with value F (θ′) is generated, we insert it into the sorted archive and then remove every solution whose objective exceeds Fmin + w. This mechanism ensures that the objective value span in each archive 

A(t,i ) remains bounded by w, effectively filtering out lower-quality local minima. The window width w is calibrated from known reference designs: it is large enough to allow a diverse set of near-optimal solutions, yet small enough to exclude any a priori inferior configurations. 

Stopping Criterion of CMSA-ES. Each CMSA-ES instance terminates as soon as any one of the following condi-tions is met: 1. Evaluation budget: The total number of objective evaluations reaches THV -EA .

2. Parameter-change tolerance: The change in the mean search vector between two successive generations of CMSA-ES falls below εparam :

μ(g+1) − μ(g) 

> 2

≤ εparam .

3. Function-value tolerance: The absolute improvement in the best objective value from one generation to the next is less than εfun :

F (g+1) min − F (g)min ≤ εfun ,

where F (g)min is the best fitness in generation g.4. Fitness-history tolerance: Over a sliding window of the last LCMSA-ES generations, no improvement exceeds 

εhist . Equivalently, the range or standard deviation of 

{F (g−L+1) min , . . . , F (g)min }

falls below εhist .In our implementation, we use the default HV-EA values for εparam , εfun , εhist and fix the same total budget limit for HV THV-EA to enforce a uniform computational effort across all descriptor-constrained searches. The specific values of these and other constants used in the implementation of the algorithms are provided in Table 1. In our experiments, the evaluation budget is exhausted only once per HV-EA run. In almost all the runs, the stopping criterion per niche is the parameter-change tolerance. 

3.4 Stopping Criterion of LDG-EA 

The proposed algorithm (external optimizer) terminates when one of the following conditions is met: 1. A maximum number of iterations ILDG-EA is reached. 2. The improvement in the best objective value over the last LLDG-EA iterations falls below a tolerance ε.3. The distribution p(t)(x) has nearly converged, where the convergence is measured according to the approxi-mated Kullback-Leibler (KL) divergence [42] to p(t−1) .This two-stage, distribution-adaptive approach balances the exploration of the descriptor space with the exploitation of high-quality design patterns, enabling efficient discovery of diverse, high-performance optical lens configurations. In our experiments, we run until the maximum number of iterations exceeds the given in advance computational budget. 

# 4 Numerical validation 

4.1 Baseline Evolutionary Algorithm 

To emphasize the benefits of descriptor-guided search, we implement a baseline algorithm that jointly changes all design parameters without intermediate descriptor learning for integer variables. Specifically, we employ the Co-variance Matrix Adaptation Evolution Strategy (CMA-ES) [43] augmented with a restart mechanism [31] and an integer-handling strategy for the material variables [44]. 7Figure 4: Reference Double-Gauss lens system. Element glasses are labeled by Schott catalog code and colored by their sorted refractive index at the d-line (587.6 nm) over the catalog. 

Joint Parameter Optimization. Unlike LDG-EA, which constrains each sub-search to a fixed behavior descriptor, the baseline CMA-ES treats the entire parameter vector θ = ( κ, d, m) as continuous (for κ, d) and integer-valued (for m). During each generation, real-valued samples for m are rounded to the nearest valid material index, following the procedure in [44] to maintain feasibility, ensure the absence of premature convergence, and avoid disruptive integer perturbations. 

Restart Strategy and Stopping Conditions. The baseline CMA-ES is given a total budget of TCMA-ES = λ·THV-EA ·

ILDG-EA evaluations of the objective function F . The algorithm starts with an identity covariance matrix and runs until it meets one of its built-in stopping conditions, analogical to those for CMSA-ES: (i) parameter-change falls below 

εparam , (ii) function-value improvement falls below εfun , or (iii) a maximum number of iterations is reached. Note that we do not consider higher-dimensional variants in this setting, because a change in the integer variables typically induces a much larger jump in the objective value than a small perturbation of the continuous variables. Consequently, near termination we expect only the continuous variables to keep changing, while the integer part remains fixed until a successful stopping condition is met. If the algorithm converges before spending all TCMA-ES evaluations, we restart it with the remaining budget and parameters following BIPOP restart scheme [31]. This restart loop continues until the full TCMA-ES evaluations are used. 

4.2 Example Lens System 

To demonstrate the capabilities of our proposed algorithms, we apply them to a classic Double-Gauss lens. Using the Double-Gauss as a reference template (Figure 4), we seek to generate alternative configurations that match its principal optical specifications while minimizing RMS spot size objective. • Effective focal length: feff = 95 .5 mm; • Full field of view: 28 ◦;• Entrance pupil diameter: DEP = 33 .33 mm; • Glass catalog: Schott [45] catalog consisting of 120 glasses; • Illumination: polychromatic design over standard spectral lines. In our reference design, the six-element Double-Gauss layout features a positive first curvature and successive refract-ing surfaces in a fixed sequence. We preserve this topology and optimize a broad set of parameters-surface curvatures, element thicknesses, inter-element spacings, and material selections, while keeping the distances from the aperture stop to the neighboring lens vertices constant. The resulting high dimensionality (24 variables in total) and complex 8interactions between them yield a vast number of local minima (in the search space, not per descriptor) with nearly identical RMS-spot sizes, making this lens system a sufficiently good example for assessing the descriptor-guided diversity of LDG-EA. During LDG-EA execution we use paraxial solve to find the distance to the sensor from the last surface [46]. However, during the optional subsequent gradient-based optimization, we treat it as a variable. 

Double-Gauss Objective Function. We define the overall objective 

F (θ) =  RMS( θ)2

| {z }

> imaging quality

+

> 5

X

> k=1

wk P 2 

> k

(θ),

where each Pk(θ) ∈ R is a penalty term and wk > 0 its corresponding weight. We implemented the following penalties: • P1: vignetting/refracted-ray penalty . A large penalty is applied if any ray is vignetted or undergoes total internal reflection during the ray trace. We used w1 = 10 .

• P2: negative optical path penalty . Enforces non-negative optical path length inside each lens element by penalizing any back-tracking rays at the element edges. We used w2 = 1 .

• P3: minimum-thickness/air-gap penalty . Penalizes element thicknesses or inter-element air gaps that fall below manufacturable limits. We used w3 = 1 .

• P4: free-working distance penalty . Penalizes the distance from the last surface to the image sensor when it deviates below a specified minimum. We used w4 = 1 .

• P5: focal-length penalty . Penalizes deviation of the effective focal length from the user-specified target. We used w5 = 1 .

The weights wk are chosen to enforce hard constraints (e.g. vignetting) with a relatively large wk, while softer con-straints (e.g. focal length tolerance) use moderate weights. This composite merit function balances image-quality optimization against practical design constraints in a single, differentiable objective. We implemented an automatic differentiable optical simulator to compute the value of this function F . The implementation follows the standard ray-tracing algorithm for geometric optics [47] and uses automatic differentiation for efficient gradient computations [37]. In our current implementation, we assume the first curvature is always positive. As a result, we disregard descriptors with negative first curvature. We intend to account for negative first curvatures in our future work. 

4.3 Results of Double-Gauss Lens Topology Optimization Wall-clock Time. All experiments were executed on a Rocky Linux 9 cluster node with 512 GB RAM and two AMD EPYC 7702 CPUs (64 cores each; 128 cores / 256 hardware threads total). We configured the runtime so that each optimization instance within a single behaviour descriptor used at most two CPU threads; consequently, a full LDG-EA run used at most 100 hardware threads concurrently. Peak memory consumption was approximately 12 GB. In our LDG-EA implementation, running local optimization within a single behavior descriptor requires approximately 4.5 minutes of wall-clock time. Since the λ descriptor evaluations within each generation are independent, full paral-lelization would limit the per-generation runtime to this single-descriptor time. Over 15 generations, the total runtime was 

15 × 4.5 min ≈ 1 h .

We allocated the same total evaluation budget to the baseline algorithm. Because the baseline lacks a parallel com-ponent, this resulted in an approximately 50 times longer wall-clock runtime. Due to the resulting runtime of over 50 hours, we executed the baseline only once, whereas we performed five independent runs of the proposed LDG-EA. 

Diversity of the Approximated Minima. Across all runs, LDG-EA produced on average 14741 local-minimum candidates (standard deviation ≈ 2049 ) over the full evaluation budget. These candidates were distributed over 

636 distinct behavior descriptors on average (standard deviation ≈ 37 ), which is below the theoretical maximum of λ × ILDG-EA = 750 . This gap indicates that the descriptor distribution stabilized during the search, leading to repeated sampling of a subset of descriptors. By Definition 1, solutions from different descriptors are never equiva-lent. Moreover, within each descriptor, LDG-EA identifies multiple pairwise-distinct local minima (sharing the same material indices and curvature-sign pattern, but differing in continuous parameters), as enforced by the Hill-Valley predicate in HV-EA. 9Figure 5: Top five lens designs from one randomly selected LDG-EA run (first row) and their locally refined counter-parts obtained with BFGS (second row). The third row reports the additional post-optimization gain in F , computed as F (θ(before) )/F (θ(after) ). Element glasses are labeled by Schott catalog code and colored by their sorted refractive index at the d-line (587.6 nm) over the catalog. In contrast, our CMA-ES baseline located only about 400 solutions overall, of which just 181 achieved objective values at least as good as the worst LDG-EA solution. Although each baseline solution used a unique glass assignment, the joint optimization approach was less systematic and discovered far fewer high-quality designs under the same computational budget. 

Quality of the Approximated Minima. Across all runs, LDG-EA produced solutions whose objective values ranged from 7 × 10 −4 up to 2.0. For reference, the unrefined Double-Gauss template (Figure 4) has F = 5 × 10 −4, which improves to 7 × 10 −5 after gradient-based optimization of its final surface and sensor distance. After applying our full LDG-EA pipeline, including gradient-based optimization, our best candidate achieved F = 3 × 10 −4.By contrast, the CMA-ES baseline’s best solution reached only F = 1 × 10 −3, and its second-best was already worse than 36 distinct LDG-EA solutions (from the same randomly selected run). In total, just 181 of the CMA-ES outputs matched or exceeded the worst LDG-EA performance. Finally, we further refined the top five solutions from one randomly selected LDG-EA run (out of five) using the BFGS quasi-Newton method [48]. This post-optimization yielded an additional improvement in F ranging from 

×1.0 (negligible) to ×3.6. Figure 5 compares these five LDG-EA solutions with their locally refined counterparts. In the BFGS stage, we optimized the distance to the sensor and all surface curvatures, while keeping all remaining design parameters fixed. Each BFGS run terminated after 1000 iterations or once the gradient norm fell below 10 −6.Overall, the parameters of solutions changed only marginally as seen in Figure 5, suggesting that LDG-EA without the gradient-based refinement already produces designs close to locally optimal. Even so, none of the solutions in all five runs surpassed the tuned reference design’s 7 × 10 −5 (we have not overpassed the baseline). However, it does not diminish LDG-EA’s practical utility: in modern optical-design workflows, RMS spot size can serve as a heuristic starting point rather than a final target [49], and LDG-EA is a fast and configurable method to generate high-quality initial designs. 

Demonstration of Descriptor Learning. We demonstrate that LDG-EA learns a non-uniform distribution over the descriptor space that increasingly favors descriptors in which the subsequent HV-EA stage tends to discover higher-quality solutions. To isolate this effect, we ablated the first stage of LDG-EA and replaced the proposed descriptor sampling with uniform random sampling. We refer to this variant as Ablated LDG-EA .Figure 6 compares the average solution quality produced by LDG-EA and Ablated LDG-EA at each LDG-EA iteration (blue: LDG-EA; salmon: Ablated LDG-EA). Panel (a) reports, across five independent runs, the average of the mean in every iteration of LDG-EA (best solutions in λ descriptors) in five runs of LDG-EA. The shaded area represents the standard deviation of those means. Panel (b) shows a representative single-run comparison. Each dot corresponds to the best solution returned after optimization within a descriptor; we plot all λ dots per iteration for both algorithms. As the blue mean curve (Figure 6 panel a) and the downward shift of the violin plots (Figure 6 panel b) illustrate, both the typical and the worst-case solution quality improve rapidly in the first few generations, confirming that the descriptor distribution quickly adapts to favor high-yield regions of the design space. After generation 5, improvement slows, indicating that exploration is giving way to the exploitation of a smaller set of promising descriptors. The occasional outlier boxes below the main trend show that LDG-EA still discovers novel, superior designs even in later generations. Together, these patterns validate that our two-stage learning mechanism effectively concentrates search 10 (a) Learning curves of the efficient sampling distribution over descriptor space. 

(b) Single-run comparison between LDG-EA and Ablated LDG-EA, depicting an Ablated LDG-EA outlier at iteration 7. Figure 6: Demonstration that LDG-EA learns a non-uniform distribution over the descriptor space that increasingly favors descriptors in which the subsequent HV-EA stage tends to discover higher-quality solutions. The figure com-pares the average solution quality produced by LDG-EA and Ablated LDG-EA at each LDG-EA iteration. effort on descriptors most likely to produce high-quality local minima, while also suggesting that a reduced learning rate or occasional random descriptor injections could help to sustain exploration, overcoming performance plateaus. The percentage of investigated descriptors is very small, since in total there are at least 10 12 different descriptors; therefore, some even better descriptors can still be found. 

# 5 Conlusion 

We have presented the Lens Descriptor-Guided Evolutionary Algorithm (LDG-EA), a two-stage, descriptor-driven framework for discovering diverse, high-quality local optima in complex optical-design spaces. By iteratively sam-pling behavior descriptors, exploiting them via HV-EA powered by CMSA-ES, and then updating the descriptor dis-tribution, LDG-EA dynamically concentrates search effort on the most promising regions of the design space. 11 Applied to the six-element Double-Gauss topology, LDG-EA demonstrated: • High diversity: ∼ 14741 local-minimum candidates grouped into 636 distinct descriptors (in average over five runs), compared with only ∼ 400 solutions from a joint CMA-ES baseline. • Competitive quality: LDG-EA produced best post-processed with gradient optimization RMS spot size 

F = 3 ×10 −4, outperforming CMA-ES’s best F = 1 ×10 −3 and approaching the tuned reference’s 7×10 −5.• Efficient learning: rapid descent of LDG-EA in mean and worst-case F across the first five generations, with the continued discovery of outlier improvements in later iterations. LDG-EA’s inherent parallelism makes it applicable on modern computing infrastructures. Furthermore, by treating RMS spot size as a heuristic initialization metric rather than a terminal criterion [49], LDG-EA provides a flexible starting point for subsequent gradient-based optimization or for full end-to-end diffraction-limited design. 

Future Work. In future work, we plan to investigate adaptive learning-rates α and periodic random descriptor in-jections to maintain exploration in later generations. We also plan to extend LDG-EA to additional lens topologies, incorporate manufacturing-tolerance models, and support multi-objective optimization criteria such as total system cost. Leveraging existing designs reported in patents as priors or warm-starts may further improve design quality and diversity, which we leave for future study. Finally, applying LADE’s [34] and NetCDE [35] mechanisms to generate a larger set of candidate peaks could improve the chances of finding superior designs. We leave the integration of those algorithms into our LDG-EA framework for future work. 

# Declaration of generative AI and AI-assisted technologies in the manuscript preparation process 

During the preparation of this work, the authors utilized ChatGPT to correct grammatical errors and rephrase some sentences to enhance clarity. After using this service, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article. 

# Acknowledgments 

This work was supported by High Tech Holland, project number TKI HTSM/22.0029. 

# References 

[1] ASML, “2022 annual report.” https://www.asml.com/en/investors/annual-report/2022 .[2] D. C. O’Shea, Elements of modern optical design , vol. 2. Wiley New York, 1985. [3] M. Laikin, Lens design . Crc Press, 2018. [4] A. Yabe, Optimization in Lens Design . 2018. [5] S. Das, S. Maity, B.-Y. Qu, and P. N. Suganthan, “Real-parameter evolutionary multimodal optimization—a survey of the state-of-the-art,” Swarm and Evolutionary Computation , vol. 1, no. 2, pp. 71–88, 2011. [6] M. v. Turnhout and F. Bociort, “Instabilities and fractal basins of attraction in optical system optimization,” 

Optics Express , vol. 17, no. 1, pp. 314–328, 2009. [7] K. Antonov, T. Tukker, T. Botari, T. B¨ ack, A. V. Kononova, and N. van Stein, “Quality-diversity driven robust evolutionary optimization of optical designs,” in Computational Optics 2024 , vol. 13023, pp. 84–95, SPIE, 2024. [8] Zemax, “Optics studio.” https://www.zemax.com/pages/opticstudio/ .[9] Synopsys, “Code v optical design software.” www.synopsys.com/optical-solutions/codev.html .[10] Code V, “Global synthesis.” https://www.synopsys.com/optical-solutions/codev/ global-synthesis.html .[11] A. P. Yow, D. Wong, Y. Zhang, C. Menke, R. Wolleschensky, and P. T¨ or¨ ok, “Artificial intelligence in optical lens design,” Artificial Intelligence Review , vol. 57, no. 8, p. 193, 2024. [12] S. Thibault, C. Gagn´ e, J. Beaulieu, and M. Parizeau, “Evolutionary algorithms applied to lens design: case study and analysis,” in Optical Design and Engineering II , vol. 5962, pp. 66–76, SPIE, 2005. 12 [13] K. H¨ oschel and V. Lakshminarayanan, “Genetic algorithms for lens design: a review,” Journal of Optics , vol. 48, no. 1, pp. 134–144, 2019. [14] G. Cˆ ot´ e, J.-F. Lalonde, and S. Thibault, “Extrapolating from lens design databases using deep learning,” Optics express , vol. 27, no. 20, pp. 28279–28292, 2019. [15] G. Cˆ ot´ e, J.-F. Lalonde, and S. Thibault, “Deep learning-enabled framework for automatic lens design starting point generation,” Optics express , vol. 29, no. 3, pp. 3841–3854, 2021. [16] G. Cˆ ot´ e, Y. Zhang, C. Menke, J.-F. Lalonde, and S. Thibault, “Inferring the solution space of microscope objec-tive lenses using deep learning,” Optics Express , vol. 30, no. 5, pp. 6531–6545, 2022. [17] S. Das and P. N. Suganthan, “Differential evolution: A survey of the state-of-the-art,” IEEE transactions on evolutionary computation , vol. 15, no. 1, pp. 4–31, 2010. [18] R. S. Hegde, “Accelerating optics design optimizations with deep learning,” Optical Engineering , vol. 58, no. 6, pp. 065103–065103, 2019. [19] W. Li, X. Jia, Y.-M. Hsu, C.-H. Liao, Y. Wang, M.-T. Lin, and J. Lee, “A novel methodology for lens matching in compact lens module assembly,” IEEE Transactions on Automation Science and Engineering , vol. 20, no. 2, pp. 741–750, 2022. [20] S. Shahane, E. Guleryuz, D. W. Abueidda, A. Lee, J. Liu, X. Yu, R. Chiu, S. Koric, N. R. Aluru, and P. M. Fer-reira, “Surrogate neural network model for sensitivity analysis and uncertainty quantification of the mechanical behavior in the optical lens-barrel assembly,” Computers & Structures , vol. 270, p. 106843, 2022. [21] X. Zhang, H. Li, and H. Yu, “Uniform design and deep learning based liquid lens optimization strategy toward improving dynamic optical performance and lowering driving force,” Optics Express , vol. 31, no. 12, pp. 20174– 20186, 2023. [22] Y. Nagata, “The lens design using the cma-es algorithm,” in Genetic and Evolutionary Computation Conference ,pp. 1189–1200, Springer, 2004. [23] C. Menke, “Application of particle swarm optimization to the automatic design of optical systems,” in Optical Design and Engineering VII , vol. 10690, pp. 293–302, SPIE, 2018. [24] B. Qian, B. Yang, Y. Liu, Q. Zhao, S. Chen, Q. Chen, and Z. Zhao, “Evolved design for telecentric optical systems based on particle swarm optimization and semi-supervised learning,” Optics Communications , vol. 546, p. 129769, 2023. [25] M. Preuss, M. G. Epitropakis, X. Li, and J. E. Fieldsend, Metaheuristics for finding multiple solutions . Springer, 2021. [26] A. V. Kononova, O. M. Shir, T. Tukker, P. Frisco, S. Zeng, and T. B¨ ack, “Addressing the multiplicity of solutions in optical lens design as a niching evolutionary algorithms computational challenge,” in Proceedings of the Genetic and Evolutionary Computation Conference Companion , pp. 1596–1604, 2021. [27] A. V. Kononova, O. M. Shir, T. Tukker, P. Frisco, S. Zeng, and T. B¨ ack, “Locating the local minima in lens design with machine learning,” in Current Developments in Lens Design and Optical Engineering XXII , vol. 11814, p. 1181402, SPIE, 2021. [28] K. Antonov, T. Botari, T. Tukker, T. B¨ ack, N. van Stein, and A. V. Kononova, “New solutions to Cooke triplet problem via analysis of attraction basins,” in Digital Optical Technologies 2023 , vol. 12624, p. 126240T, Inter-national Society for Optics and Photonics, SPIE, 2023. [29] K. Antonov, T. Tukker, T. Botari, T. B¨ ack, A. V. Kononova, and N. van Stein, “Quality–diversity-driven robust evolutionary optimization of optical designs,” Optical Engineering , vol. 64, no. 7, pp. 075106–075106, 2025. [30] S. C. Maree, T. Alderliesten, D. Thierens, and P. A. Bosman, “Real-valued evolutionary multi-modal optimiza-tion driven by hill-valley clustering,” in Proceedings of the genetic and evolutionary computation conference ,pp. 857–864, 2018. [31] N. Hansen, “Benchmarking a bi-population cma-es on the bbob-2009 function testbed,” in Proceedings of the 11th annual conference companion on genetic and evolutionary computation conference: late breaking papers ,pp. 2389–2396, 2009. [32] O. M. Shir and T. B¨ ack, “Niching with Derandomized Evolution Strategies in Artificial and Real-World Land-scapes,” Natural Computing: An International Journal , vol. 8, no. 1, pp. 171–196, 2009. [33] R. Li, J. Eggermont, O. M. Shir, M. T. Emmerich, T. B¨ ack, J. Dijkstra, and J. H. Reiber, “Mixed-integer evolution strategies with dynamic niching,” in International Conference on Parallel Problem Solving from Nature , pp. 246– 255, Springer, 2008. 13 [34] G.-Y. Lin, Z.-G. Chen, C. Liu, Y. Jiang, S. Kwong, J. Zhang, and Z.-H. Zhan, “A landscape-aware differential evolution for multimodal optimization problems,” IEEE Transactions on Evolutionary Computation , 2025. [35] X.-Y. Chen, H. Zhao, and J. Liu, “A network community-based differential evolution for multimodal optimization problems,” Information Sciences , vol. 645, p. 119359, 2023. [36] J. Braat and P. T¨ or¨ ok, Imaging optics . Cambridge university press, 2019. [37] C. Wang, N. Chen, and W. Heidrich, “do: A differentiable engine for deep lens design of computational imaging systems,” IEEE Transactions on Computational Imaging , vol. 8, pp. 905–916, 2022. [38] H. M¨ uhlenbein and G. Paass, “From recombination of genes to the estimation of distributions i. binary parame-ters,” in International conference on parallel problem solving from nature , pp. 178–187, Springer, 1996. [39] H.-G. Beyer and B. Sendhoff, “Covariance matrix adaptation revisited–the cmsa evolution strategy–,” in Inter-national Conference on Parallel Problem Solving from Nature , pp. 123–132, Springer, 2008. [40] P. A. Bosman, J. Grahl, and D. Thierens, “Benchmarking parameter-free amalgam on functions with and without noise,” Evolutionary computation , vol. 21, no. 3, pp. 445–469, 2013. [41] S. C. Maree, D. Thierens, T. Alderliesten, and P. A. Bosman, “Two-phase real-valued multimodal optimization with the hill-valley evolutionary algorithm,” Metaheuristics for Finding Multiple Solutions , pp. 165–189, 2021. [42] J. Shlens, “Notes on kullback-leibler divergence and likelihood,” arXiv preprint arXiv:1404.2000 , 2014. [43] N. Hansen and A. Ostermeier, “Completely derandomized self-adaptation in evolution strategies,” Evolutionary computation , vol. 9, no. 2, pp. 159–195, 2001. [44] T. Marty, N. Hansen, A. Auger, Y. Semet, and S. H´ eron, “Lb+ ic-cma-es: Two simple modifications of cma-es to handle mixed-integer problems,” in International Conference on Parallel Problem Solving from Nature ,pp. 284–299, Springer, 2024. [45] SCHOTT AG, “Schott | pioneer the impossible,” 2025. [46] M. J. Kidger, Intermediate optical design , vol. 134. Spie Press, 2004. [47] M. J. Kidger, Fundamental optical design , vol. 4. SPIE Optical Engineering Press, 2001. [48] J. Nocedal and S. J. Wright, Numerical optimization . Springer, 1999. [49] The Pulsar, “#DevOptical Part 10: RMS Spot Size.” Tutorial post on RMS spot size as a design heuristic. 14 A Constants 

Table 1: Key constants used in the implementation of the algorithms Symbol Value Description 

λ 50 Number of descriptors sampled per iteration 

μ 5 Number of top descriptors selected for update 

α 1 Learning rate for descriptor distribution update 

w 0.50 Quality-window width (allowable F -span in archive) 

ILDG-EA 15 Number of iterations in the algorithm LDG-EA 

B = THV-EA 10 5 Total budget of F evaluations for each HV-EA run 

TCMA-ES 50 · 15 · 10 5 Total budget of F evaluations for each CMA-ES run 

εparam 10 −10 CMSA-ES and CMA-ES mean-vector update tolerance 

εfun 10 −10 CMSA-ES and CMA-ES objective-change tolerance 

εhist 10 −5 CMSA-ES and CMA-ES fitness-history stagnation tolerance 

κmin 4 mm Absolute curvature threshold (manufacturing limit) 15