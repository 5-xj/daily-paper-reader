Title: Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed when Solving Expensive Unconstrained Multi-Objective Optimisation Problems

URL Source: https://arxiv.org/pdf/2601.21885v1

Published Time: Fri, 30 Jan 2026 02:35:53 GMT

Number of Pages: 27

Markdown Content:
# Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed when Solving Expensive Unconstrained Multi-Objective Optimisation Problems 

## Tiwonge Msulira Banda âˆ—, Alexandru-Ciprian ZÄƒvoianu 

School of Computing, Engineering and Technology, Robert Gordon University, Aberdeen, AB10 7QB, United Kingdom 

A R T I C L E I N F O 

Keywords :Surrogate models NSGA-II MOEA/D Multi-objective Optimisation Evolutionary Algorithms 

A B S T R A C T 

Multi-Objective Evolutionary Algorithms (MOEAs) have proven effective at solving Multi-Objective Optimisation Problems (MOOPs). However, their performance can be significantly hindered when applied to computationally intensive industrial problems. To address this limitation, we propose an adaptive surrogate modelling approach designed to accelerate the early-stage convergence speed of state-of-the-art MOEAs. This is important because it ensures that a solver can identify optimal or near-optimal solutions with relatively few fitness function evaluations, thereby saving both time and computational resources. Our method employs a two-loop architecture. The outer loop runs a (baseline) host MOEA which carries out true fitness evaluations. The inner loop contains an Adaptive Accelerator that leverages data-driven machine learning (ML) surrogate models to approximate fitness functions. Integrated with NSGA-II and MOEA/D, our approach was tested on 31 widely known benchmark problems and a real-world North Sea fish abundance modelling case study. The results demonstrate that by incorporating Gaussian Process Regression, one-dimensional Convolutional Neural Networks, and Random Forest Regression, our proposed approach significantly accelerates the convergence speed of MOEAs in the early phases of optimisation. 

## 1. Introduction 

Multi-Objective Optimisation Problems (MOOPs) are a class of optimisation problems that involve multiple and often conflicting objectives. These are commonly encoun-tered in industrial applications such as manufacturing and product design [41], logistics [66, 9], etc. MOOPs rarely have a single solution (i.e., that is optimal across all the objectives simultaneously), as a gain in one objective results in a decline on another. Therefore, the general goal in solving MOOPs is to identify a set of Pareto-optimal solutions (PS) that describe the best trade-offs between the considered objectives. This endeavour is complex as the PS can have an arbitrarily large size, especially when the number of objec-tives increases. For many problems, the PS is unknown and so a solver aims to find high-quality Pareto non-dominated solutions (PNs) that provide an approximation of the PS. We are particularly interested in problems that have 2 to 3 objectives, while those with 4 or more objectives are called many-objective optimisation problems [48, 33, 13]. Over the decades, population-based nature-inspired Multi-Objective Evolutionary Algorithms (MOEAs) have proved effective at solving MOOPs because they are able to identify PNs in a single optimisation run for decision makers to review and choose from [17]. Since the mid 70s, when John Holland popularised the Genetic Algorithm in his book Adaptation in Natural and Artificial Systems [28], later updated in 1992 [29], effective and powerful MOEAs have 

> âˆ—

Corresponding author  

> t.banda@rgu.ac.uk

(T.M. Banda); c.zavoianu@rgu.ac.uk (A. ZÄƒvoianu) 

> ORCID

(s): 0000-0002-2344-0397 (T.M. Banda); 0000-0003-1003-7504 

(A. ZÄƒvoianu) 

been developed, improved upon and used in various research and industrial applications [41, 66, 9, 10, 1]. State-of-the-art MOEAs use three distinct approaches: 

â€¢ MOEAs that use Pareto-dominance to guide the search process, e.g. the Strength Pareto Evolutionary Algo-rithm 2 (SPEA2) [70]; the Nondominated Sorting Genetic Algorithm 2 (NSGA-II) [22]; and the Gen-eralised Differential Evolution 3 solver (GDE3) [37]. 

â€¢ Indicator-based MOEAs, e.g. Indicator-Based Evo-lutionary Algorithm (IBEA) [68]; the S-Metric Se-lection Evolutionary Multiobjective Optimisation Al-gorithm (SMS-EMOA) [8]; and Hypervolume-based Estimation of Distribution Algorithm (HypE) [5]. 

â€¢ Reference-point based decomposition MOEAs, e.g. Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D) [64]; and NSGA-III [21]. The effectiveness of MOEAs however comes into ques-tion when they are used to solve computationally intensive MOOPs (CI-MOOPs). The criticism arises from the fact that MOEAs typically need to evaluate (tens of) thousands of candidate solutions for them to find optimal or near-optimal solutions. While this is reasonable for problems whose fit-ness function is less time consuming to evaluate, it is not feasible for CI-MOOPs. Fitness functions for the majority of industrial optimisation problems take significant amounts of time to evaluate. Others require specialised software with limited licenses. For example, the optimisation of the design of an electric motor involves optimising the geometry of the motor, selection of materials, and thermal aspects, among others. Evaluation of each candidate design solution involves 

First Author et al.: Preprint submitted to Elsevier Page 1 of 22 

> arXiv:2601.21885v1 [cs.NE] 29 Jan 2026 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

finite element analysis or computational fluid dynamics, which take a lot of time. For such a problem, one may only be able to give the MOEA a computational budget of a few hundred or thousand evaluations and hope that the algorithm would find good solutions [11, 31]. To address the challenges above, the use of surrogate models has been proposed [52]. Surrogate models (or meta models) are incorporated into a MOEA to speed up the optimisation run, either by replacing the expensive fitness function with an estimate [26, 44, 3, 51] or pre-selecting viable solutions [42, 46, 40, 61, 6]. In the former, the goal is to reduce the number of expensive fitness function calls, whereas, in the latter, the aim is to improve efficiency by only expending the expensive fitness function call on vi-able solutions. While dozens, perhaps hundreds of surrogate models have been proposed and keep being published, there are still challenges associated with them, which present opportunities for further research. For example, there are questions such as: a) How can training data be efficiently created on-the-fly for the surrogate model? b) When is the right time to introduce and exit a surrogate model during an evolutionary run? c) What are efficient ways of integrating the surrogate into a MOEA? d) What data-driven model is efficient for a surrogate? In this paper, we propose a strategy for constructing on-the-fly surrogate models to accelerate the convergence speed of existing MOEAs. Our approach addresses key challenges in surrogate-assisted optimisation above in the following unique ways. 

â€¢ To efficiently generate training data, we use solutions from the immediately preceding generation, eliminat-ing the need for extensive archives or complex data filtering mechanisms (e.g., clustering). This reduces model training time and simplifies integration. In this way, no additional data-driven models or filtering pro-cedures are required for selecting solutions for model training. 

â€¢ The surrogate is introduced early in the run (i.e. af-ter 2 generations) and remains active only while it continues to contribute to the discovery of promising solutions. A built-in adaptive exit mechanism auto-matically disables the surrogate once it stops offering improvement, ensuring that the surrogate does not unnecessarily delay the optimisation. 

â€¢ Integration with a MOEA is done by attaching the surrogate as a modular accelerator, which estimates objective values for solutions and uses a simple ran-dom selection strategy for passing surrogate solutions to the MOEA for re-evaluation with the true fitness function. This minimises disruption and avoids the overhead of complex surrogate management. 

â€¢ The approach is flexible in terms of model choice; any effective data-driven regression model can be used to approximate the objective function. While similar methods have been proposed in recent years [15, 60, 65, 62], our approach is unique in its sim-plicity, plug-and-play modularity, and the minimal overhead it imposes on the base MOEA. The rest of the paper is structured as follows: In section 2, we go through some published literature that is related to our work. In section 3, we describe our proposed approach. Section 4 contains the description of the experimental design. In section 5, we present our results and discuss them. Lastly, in section 6, we conclude and highlight areas for further research. 

## 2. Related Work 

An unconstrained MOOP with ğ‘› real-valued variables and ğ‘š objectives can be defined as: 

ğ‘€ğ‘–ğ‘›ğ‘–ğ‘šğ‘–ğ‘ ğ‘’ ğ¹ (ğ‘¥ ) = ( ğ‘“ 1(ğ‘¥ ), . . . ğ‘“ ğ‘š (ğ‘¥ )) , ğ‘š â‰¤ 3 (1) where ğ‘¥ âˆˆ â„ğ‘› represents a candidate solution and is subject to ğ‘¥ ğ‘™ â‰¤ ğ‘¥ â‰¤ ğ‘¥ ğ‘¢ , the lower and upper boundaries, and ğ‘“ 1

to ğ‘“ ğ‘š represent individual objectives (usually not more than 3) that must be optimised simultaneously. In this paper, we assume that the objectives ğ‘“ 1(ğ‘¥ ) to ğ‘“ ğ‘š (ğ‘¥ ) are expensive to evaluate. Numerous studies have explored the use of surrogate models to accelerate optimisation in Multi-Objective Evolu-tionary Algorithms (MOEAs). One of the earliest and most influential works is Jones et al. (1998) [34], who introduced Kriging to optimisation. Kriging, which is closely related to Gaussian Process Regression (GPR), is a method for approx-imating unknown functions by predicting values at unob-served points based on historical data. The Algorithm was called Efficient Global Optimisation (EGO). The method uses a weighted average of data points, with weights de-termined by the spatial correlation between them, captured by a covariance function. In optimisation, Kriging serves as a surrogate model for CI-MOOPs, providing not only predictions but also uncertainty estimates. This uncertainty helps balance exploration and exploitation, enabling more efficient optimisation by reducing the number of expensive fitness function evaluations. Originally, Kriging was used on single-objective optimi-sation problems, but later its use was extended to MOOPs. In this case, a surrogate model is constructed for each objective, for example in Pareto Efficient Global Optimiza-tion (ParEGO) [36]. Other than Kriging, various machine learning algorithms have been used as surrogates over the years, including support vector machines [3], radial basis functions [26], polynomial regression [44] and artificial neural networks (ANNs) [27] . These methods are effective, but the surrogates risk taking too long to train models as the number of objectives increases. Graphical Processing Units (GPUs) have made it in-creasingly feasible to construct surrogate models based on deep neural networks (DNNs). Although DNNs can achieve higher accuracy than traditional machine learning models, their adoption as surrogate models has been limited due to their computational complexity and the longer training times   

> First Author et al.: Preprint submitted to Elsevier Page 2 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

required. With the advent of powerful GPUs, however, this burden has been significantly reduced, leading to the emer-gence of deep learningâ€“based surrogate models. Examples include [58], where a multi-output deep neural network is used to approximate objective functions and constraints, and [25], which employs a dropout neural network. Most surrogate models are attached to an existing MOEA to assist it. Usually, state-of-the-art MOEAs are used. This approach serves well for comparison as it is easy to compare the surrogate-assisted MOEA with the unassisted baseline MOEA. Then there is the question of when it is best to introduce the surrogate. One approach is one where the MOEA alternates with the surrogate. After initialisation, the MOEA runs for several generations with the true fitness function evaluations before handing over to the surrogate when enough training data has been collected. Then the surrogate kicks in, running for several generations, produc-ing a final set of solutions which are re-evaluated by the true fitness function. This was used in [72], where ANN-based surrogate was attached to NSGA-II in what the authors called HybridOpt. In their experimentation, NSGA-II ran for 25 generations, and thereafter, the ANN-based surrogate was initialised, taking over from the MOEA, producing a final set of Pareto non-dominated solutions that was re-evaluated by the true fitness function. A similar approach was used by Nain and Deb in [45] where an ANN-based surrogate was paired with a Genetic Algorithm (GA). The key difference with the HybridOpt is that in Nain and Deb, the two alternate multiple times. Upon initialisation, the GA runs with true fitness functions multiple times, then the surrogates are built and run for several times, handing back to the GA to run for several generations. Each time the surrogate kicks in, it is trained on updated data. The final population is evaluated with the true fitness function. Although effective, this alternating framework is not ideal when the goal is achieving early convergence as the MOEA needs to run for several generations before the surrogate can influence it. Another approach for attaching surrogates to MOEAs uses a two-loop architecture, where the MOEA runs in the main or outer loop and the surrogate runs in the secondary or inner loop. There are several papers using this approach with important differences in the way training data is constructed and how the resulting output from the surrogate is treated. In Yagoubi and Baderina [60], support vector regression (SVR) based surrogate is attached to NSGA-II. After the initial population is created by NSGA-II in the main loop, the solutions are evaluated with the true function and passed on to the inner loop for training of surrogate models. The inner loop runs for several generations, using the surrogate models, after which a k-means clustering algorithm is used to cluster solutions, then non-dominated ranking is applied in each cluster to determine a winner from each cluster to be passed on to the main loop for re-evaluation with the true fit-ness function. Another algorithm, Two-stage infill Strategy and surrogate-ensemble assisted Expensive Many-objective evolutionary Optimization algorithm (TSEMO) [65] works in a similar fashion. While the algorithms outperformed the standard NSGA-II, the introduction of the clustering algorithm and another instance of non-dominated ranking increases the complexity and runtime of the solver. Kriging Assisted Reference Vector Evolutionary Algo-rithm (K-RVEA) [15] also incorporates Gaussian Processes Regression (GPR) as a surrogate to assist the Reference Vector guided Evolutionary Algorithm (RVEA) [14] in a two-loop architecture. The main loop uses RVEA and evalu-ates solutions using the true fitness function. The inner loop uses GPR to estimate the fitness functions. The algorithm uses uncertainty information from the Gaussian Processes to determine which surrogate solutions should be passed on to the main loop for re-evaluation with the true fitness function. A key distinction is in the way K-RVEA creates training data. The algorithm stores training data in archive with a pre-determined size. Individual solutions are added to the archive from recently evaluated solutions minus duplicates. If the number is large, reference point information and a clustering approach are used to select viable solutions. Again, the filtering of surrogate solutions is complex. A slightly different, but important surrogate-assisted MOEA that uses the two-loop architecture is the Classification-based Surrogate-Assisted Evolutionary Algorithm (CSEA) [46]. We label CSEA as different because the surrogate model in the solver does not approximate the objective functions directly; instead, it acts as a classifier that classifies candidate based on their estimated viability. The algorithm constructs training data by selecting a subset of reference solutions that have been evaluated using the true objec-tive functions. The reference solutions are then labelled to form a classification boundary separating promising and unpromising regions of the search space. In its original formulation, CSEA employed a Feedforward Neural Net-work (FFNN) [55] as the surrogate classifier. The inner loop uses the classifier to guide the evolutionary search by rapidly screening candidate solutions, while the outer loop periodically updates the classifier using newly acquired true evaluations. Only the most promising candidate solutions, as predicted by the classifier, are selected for evaluation with the true/expensive fitness functions. While a majority of surrogate-assisted MOEAs over-whelmingly use machine learning algorithms as surro-gate models, a recent publication proposed a new way of lightweight surrogate models that are based on interpolation functions that do not require any training [62]. The approach aims to accelerate the convergence speed of MOEAs by promoting the early creation of high-quality candidate so-lutions through pre-emptive evaluation (PE) and speculative exploration (SE) â€“ which we describe below. Both strategies rely on the fitness approximation capabilities of lightweight interpolation models based on Shepardâ€™s inverse distance weighting function [53]. By leveraging this function, the interpolation-based strategies reduce the number of fitness evaluations needed to generate high-quality Pareto front approximations during the MOEA process.   

> First Author et al.: Preprint submitted to Elsevier Page 3 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

â€¢ Pre-emptive Evaluation (PE): This approach has a filter that is used to pre-emptively evaluate a solution once it is created using Shepardâ€™s inverse distance weighting function instead of the true fitness func-tion. An individual that passes a certain threshold is selected, whereas one that doesnâ€™t is discarded. The algorithm includes a mechanism for accepting solutions if the number of failed consecutive attempts to generate a high-quality offspring exceeds a certain threshold. This way, the algorithm forces the creation of high-quality solutions, which are passed on for evaluation with the true fitness function. 

â€¢ Speculative Exploration (SE): This approach fol-lows a two-loop architecture as described before. A key feature is that a surrogate multi-objective inter-polated continuous optimisation problem (MO-ICOP) that mirrors the definition of the original problem to be solved is constructed in the inner loop and evaluated using Shepardâ€™s inverse distance weighting function instead of the true fitness function. After a predefined number of generations, the inner loop outputs sur-rogate solutions for evaluation with the true fitness function. In [62], the two approaches were incorporated into NSGA-II and the Differential Evolution-based, Coevolution-ary Multi-objective Optimization (DECMO++) [73]. An instance of NSGA-II enhanced with both strategies (NSGA-II PE+SE) significantly sped up the convergence speed of NSGA-II, whilst only using the PE strategy induced a smaller performance boost. In this paper, we adopt the two-loop architecture and, building upon recent developments in [6], introduce a novel general-purpose method for on-the-fly surrogate-based MOEA convergence acceleration featuring adaptive surrogate acti-vation and streamlined training data generation and surro-gate result integration. As such, we only use solutions from the previous generation to build the surrogate models and, instead of employing clustering algorithms, we randomly select surrogate solutions to pass to the main loop for re-evaluation using the true fitness function. Surrogates are activated as early as possible, but their usage interval is governed by an adaptive performance indicator. By limiting total surrogate usage as well as their training data to the previous generation evaluated with the true fitness function, our approach aims to strike a balance between more inten-sive ML models that require several generations to collect sufficient training data and/or rely on complex filtering algorithms on one hand, and the recent interpolation-based (light) surrogate models that do not require training but might be more prone to underfitting on the other hand. To illustrate the pros and cons of the proposed approach, we integrated the proposed adaptive on-the-fly surrogate accelerator strategy with NSGA-II and MOEA/D, which are dominance-based and reference-point-based algorithms, and carried out both comprehensive benchmark testing and a case study analysis.  

> Figure 1: Overview of the Adaptive Accelerated MOEA show-ing the two loops. The standard (host) MOEA is shown in black whereas the surrogate-based accelerator is shown in blue.

## 3. Proposed Approach 

In this section, we describe our proposed framework for accelerating the convergence speed of MOEAs when solving computationally intensive, unconstrained MOOPs. As mentioned in section 2, our approach adopts a two-loop architecture. A standard / host MOEA, in our case, NSGA-II or MOEA/D (shown in grey in Figure 1) runs in the outer loop and performs true (but expensive) fitness function eval-uations. The Adaptive Accelerator, which runs in the inner loop (highlighted in light blue in Figure 1 and detailed in lines 8-17 of Algorithm 1), is responsible for generating and evaluating candidate solutions using a surrogate model. To avoid ambiguity, we refer to solutions generated by the stan-dard / host MOEA as real solutions, and those generated by the Adaptive Accelerator as surrogate solutions. An MOEA equipped with the Adaptive Accelerator will be referred to as an Accelerated MOEA or a surrogate variant (of the standard MOEA). In terms of integration, the Adaptive Accelerator is a customised extension of its host MOEA and is positioned between the Offspring Generation and Fitness Evaluation phases of the host MOEA. The Accelerated MOEA operates as follows: When ini-tialised, the standard / host MOEA runs normally for the   

> First Author et al.: Preprint submitted to Elsevier Page 4 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

Algorithm 1 Accelerated Multi-Objective Evolutionary Algorithm (Accelerated MOEA) 

Input: Population size ğ‘ , number of main generations ğº 

Output: Non-dominated solution set  

> 1:

ğ‘šğ‘ğ‘–ğ‘›ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› 0 â† CreateInitialPopulation (ğ‘ ) 

> 2:

ğ‘šğ‘ğ‘–ğ‘›ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› 0 â† FitnessEvaluation (ğ‘šğ‘ğ‘–ğ‘›ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› 0) 

> 3:

ğ‘šğ‘ğ‘–ğ‘›ğºğ‘’ğ‘› â† 1 

> 4:

ğº ğ‘  â† âŒŠğº âˆ•2 âŒ‹ 

> 5:

while ğ‘šğ‘ğ‘–ğ‘›ğºğ‘’ğ‘› < ğº do  

> 6:

ğ‘šğ‘ğ‘–ğ‘›ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” â† SelectParents (ğ‘šğ‘ğ‘–ğ‘›ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘šğ‘ğ‘–ğ‘›ğºğ‘’ğ‘› âˆ’1 ) 

> 7:

ğ‘šğ‘ğ‘–ğ‘›ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” â† OffspringGeneration (ğ‘šğ‘ğ‘–ğ‘›ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” ) 

> 8:

ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ´ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ â† EvaluateSurrogateStatus()  

> 9:

if ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ´ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ then  

> 10:

ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™ â† TrainSurrogates (ğ‘šğ‘ğ‘–ğ‘›ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘šğ‘ğ‘–ğ‘›ğºğ‘’ğ‘› âˆ’1 ) 

> 11:

ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› 0 â† EstimateFitness (ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™, ğ‘šğ‘ğ‘–ğ‘›ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” ) 

> 12:

ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğºğ‘’ğ‘› â† 1 

> 13:

while ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğºğ‘’ğ‘› < ğº ğ‘  do  

> 14:

ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘ƒ ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  â† SelectParents (ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğºğ‘’ğ‘› âˆ’1 ) 

> 15:

ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” â† OffspringGeneration (ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘ƒ ğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘  ) 

> 16:

ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” â† EstimateFitness (ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™, ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” ) 

> 17:

ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğºğ‘’ğ‘› â† PopulationUpdate (ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğºğ‘’ğ‘› âˆ’1 , ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” ) 

> 18:

ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğºğ‘’ğ‘› â† ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğºğ‘’ğ‘› + 1  

> 19:

end while  

> 20:

ğ‘šğ‘ğ‘–ğ‘›ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” â† ConsolidateOffspring (ğ‘šğ‘ğ‘–ğ‘›ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”, ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğ‘œğ‘”ğ‘ğ‘¡ğ‘’ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘ ğ‘¢ğ‘Ÿğ‘Ÿğºğ‘’ğ‘› âˆ’1 ) 

> 21:

end if  

> 22:

ğ‘šğ‘ğ‘–ğ‘›ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” â† FitnessEvaluation (ğ‘šğ‘ğ‘–ğ‘›ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” ) 

> 23:

ğ‘šğ‘ğ‘–ğ‘›ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘šğ‘ğ‘–ğ‘›ğºğ‘’ğ‘› â† PopulationUpdate (ğ‘šğ‘ğ‘–ğ‘›ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘šğ‘ğ‘–ğ‘›ğºğ‘’ğ‘› âˆ’1 , ğ‘šğ‘ğ‘–ğ‘›ğ‘‚ğ‘“ ğ‘“ ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘” ) 

> 24:

ğ‘šğ‘ğ‘–ğ‘›ğºğ‘’ğ‘› â† ğ‘šğ‘ğ‘–ğ‘›ğºğ‘’ğ‘› + 1  

> 25:

end while  

> 26:

return nonDominatedSolutions (ğ‘šğ‘ğ‘–ğ‘›ğ‘ƒ ğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘šğ‘ğ‘–ğ‘›ğºğ‘’ğ‘› âˆ’1 )

first two generations (lines 1-7 and 23-25). This initial phase allows the evolutionary process to start shaping the Pareto front, as the initial generation (generation 0) consists of ran-domly generated solutions which are typically too scattered in the objective space to be effectively used by the subse-quent acceleration mechanism. Solutions from the next gen-eration onwards (generation 1+) are created by the solverâ€™s genetic operators and it is at this point that the Pareto Front begins to evolve. We hypothesise that for surrogates that aim at fast initial convergence, it is beneficial to in-troduce them into the host MOEA as early as possible to get maximum benefit and consider generation=2 is an appropriate surrogate initialisation threshold. The training data creating mechanism we propose is very straightforward. When the Adaptive Accelerator is activated (line 8), the recently evaluated population ( mainPopulation ) and the new offspring population ( mainOffspring ), which is unevaluated, are sent to the Adaptive Accelerator. Only the recently evaluated population is used to train surrogate models (line 10) for each objective, whereas the unevaluated offspring population is loaded as the initial population for the Adap-tive Accelerator and evaluated using the trained surrogate models (line 11). The Accelerator then proceeds with its own evolutionary process for several generations using the trained surrogate models for objective function evaluation (lines 12-19). At the end, the Accelerator outputs a set of surrogate non-dominated solutions (Surrogate PN or surro-gatePopulation ), some of which are fed back into the host MOEA in the main loop for re-evaluation using the actual fitness function. Extensive experiments we report on in section 5 demonstrate that this approach allows the Adaptive Accelerator to significantly speed up the convergence of its host MOEA at a reduced computational cost. 

3.1. Data collection and pre-processing 

As described earlier, the outer loop passes on the most recently evaluated population of solutions to the Adaptive Accelerator component. The solutions are processed accord-ingly to create training data, assigning the solution variables as input features and solution objective function values as outputs or targets. The size of the training set is always equal to the population size, which helps reduce the amount of time required to train the surrogate models, even when em-ploying more complex ML architectures and best parameter grid searches. The training data is then normalised using minimax normalisation, which involves rescaling data range between 0 and 1, by subtracting the minimum value and dividing by the range of the feature. 

3.2. Model training and fitness function estimation 

The success of any surrogate model lies in its ability to accurately estimate the objective function values of new   

> First Author et al.: Preprint submitted to Elsevier Page 5 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

solutions. Due to the fact that the objectives of a given problem can be quite heterogenous and the surrogate models need to be created on-the-fly, the Accelerator creates an individual surrogate model for each objective. Technically, any regression model can be integrated and used for this purpose, and we experimented with Random Forest Regres-sion (RFR), Gaussian Processes Regression (GPR), and 1-Dimension Convolution Neural Network (CNN). We chose to experiment with these as surrogate models due to their established use in surrogate-assisted optimisation. The three, as with many ML models are non-parametric, i.e., they do not assume a specific data distribution. This is important for our modelling strategy because we cant not make any assumptions about the distribution of the training data, es-pecially in the early phases of the optimisation. Below, we provide some details about the three chosen models: 

â€¢ Random Forest Regression (RFR): This is a ro-bust non-parametric supervised machine learning al-gorithm used for classification and regression prob-lems. Originally proposed by Breiman in 2001 [12], the algorithm is based on the principle of ensemble learning through bagging (Bootstrap Aggregating). The algorithm builds numerous independent Decision Trees during training, each trained on a random subset of data and features. For classification tasks, the final output is determined by majority voting among the trees, while for regression, it is the average of all tree predictions. An individual Decision Trees works by creating hierarchical tree-like structures that partition the data based on the values of features, leading to a prediction about a target variable. The fundamental idea behind a Decision Tree is to create a set of rules that can be used to classify or predict the value of a target variable based on the features of the data. These rules are learned by recursively partitioning the data into smaller and smaller subsets based on the most significant features [47]. RFR is robust to noise and capable of capturing complex, non-linear interactions. 

â€¢ Gaussian Processes Regression (GPR): Gaussian Process [50] is also a non-parametric, supervised ma-chine learning algorithm for both regression and clas-sification. It differs from most other machine learn-ing models in that, in addition to making an esti-mate, it also provides a measure of uncertainty, which quantifies how confident the model is of its estimate. Conceptually, a Gaussian Process can be viewed as a distribution over possible functions, fully defined by a mean function (often assumed to be zero) and a covariance function or kernel, which measures the similarity between input points. GPR is widely used in surrogate modelling, particularly when training data is limited. It operates by considering an infinite set of po-tential functions that could explain the observed data and represents relationships between data points using a covariance matrix derived from the kernel. Similar points have high covariance values, while dissimilar ones have low covariance. When making predictions, the GPR uses the kernel to evaluate how closely a new point relates to the training data and produces a predictive distribution. The mean represents the expected output and the variance indicates the modelâ€™s confidence. 

â€¢ 1-Dimension Convolution Neural Network (CNN): 

This is a type of deep learning model primarily de-signed to process sequential or time-series data. Un-like traditional fully connected CNNs, a 1D-CNN applies convolutional filters along one spatial dimen-sion, enabling it to automatically extract local pat-terns and dependencies within the sequence without the need for manual feature engineering [35]. For a surrogate model this is important because the algo-rithm learns the relationships between the decision variables. The model consists of convolutional layers that slide learnable filters across the input sequence to produce feature maps, followed by pooling layers that down sample these maps to reduce dimensionality and highlight the most important features. These extracted features are then passed through fully connected layers for final prediction. Due to its ability to learn spatial hierarchies of features directly from raw data, the 1D-CNN is both computationally efficient and highly effective in capturing local correlations and temporal structures, making it a powerful choice for a surrogate [63]. The architecture of the 1D-CNN comprised of an in-put layer accepting sequences of length corresponding to the number of the problemâ€™s decision variables, followed by two 1D convolutional layers, each with 64 filters, ReLU activation, L2 regularisation, and a kernel size of 3. Each 1D convolutional layer was followed by a dropout layer with a dropout rate of 0.2. The convolutional outputs are flattened and passed through a dense layer with 32 neurons for feature abstraction, followed by a single output neuron to produce the final prediction. The small kernel size biases the network to predominantly focus on local variable interaction patterns (dependencies between adjacent decision variables), whilst the robust multi-layer architecture enables it to capture wider / more complex interaction patterns. An adaptive kernel size might improve performance across problem classes, but it will also increase the complexity and computa-tional cost of the surrogate model, which will further increase the surrogate overheads on the base MOEA. The first stage in model training is to find the best hyper-parameters for the model and this stage is known to have a high time complexity. Apart from dataset size, the time com-plexity for hyper-parameter search depends on the number of parameter combinations sampled, the number of cross-validation (CV) folds, and the time complexity of fitting the estimator itself. To reduce the overall time complexity of the algorithm, the Adaptive Accelerator uses randomised search   

> First Author et al.: Preprint submitted to Elsevier Page 6 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

cross-validation, which has a reduced number of parameter combinations. When the best parameters are identified, a final model is trained and is used to estimate the fitness func-tion values of the particular objective. After constructing all required surrogates, the Adaptive Accelerator proceeds with the rest of the evolutionary steps, population update, selection of parents and reproduction of offspring until its exit criterion is met (i.e., a pre-defined number of inner loop generations has been evolved). Numerical experiments on standard PCs/CPUs show that given the limited sizes of the training sets and usage of randomised search CV, the full training of a surrogate model takes on average 205.4 sec-onds (3 minutes and 25 seconds). This can be undoubtedly improved on using GPU-based parallelisation for certain classes of models. As inference times are negligible, with a basic objective-wise training parallelisation, the total surro-gate usage overhead is limited by the adaptive deactivation mechanism to between 17 minutes (GPR) and 36 minutes (RFR). The full breakdown of the surrogate computational burden is presented in Table 1.             

> Table 1
> Details of average surrogate usage overheads (in seconds). The single inner loop sums up all inference times for one run of the inner loop whilst total usage shows the average overhead of applying the Adaptive Accelerator.
> Model Single training Single inner loop Total usage
> RFR 260.18 1.24 2091.47 GPR 208.86 0.27 1018.49 1D-CNN 147.05 7.94 1586.10

3.3. Parameters 

Having described how the algorithm works, it is time to introduce the key parameters of the proposed algorithm in more detail. While we describe them as parameters that may require tuning to improve algorithm performance, we are aware that introducing user-defined parameters increases algorithm complexity. In [18] and [67], it is established that minimising user-defined parameters enhances the al-gorithmâ€™s robustness, and makes comparative evaluation complicated. The evolution from NSGA [54] to NSGA-II is a good example of how removing user-defined parameters can improve usability and performance stability. Similarly, the decomposition-based [64] and reference-based [21] solvers demonstrate that adaptive or structural design choices can re-place the need for manually tuned parameters, aligning with the broader trend towards self-adaptive and parameterless evolutionary algorithms. To avoid introducing additional user-defined parameters, we adopt a "rule of 1/2" strategy that links the three pa-rameters described below to existing parameters of the host algorithm. This approach renders the proposed surrogate modelling technique effectively adaptive and apparently pa-rameterless, as the new parameters donâ€™t need to be manually specified. 

â€¢ Number of inner loop surrogate generations: When the algorithm enters the inner loop, i.e., the Adaptive Accelerator, a question arises: how many generations should it run for? A key aspect of answering this question lies in the fact that surrogate models are trained each time the Adaptive Accelerator is activated and used to estimate objective functions until the end of the inner loop run. The challenge is that if the surrogate runs only for a few inner generations, we limit its acceleration capability. On the other hand, if we run it for too long, the performance of the surrogate models degrade given that the search space shifts as the optimisation progresses. In machine learning terms, there is a data shift that happens when the distribution of data used for training models differs from that of the data used in deployment [49] (i.e., very late inner loop evaluations). Based on prelim-inary experiments, we decided to fix the number of surrogate generations by setting it to be equal to half of the generations used by the outer loop (line 4 of Algorithm 1). 

â€¢ Surrogate integration threshold: When the Adap-tive Accelerator exits, it outputs a set of surrogate PN solutions that is equal to or less than the offspring population size of the host solver. The question is, 

how many of these surrogate PN solutions should be transferred to the offspring population of the main (host) loop for re-evaluation with the true fitness func-tion? We noted during our preliminary experiments that letting the offspring population of the host MOEA comprise of surrogate solutions only led to loss of diversity and degraded performance, but if we only took a fraction of the surrogate PNs, the MOEAâ€™s performance would improve. To ensure that the sur-rogate effectively plays the role of an accelerator to the host MOEA, we introduce a threshold for the maximum number of surrogate solutions to add to the offspring population for re-evaluation. The rest of the solutions are taken from the original offspring pop-ulation generated by the host MOEA before sending them to the Accelerator. This is depicted by the step "Consolidate Offspring Population" in Figure 1 and line 20 in Algorithm 1. We experimented with 25%, 50%, 75% and 100% of the offspring population size and settled on a constant threshold value of 50%. 

â€¢ Adaptive surrogate deactivation: Whilst, all the ex-periments that we carried out indicate that for each tested problem there is a tipping point after which our Adaptive Accelerator no longer benefits its host MOEA, the exact placement (in terms of generation no.) of the tipping point is problem and even optimi-sation run specific. As such, we propose an adaptive strategy to answer the key emerging question of when is the best time to deactivate the surrogate? The idea is that whilst the surrogate is active, we monitor the host solverâ€™s archive and automatically disable the   

> First Author et al.: Preprint submitted to Elsevier Page 7 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

Algorithm 2 Adaptive Surrogate Exit Mechanism  

> 1:

Input: Archive îˆ­ğ‘¡ at generation ğ‘¡ , historicalMaximum ğœ‡  

> 2:

Output: Accelerator status (active/inactive)  

> 3: 4:

ğ‘  ğ‘¡ â† count of surrogate solutions in îˆ­ğ‘¡ generated at generation ğ‘¡ âˆ’ 2  

> 5:

|îˆ­ğ‘¡ | â† total number of solutions in archive  

> 6: 7:

ğœ’ ğ‘¡ â† ğ‘  ğ‘¡  

> |îˆ­ğ‘¡ |

âŠ³ Current proportion of surrogate solutions from penultimate generation  

> 8: 9:

if ğ‘ ğ‘¡ > ğœ‡ then  

> 10:

ğœ‡ â† ğœ’ ğ‘¡ âŠ³ Update historical maximum  

> 11:

end if  

> 12: 13:

if ğœ‡ â‰  0 and ğœ’ ğ‘¡ â‰¤ ğœ‡  

> 2

then  

> 14:

deactivate Accelerator âŠ³ Half-life threshold reached  

> 15:

else if ğ‘¡ = 50 then  

> 16:

deactivate Accelerator âŠ³ Generation limit reached  

> 17:

else  

> 18:

keep Accelerator active  

> 19:

end if 

Adaptive Accelerator when its contribution declines. Specifically, when evaluating the surrogate status at host generation ğ‘¡ (line 8 of Algorithm 1), we track the proportion of surrogate solutions currently in the archive that were generated at host generation ğ‘¡ âˆ’2, and once this percentage falls to or below half of its historical maximum (i.e., its "half-life"), the Accelerator is deactivated. Let ğœ‡ denote the historical maximum percentage of surrogate solutions from the penultimate generation and ğœ’ ğ‘¡ the current percentage of penultimate surrogate solutions in the archive. The surrogate is deactivated when: 

(ğœ’ ğ‘¡ â‰¤ ğœ‡ 

2 âˆ§ ğœ‡ â‰  0) âˆ¨ ( ğ‘¡ = 50) . (2) The reason for using the survivability of surrogate-based solutions in the host archive over two gen-erations as a proxy for their usefulness is aligned with the very rationale for creating them: obtaining superior solutions that can leapfrog the current state of the evolutionary process and hopefully accelerate its convergence once introduced in the population pool. Once surrogate-based solutions no longer re-tain a minimal survivability advantage over regular offspring, their speed-up potential is deemed insuffi-cient for accelerating the search. It is noteworthy that according to Equation 2, there is a forced Adaptive Accelerator exit at generation ğ‘¡ = 50 . The hard stop threshold was decided based on two factors: (i) the design of our Adaptive Accelerator targets the very early host MOEA convergence stage which is likely to be reached by generation no. 50 [71] and (ii) as shown by the comprehensive benchmark tests in section 5.1, on average the Adaptive Accelerator is deactivated much earlier across all MOOPs. It is important to note that our strategy and fixed pa-rameter choices promote very early surrogate usage (and deactivation) as well as a fairly balanced integration/transfer of surrogate-based results. There are two main reasons for this. Firstly, surrogates that are very well correlated with the true fitness function, but not necessarily extremely accurate are likely to still advance the search early in the run (i.e., the "exploration" stage) as most MOEAs have an evolutionary logic ultimately grounded on decisioning regarding the "or-der" between solution candidates (e.g., Pareto dominance / non-dominance, better / not better than previous candidate solution for a given decomposition vector). Towards the end of the runs (i.e., the "exploitation" stage), errors in surrogate accuracy are far more likely to translate into ordering errors when contrasting with alternative decisions based on the true fitness function. Secondly, by using surrogates to accelerate the search during "exploration", one runs the risk of exac-erbating linkage disequilibrium [2] by "jumping over" key sections of the search space where the population of the base solver would normally â€œgatherâ€ building blocks that are critical for high-quality mid/end of the run solutions. To mitigate this, in our approach, 50% of offspring are generated as per the base solver logic. 

## 4. Experimental Design 

4.1. Incorporation into Solvers 

We incorporated the Adaptive Accelerator into two very well-known and widely used MOEAs: NSGA-II [22] and MOEA/D [64]. The two are distinct in that the former is an elitist algorithm that uses Pareto dominance as a search strategy, whereas the later uses reference-point based de-composition. 

â–  NSGA-II: NSGA-II is the updated version of the Non-dominated Sorting Genetic Algorithm originally   

> First Author et al.: Preprint submitted to Elsevier Page 8 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

proposed in 1994 [54]. The algorithm employs a non-dominated sorting procedure to categorise solutions into hierarchical non-domination fronts, ranking so-lutions based on Pareto dominance, with the most optimal ones placed in the leading fronts. To main-tain population diversity within each front, NSGA-II utilises a crowding distance mechanism, which pe-nalises solutions that are too closely spaced in the objective space, thus encouraging the internal storage of a well-distributed set of solutions. In light of its efficiency and robustness, NSGA-II has been widely adopted across diverse industrial applications [57]. We integrated the Adaptive Accelerator into NSGA-II exactly as described in section 3. We parameterised the host NSGA-II based on literature recommended settings. Specifically, the population and offspring population sizes were both set to 200. We used Simu-lated Binary Crossover (SBX) [19] with a crossover probability rate of 0.8 and a crossover distribution index of 20, and Polynomial Mutation (PM) [20] with a mutation probability of 1âˆ• ğ‘› and a mutation distribution index of 20. The same settings were set for the Adaptive Accelerator NSGA-II component in the inner loop, except that the number of evaluations was set to half of that in the outer loop. 

â–  MOEA/D: MOEA/D solves MOOPs by decomposing them into a set of single-objective sub-problems, each capturing a distinct trade-off among objectives. These sub-problems are optimised simultaneously, with each one guided not only by its own search but also by infor-mation shared from neighbouring sub-problems. The neighbourhood structure is defined based on the rela-tive distances between sub-problems in the objective space. At each generation, the algorithm maintains a population comprising the best-known solution for each sub-problem [64]. We used a variant of MOEA/D with dynamic resource allocation (DRA). We param-eterised the host MOEA/D with a population size of 300, the Differential Evolution Crossover (with a crossover rate of 0.2, and a scaling factor of 0.5), and Polynomial Mutation (with probability rate of 1âˆ• ğ‘› 

and distribution index of 20) and the Tschebycheff aggregation function with the dimension set equal to the number of objectives). We also set the neighbour size to 20, the neighbourhood selection probability to 0.9, and the maximum number of replaced solutions to 2. The same settings were used for the Adaptive Ac-celerator MOEA/D component inside the inner loop, with the number of evaluations set to half of that in the outer loop. Given that the two solvers work differently, i.e. NSGA-II is generational and MOEA/D is a steady-state MOEA, we modified MOEA/D slightly in the way we capture a genera-tion. After the initial population, an iteration in the main step only contains a single solution. For us to capture a generation comparable to NSGA-II, we created an external store for these offspring solutions and count a generation when the number of individuals reached the desired population size (i.e., 300 for the results reported on in section 5). 

4.2. Benchmark Problems 

We evaluated the efficacy of the proposed Accelerated MOEAs on a test harness consisting of 31 benchmark prob-lems drawn from established suites as summarised in Table 2. The test suites are: DTLZ [23] (all 7 problems), KSW10 [38] (1 problem), LZ09 [39] (all 9 problems), WFG [30] (all 9 problems), and ZDT [67] (5 problems â€“ ZDT5 excluded because it is binary). Performance of our proposed Acceler-ated NSGA-II and Accelerated MOEAD/DRA was mainly compared against the standard NSGA-II and MOEA/D-DRA variants, but we also contrasted our results to those of interpolation-based surrogates reported in [62]. The algo-rithms were given a fixed computational budget of 50,000 fitness function evaluations for every test problem. For each benchmark problem, 100 independent runs of each solver were performed, and the mean performance was recorded to mitigate the influence of stochastic variability. The test harness was implemented using jMetalPy version 1.7.0, a comprehensive Python framework for single- and multi-objective optimisation problems with metaheuristics [7].                       

> Table 2
> Details of the 31 benchmark problems used for performance comparison.
> Problem No. of variables No. of objectives
> DTLZ1 73DTLZ2-6 12 3DTLZ7 22 3KSW10 10 2LZ09_F1-F5, F9 30 2LZ09_F6 30 3LZ09_F7-F8 10 2WFG1-9 62ZDT1, 2 30 2ZDT3, 4, 6 10 2

4.3. Performance Indicator 

We used the hypervolume indicator ( ğ» ) [69], a unary quality metric for evaluating Pareto fronts, as our main metric for measuring the performance of the solvers. The hypervolume ğ» (ğ‘ƒ ğ¹ ğ‘ ) quantifies the volume of the objec-tive subspace dominated by a candidate Pareto front ğ‘ƒ ğ¹ ğ‘ ,relative to a specified anti-optimal reference point. A higher 

ğ» indicates better performance. The ğ» is commonly used in the MOEA community because it has theoretical proof of monotonic convergence [4] and characterises the ğ‘ƒ ğ¹ ğ‘ both in terms of diversity (i.e., spread across) and convergence (i.e., distance from) the true Pareto Front of the MOOP. In itself, ğ» is not intuitive as it just comes in the form of a numerical value that depends on the choice of the anti-optimal reference point. To make it easy to interpret, we   

> First Author et al.: Preprint submitted to Elsevier Page 9 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

compute the relative hypervolume, defined as: 

ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) = 100 â‹… ğ» (ğ‘ƒ ğ¹ ğ‘ )

ğ» (ğ‘ƒ ğ¹ ğ‘¡ ) , 0 â‰¤ ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) â‰¤ 100 . (3) where ğ» (ğ‘ƒ ğ¹ ğ‘¡ ) denotes the hypervolume of the true Pareto front for each benchmark problem. We calculated ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘¡ )

for all 31 benchmark problems using their known true Pareto fronts. Using this formulation, for each problem and solver pairing, we computed the mean of the relative hypervolume achieved at the end of each generation across the 100 inde-pendent runs. As a secondary general metric for measuring solver per-formance on benchmark problems, we have used the inverse generational distance ğ¼ğºğ· (ğ‘ƒ ğ¹ ğ‘ ) [16]. Like the hypervol-ume, the ğ¼ğºğ· assesses both diversity and convergence of a candidate Pareto front. A smaller ğ¼ğºğ· indicates better performance and, in order to enable averaging across the benchmark, we have minmax normalised ğ¼ğºğ· values be-tween 0 and 1 for each test problem. For a particular comparison, we have also employed the spread metric Î”( ğ‘ƒ ğ¹ ğ‘ ) [22]. In the case of Î”, smaller values indicate that the solutions in the ğ‘ƒ ğ¹ ğ‘ are more equally distributed along the true Pareto front, the minimal value is problem specific and values larger than 1 are possible. 

## 5. Results and Discussion 

5.1. Adaptive Surrogate Deactivation 

We use Figure 2 to illustrate the impact of the pro-posed Adaptive Accelerator strategy for our two considered host solvers on the DTLZ7 and LZ09_F1 problems. On average, in both cases, after activation at generation no. 2, the GPR surrogate is only active for: 6 generations in the case of NSGA-II on DTLZ7 and 3 generations in the case of MOEA/D-DRA on LZ09_F1. Nevertheless, by the time the surrogate is deactivated based on the half-life criterion, the accelerated GPR-NSGA-II reaches a relative hypervol-ume of 75%+ whilst the accelerated GPR-MOEA/D-DRA reaches a ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) of 60%+. On average, their respective baselines, (black dotted lines) are able to match this per-formance after 20k (for NSGA-II) and 5k (for MOEA/D-DRA) extra fitness evaluations with the initial surrogate advantage being preserved throughout the run. When the GPR surrogate is active, the maximum proportion of archive surrogate solutions from the penultimate generation (i.e., ğœ‡ 

from Equation 2) reaches 32% for NSGA-II and 12% for MOEA/D-DRA. In Table 3 we present statistics of the surrogate deac-tivation points (i.e., generations) for the three NSGA-II-incorporated surrogates on the 31 benchmark problems. We remind the reader that counting starts from generation 0 (as the initial population of the solver) and the surrogate-assisted module (i.e., Adaptive Accelerator) is activated at generation 2. Generally, the minimum deactivation point for the three surrogate-assisted internal solvers across all 31 benchmark problems is at generation 5 (i.e., the Adaptive Accelerator is only active for 3 generations). The mean and maximum deactivation generation vary significantly. The highest orderly maximum deactivation points were observed for RFR-NSGA-II on the LZ09 and DTLZ problems. There are also four problems where we observed a forced deacti-vation (at generation 50) of the CNN surrogate. Thus, out of 100 independent runs, the forced deactivation occurred in 6 runs on LZ09_F1, 7 runs on LZ09_F3, 10 runs on LZ09_F4 and 5 runs on LZ09_F5 and this also increases the relative mean deactivation generation of CNN-NSGA-II for these four benchmark MOOPs. In general, the very small likelihood of a forced deactivation and the low mean deactivation points support the effectiveness of our proposed "half-life" survivability criterion. We present in Figure 3 the comparative average perfor-mance of two RFR-NSGA-II variants: a standard one with an adaptive surrogate deactivation mechanism (i.e., with exit) and one where the deactivation mechanism is turned off (i.e., no exit). Whilst always using the surrogate does provide some early convergence boost, this does come at the expense of late stage performance. Furthermore, the version without adaptive surrogate deactivation has a computational overhead that is more than 25 times higher than the adap-tive variant. These ablation results empirically confirm the usefulness of our proposed adaptive surrogate deactivation strategy based on the half-life criterion. In Figure 4 we present the comparative performance of different surrogate integration thresholds on DTLZ7 and ZDT3 â€“ two benchmark MOOPs that feature disconnected true Pareto Fronts which test the ability of solvers to main-tain diversity across different optimal regions of the search space. For this test, the average ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) convergence per-formance is complemented by the associated Î”( ğ‘ƒ ğ¹ ğ‘ ) spread plots. In the case of the baseline solver (i.e., NSGA-II), the 

Î” plots indicate that spread tends to increase or stabilise early on at a relatively high level ( > 0.75 ) and then flatten out. This is intuitive as ğ‘ƒ ğ¹ ğ‘ solutions tend to be far apart in objective space during the early and mid convergence stages (when progress towards the true Pareto Front can increase 

ğ‘ƒ ğ¹ ğ‘ gaps) and then coalesce during late convergence (when the solver focus shifts from discovering the true Pareto Front to better approximating it). In case of the NSGA-II surrogate variants, all surrogate integration thresholds (i.e., 50%, 75% and 100%) are associated with noticeable early spikes of 

Î”( ğ‘ƒ ğ¹ ğ‘ ). This is a strong indicator that surrogate-derived solutions are able to leapfrog the search when compared with the baseline. However, the magnitudes of the Î”( ğ‘ƒ ğ¹ ğ‘ ) peaks is not proportional to that of the integration thresholds and only thresholds < 100% are also associated with ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )-measured convergence boosts: i.e., 75% and especially 50% for the CNN-NSGA-II on DTLZ7 and 50% for GPR-NSGA on ZDT3. This suggests that combining surrogate-based solutions with solutions generated by the host MOEAâ€™s native operators is crucial for ensuring diversity retention (i.e., mitigating potential linkage disequilibrium), whilst also motivating our fixed surrogate integration threshold of 50%.   

> First Author et al.: Preprint submitted to Elsevier Page 10 of 22

Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

(a) NSGA-II (b) MOEA/D-DRA 

Figure 2: Mean performance of GPR-based Adaptive Accelerator for NSGA-II on DTLZ7 and MOEA/D-DRA on LZ09_F1 across the 100 independent runs. The shaded part is the mean interval the surrogate was active. The brown line indicates the mean value of ğœ’ ğ‘¡ â€“ the percentage of surrogate solutions from the penultimate generation that are in the archive at generation ğ‘¡ (read from the right y-axis). 

Table 3 

Adaptive Accelerator deactivation point statistics for the surrogate-enhanced NSGA-II variants on the 31 benchmark problems. 

Problem RFR-NSGA-II GPR-NSGA-II CNN-NSGA-II 

Min Mean Max Min Mean Max Min Mean Max DTLZ1 5 10.11 26 5 6.21 14 5 6.48 9DTLZ2 5 7.5 12 5 5.31 9 5 5.1 7DTLZ3 5 8.91 19 5 6.15 9 5 6.13 9DTLZ4 5 7.79 15 5 6.0 11 5 6.34 10 DTLZ5 5 8.01 13 5 5.53 7 5 5.12 7DTLZ6 9 16.0 25 5 7.74 13 6 6.95 8DTLZ7 5 11.21 18 5 8.19 13 5 7.05 9KSW 5 7.18 13 5 5.88 9 5 5.79 9LZ09_F1 5 8.24 16 5 6.46 12 5 12.42 50 LZ09_F2 5 8.25 22 5 5.48 9 5 5.67 8LZ09_F3 5 7.74 16 5 5.64 8 5 8.61 50 LZ09_F4 5 7.32 20 5 5.73 7 5 10.32 50 LZ09_F5 5 6.82 14 5 5.3 8 5 7.38 50 LZ09_F6 5 9.58 20 5 6.04 9 5 6.86 11 LZ09_F7 5 7.98 23 5 5.42 9 5 6.1 9LZ09_F8 5 7.43 18 5 5.52 9 5 6.06 9LZ09_F9 5 8.07 22 5 5.64 10 5 5.66 12 WFG1 5 8.67 15 5 5.22 6 5 6.44 8WFG2 5 7.15 15 5 5.43 8 5 5.75 8WFG3 5 6.81 11 5 5.2 8 5 5.27 8WFG4 5 5.83 9 5 5.43 8 5 5.58 8WFG5 5 6.92 11 5 6.0 9 5 6.0 7WFG6 5 6.89 11 5 6.0 9 5 5.97 7WFG7 5 5.3 7 5 5.82 8 5 5.36 7WFG8 6 6.95 8 5 6.85 7 6 6.91 7WFG9 5 6.04 9 5 5.56 7 5 5.91 8ZDT1 5 9.37 21 5 5.13 10 5 6.04 8ZDT2 5 6.53 11 5 7.97 16 5 6.64 8ZDT3 5 9.31 17 5 6.26 10 5 5.78 7ZDT4 5 6.63 10 5 7.04 10 5 6.28 8ZDT6 5 6.53 19 5 7.22 18 5 6.43 11 First Author et al.: Preprint submitted to Elsevier Page 11 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs  

> Figure 3: Comparison with full-on surrogate version (i.e., no exit) for the RFR-NSGA-II variant.

5.2. Mean Performance on MOOP Benchmark Set 

We present the mean ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) performance across all 31 benchmark problems that we have considered in Figures 5 and 6. The results indicate that our Adaptive Accelerators successfully speed up the general convergence of NSGA-II, with GPR-NSGA-II and CNN-NSGA-II notably doing so, followed by RFR-NSGA-II. The same is largely true for MOEA/D-DRA early convergence as both GPR-MOEA/D-DRA and CNN-MOEA/D-DRA outperform the baseline solver whilst the early boost provided by RFR-MOEA/D-DRA is more subdued. However, the overall MOEA/D-DRA Adaptive Accelerator performance is more nuanced as the early performance improvement of the GPR and RFR variants come at the expense of late-stage performance attainment whilst the RFR variant tracks the end-of-run baseline solver performance much better. All results and the associated code base used to produce them are available online at <web respository withheld>. In order to better illustrate the convergence behaviour of the Accelerated MOEAs, we computed their respective 

ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) performance gain (%) by subtracting the mean relative hypervolume of the baseline MOEA from that ob-tained by each accelerated variant at each generation. The results are presented in Figure 7 and provide further support that our surrogate-based strategy is successful at speeding up convergence, especially during the early generations. For instance, in the case of NSGA-II (Figure 7a), all three surrogate models accelerate the benchmark-wide average convergence by at least 20% in the first 10 generations and by at least 10% by the 20th generation. The CNN and GPR surrogates are able to induce a mean performance gain of about 10% until the 70th generation. The performance gain for MOEA/D-DRA surrogates (Figure 7b) are in line with previous observations, with GPR-MOEA/D-DRA attaining noticeable mean gains of about 80% over its baseline very early in the runs. However, despite the initial positive impact, both the the GPR and CNN variants of MOEA/D-DRA start to underperform the baseline after the 50th generation, ending the runs at a 10% mean deficit. The end-of-the-run performance drop of the RFR variant is less pronounced, but its early generations boost is also lackluster (max 10%). In the case of the three surrogate-accelerated variants of NSGA-II, we also performed a ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) comparison with the lightweight interpolation-based surrogate mod-elling strategy proposed in [62]. The results we obtained are shown in Figure 8 and indicate that our Adaptive Acceleration strategy using GPR and CNN surrogate mod-els consistently achieves faster convergence than NSGA-II PE+SE â€“ the best performing interpolation-based approach. The RFR surrogate also initially outperforms both NSGA-II PE and NSGA-II PE+SE but is overtaken by the latter around generation 20. The mean end-of-the-run ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )

attainment is roughly similar across all surrogate-enhanced MOEAs after equalising around 25k fitness evaluations (i.e., as the solvers enter the late convergence stage). These results indicate that, when compared with a simpler strategy, the increased complexity of our on-the-fly surrogate models (coupled with an effective adaptive deactivation strategy) is justified by their superior capability to generally accelerate NSGA-II convergence. 

5.3. Statistical Analysis of MOOP Benchmark Set Performance 

We applied the Mann-Whitney U Test [43] to determine if the mean relative hypervolumes attained by the surrogate variants on each of the 31 benchmark problems at every generation are statistically different from those obtained by their respective baseline solvers. The Mann-Whitney U test is a non-parametric statistical test used to compare two independent groups. Using a significance level of 0.05, we carried out a one-sided test with a null hypothesis that there is no statistical difference between the observed mean 

ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) of the baseline solvers and their accelerated vari-ants. The alternative was that the mean ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) values achieved by the baseline solver was lower than that of the tested accelerated variant. The statistical significance results we obtained are aggre-gated in Figure 9a for NSGA-II and Figure 9b for MOEA/D-DRA. In these plots we show the number of benchmark problems where we rejected the null hypothesis in favour of the alternative hypothesis (i.e., that the surrogate variant outperform the baseline solver). The results follow an intu-itive pattern for both the NSGA-II and the MOEA/D-DRA surrogate-accelerated variants, confirming that surrogate-driven outperformance of the baseline solvers occurs across many problems during the early stages of the optimisation. For instance, Figure 9a indicates that, in generation 11 (i.e., after 2400 fitness function evaluations), RFR-NSGA-II outperformed NSGA-II in 19 problems, CNN-NSGA-II in 17 problems and GPR-NSGA-II in 11. As expected, as the generations progress and the solvers reach their late convergence stage, the number of problems where the ac-celerated solvers outperform the baseline tends to decline from approx 1/2 of the total number of problems to approx   

> First Author et al.: Preprint submitted to Elsevier Page 12 of 22

Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

(a) ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )-measured convergence (b) ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )-measured convergence 

(c) Associated Î”( ğ‘ƒ ğ¹ ğ‘ ) (d) Associated Î”( ğ‘ƒ ğ¹ ğ‘ )

Figure 4: Mean comparative performance (convergence and spread) of CNN-NSGA-II and GPR-NSGA-II variants with surrogate integration thresholds of 50%, 75% and 100%. 

(a) NSGA-II - ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) (b) MOEA/D-DRA - ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )

Figure 5: Mean comparative performance of the surrogate-enhanced NSGA-II and MOEA/D-DRA and their standard versions on all 31 problems using the hypervolume ( ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )) metric. First Author et al.: Preprint submitted to Elsevier Page 13 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs        

> (a) NSGA-II -ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )
> (b) MOEA/D-DRA -ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )
> Figure 6: ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )comparative performance of the surrogate-enhanced NSGA-II and MOEA/D-DRA and their standard versions after every 2000 fitness evaluations. Each box plot is based on 3100 values (31 MOOPs times 100 runs per problem).

1/4 of the benchmark set (especially for RFR variants). In this respect, CNN variants (especially CNN-MOEA/D-DRA) demonstrate a noteworthy stability between 1/2 and 1/3 benchmark set outperformance. 

5.4. Performance on Individual Problems and Problem Suites 

We present, in Figures 10 to 14, the average comparative performance of the standard solvers against their surrogate-accelerated variants employing RFR, CNN and GPR on the 5 problem families we considered. To help with readabil-ity, whilst we do comment on individual problem perfor-mance, individual problem performance plots considering both ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) and ğ¼ğºğ· (ğ‘ƒ ğ¹ ğ‘ ) for all 31 benchmark prob-lems are shown in the Appendix. 

DTLZ Suite: Figure 10 shows the mean performance of the two groups of solvers on the DTLZ suite of problems (7 problems). The results show that NSGA-II convergence can be considerably improved by the Adaptive Accelera-tor using all three models (RFR, CNN and GPR) as the surrogate variants reach an average relative hypervolume of 50% by generation 50, whereas, the standard NSGA-II only attains this ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) at generation 80. The over-all ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) convergence profile of NSGA-II surrogate-accelerated variants on the DTLZ suite is influenced by their notable outperformance on the DTLZ7 problem where CNN and GPR achieve 80% ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) at generation 10, whereas the baseline NSGA-II only achieves that performance at generation 95. In the case of MOEA/D-DRA, all surrogate-assisted variants and especially the CNN one perform well on DTLZ2, DTLZ5, DTLZ6 and DTLZ7. However, both CNN- and GPR-MOEA/D-DRA fail to reach meaningful 

ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) values on DTLZ1 and DTLZ3, leading to the subpar performance over the entire suite that is shown in      

> First Author et al.: Preprint submitted to Elsevier Page 14 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs
> (a) NSGA-II (b) MOEA/D-DRA
> Figure 7: Performance gains of surrogate-enhanced MOEAs compared to their respective baseline solvers.
> Figure 8: Comparison with lightweight surrogate models pro-posed in [62].

Figure 10b. It is very important to note that the Appendix 

ğ¼ğºğ· (ğ‘ƒ ğ¹ ğ‘ ) plots confirm that all tested solvers are able to converge on any benchmark problem. However, for a few problems the best ğ‘ƒ ğ¹ ğ‘ some solvers discover within the allo-cated computation budget are not ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) meaningful (i.e., yield a value close to 0) given the reference point used in the benchmark formulation to compute the ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ). In the Appendix (Figure 20), we also present the comparative per-formance on DTLZ5 and DTLZ7 versions with 4 objectives. The performance boosts delivered by all surrogate-assisted variants in these preliminary tests are on par, if not larger (in the case of RFR and NSGA-II) than those observed in the multi-objective versions of these two problems, indicating that the proposed Adaptive Accelerator strategy has potential to scale well on many-objective optimisation scenarios. 

KSW: The ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) performance of the solvers on the KSW problem is shown in Figure 11, where a trend similar to that exhibited on the DTLZ suite is observed. On the one hand, the three NSGA-II accelerators induced a consistently enhanced performance throughout the runtime. On the other hand, the MOEA/D-DRA surrogate-based variants failed to generate similar improvement, with CNN-MOEA/D-DRA determining a noticeable drop in convergence performance. 

LZ09 Suite: Comparative performance ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) re-sults on the LZ09 problem suite (9 problems) are shown in Figure 12. These results indicate that, over the prob-lem suite, the surrogates provide at best a minimal early convergence boost for NSGA-II. The early convergence ac-celeration is more evident for MOEA/D-DRA, with GPR-MOEA/D-DRA achieving 32.6% suite average ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) at generation 10, followed by CNN-MOEA/D-DRA achiev-ing 24.7%, RFR-MOEA/D-DRA achieving 20.3%, and the baseline solver achieving 19.2%. On individual benchmark problems, GPR-MOEA/D-DRA consistently achieves good performance across both ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) and ğ¼ğºğ· (ğ‘ƒ ğ¹ ğ‘ ) met-rics in the first 50 generations, apart from LZ09_F2 and LZ09_F9. It is also worthwhile that the mean end-of-the-run 

ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) attainment over the entire LZ09 suite is slightly lower for the surrogate-based variants. 

WFG Suite: Figure 13 shows the comparative ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )

results of the two groups of solvers on the WFG problem suite which comprises of 9 individual problems. Over the entire suite, surrogate-based variants have a slight conver-gence speed edge in the case of NSGA-II and show a slight disadvantage for MOEA/D-DRA. On individual problems, GPR-NSGA-II and CNN-NSGA-II surrogates perform very well on WFG1, WFG5, WFG6 and WFG8 as confirmed by both ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) and ğ¼ğºğ· (ğ‘ƒ ğ¹ ğ‘ ) metrics. GPR-MOEAD/D-DRA demonstrates some convergence speed improvements on WFG1 and WFG8, but all MOEA/D-DRA surrogate variants underperform on WFG9 on the ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) metric. 

ZDT Suite: Comparative ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) results on the ZDT problem suite (5 problems) are shown in Figure 14 and they indicate that GPR- and CNN-based Adaptive Accelerators   

> First Author et al.: Preprint submitted to Elsevier Page 15 of 22

Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

(a) NSGA-II (b) MOEA/D-DRA 

Figure 9: Comparative statistical test results showing the number of benchmark problems in which surrogate-assisted variants statistically outperformed their baseline solver at each generation. 

(a) NSGA-II (b) MOEA/D-DRA 

Figure 10: Comparative mean performance of the surrogate-enhanced NSGA-II and MOEA/D-DRA variants and their baseline versions on the DTLZ problem suite (7 problems). 

(a) NSGA-II (b) MOEA/D-DRA 

Figure 11: Comparative mean performance of the surrogate-enhanced NSGA-II and MOEA/D-DRA variants and their baseline versions on the KSW problem (1 problem). First Author et al.: Preprint submitted to Elsevier Page 16 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs     

> (a) NSGA-II (b) MOEA/D-DRA
> Figure 12: Comparative performance of the surrogate-enhanced NSGA-II and MOEA/D-DRA variants and their baseline versions on the LZ09 problem suite (9 problems).
> (a) NSGA-II (b) MOEA/D-DRA
> Figure 13: Comparative performance of the surrogate-enhanced NSGA-II and MOEA/D-DRA variants and their baseline versions on the WFG problem suite (9 problems).

are able to drive notable convergence speed boosts for both NSGA-II and MOEA/D-DRA. The versions using RFR are also able to improve over baseline performance (especially in the case of NSGA-II), but the gains in convergence speed are far inferior to those demonstrated by the other two modelling strategies. Drilling down to individual problems, the CNN and GPR-based variants have a vastly superior convergence performance â€“ across both ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) and ğ¼ğºğ· (ğ‘ƒ ğ¹ ğ‘ ) â€“ on the entire ZDT suite for NSGA-II and on ZDT1, ZDT2 and ZDT6 for MOEA/D-DRA. 

5.5. Case Study: North-East Atlantic Fish Stock Assessment Surveys 

In this section, we describe a real-world computationally intensive problem that provided us with a relevant case study to further evaluate the efficacy of our surrogate-based MOEA convergence acceleration technique. The context is fish stock assessment in the North-East Atlantic, where the International Council for Exploration of the Seas (ICES) carries out bottom-trawl surveys. Protocols for conducting the surveys are detailed in the Manual for the North Sea International Bottom Trawl Surveys [32]. According to the manual, the surveys aim at providing consistent and stan-dardised data for examining spatial and temporal changes in (a) the distribution and relative abundance of fish and fish assemblages; and (b) the biological parameters of commer-cial fish species for stock assessment purposes. The surveys make use of the segmentation of the North-East Atlantic into statistical grid rectangles measuring 1 degree in longitude by 0.5 degrees in latitude. Survey vessels typically conduct one or two trawling hauls within each grid cell, each lasting approximately 30 minutes. However, greater importance is placed on covering all grid cells rather than having two samples collected in a grid cell. Occasionally, coverage gaps     

> First Author et al.: Preprint submitted to Elsevier Page 17 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs
> (a) NSGA-II (b) MOEA/D-DRA
> Figure 14: Comparative performance of the surrogate-enhanced NSGA-II and MOEA/D-DRA variants and their baseline versions on the ZDT problem suite (5 problems).

occur due to adverse weather or technical failures of the vessels. Offshore wind energy plays a pivotal role in the UKâ€™s strategy to decarbonise its energy system and meet net-zero targets. The UK government has set bold offshore wind capacity goals: 50 GW by 2030 â€“ of which 5 GW is expected to come from floating wind â€“ and potentially scaling up to 140 GW by 2050 [24]. However, once offshore wind farms are established, the surrounding areas become hazardous for fishing and survey vessels to operate. This impacts two primary stakeholder groups: 

â€¢ Policy makers must understand how the placement of offshore wind farms will influence fishing activities. Their challenge lies in identifying which grid cells should be excluded from development to minimise disruption. 

â€¢ Fisheries scientists and survey teams face reduced spatial coverage for the survey, raising concerns about the accuracy of survey-based models. The question becomes: How robust are fish abundance estimates if some areas are no longer surveyed? To explore this issue, we modelled it as a binary opti-misation problem with three objectives, focusing solely on Haddock populations for simplicity. Using DATRAS data from 2010 to 2023, we divided the North East Atlantic into 439 grid cells, with each candidate solution represented by a binary encoding indicating which cells were selected for inclusion in the abundance index calculations. We adopted a binary encoding scheme because each decision variable represents whether a grid cell is selected (1) or not (0). The first and second objectives measured the impact on the abundance index for quarters 1 and 3/4, respectively, while the third objective minimised the number of excluded (deselected) grid cells (i.e., cell value = 0). The impact was measured by comparing a solutions index with the true index, both produced using Delta -lognormal by age over time as follows. Difference = âˆ’ 

> ğ‘›

âˆ‘

> ğ‘– =1

|||||

ğ‘¦ â€²

> ğ‘–

ğ‘¦ ğ‘– 

âˆ’ 1 

|||||

(4) where ğ‘¦ is the true index value, ğ‘¦ â€² is the new index value based on a given solution candidate (i.e., with measurements from deselected cells excluded), and ğ‘– = 1 â€¦ ğ‘› is the period (i.e. from year 2010 to 2023). Computing the Haddock index of abundance over a 14-year period is a very computationally intensive task. The delta-lognormal model generates indices for eight age groups in quarter 1 and nine age groups in quarter 3/4, for each year. Depending on the specific candidate solution, cal-culating these indices for both quarters takes approximately 15 to 30 minutes. As a result, evaluating the fitness of a single solution can require between 30 minutes and 1 hour. Given these computational demands, running an optimisation with, for example, a budget of 50,000 fitness function evaluations becomes highly time-consuming. Evaluating 50k solutions sequentially â€“ each taking a minimum of 30 minutes â€“ would require over 1,042 days to complete (wall-clock time). Clearly, a sequential approach is impractical for this scale of problem. To address this, we developed a custom population eval-uator within the jMetalPy framework to enable parallel evaluation on an HTCondor-based computing cluster [56]. The HTCondor cluster is composed of 4 machines, each with the following specifications: 24 CPUs, 64GB memory, NVIDIA RTX A5000 GPU, Ubuntu 22.04.5 LTS operating system. This allowed us to evaluate candidate solutions in parallel. We then compared the performance of standard NSGA-II against three NSGA-II-based surrogate-assisted variants on the fish abundance modelling problem, using the same parameters as in the benchmark problems and a   

> First Author et al.: Preprint submitted to Elsevier Page 18 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs

computational budget of 40,000 fitness evaluations. With parallel execution, a single optimisation run was completed in approximately 11 days. We only managed to do a single run for the baseline NSGA-II solver and its three surrogate-based variants as it was impractical to repeat the experiments say 100 times as we did with the benchmarks. Furthermore, due to the complexity of the problem (bi-nary encoding of size 439), and the desire to give all solvers a strong starting point, the initial population was generated using a combination of two approaches: 50% of the solutions were generated randomly to introduce diversity, while the remaining 50% were solutions derived through a "greedy" strategy that also aimed to create a baseline for the MOEA results. This greedy strategy involved a leave-one-out ap-proach to first identify grid cells with the single highest total impact across the quarters of interest when excluded from the index. Afterwards, "greedy" solution candidates were iteratively created by adding individual exclusion cells in descending order of their leave-one-out total impact. Ulti-mately, 50% of the MOEAs population was selected from the pool of "greedy" solution candidates, ensuring that we maintained a representative spread across the size of the excluded cliques of cell. Comparative results are shown in Figure 15, where the three surrogates accelerate the convergence speed of NSGA-II in the early generations. As seen with several benchmark problems, the early-stage acceleration delivered by GPR-NSGA-II and CNN-NSGA-II is noticeable. When compared to the greedy baseline, the Adaptive Accelerators using CNN and GPR models are able to maintain a lead of 1 to 2 days over the standard solver in the first third of the run. Towards the end of the optimisation run, the performance of the GPR and CNN variants is matched end even slightly surpassed by the standard NSGA-II run. RFR-NSGA-II does not provide any early stage convergence improvement, but outperforms all solvers on end-of-the-run relative hyper-volume attainment. In terms of surrogate exit points, all three surrogates exited at generation 6, having run for 5 generations. Given that the average training time for a single surrogate model was 3 minutes and 46 seconds (RFR), 3 minutes and 49 seconds (CNN) and 1 minute and 52 seconds (GPR), the total computational burden associated with surrogate model training was negligible. However, the performance of all three surrogate-based variants on the case study scenario indicate their general usefulness (speed and quality attainment) within a computational approach designed to offer fisheries scientists a means to stress test abundance indices when aiming to assess and ultimately improve their robustness. 

## 6. Conclusions 

In this paper we have demonstrated the efficacy of an on-the-fly surrogate modelling technique designed to accel-erate the early stage convergence speed of multi-objective evolutionary algorithms. Despite being data-driven , the proposed technique is not resource-intensive as it only uses  

> Figure 15: Comparative performance of surrogate-enhanced NSGA-II and its standard version on the Haddock Survey optimisation.

the most recently evaluated population to train models and also features an adaptive surrogate deactivation strategy. Using a streamlined configuration (with fixed parameter settings), the tested surrogate-enhanced solvers proved to accelerate the mean early convergence speed of NSGA-II and MOEA/D-DRA on a test aggregating 31 widely known benchmark problems. In particular, the convergence speed gains demonstrated by our NSGA-II Adaptive Ac-celerator variants using GPR and CNN models over this benchmark set clearly surpassed those achieved by recently proposed interpolation-based approaches. To demonstrate performance on multi-objective scenarios with a wide range of characteristics, we also applied our acceleration tech-nique on a real-world computationally-intensive fisheries modelling problem in the North-East Atlantic that features a fairly large binary encoding and a specialised initialisation. The case study results show valuable convergence improve-ments when compared with the standard MOEA approach. Overall, numerical results indicate that our streamlined Adaptive Acceleration approach can demonstrably enhance the early-stage convergence of MOEAs. Consequently, when addressing computationally intensive optimisation prob-lems, practitioners can obtain a high-quality set of PN so-lutions with substantially fewer fitness function evaluations. This, in turn, resulting in significant savings in both time and computational resources. Even though it is generally successful in accelerating the convergence of both tested MOEAs over the bench-mark set on average, our streamlined strategy also displays limitations as some surrogate-MOEA pairs underperform on specific test problems and even problem families (e.g., CNN and GPR variants of MOEA/D-DRA on DTLZ1 and DTLZ3). At the same time, RFR variants that tend to deliver a more luckluster performance on average perform very well on DTLZ3 and DTLZ6 (for NSGA-II) and much better than other MOEA/D-DRA surrogates on DTLZ1, DTLZ3, DTLZ4 and LZ09_F6. Whilst tempting to simply interpret   

> First Author et al.: Preprint submitted to Elsevier Page 19 of 22

Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

this as a corollary of the No Free Lunch Theorems [59], the differential behaviour of the RFR variants also offers signif-icant potential for a further future refinement of our strategy based on ensemble ML models. This future refinement could also explore a potential update for the fixed parameterisa-tion we propose in Section 3.3. The current "rule of 1/2" fixed setting (i.e., half-life surrogate deactivation criterion, 50% surrogate solutions to be re-integrated, max surrogate generations set to 1/2 of host solver setting) and limitation to a single generation for training produces stable results and reduces the complexity of our proposed approach, but a thorough sensitivity analysis might explore if tailoring these setting can improve the general performance of Adaptive Accelerators to a level that warrants the associated increase in complexity. Furthermore, future research will investigate the effect of offspring population size on the performance of the Adap-tive Accelerators. We will also compare the performance of the Adaptive Accelerators with other similar surrogate modelling approaches such as the Kriging assisted Reference Vector Guided Evolutionary Algorithm (K-RVEA) [15] the Classification-based Surrogate-assisted Evolutionary Algo-rithm (CSEA) [46] This will give more opportunity for a comprehensive comparison of algorithmic behaviour and run-time performance. 

## Acknowledgments 

This work has been supported by the COMET-K2 â€Cen-ter for Symbiotic Mechatronicsâ€ of the Linz Center of Mechatronics (LCM) funded by the Austrian federal govern-ment and the federal state of Upper Austria. We also would like to acknowledge <name withheld> for providing the case study and associated code for running the delta-lognormal models. 

## References 

[1] Abdulrahman, M.M., Niu, Y., 2023. Multi-objective evolutionary algorithm with decomposition for enhanced community detection in signed networks. KHWARIZMIA 2023, 10â€“23. doi: 10.70470/ khwarizmia/2023/002 .[2] Altenberg, L., 1995. The schema theorem and priceâ€™s theorem, Elsevier. volume 3 of Foundations of Genetic Algorithms ,pp. 23â€“49. URL: https://www.sciencedirect.com/science/ article/pii/B9781558603561500066 , doi: https://doi.org/10.1016/ B978-1-55860-356-1.50006-6 .[3] AndrÃ©s, E., Salcedo-Sanz, S., Monge, F., PÃ©rez-Bellido, A., 2012. Efficient aerodynamic design through evolutionary programming and support vector regression algorithms. Expert Systems with Applica-tions 39, 10700â€“10708. doi: 10.1016/j.eswa.2012.02.197 .[4] Auger, A., Bader, J., Brockhoff, D., Zitzler, E., 2009. Theory of the hypervolume indicator: optimal Î¼-distributions and the choice of the reference point, in: Proceedings of the Tenth ACM SIGEVO Workshop on Foundations of Genetic Algorithms, Association for Computing Machinery, New York, NY, USA. p. 87â€“102. URL: https: //doi.org/10.1145/1527125.1527138 , doi: 10.1145/1527125.1527138 .[5] Bader, J., Zitzler, E., 2011. Hype: An algorithm for fast hypervolume-based many-objective optimization. Evolutionary Computation 19, 45â€“76. doi: 10.1162/evco_a_00009 .[6] Banda, T.M., ZÄƒvoianu, A.C., 2024. A dominance-based surrogate classifier for multi-objective evolutionary algorithms, in: Bramer, M., Stahl, F. (Eds.), Artificial Intelligence XLI, Springer Nature Switzerland, Cham. pp. 268â€“281. [7] BenÃ­tez-Hidalgo, A., Nebro, A.J., GarcÃ­a-Nieto, J., Oregi, I., Del Ser, J., 2019. jmetalpy: A python framework for multi-objective optimiza-tion with metaheuristics. Swarm and Evolutionary Computation 51, 100598. doi: https://doi.org/10.1016/j.swevo.2019.100598 .[8] Beume, N., Naujoks, B., Emmerich, M., 2007. Sms-emoa: Multiob-jective selection based on dominated hypervolume. European Journal of Operational Research 181, 1653â€“1669. doi: 10.1016/j.ejor.2006. 08.008 .[9] Bevilacqua, V., Costantino, N., Dotoli, M., Falagario, M., Sciancale-pore, F., 2012. Strategic design and multi-objective optimisation of distribution networks based on genetic algorithms. International Jour-nal of Computer Integrated Manufacturing 25, 1139â€“1150. doi: 10. 1080/0951192x.2012.684719 .[10] Bramerdorfer, G., Tapia, J.A., Pyrhonen, J.J., Cavagnino, A., 2018. Modern electrical machine design optimization: Techniques, trends, and best practices. IEEE Transactions on Industrial Electronics 65, 7672â€“7684. doi: 10.1109/TIE.2018.2801805 .[11] Bramerdorfer, G., Zavoianu, A.C., 2017. Surrogate-based multi-objective optimization of electrical machine designs facilitating tol-erance analysis. IEEE Transactions on Magnetics 53, 1â€“11. doi: 10. 1109/tmag.2017.2694802 .[12] Breiman, L., 2001. Random forests. Machine Learning 45, 5â€“32. doi: 10.1023/a:1010933404324 .[13] Chand, S., Wagner, M., 2015. Evolutionary many-objective opti-mization: A quick-start guide. Surveys in Operations Research and Management Science 20, 35â€“42. doi: 10.1016/j.sorms.2015.08.001 .[14] Cheng, R., Jin, Y., Olhofer, M., Sendhoff, B., 2016. A reference vector guided evolutionary algorithm for many-objective optimization. IEEE Transactions on Evolutionary Computation 20, 773â€“791. doi: 10. 1109/tevc.2016.2519378 .[15] Chugh, T., Jin, Y., Miettinen, K., Hakanen, J., Sindhya, K., 2018. A surrogate-assisted reference vector guided evolutionary algorithm for computationally expensive many-objective optimization. IEEE Transactions on Evolutionary Computation 22, 129â€“142. doi: 10. 1109/tevc.2016.2622301 .[16] Coello, C.A.C., CortÃ©s, N.C., 2005. Solving multiobjective op-timization problems using an artificial immune system. Genetic Programming and Evolvable Machines 6, 163â€“190. doi: 10.1007/ s10710-005-6164-x .[17] Coello, C.A.C., Lamont, G.B., Veldhuizen, D.A.V., 2007. Evolu-tionary algorithms for solving multi-objective problems. Genetic and Evolutionary Computation Series, Springer. [18] Deb, K., 2001. Multi-Objective Optimization using Evolutionary Algorithms. volume 16. John Wiley & Sons. [19] Deb, K., Agrawal, R.B., 1995. Simulated binary crossover for continuous search space. Complex systems 9, 115â€“148. [20] Deb, K., Goyal, M., 1996. A combined genetic adaptive search (geneas) for engineering design. Computer Science and informatics 26, 30â€“45. [21] Deb, K., Jain, H., 2014. An evolutionary many-objective opti-mization algorithm using reference-point-based nondominated sort-ing approach, part i: Solving problems with box constraints. IEEE Transactions on Evolutionary Computation 18, 577â€“601. doi: 10. 1109/tevc.2013.2281535 .[22] Deb, K., Pratap, A., Agarwal, S., Meyarivan, T., 2002a. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation 6, 182â€“197. doi: 10.1109/4235.996017 .[23] Deb, K., Thiele, L., Laumanns, M., Zitzler, E., 2002b. Scal-able multi-objective optimization test problems, in: Proceedings of the 2002 Congress on Evolutionary Computation. CECâ€™02 (Cat. No.02TH8600), pp. 825â€“830 vol.1. doi: 10.1109/CEC.2002.1007032 .[24] Department for Energy Security and Net Zero, 2023. Offshore wind net zero investment roadmap. URL: https://assets. publishing.service.gov.uk/media/64a54c674dd8b3000f7fa4c9/ offshore-wind-investment-roadmap.pdf .

First Author et al.: Preprint submitted to Elsevier Page 20 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

[25] Guo, D., Wang, X., Gao, K., Jin, Y., Ding, J., Chai, T., 2022. Evo-lutionary optimization of high-dimensional multiobjective and many-objective expensive problems assisted by a dropout neural network. IEEE Transactions on Systems, Man, and Cybernetics: Systems 52, 2084â€“2097. doi: 10.1109/tsmc.2020.3044418 .[26] Hardy, R.L., 1971. Multiquadric equations of topography and other irregular surfaces. Journal of Geophysical Research 76, 1905â€“1915. doi: 10.1029/jb076i008p01905 .[27] Haykin, S., 1999. Neural Networks: A Complrehensive Foundation. 2nd edition ed., Pearson Prentice Hall. [28] Holland, J.H., 1975. Adaptation in Natural and Artificial Systems. University of Michigan Press. [29] Holland, J.H., 1992. Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence. MIT press. [30] Huband, S., Hingston, P., Barone, L., While, L., 2006. A review of multiobjective test problems and a scalable test problem toolkit. IEEE Transactions on Evolutionary Computation 10, 477â€“506. [31] Huber, M.C., FuhrlÃ¤nder, M., SchÃ¶ps, S., 2022. Multi-objective yield optimization for electrical machines using gaussian processes to learn faulty design. IEEE Transactions on Industry Applications 59, 1340â€“ 1350. doi: 10.1109/TIA.2022.3211250 .[32] ICES, 2019. SISP 10 â€“ Manual for the North Sea International Bottom Trawl Surveys. Technical Report. International Council for the Exploration of the Sea (ICES). URL: https: //ices-library.figshare.com/articles/report/SISP_10_Manual_ for_the_North_Sea_International_Bottom_Trawl_Surveys/19051250 ,doi: 10.17895/ices.pub.5713 .[33] Ishibuchi, H., Tsukamoto, N., Nojima, Y., 2008. Evolutionary many-objective optimization: A short review. 2008 IEEE Congress on Evolutionary Computation, CEC 2008 , 2419â€“2426doi: 10.1109/CEC. 2008.4631121 .[34] Jones, D.R., Schonlau, M., Welch, W.J., 1998. Efficient global optimization of expensive black-box functions. Journal of Global Optimization 13, 455â€“492. doi: 10.1023/a:1008306431147 .[35] Kiranyaz, S., Ince, T., Hamila, R., Gabbouj, M., 2015. Convo-lutional neural networks for patient-specific ecg classification, in: 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), IEEE. pp. 2608â€“2611. doi: 10.1109/embc.2015.7318926 .[36] Knowles, J., 2006. Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation 10, 50â€“66. doi: 10. 1109/tevc.2005.851274 .[37] Kukkonen, S., Lampinen, J., 2005. Gde3: The third evolution step of generalized differential evolution. 2005 IEEE Congress on Evo-lutionary Computation, IEEE CEC 2005. Proceedings 1, 443â€“450. doi: 10.1109/CEC.2005.1554717 .[38] Kursawe, F., 1991. A variant of evolution strategies for vector optimization, in: Schwefel, H.P., MÃ¤nner, R. (Eds.), Parallel Problem Solving from Nature, Springer Berlin Heidelberg, Berlin, Heidelberg. pp. 193â€“197. [39] Li, H., Zhang, Q., 2008. Multiobjective optimization problems with complicated pareto sets, moea/d and nsga-ii. IEEE transactions on evolutionary computation 13, 284â€“302. [40] Li, J., Wang, P., Dong, H., Shen, J., Chen, C., 2022. A classifi-cation surrogate-assisted multi-objective evolutionary algorithm for expensive optimization. Knowledge-Based Systems 242, 108416. doi: 10.1016/j.knosys.2022.108416 .[41] Lihui Wang, Amos H. C. Ng, K.D. (Ed.), 2011. Multi-objective Evolutionary Optimisation for Product Design and Manufacturing. Springer London. doi: 10.1007/978-0-85729-652-8 .[42] Loshchilov, I., Schoenauer, M., Sebag, M., 2010. Dominance-based pareto-surrogate for multi-objective optimization, in: Deb, K., Bhattacharya, A., Chakraborti, N., Chakroborty, P., Das, S., Dutta, J., Gupta, S.K., Jain, A., Aggarwal, V., Branke, J., Louis, S.J., Tan, K.C. (Eds.), Simulated Evolution and Learning, Springer Berlin Hei-delberg, Berlin, Heidelberg. pp. 230â€“239. [43] Mann, H.B., Whitney, D.R., 1947. On a test of whether one of two random variables is stochastically larger than the other. The Annals of Mathematical Statistics 18, 50â€“60. doi: 10.1214/aoms/1177730491 .[44] Myers, R.H., Montgomery, D.C., Anderson-Cook, C.M., 2016. Re-sponse Surface Methodology Process and Product Optimization Us-ing Designed Experiments. Wiley & Sons, Incorporated, John. [45] Nain, P., Deb, K., . Computationally effective search and optimization procedure using coarse to fine approximations, in: The 2003 Congress on Evolutionary Computation, 2003. CEC â€™03., IEEE. pp. 2081â€“2088. doi: 10.1109/cec.2003.1299929 .[46] Pan, L., He, C., Tian, Y., Wang, H., Zhang, X., Jin, Y., 2019. Aclassification-based surrogate-assisted evolutionary algorithm for ex-pensive many-objective optimization. IEEE Transactions on Evolu-tionary Computation 23, 74â€“88. doi: 10.1109/tevc.2018.2802784 .[47] Probst, P., Wright, M.N., Boulesteix, A., 2019. Hyperparameters and tuning strategies for random forest. WIREs Data Mining and Knowledge Discovery 9. doi: 10.1002/widm.1301 .[48] Purshouse, R.C., Fleming, P.J., 2007. On the evolutionary optimiza-tion of many conflicting objectives. IEEE Transactions on Evolution-ary Computation 11, 770â€“784. doi: 10.1109/tevc.2007.910138 .[49] Rahmani, K., Thapa, R., Tsou, P., Casie Chetty, S., Barnes, G., Lam, C., Foon Tso, C., 2023. Assessing the effects of data drift on the performance of machine learning models used in clinical sepsis prediction. International Journal of Medical Informatics 173, 104930. doi: 10.1016/j.ijmedinf.2022.104930 .[50] Rasmussen, C.E., Williams, C.K.I., 2008. Gaussian processes for ma-chine learning. Adaptive computation and machine learning. 3. print ed., MIT Press, Cambridge, Massachusetts. Includes bibliographical references and indexes. [51] Ratle, A., 2001. Kriging as a surrogate fitness landscape in evolutionary optimization. Artificial Intelligence for Engineer-ing Design, Analysis and Manufacturing 15, 37â€“49. doi: 10.1017/ s0890060401151024 .[52] Santana-Quintero, L.V., MontaÃ±o, A.A., Coello, C.A.C., 2010. Areview of techniques for handling expensive functions in evolutionary multi-objective optimization, in: Tenne, Y., Goh, C.K. (Eds.), Com-putational Intelligence in Expensive Optimization Problems. Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 29â€“59. URL: https://doi. org/10.1007/978-3-642-10701-6_2 , doi: 10.1007/978-3-642-10701-6_2 .[53] Shepard, D., 1968. A two-dimensional interpolation function for irregularly-spaced data, in: Proceedings of the 1968 23rd ACM na-tional conference on -, ACM Press. pp. 517â€“524. doi: 10.1145/800186. 810616 .[54] Srinivas, N., Deb, K., 1994. Muiltiobjective optimization using non-dominated sorting in genetic algorithms. Evolutionary computation 2, 221â€“248. [55] Svozil, D., Kvasnicka, V., Pospichal, J., 1997. Introduction to multi-layer feed-forward neural networks. Chemometrics and Intelligent Laboratory Systems 39, 43â€“62. doi: 10.1016/s0169-7439(97)00061-0 .[56] Thain, D., Tannenbaum, T., Livny, M., 2005. Distributed computing in practice: the condor experience. Concurrency and Computation: Practice and Experience 17, 323â€“356. doi: 10.1002/cpe.938 .[57] Verma, S., Pant, M., Snasel, V., 2021. A comprehensive review on nsga-ii for multi-objective combinatorial optimization problems. IEEE Access 9, 57757â€“57791. [58] Wang, Y., Zou, X., Wu, Y., 2025. Deep neural network-based surrogate-assisted evolutionary algorithm for expensive con-strained optimization. Cluster Computing 28. doi: 10.1007/ s10586-025-05169-4 .[59] Wolpert, D., Macready, W., 1997. No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation 1, 67â€“82. doi: 10.1109/4235.585893 .[60] Yagoubi, M., Bederina, H., 2023. Surrogate-assisted nsga-ii algo-rithm for expensive multiobjective optimization, in: Proceedings of the Companion Conference on Genetic and Evolutionary Computa-tion, ACM. pp. 431â€“434. doi: 10.1145/3583133.3590746 .[61] Yuan, Y., Banzhaf, W., 2022. Expensive multiobjective evolutionary optimization assisted by dominance prediction. IEEE Transactions 

First Author et al.: Preprint submitted to Elsevier Page 21 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

on Evolutionary Computation 26, 159â€“173. doi: 10.1109/tevc.2021. 3098257 .[62] ZÄƒvoianu, A.C., Lacroix, B., McCall, J., 2022. Lightweight interpolation-based surrogate modelling for multi-objective con-tinuous optimisation, in: Moreno-DÃ­az, R., Pichler, F., Quesada-Arencibia, A. (Eds.), Computer Aided Systems Theory â€“ EURO-CAST 2022, Springer Nature Switzerland, Cham. pp. 53â€“60. [63] Zhang, H., Li, Y., Zhang, Y., Shen, Q., 2017. Spectral-spatial classi-fication of hyperspectral imagery using a dual-channel convolutional neural network. Remote Sensing Letters 8, 438â€“447. doi: 10.1080/ 2150704x.2017.1280200 .[64] Zhang, Q., Li, H., 2007. Moea/d: A multiobjective evolutionary algo-rithm based on decomposition. IEEE Transactions on Evolutionary Computation 11, 712â€“731. doi: 10.1109/TEVC.2007.892759 .[65] Zhao, Y., Zhao, J., Zeng, J., Tan, Y., 2022. A two-stage infill strategy and surrogate-ensemble assisted expensive many-objective optimization. Complex & Intelligent Systems 8, 5047â€“5063. doi: 10. 1007/s40747-022-00751-4 .[66] Zhu, Y.h., Luo, Y.z., 2015. Multi-objective optimisation and decision-making of space station logistics strategies. International Journal of Systems Science 47, 3132â€“3148. doi: 10.1080/00207721.2015.1091898 .[67] Zitzler, E., Deb, K., Thiele, L., 2000. Comparison of multiobjective evolutionary algorithms: Empirical results. Evolutionary Computa-tion. 8, 173â€“195. doi: 10.1162/106365600568202 .[68] Zitzler, E., KÃ¼nzli, S., 2004. Indicator-based selection in multi-objective search, in: Yao, X., Burke, E.K., Lozano, J.A., Smith, J., Merelo-GuervÃ³s, J.J., Bullinaria, J.A., Rowe, J.E., TiÅˆo, P., KabÃ¡n, A., Schwefel, H.P. (Eds.), Parallel Problem Solving from Nature - PPSN VIII, Springer Berlin Heidelberg, Berlin, Heidelberg. pp. 832â€“842. [69] Zitzler, E., Thiele, L., 1998. Multiobjective optimization using evolutionary algorithms â€” a comparative case study, in: Lecture Notes in Computer Science. Springer Berlin Heidelberg, pp. 292â€“301. doi: 10.1007/bfb0056872 .[70] Zitzler, E., Thiele, L., 1999. Multiobjective evolutionary algorithms: a comparative case study and the strength pareto approach. IEEE transactions on Evolutionary Computation 3, 257â€“271. [71] ZÄƒvoianu, A.C., 2015. Enhanced evolutionary algorithms for solv-ing computationally-intensive multi-objective optimization problems. Phd thesis. Johannes Kepler University. [72] ZÄƒvoianu, A.C., Bramerdorfer, G., Lughofer, E., Silber, S., Amrhein, W., Peter Klement, E., 2013. Hybridization of multi-objective evo-lutionary algorithms and artificial neural networks for optimizing the performance of electrical drives. Engineering Applications of Artificial Intelligence 26, 1781â€“1794. doi: 10.1016/j.engappai.2013. 06.002 .[73] ZÄƒvoianu, A.C., Saminger-Platz, S., Lughofer, E., Amrhein, W., 2018. Two enhancements for improving the convergence speed of a robust multi-objective coevolutionary algorithm, in: Proceedings of the Ge-netic and Evolutionary Computation Conference, ACM. pp. 793â€“800. doi: 10.1145/3205455.3205549 .

## A. Appendix 

In this Appendix, we plot the ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) and ğ¼ğºğ· (ğ‘ƒ ğ¹ ğ‘ )

comparative performance of the NSGA-II and MOEA/D based solvers on the individual 31 benchmark problems in Figures 16 to 19. In Figure 20 we show the comparative performance of the solvers on two DTLZ problem versions with 4 objectives (i.e., many objective optimisation problem extensions). 

First Author et al.: Preprint submitted to Elsevier Page 22 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

Figure 16: Comparison of NSGA-II and its associated surrogate-enhanced solvers on individual benchmark problems using ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )

â€“ i.e., the relative hypervolume â€“ as a performance indicator.. First Author et al.: Preprint submitted to Elsevier Page 23 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

Figure 17: Comparison of MOEA/D and its associated surrogate-enhanced solvers on individual benchmark problems using 

ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ ) â€“ i.e., the relative hypervolume â€“ as a performance indicator. First Author et al.: Preprint submitted to Elsevier Page 24 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

Figure 18: Comparison of NSGA-II and its associated surrogate-enhanced solvers on individual benchmark problems using 

ğ¼ğºğ· (ğ‘ƒ ğ¹ ğ‘ ) â€“ i.e., the normalised inverse generational distance â€“ as a performance indicator. First Author et al.: Preprint submitted to Elsevier Page 25 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

Figure 19: Comparison of MOEA/D and its associated surrogate-enhanced solvers on individual benchmark problems using 

ğ¼ğºğ· (ğ‘ƒ ğ¹ ğ‘ ) â€“ i.e., the normalised inverse generational distance â€“ as a performance indicator. First Author et al.: Preprint submitted to Elsevier Page 26 of 22 Adaptive Surrogate-Based Strategy for Accelerating Convergence Speed in MOEAs 

(a) NSGA-II (b) NSGA-II 

(c) MOEA/D-DRA (d) MOEA/D-DRA 

Figure 20: ğ»ğ‘£ (ğ‘ƒ ğ¹ ğ‘ )-measured mean comparative performance of NSGA-II and MOEA/D-DRA based solvers on DTLZ5 and DTLZ7 versions with 4 objectives. First Author et al.: Preprint submitted to Elsevier Page 27 of 22