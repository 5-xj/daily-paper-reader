# SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents
# SWE-Replay：软件工程智能体的高效测试时缩放

**Authors**: Yifeng Ding, Lingming Zhang \\
**Date**: 2026-01-29 \\
**PDF**: https://arxiv.org/pdf/2601.22129v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 6.0 \\
**Evidence**: Efficient test-time scaling for agents aligns with efficient automatic algorithm design \\

---

## Abstract
Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.

## 摘要
测试时缩放（Test-time

---

## 速览摘要（自动生成）

**问题**：软件工程智能体在推理侧扩展时计算成本极高，且现有基于价值评估的方法存在模型失准和泛化性差的问题