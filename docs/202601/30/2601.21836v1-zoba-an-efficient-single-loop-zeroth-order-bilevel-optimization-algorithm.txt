Title: ZOBA: An Efficient Single-loop Zeroth-order Bilevel Optimization Algorithm

URL Source: https://arxiv.org/pdf/2601.21836v1

Published Time: Fri, 30 Jan 2026 02:24:56 GMT

Number of Pages: 79

Markdown Content:
# ZOBA: An Efficient Single-loop Zeroth-order Bilevel Optimization Algorithm 

Marco Rando 1 and Samuel Vaiter 21Universit´ e Cˆ ote d’Azur, Inria, CNRS, LJAD, Nice, France 

> 2

CNRS & Universit´ e Cˆ ote d’Azur, LJAD, Nice, France 

Abstract 

Bilevel optimization problems consist of minimizing a value function whose evaluation depends on the solution of an inner optimization problem. These problems are typically tackled using first-order methods that require computing the gradient of the value function ( the hypergradient ). In several practical settings, however, first-order information is unavailable ( zeroth-order setting ), rendering these methods inapplicable. Finite-difference methods provide an alternative by approximating hypergradients using function evaluations along a set of directions. Nevertheless, such surrogates are notoriously expensive, and existing finite-difference bilevel methods rely on two-loop algorithms that are poorly parallelizable. In this work, we propose ZOBA, the first finite-difference single-loop algorithm for bilevel optimization. Our method leverages finite-difference hypergradient approximations based on delayed information to eliminate the need for nested loops. We analyze the proposed algorithm and establish convergence rates in the non-convex setting, achieving a complexity of O(p(d + p)2ε−2), where p and d denote the dimension of inner and outer spaces respectively, which is better than prior approaches based on Hessian approximation. We further introduce and analyze HF-ZOBA, a Hessian-free variant that yields additional complexity improvements. Finally, we corroborate our findings with numerical experiments on synthetic functions and a real-world black-box task in adversarial machine learning. Our results show that our methods achieve accuracy comparable to state-of-the-art techniques while requiring less computation time. 

Keywords: Zeroth-order Optimization, Black-box Optimization, Bilevel Optimization, Stochastic Optimization. 

AMS Mathematics Subject Classification: 90C56, 90C46, 90C15, 90C26, 90C30. 

# 1 Introduction 

Bilevel optimization problems involve minimizing an outer objective ( the value function ) whose evaluation depends on the solution of an inner optimization problem. This class of problems arises naturally in a wide range of machine learning applications, including hyperparameter optimization [ 6, 37 ], meta-learning [ 18 ], representation learning [ 26 ], and many others. These problems are typically addressed using first-order algorithms that aim to compute the gradient of the value function, the hypergradient [ 21 , 28 , 12 , 14 , 4]. In practice, computing the hypergradient exactly is prohibitively expensive or infeasible, since at each iteration it requires the action of the Hessian (and its inverse) of the inner objective, as well as a minimizer of the inner problem. For this reason, first-order bilevel methods approximate the hypergradient by relying on an approximate inner solution and inverse-Hessian actions obtained through iterative procedures. This naturally leads to a two-loop algorithmic structure, where inner loops construct these approximations by exploiting gradients and, in some cases, Hessians of the inner and outer objectives. However, in many practical scenarios, such information is either unavailable or prohibitively expensive to compute, and only function evaluations of the inner and outer objectives are accessible. Such problems are referred to as black-box bilevel optimization problems and are particularly relevant, as they arise in many real-world applications such as meta-training, complex control problem, fine-tuning large language models (LLMs) and many others - see, 1

> arXiv:2601.21836v1 [math.OC] 29 Jan 2026

e.g., [ 47 , 50 , 40 , 49 , 30 , 22 , 13 ]. To tackle black-box bilevel problems, several classes of algorithms have been proposed [ 51 , 9, 13 , 1], and finite-difference is one of these. Finite-difference methods are iterative procedures that mimic first-order strategies by replacing gradients with surrogates constructed from function evaluations along a set of directions [ 34 , 20 , 41 , 16 , 8]. Designing efficient finite-difference methods for bilevel optimization is particularly challenging. Approximating all first- and second-order quantities involved in the hypergradient computations with finite-difference is extremely expensive, especially in high-dimensional settings, where a large number of function evaluations is required to obtain reliable gradient and Hessian surrogates. Existing finite-difference methods for black-box bilevel optimization [ 1, 2] typically mimic the first-order two-loop structure and attempt to mitigate the resulting computational cost by using computationally cheap but inaccurate gradient and Hessian approximations in the inner loops. To control the resulting approximation errors, these methods have to employ small step sizes and perform many inner-loop iterations, leading to significant computational overhead and poor parallelization in practice. More recent approaches [ 2] aim to reduce this burden by regularization-based reformulations that avoid explicit Hessian approximation. While reducing per-iteration cost, these methods introduce bias into the bilevel objective and require careful tuning of regularization parameters, which may negatively affect solution accuracy. Moreover, the underlying two-loop structure is preserved, thereby retaining the same limitations in terms of parallelization. Recent advances in first-order bilevel optimization have shown that the two-loop structure can be avoided by using single-loop algorithms that rely on approximate hypergradients computed using delayed information [ 14 ]. These methods offer substantial practical advantages, as they allow all quantities involved in hypergradient computation to be evaluated in parallel and significantly reduce per-iteration computational cost while preserving convergence guarantees. However, extending these ideas efficiently to black-box setting is far from straightforward. In particular, it is not evident that mimicking a single-loop scheme can achieve meaningful computational savings while sufficiently controlling finite-difference errors to preserve convergence and get reasonable complexity results. As a result, to the best of our knowledge, no single-loop finite-difference method for bilevel optimization has been previously proposed. In this work, we propose ZOBA, the first single-loop finite-difference algorithm for black-box bilevel optimiza-tion. Our method relies on delayed information and enables full parallelization in the approximation of the hypergradient. Moreover, ZOBA tackles the approximation cost per iteration by reusing the same function evaluations to compute multiple components of the hypergradient, thereby significantly reducing the number of required function evaluations. We analyze the proposed algorithm and derive convergence guarantees for non-convex bilevel problems. In particular, we show that ZOBA achieves a complexity of O(p(d + p)2ε−2), for an accuracy ε ∈ (0 , 1) and where p and d denote the dimensions of the inner and outer variables, respectively, improving upon existing zeroth-order bilevel methods that rely on explicit Hessian approximations. We further introduce and analyze HF-ZOBA, a Hessian-free variant that entirely avoids explicit Hessian approximations and does not rely on regularization-based strategies, yielding additional improvements in the complexity results. Finally, we corroborate our theoretical findings through experiments on synthetic objectives and on the real-world task of computing minimal-distortion universal adversarial perturbations in black-box settings. The numerical results show that our methods, despite the additional errors introduced by delayed information and finite-difference surrogates, achieve solution quality comparable to state-of-the-art finite-difference algorithms for bilevel optimization while providing significant improvements in runtime. The paper is organized as follows. In Section 2, we formally define the problem, introduce our algorithms, and discuss their relationship with related work. In Section 3, we state and discuss the main theoretical results. Section 4 presents numerical experiments, and Section 5 concludes the paper with final remarks. 

# 2 Problem Setting & Algorithm 

Let (Ω 1, F1, P1) and (Ω 2, F2, P2) be probability spaces, and let Z1, Z2 be measurable spaces. Let ζ : Ω 1 → Z 1,

ξ : Ω 2 → Z 2 be two random variables and let f : Rp × Rd × Z 1 → R and g : Rp × Rd × Z 2 → R be two functions. 2Consider the minimization of a function Ψ : Rd → R defined as 

min 

> x∈Rd

Ψ( x) := F (z∗(x), x ) = Eζ [f (z∗(x), x, ζ )] 

s.t. 

z∗(x) ∈ arg min 

> z∈Rp

G(z, x ) = Eξ [g(z, x, ξ )] .

(1) Under standard regularity assumptions [ 21 ], the function Ψ is differentiable, and its gradient (the hypergradient )admits the following representation 

∇Ψ( x) = ∇xF (z∗(x), x ) + ∇2 

> xz

G(z∗(x), x ), v ∗(x), (2) where 

v∗(x) = − ∇2 

> zz

G(z∗(x), x )−1 ∇z F (z∗(x), x ). (3) Here, ∇x and ∇z denote gradients with respect to x and z, respectively, while ∇2 

> xz

and ∇2 

> zz

denote the corresponding Hessian blocks. We consider the stochastic zeroth-order setting, in which no first- or higher-order information of f and g is available. Moreover, exact evaluations of F and G are inaccessible, and only noisy evaluations of f and g can be obtained. To address Problem (1) , we propose a stochastic zeroth-order algorithm, namely, an iterative procedure that relies solely on evaluations of f and g. At each iteration k, our method constructs a hypergradient surrogate at the current iterate xk by approximating gradient and Hessian terms with mini-batch finite differences and using auxiliary sequences zk and vk to track z∗(xk) and v∗(xk). We next describe how these sequences are built ( auxiliary sequence construction ) and how the hypergradient is approximated ( hypergradient approximation ). 

Auxiliary sequence construction. At every iteration k ∈ N, let (ξi,k )b1 

> i=1

be realizations of independent copies of the random variable ξ. For each i ∈ [b1] := {1, . . . , b 1}, let  w(i,j )

> k

ℓ1 

> j=1

⊂ Rp be vectors sampled i.i.d. from 

N (0 , I p). For hk > 0, let g+,k i,j := g(zk + hkw(i,j ) 

> k

, x k, ξ i,k ) and g−,k i,j := g(zk − hkw(i,j ) 

> k

, x k, ξ i,k ). Our algorithm approximates ∇z G(zk, x k) with the following surrogate 

Dkz := 1

b1ℓ1

> b1

X

> i=1
> ℓ1

X

> j=1

g+,k i,j − g−,k i,j 

2hk

w(i,j ) 

> k

. (4) Let ρk > 0 be a stepsize, we define the sequence (zk)k≥0 with the following iteration 

zk+1 = zk − ρkDkz . (5) Then, denoting with gki := g(zk, x k, ξ i,k ) and cki,j := ( g+,k i,j + g−,k i,j − 2gki )/(2 h2

> k

), our method builds the surrogate of ∇2 

> zz

G(zk, x k) as follows 

ˆ∇2 

> zz

gk := 1

b1ℓ1

> b1

X

> i=1
> ℓ1

X

> j=1

cki,j 



w(i,j ) 

> k

w(i,j )⊤ 

> k

− Ip



.

Notice that to compute this quantity we need to perform just b1 evaluations since g+,k i,j and g−,k i,j are already computed for eq. (5) . Let b2, ℓ 2 > 0 and let (ζi,k )b2 

> i=1

be realizations of independent copies of ζ. Let f z,k, + 

> i,j

=

f (zk + hkw(i,j ) 

> k

, x k, ζ i,k ) and f ki := f (zk, x k, ζ i,k ). Our algorithm approximates ∇z F (zk, x k) as 

ˆ∇z fk := 1

b2ℓ2

> b2

X

> i=1
> ℓ2

X

> j=1

f z,k, + 

> i,j

− f ki

hk

w(i,j ) 

> k

.

Then, our procedure build the following search direction 

Dkv := ˆ∇2 

> zz

gkvk + ˆ∇z fk. (6) Thus, we construct the next auxiliary iterate vk+1 as 

vk+1 = vk − ρkDkv . (7) 3Hypergradient approximation. For i = 1 , · · · , b 1 and j = 1 , · · · , ℓ 1, let ˆg+,k i,j := g(zk + hkw(i,j ) 

> k

, x k +

hku(i,j ) 

> k

, ξ i,k ), ˆg−,k i,j = g(zk − hkw(i,j ) 

> k

, x k − hku(i,j ) 

> k

, ξ i,k ) and ski,j = (ˆ g+,k i,j + ˆ g−,k i,j − 2gki )/(2 h2

> k

). Our method approximates ∇2 

> xz

G(zk, x k) as 

ˆ∇2 

> xz

gk := 1

b1ℓ1

> b1

X

> i=1
> ℓ1

X

> j=1

ski,j u(i,j ) 

> k

w(i,j )⊤ 

> k

.

Notice that such surrogate requires only 2b1ℓ1 function evaluations since (gki )b1 

> i=1

are already computed for previous quantities. Let f x,k, + 

> i,j

= f (zk, x k + hku(i,j ) 

> k

, ζ i,k ), consider the following approximation of ∇xF

ˆ∇xfk := 1

ℓ2b2

> b2

X

> i=1
> ℓ2

X

> j=1

f x,k, + 

> i,j

− f ki

hk

u(i,j ) 

> k

.

Notice that since (f ki )b2 

> i=1

are already computed for eq. (6) , such approximation requires b2ℓ2 function evalua-tions. The hypergradient surrogate is thus built as follows 

Dkx := ˆ∇2 

> xz

gkvk + ˆ∇xfk. (8) Finally, the iterate is updated with the following iteration 

xk+1 = xk − γkDkx. (9) We summarize the entire procedure in Algorithm 1. 

Algorithm 1 ZOBA: Zeroth-Order Bilevel Algorithm  

> 1:

Input: x0 ∈ Rd, z0, v 0 ∈ Rp, stepsizes (γk), (ρk), discretization parameter hk, minibatch sizes b1, b 2, number of directions ℓ1, ℓ 2. 

> 2:

Let b = max( b1, b 2) and ℓ = max( ℓ1, ℓ 2). 

> 3:

for k = 0 , 1, 2, . . . do  

> 4:

Sample i.i.d. ξ1,k , . . . , ξ b,k and ζ1,k , . . . , ζ b,k . 

> 5:

For each i ∈ [b], sample (w(i,j ) 

> k

)ℓj=1 ⊂ Rp and (u(i,j ) 

> k

)ℓj=1 ⊂ Rd from N (0 , I p) and N (0 , I d). 

> 6:

Compute 

zk+1 = zk − ρkDkz (zk, x k)

vk+1 = vk − ρkDkv (zk, x k)

xk+1 = xk − γkDkx(zk, x k) 

> 7:

end for 

Given initial guesses z0, v 0 ∈ Rp and x0 ∈ Rd, at each iteration k ∈ N, the algorithm gets two mini-batches of b = max( b1, b 2) realizations (ξi,k )bi=1 and (ζi,k )bi=1 of independent copies of ξ and ζ. For each i ∈ [b], it then samples ℓ = max( ℓ1, ℓ 2) i.i.d. Gaussian directions (w(i,j ) 

> k

)ℓj=1 and (u(i,j ) 

> k

)ℓj=1 from N (0 , I p) and N (0 , I d).Using these samples, the algorithm constructs the search directions Dkz , Dkv , and Dkx defined in eqs. (4) , (6) ,and (8) , and updates the sequences zk, vk, and xk accordingly. The update rules are guided by the following intuitions. i) Eq. (5) corresponds to a single step of a mini-batch finite-difference method applied to the inner objective z 7 → G(z, x k), yielding a one-step approximation of z∗(xk) (under assumptions, such as convexity in 

z). ii) Eq. (7) is a single mini-batch finite-difference step applied to the quadratic objective associated to the linear system in eq. (3) , using zk in place of z∗(xk). iii) Finally, the iterate xk is updated using the resulting hypergradient surrogate. Notice that the updates of the sequences are performed in parallel. This is due the use of delayed information. In particular, vk+1 is computed using zk instead of zk+1 ; since zk depends on xk−1,this can be interpreted as a one-step delay as zk would be an approximation of z∗(xk−1). While this delay induces additional error, it enables reuse the function evaluations employed in Dkz to estimate the Hessian block ∇2 

> zz

G(zk, x k), reducing its cost from b1(2 ℓ1 + 1) to b1 evaluations. A similar delay appears in the outer 4update, where xk+1 is computed using zk and vk instead of their updated counterparts, with vk itself being affected by a delayed update. While this further propagates approximation error, it enables reuse of function evaluations in the computation of Dkx. As a result, the total number of function evaluations per iteration is 

b1(4 ℓ1 + 1) + b2(2 ℓ2 + 1) .

Hessian-free variant. Algorithm 1 constructs hypergradient surrogates by explicitly approximating the Hessian blocks in eq. (2) via finite differences. Since accurate finite difference Hessian approximations requires a large number of function evaluations [ 2 ], we introduce a Hessian-free variant that avoids explicit Hessian approximation. Instead, we approximate Hessian–vector products using first-order finite differences and estimate the resulting gradients with finite-differences. Let ˆg+,z,k i,j (z, x ) = g(z + hkw(i,j ) 

> k

, x, ξ i,k ), ˆg+,x,k i,j (z, x ) = 

g(z, x + hku(i,j ) 

> k

, ξ i,k ) and ˆgki (z, x ) = g(z, x, ξ i,k ). We define the following minibatch gradient approximation in variable z and x for arbirtrary z, x as 

ˆ∇z gk(z, x ) := 1

b1ℓ1

> b1

X

> i=1
> ℓ1

X

> j=1

ˆg+,z,k i,j (z, x ) − ˆgki (z, x )

hk

w(i,j ) 

> k

.

ˆ∇xgk(z, x ) := 1

b1ℓ1

> b1

X

> i=1
> ℓ1

X

> j=1

ˆg+,x,k i,j (z, x ) − ˆgki (z, x )

hk

u(i,j ) 

> k

.

Let ¯h > 0, we approximate ∇2 

> zz

G(zk, x k)vk and ∇2 

> xz

G(zk, x k)vk with the following estimators 

Hkzz := 1¯hk

( ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k)) 

Hkxz := 1¯hk

( ˆ∇xgk(zk + ¯hkvk, x k) − ˆ∇xgk(zk, x k)) .

Using these approximations, we define the search directions 

ˆDkz (zk, x k) := ˆ∇z g(zk, x k),

ˆDkv (zk, x k) := Hkzz + ˆ∇z f (zk, x k),

ˆDkx(zk, x k) := Hkxz + ˆ∇xf (zk, x k).

(10) Thus, we define the following algorithm 

Algorithm 2 HF-ZOBA: Hessian-free Zeroth-Order Bilevel Algorithm  

> 1:

Input: x0 ∈ Rd, z0, v 0 ∈ Rp, stepsizes (γk), (ρk), smoothing parameter hk, minibatch sizes b1, b 2, directions 

ℓ1, ℓ 2. 

> 2:

Let b = max( b1, b 2) and ℓ = max( ℓ1, ℓ 2). 

> 3:

for k = 0 , 1, 2, . . . do  

> 4:

Sample i.i.d. ξ1,k , . . . , ξ b,k and ζ1,k , . . . , ζ b,k . 

> 5:

For each i ∈ [b], sample (w(i,j ) 

> k

)ℓj=1 ⊂ Rp and (u(i,j ) 

> k

)ℓj=1 ⊂ Rd from N (0 , I p) and N (0 , I d). 

> 6:

Compute 

zk+1 = zk − ρk ˆDkz (zk, x k)

vk+1 = vk − ρk ˆDkv (zk, x k)

xk+1 = xk − γk ˆDkx(zk, x k) 

> 7:

end for 

The Hessian-free variant (HF-ZOBA) preserves the structure of Algorithm 1, differing only by replacing the search directions with the ones in eq. (10) . The sequences zk, vk, and xk are updated in parallel, and 5delayed information is exploited to reuse function evaluations across all updates. In particular, function values used to compute ˆ∇z gk and ˆ∇z fk are reused in Hkzz , H kxz and ˆ∇xfk. As a result, HF-ZOBA requires 

2b1(2 ℓ1 + 1) + b2(2 ℓ2 + 1) function evaluations per iteration, which is higher than ZOBA, but without explicit Hessian estimation. In the next section, we review the related works and compare our algorithm with the state-of-the-arts methods. 

2.1 Related Works 

Several algorithms have been proposed for black-box bilevel optimization. We review the most related ones, highlighting differences with our methods, and briefly summarize other approaches. 

Finite-difference methods. Several finite-difference approaches have been proposed for bilevel optimization. In [ 23 ] a finite-difference based algorithm for bilevel optimization is introduced. However, exact function values and access to an approximation of z∗ via first-order methods are assumed to be available and no convergence guarantees are provided. In [ 49 ], a finite-difference bilevel method for LLMs fine-tuning is proposed and analyzed in non-convex setting. However, their approach assumes access to gradients of the objectives, which are unavailable in our setting. In [ 1], the authors propose ZDSBA, a method for black-box bilevel optimization that approximates gradients and Hessians using single-sample, single-direction finite difference surrogates. The authors analyze their approach and obtain a complexity of O

 (d+p)4 

> ε3

log d+pε



in non-convex setting. This is improved in ZMDSBA [ 2] which extends ZDSBA by using mini-batch finite-difference to approximate ∇xF

and ∇2 

> xz

G, yielding a complexity of O

 p(d+p)2 

> ε2

log 1

> ε



in the same setting. However, both our algorithms achieve better complexity. Moreover, ZDSBA and ZMDSBA rely on a two-loop structure, where the inner loops approximate z∗ and v∗. In these loops, single-sample, single-direction finite-difference surrogates are used to estimate gradients and Hessians. While cheap, these approximations are highly inaccurate, and in high dimensions many inner iterations are required for reliable estimates. In contrast, our methods adopt a single-loop design and approximate z∗ and v∗ in a single step using multi-direction, mini-batch finite differences. As observed in prior work on finite differences [ 32 , 46 , 41 , 43 , 44 ], a single step of finite-difference methods with surrogates constructed with multiple directions provide better performance than many steps each with surrogate constructed with single-direction. Moreover, due to the single loop design, at every iteration all function evaluations can be parallelized while inner-loop iterations cannot. In [ 2], the authors also propose a Hessian-free variant (Opt-ZMDSBA) achieving a complexity of O  (d + p)ε−2. While HF-ZOBA matches this complexity, our approach offers several key advantages. First, Opt-ZMDSBA, although Hessian-free, is still a two-loop algorithm. Here the inner loop is used to approximate z∗, and therefore it inherits the same limitations of ZDSBA and ZMDSBA in high-dimensional settings, where many non-parallelizable inner iterations are required. Second, Opt-ZMDSBA relies on a regularization-based reformulation that transforms the bilevel problem into a single-level problem, introducing a regularization parameter λ that must be carefully tuned to achieve accurate solutions. In contrast, HF-ZOBA operates directly on the original bilevel problem without regularization, remaining Hessian-free, single-loop, and fully parallelizable, allowing better practical performance in runtime. Finally, our gradient and Hessian surrogates are more general than those in [ 2 , 1], recovering the estimators of [1] when b1 = b2 = ℓ1 = ℓ2 = 1 and those of [2] when b1 = b2 > 0 and ℓ1 = ℓ2 = 1 .

Other methods. Different other strategies were proposed in the literature. In [ 13 ], a zeroth-order trust-region method for bilevel optimization is proposed. However, only the deterministic setting with exact function evaluations is considered, and no convergence proof is provided. In [ 17 ], another trust-region algorithm is introduced and analyzed. However, the authors assume exact function values and first-order information of the inner objective to be available. Other strategies proposed are meta-heuristics. These are based on genetic algorithms [ 52 , 29 ], simulated annealing [ 45 , 54 ], particle swarm optimization [ 3, 19 ], and other techniques - see [ 9] for a review. While these approaches perform well in practice, they typically lack convergence guarantees. 63 Main Results 

In this section, we analyze Algorithms 1 and 2, providing convergence rates for different choices of parameters. More precisely, we provide bounds on the average squared norm of the gradient of the value function Ψ. In particular, we use the notation 

αK := 

> K

X

> k=0

γk E

h

∥∇ Ψ( xk)∥2i! 

AK ,

where AK = PKk=0 γk. In the following, we call complexity the number of function evaluations required to get 

αK ≤ ε with ε ∈ (0 , 1) . To improve readability, we define Rinit := Ψ( x0) − min Ψ + ϕ0 

> z

∥z0 − z∗(x0)∥2 + ϕ0 

> v

∥v0 −

v∗(x0)∥2, where ϕ0 

> z

, ϕ 0 

> v

> 0 are constants. To perform the analysis, we consider targets satisfying the following hypothesis. 

Assumption 3.1 (Unbiasedness and Bounded Variance) . Stochastic gradients and Hessians of inner and outer targets are unbiased estimators, i.e ∇F (z, x ) = Eζ [∇f (z, x, ζ )] , ∇G(z, x ) = Eξ [∇g(z, x, ξ )] and ∇2G(z, x ) = 

Eξ

∇2g(z, x, ξ ). Moreover, there exist constants σ1,G , σ 1,F ≥ 0 s.t. for every z ∈ Rp and x ∈ Rd,

Eζ [∥∇ f (z, x, ζ ) − ∇ F (z, x )∥2] ≤ σ21,F 

Eξ [∥∇ g(z, x, ξ ) − ∇ G(z, x )∥2] ≤ σ21,G .

Assumption 3.2 (Regularity of outer objective) . There exist L0,F , L 1,F > 0 such that for every ζ ∈ Z 1

the function (z, x ) 7 → f (z, x, ζ ) is L0,F -Lipschitz continuous, differentiable and its gradient is L1,F -Lipschitz continuous. 

Assumption 3.3 (Regularity of inner objective) . There exist L1,G , L 2,G , μ G > 0 such that, for every ξ ∈ Z 2 the function (z, x ) 7 → g(z, x, ξ ) is twice differentiable, its gradient is L1,G -Lipschitz and its Hessian is L2,G -Lipschitz. Moreover, for every x ∈ Rd, the function z 7 → g(z, x, ξ ) is μG-strongly convex. This setting is standard in stochastic zeroth-order bilevel optimization and has been adopted in prior work [ 1, 2]. Unlike these approaches, which additionally assume boundedness of the optimal solution of the inner problem, our analysis does not rely on such an assumption. We next present convergence rates under this regime for both algorithms. For readability, explicit constants of theorems and corollaries are reported in the proofs - see Appendices C and D. We begin with the main result for Algorithm 1. 

Theorem 3.4 (Convergence Rate of ZOBA) . Let Assumptions 3.1,3.2,3.3 hold. For every k ∈ N, let 

zk, v k, x k be the sequences generated by Algorithm 1. Let ρk < min 



1, μG 

> 2ω3(μG+4)

, μG

> 2 ¯Cv



, γk < cγ ρk and 

hk = O



min 

q b1ℓ1+p3 

> p4

, 1√p4+pd 3 , d −5/2

 

, where cγ , ω 3 > 0 and ¯Cv = O( p(p+6) 2 

> b1ℓ1

) are defined in the proofs. Then, for K > 0,

αK ≤ 2

AK

Rinit +

> K

X

> k=0

¯Ck

!

,

where ¯Ck ≥ 0 is a sequence of errors depending on γk, ρ k, h k defined in the proof. 

In the following corollary, we derive rates for specific choices of the parameters. 

Corollary 3.5. Under same assumptions of Theorem 3.4. (I) Let γk = γ, ρk = ρ and hk = h, we have 

αK ≤ Rinit 

γ(K + 1) + O

 ρ3 + ρ2

γ + γ



+ O

 ρ + ρ2 + ρ3

γ + γ



h2.

(II) Let γk = γ(k + 1) −α1 , ρk = ρ(k + 1) −α2 and hk = h(k + 1) −α3 with γ, ρ, h > 0 and α1, α 2 ∈ (1 /2, 1) and 

α3 > 1,

αK ≤ Rinit 

γ(K + 1) 1−α1 + O

 1(K + 1) 1−α1



.

7(III) Fix an accuracy ε ∈ (0 , 1) . Let γ =

r

> 1
> C5,1

 ∆1 

> K+1

+ C3μG ¯ϕz ρ3 + C6,1ρ2



, ρ = ε√16 C5,1C6,1

and h = q ε

> 2C(γ,ρ )

where the constants C3, ¯ϕz , C 6,1, ∆1, C 5,1, C (γ, ρ ) > 0 are defined in the proof. Let b1ℓ1 = O(⌊p(d + p)2/c 1⌋) and 

b2ℓ2 = O(⌊p(d + p)2/c 2⌋) for c1, c 2 ∈ R+. Let K + 1 ≥ O ( p(d+p)2 

> b1ℓ1

ε−2). Then, αK ≤ ε and the complexity is 

O  p(d + p)2ε−2.

In the following theorem and corollary, we show the rate for Algorithm 2 and we specialize the result with different choices of parameters. 

Theorem 3.6 (Convergence Rate of HF-ZOBA) . Under Assumptions 3.1,3.2,3.3. Let ρk < min 



1, 116 CB, 1L21,G 

, 14CA, 1L21,G 



,

γk < ¯cγ ρk, ¯hk =

(ˆhk/∥vk∥, ∥vk∦ = 0 ˆhk, otherwise , and hk ≤ μG

> 4

√(μG+2) ¯CL

p ¯ϕv ˆhk where ¯cγ > 0, C A, 1 = O (p/b 1ℓ1),

CB, 1 = O (p/b 1ℓ1), ˆhk > 0, ¯CL = O( (p+6) 3+( d+6) 3 

> b1ℓ1

) and ¯ϕv > 0 are defined in the proof. Let zk, v k, x k be the sequences generated by eq. (10) . Then, for K > 0,

αK ≤ 2

AK

Rinit +

> K

X

> k=0

ˆCk

!

.

Where ˆCk ≥ 0 is a sequence of errors depending on γk, ρ k, h k defined in the proof. 

Corollary 3.7. Under same assumptions of Theorem 3.6. (I) Let γk = γ, ρk = ρ, hk = h and ˆhk = ˆh. Then, 

αK ≤ Rinit 

γ(K + 1) + O



γ + ρ3

γ



+ O



1 + ρ2

γ + γ



ˆh2

+ O



1 + ρ2

γ + ρ3

γ



h2 + O

 ρ2

γ + γ

 h2

ˆh2 .

(II) Let γk = γ(k + 1) −α1 , ρk = ρ(k + 1) −α2 , hk = h(k + 1) −α3 and ˆhk = ˆh(k + 1) −α4 with γ, ρ, h, ˆh > 0,

α1, α 2 ∈ (1 /2, 1) , α3, α 4 > 1/2 and α3 − α4 ≥ 0,

αK ≤ Rinit 

γ(K + 1) 1−α1 + O

 1(K + 1) 1−α1



.

(III) Fix an accuracy ε ∈ (0 , 1) . Let ¯∆4 = O( d+pb1ℓ1 + d+pb2ℓ2 ) and fix K + 1 ≥ 32 Rinit ¯∆4 

> ˆcγ

ε−2. Let ρ =

q 2Rinit   

> ¯cγ¯∆4(K+1)

,

γ = ˆ cγ ρ where ˆcγ < ¯cγ = O(( b1ℓ1)/(d + p)) . Let h = ˆh2 and ˆh ≤ min 



1,

r ˆcγ ε

> 2

( ¯∆1+ ¯∆2+ ¯∆3)



where ¯∆1, ¯∆2, ¯∆3 > 0

are defined in the proof. Let b1ℓ1 = O(⌊(d + p)/c 1⌋) and b2ℓ2 = O(⌊(d + p)/c 1⌋) for c1, c 2 ∈ R+. Then, αK ≤ ε

and the complexity is O(( d + p)ε−2).

Discussion. The bounds in Theorems 3.4 and 3.6 depend on two terms: the initialization and an error component. Such errors arise from the use of stochastic information, delayed information, zeroth-order approximations of stochastic gradients and Hessians, and (in case of HF-ZOBA) the first-order finite-difference approximation of Hessian–vector products. In Corollaries 3.5 and 3.7 we provide two parameter regimes. The first (I) uses constant stepsizes, which is the most common choice in practice, while the second (II) ensures asymptotic convergence. Under choice (I), both algorithms converge to a neighborhood of a stationary point where the size is determined by stochastic variance (the term depending only on γ and ρ) and finite-difference bias (i.e. the term depending on h), consistent with classical results for stochastic zeroth-order methods - see e.g. [ 34 , 20 ]. For HF-ZOBA, this neighborhood size further depends on the accuracy of the Hessian–vector product approximation, as reflected by the ˆh and h/ ˆh terms, which respectively represent the finite-difference bias of the first-order Hessian surrogate and the zeroth-order approximation error of the gradients used within 8it. For both algorithms, expressing the outer stepsize γ as a fraction of ρ makes explicit that taking ρ (and h)smaller shrinks the size of such neighborhoods but slows the decay of the initialization term through its 1/γ 

dependence, mirroring the behavior of first-order stochastic methods with constant stepsize. Under choice (II), both algorithms recover the convergence rates of first-order stochastic bilevel methods [ 14 , 21 , 12 ] up to dependence on dimension. In particular, choosing α1 arbitrarily close to 1/2 yields rates approaching 

O(( K + 1) −1/2), which is order-optimal [ 2]. Moreover, for HF-ZOBA, guaranteeing convergence requires the parameter hk to decay faster than ˆhk, to compensate for the zeroth-order bias in the Hessian–vector approximation. In (III) we derive the complexities of the proposed algorithms. As shown in the corollaries, both methods improve upon ZDSBA [ 1], which attains a complexity of O

 (d+p)4

ε3 log 

 d+pε

 

. Moreover, they also improve over ZMDSBA [ 2 ], which achieves a complexity of O

 p(d+p)2

ε2 log   1

ε



. As in ZMDSBA, ZOBA retains the O(p(d + p)2) term, which is consistent with the observation in [ 2] that this dimensional dependence arises from explicit Hessian approximations. HF-ZOBA matches the complexity of Opt-ZMDSBA [ 2], which is indicated to be optimal in this regime. However, in contrast to Opt-ZMDSBA and ZMDSBA, whose such dimension dependence in the complexity partially comes from inner-loop iterations, our methods’ dependence arises from directions and batch sizes, which can be fully parallelized, yielding faster practical runtime. 

# 4 Experiments 

In this section, we present numerical experiments to evaluate the empirical performance of the proposed methods. We compare ZOBA and HF-ZOBA with ZDSBA [ 1 ], ZMDSBA [ 2], and Opt-ZMDSBA [ 2]. We consider two problems: a synthetic quadratic bilevel problem and the minimal universal black-box adversarial attack. For each algorithm and each task, hyperparameters are tuned via grid search. In both experiments, we run the algorithms with a fixed budget of 10 5 function evaluations. All experiments are repeated 10 times, and the mean and standard deviation of the results are reported in Figures 1 and 2. Additional details are provided in Appendix B. 

Synthetic target. Here, we evaluate the proposed methods on a synthetic quadratic bilevel problem. We consider settings with equal inner and outer dimensions p = d, and perform experiments considering d ∈{25 , 50 , 100 }.0 20000 40000 60000 80000 100000                                            

> # function evaluations
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ
> Synthetic Objective [ d= 25, p= 25]
> ZDSBA
> ZMDSBA
> OPT-ZMDSBA
> ZOBA
> HF-ZOBA 020000 40000 60000 80000 100000
> # function evaluations
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ
> Synthetic Objective [ d= 50, p= 50] 020000 40000 60000 80000 100000
> # function evaluations
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ
> Synthetic Objective [ d= 100, p= 100] 10 −3 10 −2 10 −1 10 010 110 2
> Time (s)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ
> Synthetic Objective [ d= 25, p= 25] 10 −3 10 −2 10 −1 10 010 110 2
> Time (s)
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ
> Synthetic Objective [ d= 50, p= 50] 10 −3 10 −2 10 −1 10 010 110 2
> Time (s)
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ
> Synthetic Objective [ d= 100, p= 100]

Figure 1: Comparison of algorithms on synthetic bilevel problems with varying dimensions: objective value versus function evaluations (first row) and versus wall-clock time in seconds (second row). In Figure 1 (first row) we report the normalized function value gap i.e. (Ψ( xk) − min Ψ) /(Ψ( x0) − min Ψ) as a function of the number of stochastic function evaluations. We observe that ZDSBA performs worst across all problem sizes, which is consistent with its theoretical complexity. For d = 25 and d = 50 , our methods attain performance comparable to ZMDSBA and Opt-ZMDSBA, while in the case d = 100 they yield higher performance. 9This improvement can be attributed to two main factors. First, our gradient and Hessian surrogates allow flexible choices of the number of samples b1, b 2 and directions ℓ1, ℓ 2, enabling a more favorable trade-off between stochastic noise and approximation accuracy, whereas ZMDSBA and Opt-ZMDSBA are restricted to ℓ1 = ℓ2 = 1 

by design. Second, ZMDSBA and Opt-ZMDSBA rely on a two-loop structure in which the inner loop is used to approximate v∗ (or its surrogate in Opt-ZMDSBA) and the inner solution z∗ via multiple iterations based on gradient or Hessian approximations with a single Gaussian direction. As the dimension increases, more such inner iterations are required; this behavior is also reflected in their theoretical complexity, where dimension-dependent terms appear in the required number of inner-loop iterations. In contrast, our single-loop methods control the accuracy of such approximations through the number of directions used in the finite-difference gradient surrogates, confirming (and extending to this bilevel setting) the well-known observation in the finite-difference literature that using multiple directions leads to more accurate gradient estimates and higher practical performance than single-direction schemes - see e.g. [ 32 , 16 , 46 , 7 , 8, 41 , 43 , 42 , 44 ]. Moreover, despite the theoretical guarantees of Hessian-free approaches (Opt-ZMDSBA and HF-ZOBA), the methods based on explicit Hessian approximations (ZMDSBA and ZOBA) achieve comparable or better performance. A similar phenomenon was reported in [ 2], suggesting that for problems in which coarse Hessian surrogates already provide sufficiently accurate approximations, faster convergence may be observed in practice. In Figure 1 (second row), we report the normalized function value gap as a function of wall-clock time. We observe that our methods complete the experiments in significant less time than competing methods. This gain stems from the parallelization and information reuse enabled by delayed updates, unlike ZDSBA, ZMDSBA, and Opt-ZMDSBA, which rely on non-parallelizable inner loops. 

Minimal distortion universal perturbation. We evaluate our methods on generating minimal-distortion universal adversarial perturbations (MD-UAP) for a multiclass MNIST classifier [ 15 ]; see Appendix B.2 for details. We tackle this problem using subspace-based black-box attacks [ 53 ], which parameterize the perturbation as, e.g., δ = Xz , where X ∈ Rdimg ×p defines a low-dimensional subspace where dimg = 784 is the dimension of images and z ∈ Rp denotes the coefficients. While most existing approaches fix or compute X heuristically, we instead optimize it by formulating MD-UAP as a black-box bilevel optimization problem, where the outer objective measures the average distortion and the inner objective measures the average confidence of correct classification. Through this reformulation, the bilevel algorithm seeks a subspace X that minimizes the average distortion while finding coefficients z⋆(X) ∈ Rp that minimize the average confidence within that subspace, yielding the parameterization δ = Xz ⋆(X). To the best of our knowledge, this is the first work to formulate subspace-based MD-UAP by explicitly optimizing the subspace within a bilevel framework. We set p = 100 

and construct perturbations for images labeled 4. In Figure 2 (first row), we report the outer (distortion) and inner (confidence) objective values along the sequences Xk and zk, together with the classification accuracy, as functions of the number of stochastic function evaluations. As in the previous experiment, values are repeated according to the number of stochastic evaluations performed per iteration. We observe that our methods achieve performance comparable to ZMDSBA and Opt-ZMDSBA, and better performance than ZDSBA. Notice that for all methods, the average distortion initially increases before reaching a plateau; this behavior is expected since the perturbation is initialized with very small magnitude and must grow in norm to effectively reduce the classification confidence. Although the final performance is similar across the considered methods, our approaches reach comparable accuracy and confidence levels in fewer iterations by leveraging multiple function evaluations per iteration to construct high-quality gradient and Hessian surrogates. This yields a practical advantage due to full parallelization of the evaluations. Such an effect is illustrated in Figure 2 (second row), where the same quantities are plotted against wall-clock time. Here, we observe that our methods achieve substantially better runtime performance than competing approaches. Notice that ZOBA requires many function evaluations per iteration due to costly Hessian-block estimation. In contrast, HF-ZOBA requires fewer evaluations for estimating gradients and, by preserving a single-loop structure, achieves higher performance within the same wall-clock time. 10 0 20000 40000 60000 80000 100000                    

> # Function evaluations
> 1.1 × 10 −1
> 1.2 × 10 −1
> 1.3 × 10 −1
> 1.4 × 10 −1
> 1.5 × 10 −1
> 1.6 × 10 −1
> F(zk, xk)
> F(z0, x0)
> Distortion
> ZDSBA
> ZMDSBA
> OPT-ZMDSBA
> ZOBA
> HF-ZOBA
> 020000 40000 60000 80000 100000
> # Function evaluations
> 10 −1
> 10 0
> G(zk, xk)
> G(z0, x0)
> Confidence
> 020000 40000 60000 80000 100000
> # Function evaluations
> 10 −1
> 10 0
> Accuracy
> Classification Accuracy
> 10 −1 10 010 110 2
> Time (s)
> 0.11
> 0.12
> 0.13
> 0.14
> 0.15
> 0.16
> F(zk, xk)
> F(z0, x0)
> 10 −1 10 010 110 2
> Time (s)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> G(zk, xk)
> G(z0, x0)
> 10 −1 10 010 110 2
> Time (s)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> Accuracy

Adversarial Objective [ y = 4] Figure 2: Comparison of algorithms on minimal distortion universal perturbation attack. Normalized outer function values and inner function values on iterations zk, x k and classification accuracy are reported as function of number of function evaluations (first row) and versus wall-clock time in seconds (second row). 

# 5 Conclusion 

In this work, we introduced ZOBA, the first single-loop algorithm for fully zeroth-order bilevel optimization, and HF-ZOBA, its Hessian-free variant. We analyzed both algorithms and derived convergence rates. Our experiments show that single-loop approaches provide clear practical advantages in wall-clock time over current two-loop state-of-the-art methods, making them particularly suitable for computationally expensive problems. Future directions include extending the analysis to non-smooth settings, relaxing the strong-convexity assumptions and developing adaptive parameter selection strategies. 

Acknowledgments. This work has been supported by the French government, through the 3IA Cote d’Azur Investments in the project managed by the National Research Agency (ANR) with the reference number ANR-23-IACL-0001, the ANR project PRC MAD ANR-24-CE23-1529 and the support of the “France 2030” funding ANR-23-PEIA-0004 (PDE-AI). 

# References 

[1] Alireza Aghasi and Saeed Ghadimi. Fully zeroth-order bilevel programming via gaussian smoothing. 

Journal of Optimization Theory and Applications , 205(2):31, Mar 2025. [2] Alireza Aghasi, Jeongyeol Kwon, and Saeed Ghadimi. Optimal zeroth-order bilevel optimization, 2025. [3] Maria Jo ˜ao Alves and Jo ˜ao Paulo Costa. An algorithm based on particle swarm optimization for multiob-jective bilevel linear problems. Applied Mathematics and Computation , 247:547–561, 2014. [4] Michael Arbel and Julien Mairal. Amortized implicit differentiation for stochastic bilevel optimization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022. [5] Jens Bayer, Stefan Becker, David M ¨unch, Michael Arens, and J ¨urgen Beyerer. Traversing the subspace of adversarial patches. Machine Vision and Applications , 36(3):70, Apr 2025. 11 [6] Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural Computation , 12(8):1889–1900, 2000. [7] Hanqin Cai, Yuchen Lou, Daniel Mckenzie, and Wotao Yin. A zeroth-order block coordinate descent algorithm for huge-scale black-box optimization. In Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 1193–1203, 18–24 Jul 2021. [8] HanQin Cai, Daniel McKenzie, Wotao Yin, and Zhenliang Zhang. Zeroth-order regularized optimization (zoro): Approximately sparse gradients and adaptive sampling. SIAM Journal on Optimization , 32(2):687– 714, 2022. [9] Jos ´e-Fernando Camacho-Vallejo, Carlos Corpus, and Juan G. Villegas. Metaheuristics for bilevel optimiza-tion: A comprehensive review. Computers & Operations Research , 161:106410, 2024. [10] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP) , pages 39–57, 2017. [11] P.Y. Chen, H. Zhang, Y. Sharma, J. Yi, and C.J. Hsi. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security , AISec ’17, pages 15–26, New York, NY, USA, 2017. Association for Computing Machinery. [12] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 25294–25307. Curran Associates, Inc., 2021. [13] Andrew R Conn and Lu ´ıs Nunes Vicente. Bilevel derivative-free optimization and its application to robust optimization. Optimization Methods and Software , 27(3):561–577, 2012. [14] Mathieu Dagr ´eou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 26698–26710. Curran Associates, Inc., 2022. [15] L. Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. 

IEEE Signal Processing Magazine , 29(6):141–142, 2012. [16] John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory ,61(5):2788–2806, 2015. [17] Matthias J. Ehrhardt and Lindon Roberts. Inexact derivative-free optimization for bilevel learning. Journal of Mathematical Imaging and Vision , 63(5):580–600, Jun 2021. [18] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 1568–1577. PMLR, 10–15 Jul 2018. [19] Ya Gao, Guangquan Zhang, Jie Lu, and Hui-Ming Wee. Particle swarm optimization for bi-level pricing problems in supply chains. Journal of Global Optimization , 51(2):245–254, Oct 2011. [20] Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization , 23(4):2341–2368, 2013. [21] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246 , 2018. 12 [22] Mitchell ´Angel G ´omez-Ortega, Miguel Gabriel Villarreal-Cervantes, Mario Aldape-P ´erez, Alam Gabriel Rojas-L ´opez, Daniel Molina-P ´erez, and Ram ´on Silva-Ortigoza. Evolutionary bi-level neural architecture search with training: A framework for color classification. Scientific Reports , 15(1):38572, Nov 2025. [23] Bin Gu, Guodong Liu, Yanfu Zhang, Xiang Geng, and Heng Huang. Optimizing large-scale hyperparameters via automated learning algorithm. arXiv preprint , abs/2102.09026, 2021. [24] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. Fern ´andez del R ´ıo, M. Wiebe, P. Peterson, P. G ´erard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. Nature , 585(7825):357–362, September 2020. [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) , December 2015. [26] Yudi Huang, Yujie Mo, Yujing Liu, Ci Nie, Guoqiu Wen, and Xiaofeng Zhu. Multiplex graph representation learning via bi-level optimization. In Kate Larson, editor, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24 , pages 2081–2089. International Joint Conferences on Artificial Intelligence Organization, 8 2024. Main Track. [27] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering , 9(3):90–95, 2007. [28] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 4882–4892. PMLR, 18–24 Jul 2021. [29] Hecheng Li and Yuping Wang. A genetic algorithm for solving a special class of nonlinear bilevel programming problems. In Yong Shi, Geert Dick van Albada, Jack Dongarra, and Peter M. A. Sloot, editors, Computational Science – ICCS 2007 , pages 1159–1162, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg. [30] Jason Zhi Liang and Risto Miikkulainen. Evolutionary bilevel optimization for complex control tasks. In 

Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation , GECCO ’15, page 871–878, New York, NY, USA, 2015. Association for Computing Machinery. [31] S. Liu, B. Kailkhura, P.Y. Chen, P. Ting, S. Chang, and L. Amini. Zeroth-order stochastic variance reduction for nonconvex optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. [32] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is competitive for reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems , NIPS’18, page 1805–1814, Red Hook, NY, USA, 2018. Curran Associates Inc. [33] Yurii Nesterov. Nonlinear Optimization , pages 3–58. Springer International Publishing, Cham, 2018. [34] Yurii Nesterov and Vladimir Spokoiny. Random Gradient-Free Minimization of Convex Functions. Founda-tions of Computational Mathematics , 17(2):527–566, April 2017. [35] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP) , pages 582–597, 2016. 13 [36] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32 , pages 8024–8035. Curran Associates, Inc., Red Hook, NY, USA, 2019. [37] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning ,volume 48 of Proceedings of Machine Learning Research , pages 737–746, New York, New York, USA, 20–22 Jun 2016. PMLR. [38] Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical University of Denmark , 7(15):510, 2008. [39] B. T. Polyak. Introduction to optimization. Optimization Software Inc., Publications Division, New York ,1:32, 1987. [40] Dhaval Pujara and Ankur Sinha. A review of bilevel optimization: Methods, emerging applications, and recent advancements, 2025. [41] Marco Rando, Cesare Molinari, Lorenzo Rosasco, and Silvia Villa. An optimal structured zeroth-order algorithm for non-smooth optimization. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems , volume 36, pages 36738–36767. Curran Associates, Inc., 2023. [42] Marco Rando, Cesare Molinari, Lorenzo Rosasco, and Silvia Villa. A structured tour of optimization with finite differences, 2025. [43] Marco Rando, Cesare Molinari, Silvia Villa, and Lorenzo Rosasco. Stochastic zeroth order descent with structured directions. Computational Optimization and Applications , 89(3):691–727, Dec 2024. [44] Marco Rando, Cheik Traor ´e, Cesare Molinari, Lorenzo Rosasco, and Silvia Villa. A structured proximal stochastic variance reduced zeroth-order algorithm. arXiv preprint arXiv:2506.23758 , 2025. [45] Kemal H. Sahin and Amy R. Ciric. A dual temperature simulated annealing approach for solving bilevel programming problems. Computers & Chemical Engineering , 23(1):11–25, 1998. [46] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning, 2017. [47] Mandar S. Sapre and Ishaan R. Kale. A Brief Review of Bilevel Optimization Techniques and Their Applications ,pages 1179–1202. Springer Nature Singapore, Singapore, 2024. [48] George AF Seber and Alan J Lee. Linear regression analysis . John Wiley & Sons, 2003. [49] Reza Shirkavand, Peiran Yu, Qi He, and Heng Huang. Bilevel zofo: Efficient llm fine-tuning and meta-training. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. [50] Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: From classical to evolutionary approaches and applications. IEEE Transactions on Evolutionary Computation , 22(2):276–295, 2018. [51] El-Ghazali Talbi. A Taxonomy of Metaheuristics for Bi-level Optimization , pages 1–39. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. [52] Guang-min Wang, Xian-jia Wang, Zhong-ping Wan, and Shi-hui Jia. An adaptive genetic algorithm for solving bilevel linear programming problem. Applied Mathematics and Mechanics , 28(12):1605–1612, Dec 2007. 14 [53] Lu Wang, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Yuan Jiang. Spanning attack: reinforce black-box attacks with unlabeled data. Machine Learning , 109(12):2349–2368, Dec 2020. [54] Vincent F. Yu, Shih-Wei Lin, Wenyih Lee, and Ching-Jung Ting. A simulated annealing heuristic for the capacitated location routing problem. Computers & Industrial Engineering , 58(2):288–299, 2010. Scheduling in Healthcare and Industrial Systems. 15 A Cost Comparison with Finite-Difference Bilevel Optimization Methods 

In this appendix, we compare our methods with state-of-the-art finite-difference algorithms in terms of the function evaluations performed at each iteration. More precisely, we analyze how many function evaluations are required per iteration by state-of-the-art methods and how these evaluations are used. 

ZDSBA [ 1]. At each iteration k ∈ N, ZDSBA first approximates the inner solution z∗(xk) at the current iterate xk by constructing the sequence (zk)i = 1 N inner through Ninner > 0 iterations of a stochastic zeroth-order method. This procedure involves the construction of a stochastic gradient surrogate of the inner objective at every step. Such a surrogate is built using finite differences with a single sample and a single Gaussian direction. Each inner iteration therefore requires two function evaluations, resulting in a total cost of 2Ninner function evaluations. Next, the method approximates v∗(xk) by constructing a sequence (vk)k = 1 N inverse via Ninverse 

iterations of a stochastic zeroth-order method applied to the quadratic form in eq. (3) , where z∗(xk) is replaced by zNinner . Each iteration again relies on single-sample, single-direction finite-difference estimators. Since this quadratic form requires approximations of both the gradient of the outer objective with respect to the first variable and the corresponding Hessian block, each iteration incurs a cost of 5 function evaluations. Finally, the Hessian cross-block and the gradient of the outer objective with respect to the second variable are approximated. Again, such approximations are built using single-sample, single Gaussian-direction estimators, incurring an additional cost of 5 function evaluations. Therefore, the total number of function evaluations per iteration k is 

2Ninner + 5 Ninverse + 5 .

ZMDSBA [ 2 ]. ZMDSBA generalizes ZDSBA by allowing multiple samples in the estimation of the Hessian cross-block and the gradient of the outer objective with respect to the second variable. Specifically, these quantities are approximated using finite differences with a single Gaussian direction and Nsample samples (i.e. our gradient/hessian estimators with ℓ1 = ℓ2 = 1 and b1 = b2 = Nsample ). The other steps are identical to ZDSBA. As a result, the total number of function evaluations per iteration becomes 

2Ninner + 5 Ninverse + 5 Nsample .

OPT-ZMDSBA [ 2]. OPT-ZMDSBA is based on a regularization-based reformulation of the bilevel problem, where the inner optimization is incorporated via a penalty function. This approach allows the algorithm to avoid explicit Hessian approximations. At each iteration k, the algorithm constructs approximations of z∗(xk)

with a sequence (zk)Ninner  

> k=1

and an auxiliary sequence (yk)Ninner  

> k=1

by performing Ninner iterations of a stochastic zeroth-order method using single-sample, single Gaussian direction gradient surrogates. More precisely, these sequences require computing three gradient surrogates of the inner and outer objectives per iteration, resulting in a total cost of 6Ninner function evaluations. Then, the algorithm constructs a search direction to update the iterate. Such a directions requires computing three surrogate gradients of the inner and outer objectives using single-direction estimators with Nsample samples, incurring a cost of 6Nsample function evaluations. Therefore, the total number of function evaluations per iteration is 

6Ninner + 6 Nsample .

In Table 1, we summarize these methods by reporting the number of function evaluations per iteration (# FE) and their theoretical complexity, i.e. the number of function evaluations required to achieve ε ∈ (0 , 1) accuracy. 16 Table 1: Comparison of bilevel zeroth-order algorithms in terms of complexity and number of function evaluations performed at every iteration (# FE). Method Loops # FE Complexity ZDSBA two-loops 2Ninner + 5 Ninverse + 5 O(( d + p)4ε−3 log(( d + p)/ε )) 

ZMDSBA two-loops 2Ninner + 5 Ninverse + 5 Nsample O(p(d + p)2ε−2 log(1 /ε )) 

OPT-ZMDSBA two-loops 6Ninner + 6 Nsample O(( d + p)ε−2)

ZOBA single-loop b1(4 ℓ1 + 1) + b2(2 ℓ2 + 1) O(p(d + p)2ε−2)

HF-ZOBA single-loop 2b1(2 ℓ1 + 1) + b2(2 ℓ2 + 1) O(( d + p)ε−2)

In Table 1, we observe that in the limit case b1 = b2 = ℓ1 = ℓ2 = Ninner = Nsample = Ninverse = 1 , our methods require fewer function evaluations per iteration than other finite-difference bilevel algorithms. This reduction stems from the reuse of function evaluations enabled by the use of delayed information in ZOBA and HF-ZOBA. In addition, the function evaluations required by ZOBA and HF-ZOBA can be performed in parallel across directions and batch samples, while the evaluations in ZDSBA, ZMDSBA and OPT-ZMDSBA are inherently sequential due to their inner-loop structure. As a result, the effective wall-clock cost of ZOBA and HF-ZOBA can be significantly lower in practice, even when the number of function evaluations is comparable or even higher. 

# B Experimental Details 

In this appendix we provide all details on the experimental setup and parameter tuning used to perform the experiments reported in Section 4. We implemented all scripts in Python 3 (version 3.11) and used the NumPy (version 1.25.0) [ 24 ], PyTorch (version 2.0.1) [ 36 ], and Matplotlib (version 3.10.8) [ 27 ] libraries. This appendix is organized as follows. In Appendix B.1, we provide details of the experiments on the synthetic quadratic problem while in Appendix B.2, we provide details of the experiments on the minimal-distortion universal perturbation task. Details on the machine used to perform the experiments are reported in Table 2. Table 2: Machine used to perform the experiments Feature CPU 64 x AMD EPYC 7301 16-Core GPU 1 x NVIDIA Quadro RTX 6000 RAM 256 GB 

B.1 Synthetic Problem 

Here we describe the synthetic problem considered in Section 4. We consider the minimization of the following function 

min 

> x∈Rd

Ψ( x) := F (z∗(x), x ) := 12n

> n

X

> i=1

(Ciz∗(x) − Dix − bi)2 + 12 ∥x − ¯x∥22

s.t. z∗(x) ∈ arg min  

> z∈Rp

G(z, x ) := 12m

> m

X

> j=1

(Aj z − Bj x − aj )2 ,

(11) where A ∈ Rm×p, B ∈ Rm×d, C ∈ Rn×p, D ∈ Rn×d are Gaussian matrices where every entry is sampled from 

N (0 , 1) . For fixed ¯z ∈ Rp and ¯x ∈ Rd, we define for every j = 1 , · · · , m and i = 1 , · · · , n a := A¯z − B ¯x and b := C ¯z − D¯x. 

In the experiments in Section 4, we fixed n = m = 1000 , ¯z = [2 , · · · , 2] ∈ Rp and ¯x = [1 , . . . , 1] ∈ Rd. We considered the setting d = p = 25 , 50 , 100 .17 Parameter Tuning. All parameters of every algorithm were tuned via grid search. For every algorithm and each parameter configuration in its parameter grid, the algorithm was run 6 times with a fixed budget of 10 5

function evaluations, using 6 different random initializations. The same set of initializations was used across all parameter configurations to ensure a fair comparison. The optimal parameter configuration was selected as the one minimizing, on average over the runs, the normalized function value gap at the final iterate, 

Ψ( xK ) − min Ψ Ψ( x0) − min Ψ ,

where xK denotes the iterate obtained at the last iteration performed K. Initializations of x0 and z0 were sampled independently from the uniform distribution over the hypercubes [−5.0, 10 .0] d and [−5.0, 10 .0] p,respectively. The initialization of v0 (i.e. the approximation of the inverse Hessian-vector product) was set to the zero vector. Once the optimal parameter configuration was identified, the algorithm was run 11 additional times using the same evaluation budget. To mitigate the effect of initialization overhead (e.g., library loading) on runtime measurements, the first run was discarded, and all reported results were computed by averaging over the remaining 10 runs. 

Search Spaces. For all algorithms, every discretization parameter used are fixed to h = 10 −3. Every stepsize parameter (used by all algorithms) was tuned over the set {10 −5, 10 −4, 10 −3, 10 −2}. For ZOBA and HF-ZOBA, the number of directions ℓ1 and ℓ2 used to approximate stochastic gradients and Hessians was selected from 

{1, 10 , 50 , 100 }. Every batch sizes ( b1, b 2 in ZOBA, HF-ZOBA and sk in ZMDSBA and OPT-ZMDSBA of [ 2]) were tuned over {1, 10 , 50 , 100 }. Notice that the number of directions and batch sizes are not required to be equal, i.e., ℓ1, ℓ 2, b 1, b 2 can all take different values. The number of inner-loop iterations used to approximate z∗ and 

v∗ for ZDSBA, ZMDSBA, and OPT-ZMDSBA was tuned over {1, 10 , 50 }. Finally, the regularization parameter λ

for OPT-ZMDSBA was selected from {1.0, 10 .0, 100 .0}. All parameters tuned, their ranges and the algorithms that used them are summarized in Table 3. For each method, the full search grid was defined as the Cartesian product of the candidate sets corresponding to its parameters. Table 3: Parameter grids used for each algorithm. 

Parameter Algorithm(s) Candidate Values 

Stepsizes All algorithms {10 −5, 10 −4, 10 −3, 10 −2}

Number of directions ZOBA, HF-ZOBA {1, 10 , 25 , 50 , 100 }

Batch sizes ZOBA, HF-ZOBA, ZMDSBA, OPT-ZMDSBA {1, 10 , 50 , 100 }

Inner-loop iterations ZDSBA, ZMDSBA, OPT-ZMDSBA {1, 10 , 50 }

Regularization OPT-ZMDSBA {1.0, 10 .0, 100 .0}

Impact of the number of directions and batchsize. We study how the performance of our algorithms is affected by the choice of the number of directions and the batch-size parameters. Specifically, we consider the optimization of the synthetic objective defined in eq. (11) with p = d = 50 and n = m = 1000 . We fix a budget of 10 4 function evaluations (i.e., fewer than those used in Section 4) and run ZOBA and HF-ZOBA with a constant stepsize ρ, chosen from a grid of 20 logarithmically spaced values between 10 −5 and 1.0. The outer stepsize is set as a fraction of ρ, namely γ = ρ/c and, in particular, we fix c = 5 .0. The discretization parameters are fixed to h = ˆh = 10 −3. To reduce the number of hyperparameters, we impose b := b1 = b2

and ℓ := ℓ1 = ℓ2. Each experiment is repeated five times, and we report the mean and standard deviation of the results. In Figure 3, we plot the normalized objective value progress (Ψ( xK ) − min Ψ) /(Ψ( x0) − min Ψ) at the final iterate xK obtained by ZOBA as a function of the stepsize ρ. If the algorithm diverges, this quantity is clipped to one. From Figure 3, we observe that increasing the number of directions ℓ, i.e., improving the quality of the stochastic gradient approximations for both the inner and outer objectives, makes ZOBA more stable and allows for the use of larger stepsize ρ. Moreover, increasing ℓ generally leads to better or comparable final performance in terms of solution quality, while also being potentially more time-efficient in practice since 18 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ

b = 1          

> ℓ= 1
> ℓ= 10
> ℓ= 25
> ℓ= 50
> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00

b = 10      

> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.6
> 0.7
> 0.8
> 0.9
> 1.0

b = 25      

> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.65
> 0.70
> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00

b = 50 

ZOBA 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.80
> 0.85
> 0.90
> 0.95
> 1.00
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ

ℓ = 1          

> b= 1
> b= 10
> b= 25
> b= 50
> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00

ℓ = 10      

> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.6
> 0.7
> 0.8
> 0.9
> 1.0

ℓ = 25      

> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.75
> 0.80
> 0.85
> 0.90
> 0.95
> 1.00

ℓ = 50 Figure 3: Normalized objective value progress at the final iterate obtained by running Algorithm 1 with different stepsizes ρ, numbers of directions ℓ, and batch sizes b. If the algorithm diverges, the quantity 

(Ψ( xK ) − min Ψ) /(Ψ( x0) − min Ψ) is clipped to one. In the first row, every plot show how this quantity change for different values of ℓ by fixing the batch size b; in the second row, instead, every plot show how such value change for different values of b by fixing ℓ.function evaluations can be computed in parallel. When ℓ (and eventually b) becomes too large, performance deteriorates. This behavior is due to the limited budget of function evaluations: large values of ℓ and b consume many evaluations per iteration, resulting in too few optimization steps to make meaningful progress. Comparing the first and second rows of Figure 3, we further observe that although ℓ and b play a similar conceptual role (both improving the quality of gradient and Hessian surrogates), fixing b and increasing ℓ may yield better results in practice. For example, fixing b = 1 and increasing ℓ achieves a lower normalized objective gap 

(Ψ( xK ) − min Ψ) /(Ψ( x0) − min Ψ) than fixing ℓ = 1 and increasing b. Moreover, this advantage diminishes or vanishes when the fixed parameter takes a large value. This behavior can again be attributed to the limited evaluation budget, or to the fact that when either ℓ or b is sufficiently large, the variance becomes small and the two parameters play increasingly similar roles. One possible explanation of that phenomenon is that, in some regimes, stochastic (or mini-batch) gradients are already sufficiently accurate and it is also easier to obtain good zeroth-order approximations via multiple directions than improving surrogate quality through batching with poor approximations i.e. good approximations of high-variance stochastic gradients (large ℓ, small b) may thus be preferable to poor approximations of low-variance minibatch gradients (small ℓ, large b). A similar phenomenon has been observed in [ 44 ], suggesting that this behavior is not specific in bilevel optimization but it can also arise in other stochastic settings. These findings provide empirical evidence suggesting that achieving a good balance between ℓ and b is important for maximizing the performance of the method, and they support the use of flexible surrogates such as ours, rather than special cases in which ℓ is fixed to 1 and only b varies, as in [1, 2]. 19 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ

b = 1          

> ℓ= 1
> ℓ= 10
> ℓ= 25
> ℓ= 50
> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0

b = 10      

> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0

b = 25      

> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0

b = 50 

HF-ZOBA 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0
> Ψ( xk) −  min ΨΨ( x0) −  min Ψ

ℓ = 1          

> b= 1
> b= 10
> b= 25
> b= 50
> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0

ℓ = 10      

> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0

ℓ = 25      

> 10 −5 10 −4 10 −3 10 −2 10 −1 10 0

ρ

> 0.4
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9
> 1.0

ℓ = 50 Figure 4: Normalized objective value progress at the final iterate obtained by running Algorithm 2 with γ = ρ/ 5

for different stepsizes ρ, numbers of directions ℓ, and batch sizes b. If the algorithm diverges, the quantity 

(Ψ( xK ) − min Ψ) /(Ψ( x0) − min Ψ) is clipped to one. In the first row, every plot show how this quantity change for different values of ℓ by fixing the batch size b; in the second row, instead, every plot show how such value change for different values of b by fixing ℓ.In Figure 4, HF-ZOBA exhibits a similar pattern: increasing ℓ or b improves stability and allows for larger stepsizes 

ρ, and fixing a small b while increasing ℓ achieves lower normalized objective gaps than the opposite. Comparing Figures 4 and 3, HF-ZOBA achieves significantly better results than ZOBA under the limited evaluation budget, even though both methods perform similarly in Section 4. This can be attributed to the smaller budget: while higher budgets allow using more directions and larger batch sizes, under limited evaluations fewer directions or smaller batch sizes must be used. ZOBA relies on explicit Hessian approximations, which can be noisy with few directions, leading to higher variance in the updates. As a result, HF-ZOBA, which relies solely on gradient surrogates, does not suffer from this variance, allowing for more precise steps and thus better performance under limited evaluation budget settings. 

B.2 Minimal-distortion Universal Perturbation 

In this appendix, we report the experimental details of the minimal-distortion universal adversarial attack experiment presented in Section 4. More precisely, we describe the problem and we propose our bilevel formulation. We describe how we implemented and trained the classifier and how we choose the parameters of the optimizer. 

Problem Description & Bilevel Formulation. The goal of the minimal-distortion universal perturbation problem is the following: let ˆf : Rdimg → RC be a trained multiclass classifier and let Stest = {(ˆ xi, y i)}mi=1 with 

ˆxi ∈ Rdimg and yi ∈ { 1, . . . , C } be a test dataset. Let ψ : Rdimg × Rdimg → Rdimg be a manipulation function that, given a data point ˆx and a perturbation δ, returns the perturbed data point with perturbation δ. Given a function L(y, ˆf (x)) that measures the confidence of classifying x with label y and a distortion measure D(ˆ x, δ )

that quantifies the amount of distortion obtained by perturbing ˆx with δ, the minimal-distortion universal 20 perturbation problem can be formulated as the following constrained optimization problem: 

min 

> δ∈Rdimg

1

m

> m

X

> i=1

D(ˆ xi, δ ) s.t. δ ∈ arg min 

> δ′∈Rdimg

1

m

> m

X

> i=1

L(yi, ˆf (ψ(ˆ xi, δ ))) .

Based on prior observations that adversarial perturbations tend to concentrate in low-dimensional subspaces of the input space (see, e.g., [ 11 , 5]) and the fact that zeroth-order methods can perform poorly in high-dimensional settings due to their dependence on dimension in the convergence rates (see, e.g., [ 34 ]), we restrict the attack to a low-dimensional subspace. More precisely, we parametrize the perturbation as δ = Az , with 

A ∈ Rdimg ×p and z ∈ Rp with p < d img , and we rewrite the minimal-distortion universal perturbation problem as a zeroth-order bilevel optimization problem, where the outer problem chooses the transformation A that minimizes the distortion measure and the inner problem selects the coefficients z that minimize the classification confidence. The idea of restricting the attack to a lower-dimensional subspace has been proposed in previous works and is common, for instance, in spanning attacks [ 53 ]. In such works, the subspace is generally either heuristically fixed or selected at random; in contrast, we propose to optimize it. This approach relies on the intuition that minimizing the average distortion is an ”easier” problem than minimizing classification confidence, which is instead recast as the inner optimization problem in Rp with a smaller p. In our experiments, we trained a multiclass classifier ˆf on the MNIST dataset [ 15 ]. We fixed the manipulation function to be the Carlini & Wagner transformation [10], i.e., for all ˆx, δ ∈ Rdimg ,

ψ(ˆ x, δ ) = 12 tanh  tanh −1 (2ˆ x) + δ.

We considered D to be the ℓ1 norm as the distortion measure, i.e., for all ˆx, δ ∈ Rdimg ,

D(ˆ x, δ ) = ∥ˆx − ψ(ˆ x, δ )∥1.

The accuracy criterion L is fixed to be the black-box untargeted loss proposed in [11], i.e., 

L



y, ˆf (ˆ x)



= max 



max  

> t̸=y

log 

h ˆf (ˆ x)

i

> t

− log 

h ˆf (ˆ x)

i

> y

, −κ



,

with κ = −1.0. Informally, this loss measures how much the confidence of predicting the true class y is smaller than the largest confidence of classfing the input in one of the other classes. If the true class already has the lowest confidence among all classes, the loss is clipped at −κ to prevent excessively negative values. By minimizing this loss, the optimizer increases the relative confidence of an incorrect class compared to the true class, encouraging missclassification. We therefore consider the following bilevel optimization problem. 

min 

> A∈Rdimg ×p

1

m

> m

X

> i=1

∥ˆxi − ψ(ˆ xi, Az ∗(A)) ∥1 s.t. z∗(A) ∈ arg min 

> z∈Rp

1

m

> m

X

> i=1

L(yi, ˆf (ψ(ˆ xi, Az ))) .

In our experiments, since we considered the MNIST dataset, the number of pixels in the images is dimg = 784 ,and we fixed p = 100 .

Data Preprocessing. We downloaded the training and test sets of MNIST dataset [ 15 ] and we normalized the images to be in [−0.5, 0.5] dimg as suggested in previous works - see e.g. [31, 42]. 

Network Architecture. The multiclass classifier has been implemented as a convolutional neural network (CNN), following the architecture described in [ 42 ]. The network consists of five convolutional layers with ReLU activations, interleaved with max-pooling layers, and followed by fully connected layers. Specifically, the first two convolutional layers use 3 × 3 kernels. The first layer has 1 input channel and 32 output channels, while the second layer has 32 input channels and 64 output channels. These two layers are followed by a 2 × 2

max-pooling layer with stride 2. The third and fourth convolutional layers have 64 input and output and use 

3 × 3 kernels. A second 2 × 2 max-pooling layer with stride 2 is applied thereafter. The resulting feature maps 21 are flattened and passed through two fully connected layers, each with 200 hidden units and ReLU activation. The final output layer is a fully connected layer with 10 units and a softmax activation, corresponding to the 10 

classes of the MNIST dataset. The network architecture is summarized in Table 4. Table 4: CNN architecture Layer Type Parameters Activation 1 Convolutional in= 1, out= 32 , kernel= 3 × 3 ReLU 2 Convolutional in= 32 , out= 64 , kernel= 3 × 3 ReLU 3 Max Pooling kernel= 2 × 2, stride= 2 –4 Convolutional in= 64 , out= 64 , kernel= 3 × 3 ReLU 5 Convolutional in= 64 , out= 64 , kernel= 3 × 3 ReLU 6 Max Pooling kernel= 2 × 2, stride= 2 –7 Dense in= 1024 , out= 200 ReLU 8 Dense in= 200 , out= 200 ReLU 9 Dense in= 200 , out= 10 Softmax 

Training Details. We describe here the training procedure and hyperparameters used to train the classifier. In particular, we adopt the defensive distillation technique introduced in [ 35 ] to improve robustness against adversarial perturbations. Defensive distillation is a training procedure based on a teacher-student framework. Let D = {(ˆ xi, y i)}ni=1 denote the labeled training dataset, where ˆxi ∈ Rdimg and yi ∈ { 1, . . . , C } are the data points and labels respectively, with C > 1 denoting the number of classes. In the first stage, a teacher network 

ˆfteacher : Rdimg → RC is trained using a softened softmax output with temperature T > 1. Specifically, given the logits xlog ∈ RC produced by the network for an input ˆx, the temperature-scaled softmax is defined as 

[softmax T (xlog )] i = exp( xlog  

> i

/T )

PCj=1 exp( xlog  

> j

/T ) , i = 1 , . . . , C. 

The teacher model is trained by minimizing the standard cross-entropy loss between the softened predictions and the ground-truth labels. In the second stage, the trained teacher network is used to generate soft labels 

¯yi = softmax T ( ˆfteacher (ˆ xi)) for i = 1 , . . . , n and a student network ˆfstudent , with the same architecture as the teacher, is then trained to mimic such soft predictions by minimizing the cross-entropy loss with respect to these instead of the hard labels yi. After training, the temperature is reset to T = 1 and the student model is used for inference. In our experiments, the temperature was set to T = 100 during both the teacher and student training stages, as suggested in [ 35 ]. Both the student and teacher networks are initialized from the weights obtained after one SGD step with temperature T = 1 , starting from random initialization, with all weights using Kaiming uniform initialization [ 25 ] and biases set to zero. The training dataset is split into two disjoint subsets, with 80% of the data used for training and 20% for validation. All models were trained on training part using the SGD optimizer with learning rate 0.01 , momentum 0.9, and batch size 32 . Early stopping based on the validation loss was employed, with a patience of 5 epochs, as suggested in [ 42 ]. Training was performed for up to 100 epochs. Additionally, dropout with rate 0.8 was applied to the last hidden layer before the output layer. A summary of the hyperparameters is provided in Table 5. After training, we assessed the performance of the student model by evaluating its training and test accuracy, achieving a training accuracy of 99 .16% and a test accuracy of 98 .87% .

Parameter Tuning. The procedure to tune parameters of the optimization algorithms is the same of the one adopted in the synthetic problem. Parameters are tuned with grid search. For every algorithm and every parameter configuration of the respective grid, we run such an algorithm 6 times with a fixed budget of 10 5

function evaluations. Since we cannot access to the function values of Ψ, we selected the best configuration of parameters for each algorithm as the one which minimize the average classification accuracy all over the runs computed at the last iteration performed. Initializations of x0 and z0 are sampled from Gaussian distributions 22 Table 5: Training hyperparameters Parameter Optimizer SGD with momentum Learning rate 0.01 

Momentum 0.9

Batch size 32 

Max epochs 100 

Temperature 100 

Early stopping Patience of 5 epochs (on validation loss) Dropout 0.8

with 0 mean and 0.5 of standard deviation while the initialization of v0 is set to the zero vector. As in the synthetic problem, algorithms with optimal configuration are re-executed 5 times to produce the plots in Section 4. Parameter grids used are the same for the synthetic problems (see Table 3) with the only difference that the stepsize parameters are tuned over the set {10 −7, 10 −6, 10 −5, 10 −4, 10 −3, 10 −2}.

Additional Experiments. We repeated the adversarial experiment of Section 4 by constructing universal perturbations for images labeled with y = 1 , y = 6 , and y = 9 . In Figure 5, we report, for each group of images with labels 1, 6, 4 (used in Section 4), and 9, the normalized outer (distortion) and inner (confidence) objective values along the sequences xk and zk, together with classification accuracy, as functions of the number of stochastic function evaluations (top row of each plot) and wall-clock time (bottom row of each plot). As in Section 4, values plotted against the number of stochastic evaluations are repeated according to the number of evaluations performed per iteration. From Figure 5, we observe again that our methods achieve comparable performance to state-of-the-art algorithms in significantly less time. In particular, ZOBA requires a large number of function evaluations per iteration, since obtaining reliable estimators of Hessian blocks is costly. This limits performance in budget-constrained settings, where only a few updates can be performed. In contrast, HF-ZOBA avoids this limitation by requiring fewer function evaluations per iteration, as it approximates Hessian–vector products using finite differences rather than explicitly estimating Hessian blocks. Moreover, by leveraging delayed information and a single-loop structure, HF-ZOBA reuses function evaluations to compute multiple terms of the search direction while retaining the benefits of parallelization. As a result, HF-ZOBA runs faster than other approaches while maintaining comparable performance, and without relying on a regularized version of the problem (as in Opt-ZMDSBA [2]), which can introduce additional bias and reduce performance. 23 0 20000 40000 60000 80000 100000 

# Function evaluations 

6 × 10 −2 

7 × 10 −2 

8 × 10 −2 

9 × 10 −2 

> F(zk, xk)
> F(z0, x0)

Distortion 

ZDSBA 

ZMDSBA 

OPT-ZMDSBA 

ZOBA 

HF-ZOBA 

0 20000 40000 60000 80000 100000 

# Function evaluations 

10 −1 

10 0

> G(zk, xk)
> G(z0, x0)

Confidence 

0 20000 40000 60000 80000 100000 

# Function evaluations 

10 −1 

10 0

> Accuracy

Classification Accuracy 

10 −1 10 0 10 1 10 2

Time (s) 

0.06 

0.07 

0.08 

0.09 

> F(zk, xk)
> F(z0, x0)

10 −1 10 0 10 1 10 2

Time (s) 

0.2 

0.4 

0.6 

0.8 

1.0 

> G(zk, xk)
> G(z0, x0)

10 −1 10 0 10 1 10 2

Time (s) 

0.2 

0.4 

0.6 

0.8 

1.0 

> Accuracy

Adversarial Objective [ y = 1] 0 20000 40000 60000 80000 100000 

# Function evaluations 

1.1 × 10 −1 

1.2 × 10 −1 

1.3 × 10 −1 

1.4 × 10 −1 

1.5 × 10 −1 

1.6 × 10 −1 

> F(zk, xk)
> F(z0, x0)

Distortion 

ZDSBA 

ZMDSBA 

OPT-ZMDSBA 

ZOBA 

HF-ZOBA 

0 20000 40000 60000 80000 100000 

# Function evaluations 

10 −1 

10 0

> G(zk, xk)
> G(z0, x0)

Confidence 

0 20000 40000 60000 80000 100000 

# Function evaluations 

10 −1 

10 0

> Accuracy

Classification Accuracy 

10 −1 10 0 10 1 10 2

Time (s) 

0.11 

0.12 

0.13 

0.14 

0.15 

0.16 

> F(zk, xk)
> F(z0, x0)

10 −1 10 0 10 1 10 2

Time (s) 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> G(zk, xk)
> G(z0, x0)

10 −1 10 0 10 1 10 2

Time (s) 

0.0 

0.2 

0.4 

0.6 

0.8 

> Accuracy

Adversarial Objective [ y = 4] 0 20000 40000 60000 80000 100000 

# Function evaluations 

1.2 × 10 −1 

1.3 × 10 −1 

1.4 × 10 −1 

1.5 × 10 −1 

1.6 × 10 −1 

1.7 × 10 −1 

> F(zk, xk)
> F(z0, x0)

Distortion 

ZDSBA 

ZMDSBA 

OPT-ZMDSBA 

ZOBA 

HF-ZOBA 

0 20000 40000 60000 80000 100000 

# Function evaluations 

10 −1 

10 0

> G(zk, xk)
> G(z0, x0)

Confidence 

0 20000 40000 60000 80000 100000 

# Function evaluations 

10 −1 

10 0

> Accuracy

Classification Accuracy 

10 −1 10 0 10 1 10 2

Time (s) 

0.12 

0.13 

0.14 

0.15 

0.16 

0.17 

> F(zk, xk)
> F(z0, x0)

10 −1 10 0 10 1 10 2

Time (s) 

0.2 

0.4 

0.6 

0.8 

1.0 

> G(zk, xk)
> G(z0, x0)

10 −1 10 0 10 1 10 2

Time (s) 

0.2 

0.4 

0.6 

0.8 

> Accuracy

Adversarial Objective [ y = 6] 0 20000 40000 60000 80000 100000 

# Function evaluations 

1.2 × 10 −1 

1.3 × 10 −1 

1.4 × 10 −1 

1.5 × 10 −1 

1.6 × 10 −1 

1.7 × 10 −1 

> F(zk, xk)
> F(z0, x0)

Distortion 

ZDSBA 

ZMDSBA 

OPT-ZMDSBA 

ZOBA 

HF-ZOBA 

0 20000 40000 60000 80000 100000 

# Function evaluations 

10 −1 

10 0

> G(zk, xk)
> G(z0, x0)

Confidence 

0 20000 40000 60000 80000 100000 

# Function evaluations 

10 −1 

10 0

> Accuracy

Classification Accuracy 

10 −1 10 0 10 1 10 2

Time (s) 

0.12 

0.13 

0.14 

0.15 

0.16 

0.17 

> F(zk, xk)
> F(z0, x0)

10 −1 10 0 10 1 10 2

Time (s) 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

> G(zk, xk)
> G(z0, x0)

10 −1 10 0 10 1 10 2

Time (s) 

0.2 

0.4 

0.6 

0.8 

> Accuracy

Adversarial Objective [ y = 9] Figure 5: Comparison of algorithms on minimal-distortion universal perturbation attacks across different test sets with different labels y. For each test set, we report the normalized outer function values, inner function values at iterations zk, x k, and classification accuracy. The top row of each block shows these metrics as a function of the number of function evaluations, while the bottom row shows them versus wall-clock time (in seconds). 

# C Auxiliary Results 

In this appendix, we introduce the main concepts and report and prove the lemmas and propositions required to prove the main theorems. 

Notation. In the following, we denote by Ip the identity matrix p × p. For a function f : Rp × Rd →

R, we indicate with ∇z f (z, x ) and ∇xf (z, x ), the gradient with respect to the first and second variable respectively. Moreover, we denote with Ek[·] the conditional expectation conditioning on the entire history of every random variable (w(i,j )

t )t<k , (u(i,j )

t )t<k , (ξi,t )t<k and (ζi,t )t<k for every i = 1 , · · · , b 1 and j = 1 , · · · , ℓ 1 i.e. the expectation is taken only on random variables at iteration k

Ek[·] := Eξ1,k ,ζ 1,k 

h

· · · 

h

Eξb1,k ,ζ b1,k 

h

Ew(1 ,1) 

k ,u (i,j )

k

h

· · · Ew(b1,ℓ 1)

k ,u (b1,ℓ 1)

k

[·]

iiii 

.

Moreover, we denote with EW (i)

k

[·] the conditional expectation on w(i,j )

k for j = 1 , · · · , ℓ 1 i.e. the expectation is taken only on w(i,j )

k for every j = 1 , · · · , ℓ 1 (i.e. i is fixed) 

EW (i)

k

[·] := Ew(i, 1) 

k

h

Ew(i, 2) 

k

h

· · · Ew(i,ℓ 1)

k

[·]

ii 

.

Similarly, we denote with EWk [·] the conditional expectation on w(i,j )

k for every i = 1 , · · · , b 1 and j = 1 , · · · , ℓ 1

i.e. the expectation is taken only on w(i,j )

k for every i = 1 , · · · , b 1 and j = 1 , · · · , ℓ 1

EWk [·] := EW (1) 

k

h

· · · EW (b1)

k

[·]

i

= Ew(1 ,1) 

k

h

Ew(1 ,2) 

k

h

· · · Ew(1 ,ℓ 1)

k

h

Ew(2 ,1) 

k

h

· · · Ew(b1,ℓ 1)

k

[·]

iiii 

.

24 For i = 1 , · · · , b 2, we denote with 

EU (i)

> k

[·] = Eu(i, 1) 

> k

h

Eu(i, 2) 

> k

h

· · · Eu(i,ℓ 2)

> k

[·]

ii 

and EUk [·] = EU (1) 

> k

h

· · · EU (b2)

> k

[·]

i

.

Similarly, we denote with 

Eξk [·] = Eξ1,k [· · · Eξb1,k [·]] and Eζk [·] = Eζ1,k [· · · Eζb2,k [·]] .

Lemma C.1. Let u ∼ N (0 , I d). Then, for p ∈ [0 , 2] ,

Eu[∥u∥p] ≤ dp/ 2.

For p ≥ 2,

Eu[∥u∥p] ≤ (d + p)p/ 2.

Moreover, let A, B ∈ Rd×d be symmetric matrices. Then, 

Eu[u⊤Au ] = tr (A), Eu[u⊤Auu ⊤Bu ] = 2 tr (AB ) + tr (A)tr (B),

Eu[uu ⊤uu ⊤] = ( d + 2) Id.

where tr (A) denotes the trace of matrix A.Proof. These are standard results for Gaussian variables and can be found in several textbooks - see e.g. [34, 38, 48]. The following appendices collect auxiliary results used in the proofs of the main theorems and corollaries. In Appendix C.1, we present lemmas establishing bounds and regularity properties of the quantities arising in bilevel optimization under the considered assumptions. In Appendix C.2, we review the Gaussian smoothing framework for the bilevel problem and states the associated lemmas and properties used to analyze our algorithms. In Appendix C.3, we provide bounds on the sequences generated by Algorithm 1 together with the corresponding descent lemma and in Appendix C.4 we report analogous results for Algorithm 2. 

C.1 Auxiliary results for Bilevel Optimization 

Lemma C.2 (Regularity of Ψ, z∗ and v∗). Under Assumptions 3.2,3.3, the following holds: (I) the function Ψ is LΨ-smooth for some LΨ > 0.(II) there exists L∗ > 0 such that for every x1, x 2 ∈ Rd,

∥z∗(x1) − z∗(x2)∥ ≤ L∗∥x1 − x2∥ and ∥v∗(x1) − v∗(x2)∥ ≤ L∗∥x1 − x2∥,

where z∗(·) ∈ arg min 

> z∈Rp

G(z, ·) and v∗(·) is defined in eq. (3) .Proof. The point (I) has been proved in [21] and (II) has been proved in [14]. 

Lemma C.3 (Bound on norm of v∗). Under Assumptions 3.2, 3.3. Let v∗(·) be defined as in eq. (3) . Then, for every x ∈ Rd,

∥v∗(x)∥ ≤ L0,F 

μG

.

Proof. Since v∗(x) = −[∇2 

> zz

G(z∗(x), x )] −1∇z F (z∗(x), x ), G is μG-strongly convex and F is L0,F -Lipschitz continuous, we have 

∥v∗(x)∥ = [∇2 

> zz

G(z∗(x), x )] −1∇z F (z∗(x), x ) ≤ L0,F 

μG

.

25 C.2 Gaussian Smoothing for Bilevel Optimization 

The analysis of Algorithm 1 and 2 rely on the fact that the surrogates used to construct the search directions (in eq. (4) , (6) and (8) for Algorithm 1 and eq. (10) for Algorithm 2) are unbiased estimators of gradients and Hessians of a smooth approximation of the inner and outer objective functions f and g. More precisely, let 

(Ω , F, P) be a probability space and Z be a measurable space. Let f : Rp × Rd × Z → R and let ζ : Ω → Z be a random varianble. Then for any h, η ≥ 0, we define the following smoothed version of f for every z ∈ Rp,

x ∈ Rd and every realization of ζ.

fh,η (z, x, ζ ) = Ew,u [f (z + hw, x + ηu, ζ )] , (12) where w ∼ N (0 , I p) and u ∼ N (0 , I d). For h, η > 0, the function fh,η is differentiable even when f is not, and its properties depends on those of f [1] - see Proposition C.4. Note that this construction differs from the standard Gaussian smoothing considered in [ 34 ], as it applies distinct smoothing parameters to different variable blocks. This flexibility is advantageous in bilevel optimization, where we want to compute gradients and hessian only a of subset of variables. In the following proposition we show some properties of the smoothing. 

Proposition C.4 (Properties of the smoothing) . Let (Ω , F, P) be a probability space and let Z be a measurable space. Let f : Rp × Rd × Z → R and let fh,η be the smooth surrogate of f defined as in eq. (12) . Then, the following hold: (I) If f is convex (or μf strongly convex) then fh,η is convex (or μf strongly convex). (II) If f is L0-Lipschitz then we have that fh,η is L0-Lipschitz. (III) If f is L1-smooth then we have that fh,η is L1-smooth. Proof. The results on convexity, Lipschitz continuity, and smoothness were proved in [1]. In the following lemma we show that gradients and hessians of the smoothed approximation of a target can be expressed as expectation of finite difference. 

Lemma C.5 (Gradient and Hessian of Smoothing) . Let (Ω , F, P) be a probability space and let Z be a measurable space. Let ζ : Ω → Z , and let w ∼ N (0 , I p) and u ∼ N (0 , I d). Let f : Rp × Rd × Z → R, and let fh,η denote the smooth surrogates of f defined as in eq. (12) . Then, for every z ∈ Rp, and x ∈ Rd,

∇z fh,η (z, x, ζ ) = Ew,u 

 f (z + hw, x + ηu, ζ )

h w



and ∇xfh,η (z, x ) = Ew,u 

 f (z + hw, x + ηu, ζ )

η u



.

(13) 

Moreover, 

∇2 

> zz

fh,η (z, x, ζ ) = Ew,u 

 f (z + hw, x + ηu, ζ )

h2 (ww ⊤ − Ip)



,

∇2 

> xz

fh,η (z, x, ζ ) = Ew,u 

 f (z + hw, x + ηu, ζ )

ηh uw ⊤



.

(14) 

Proof. Although this result was stated in [ 1], no explicit proof was given. We therefore provide a proof here for completeness. By the definition of the smooth surrogate (eq. (12)), we have 

fh,η (z, x, ζ ) = Ew,u [f (z + hw, x + ηu, ζ )] .

Since w, u are independent and f is absolutely measurable, by Fubini’s Theorem, we have 

fh,η (z, x, ζ ) = Ew,u [f (z + hw, x + ηu, ζ )] = Ew[Eu[f (z + hw, x + ηu, ζ )]] . (15) Now, we prove eq. (13). Let y = z + hw . Thus, by substitution, we have 

fh,η (z, x, ζ ) = 1(√2π)p/ 2

Z

Eu[f (z + hw, x + ηu, ζ )] e− ∥w∥22 dw 

= 1(√2π)p/ 2

Z

Eu[f (y, x + ηu, ζ )] e− ∥y−x∥22h2 dy hp .

26 Therefore, computing the gradient in z, we have 

∇z fh,η (z, x, ζ ) = 1(√2π)p/ 2

Z

Eu[f (y, x + ηu, ζ )] ∇z e− ∥y−z∥22h2 dy hp

= 1(√2π)p/ 2

Z

Eu[f (y, x + ηu, ζ )] 

 y − zh2



e− ∥y−z∥22h2 dy hp

= 1(√2π)p/ 2

Z

Eu

 f (z + hw, x + ηu, ζ )

h w



e− ∥w∥22 dw 

= Ew,u 

 f (z + hw, x + ηu, ζ )

h w



.

Similarly we can compute the gradient in x i.e. ∇xfh,η (z, x, ζ ). Now, we prove eq. (14) . Setting y = z + hw , by substitution, 

fh,η (z, x, ζ ) = 1(√2π)p/ 2

Z

Eu[f (y, x + ηu, ζ )] e− ∥y−z∥22h2 dy hp . (16) Computing the Hessian block in zz , we have 

∇2 

> zz

fh,η (z, x, ζ ) = 1(√2π)p/ 2

Z

Eu[f (y, x + ηu, ζ )] ∇2

> zz



e− ∥y−z∥22h2

 dy hp

= 1(√2π)p/ 2

Z

Eu[f (y, x + ηu, ζ )] 

 (y − z)( y − z)⊤

h4 − 1

h2 Ip



e− ∥y−z∥22h2 dy hp

= 1(√2π)p/ 2

Z

Eu

 f (y, x + ηu, ζ )

h2

 ww ⊤ − Ip



e− ∥w∥22 dw 

= Ew,u 

 f (y, x + ηu, ζ )

h2

 ww ⊤ − Ip



.

Now, to get the expression of the cross-block, we restart from eq. (16) . Let ¯y = x + ηu , by substitution we have 

fh,η (z, x, ζ ) = 1(√2π)p/ 2

1(√2π)d/ 2

Z Z 

f (y, x + ηu, ζ )e− ∥u∥22 du 



e− ∥y−z∥22h2 dy hp

= 1(√2π)p/ 2

1(√2π)d/ 2

Z Z 

f (y, ¯y, ζ )e− ∥ ¯y−x∥22η2 d¯yηd



e− ∥y−z∥22h2 dy hp

= 1(√2π)p/ 2

1(√2π)d/ 2

Z Z

f (y, ¯y, ζ )e− ∥ ¯y−x∥22η2 e− ∥y−z∥22h2 d¯yηd

dy hp .

Computing the mixed block xz we have, 

∇2 

> xz

fh,η (z, x, ζ ) = 1(√2π)p/ 2

1(√2π)d/ 2

Z Z

f (y, ¯y, ζ )∇2

> xz



e− ∥ ¯y−x∥22η2 e− ∥y−z∥22h2

 d¯yηd

dy hp

= 1(√2π)p/ 2

1(√2π)d/ 2

Z Z

f (y, ¯y, ζ )

 (y − z)(¯ y − x)⊤

h2η2



e− ∥ ¯y−x∥22η2 e− ∥y−z∥22h2 d¯yηd

dy hp

= 1(√2π)p/ 2

1(√2π)d/ 2

Z Z f (z + hw, x + ηu, ζ )

hη uw ⊤e− ∥u∥22 e− ∥w∥22 dudw 

= Ew,u 

 f (z + hw, x + ηu, ζ )

hη uw ⊤



.

Remark C.6 . Notice that by symmetry, the gradient of smoothing can be expressed as expectation of forward or 27 central finite difference 

∇z fh,η (z, x, ζ ) = Ew,u 

 f (z + hw, x + ηu, ζ )

h w



= Ew,u 

 f (z + hw, x + ηu, ζ ) − f (z, x, ζ )

h w



= Ew,u 

 f (z + hw, x + ηu, ζ ) − f (z − hw, x − ηu, ζ )2h w



.

Analogously, we can express the gradient with respect to the second variable. Moreover, also the hessian blocks can be expressed as finite difference 

∇2 

> zz

fh,η (z, x, ζ ) = Ew,u 

 f (z + hw, x + ηu, ζ )

h2 (ww ⊤ − Ip)



= Ew,u 

 f (z + hw, x + ηu, ζ ) + f (z − hw, x − ηu, ζ ) − 2f (z, x, ζ )2h2 (ww ⊤ − Ip)



,

∇2 

> xz

fh,η (z, x, ζ ) = Ew,u 

 f (z + hw, x + ηu, ζ )

ηh uw ⊤



= Ew,u 

 f (z + hw, x + ηu, ζ ) + f (z − hw, x − ηu, ζ ) − 2f (z, x, ζ )2ηh uw ⊤



.

Notice that if we define F (z, x ) = Eζ [f (z, x, ζ )] , we can define the smoothed surrogate Fh,η (z, x ) as in eq. (12) i.e. let w ∼ N (0 , I p) and u ∼ N (0 , I d),

Fh,η (z, x ) = Ew,u [F (z + hw, x + ηu )] .

Moreover, we have 

Fh,η (z, x ) = Ew,u [F (z + hw, x + ηu )] = Ew,u [Eζ [f (z + hw, x + ηu, ζ )]] = Eζ [Ew,u [f (z + hw, x + ηu, ζ )]] = Eζ [fh,η (z, x, ζ )] .

In the following lemma, we report the error of using the gradient of the smoothing instead of the exact gradient of the target. 

Lemma C.7 (Smoothing Error) . Let F : Rp × Rd → R be a L1,F -smooth function and let Fh,η denotes its gaussian smoothed surrogate defined as in eq. (12) . Then, for every z ∈ Rp and x ∈ Rd,

(∀h > 0, η ≥ 0) ∥∇ z Fh,η (z, x ) − ∇ z F (z, x )∥ ≤ L1,F 

2 (p + 3) 3/2h + L1,F η2

2h d√p. 

(∀h ≥ 0, η > 0) ∥∇ xFh,η (z, x ) − ∇ xF (z, x )∥ ≤ L1,F p√d

2

h2

η + L1,F 

2 (d + 3) 3/2η. 

(17) 

Let F be a twice differentiable functions with L2,G -Lipschitz continuous hessian. Then for every h > 0 and η ≥ 0 we have 

∥∇ z Fh,η (z, x ) − ∇ z F (z, x )∥ ≤ 2LG, 2

3



h2(p + 4) 2 + η3

h

√p(d + 3) 2/3



,

and, 

∥∇ 2 

> zz

Fh,η (z, x ) − ∇ 2 

> zz

F (z, x )∥ ≤ 2L2,G 

3



h(d + 5) 5/2 + h(d + 3) 3/2 + η3

h2 (d + 1)( p + 3) 3/2



. (18) 

Proof. These results have been proved in [1]. Now, building on the previous lemmas, we introduce a result that bounds the error of a smoothed approximate hypergradient computed for arbitrary z ∈ Rp and v ∈ Rp, rather than for the exact inner solution and the true inverse Hessian action. 28 Lemma C.8 (Bilevel Smoothing Error) . Let Assumptions 3.2 and 3.3 holds. For every h1, η 1, h 2, η 2 > 0 and every 

x ∈ Rd, z ∈ Rp, let Gh1,η 1 (z, x ) and Fh2,η 2 (z, x ) be the smooth surrogates of the inner and outer targets in eq. (1) 

respectively. Then, for every z, v ∈ Rp and x ∈ Rd,

∥∇ 2 

> xz

Gh1,η 1 (z, x )v + ∇xFh2,η 2 (z, x ) − ∇ Ψ( x)∥ ≤ 



L1,F + L0,F L1,G 

μG



∥z − z∗(x)∥ + L1,G ∥v − v∗(x)∥

+ L0,F 

μG

2L2,G 

3



h1(d + 5) 5/2

+ h1(d + 3) 3/2 + η31

h21

(d + 1)( p + 3) 3/2



+ L1,F p√d

2

h22

η2

+ L1,F 

2 (d + 3) 3/2η2.

Proof. Since ∇Ψ( x) := ∇xF (z∗(x), x ) + ∇22,1G(z∗(x), x )v∗(x), we have 

∥∇ 2 

> xz

Gh1,η 1 (z, x )v + ∇xFh2,η 2 (z, x ) − ∇ Ψ( x)∥ ≤ ∥∇ xFh2,η 2 (z, x ) − ∇ xF (z∗(x), x )∥

| {z }

> A

+ ∥∇ 2 

> xz

Gh1,η 1 (z, x )v − ∇ 2 

> xz

G(z∗(x), x )v∗(x)∥

| {z }

> B

.

To bound A, we add and subtract ∇xF (z, x ),

∥∇ xFh2,η 2 (z, x ) − ∇ xF (z∗(x), x )∥ ≤ ∥∇ xFh2,η 2 (z, x ) − ∇ xF (z, x )∥ + ∥∇ xF (z, x ) − ∇ xF (z∗(x), x )∥.

By eq. (17) and L1,F -smoothness we have 

∥∇ xFh2,η 2 (z, x ) − ∇ xF (z∗(x), x )∥ ≤ L1,F p√d

2

h22

η2

+ L1,F 

2 (d + 3) 3/2η2 + L1,F ∥z − z∗(x)∥.

To bound B, we add and subtract ∇2 

> xz

Gh1,η 1 (z, x )v∗(x),

∥∇ 2 

> xz

Gh1,η 1 (z, x )v − ∇ 2 

> xz

G(z∗(x), x )v∗(x)∥ ≤ ∥∇ 2 

> xz

Gh1,η 1 (z, x )v − ∇ 2 

> xz

Gh1,η 1 (z, x )v∗(x)∥

| {z }

> C

+ ∥∇ 2 

> xz

Gh1,η 1 (z, x )v∗(x) − ∇ 2 

> xz

G(z∗(x), x )v∗(x)∥

| {z }

> D

.

We bound C, by L1,G -smoothness of G,

∥∇ 2 

> xz

Gh1,η 1 (z, x )v − ∇ 2 

> xz

Gh1,η 1 (z, x )v∗(x)∥ = ∥∇ 2 

> xz

Gh1,η 1 (z, x )( v − v∗(x)) ∥≤ ∥∇ 2 

> xz

Gh1,η 1 (z, x )∥∥ v − v∗(x)∥≤ L1,G ∥v − v∗(x)∥.

We bound D, by adding and subtracting ∇2 

> xz

G(z, x )v∗(x)

∥∇ 2 

> xz

Gh1,η 1 (z, x )v∗(x) − ∇ 2 

> xz

G(z∗(x), x )v∗(x)∥ ≤ ∥ (∇2 

> xz

G(z, x ) − ∇ 2 

> xz

G(z∗(x), x )) v∗(x)∥

+ ∥∇ 2 

> xz

Gh1,η 1 (z, x )v∗(x) − ∇ 2 

> xz

G(z, x )v∗(x)∥

Since hessian of G is L2,G -Lipschitz, adding and subtracting ∇2 

> xz

G(z, x )v∗(x),

∥∇ 2 

> xz

Gh1,η 1 (z, x )v∗(x) − ∇ 2 

> xz

G(z∗(x), x )v∗(x)∥ ≤ ∥ (∇2 

> xz

G(z, x ) − ∇ 2 

> xz

G(z∗(x), x )) v∗(x)∥

+ ∥∇ 2 

> xz

Gh1,η 1 (z, x )v∗(x) − ∇ 2 

> xz

G(z, x )v∗(x)∥≤ L1,G ∥z − z∗(x)∥∥ v∗(x)∥

+ ∥∇ 2 

> xz

Gh1,η 1 (z, x ) − ∇ 2 

> xz

G(z, x )∥∥ v∗(x)∥

29 By Lemma C.7 (eq. (18)) and Lemma C.3, we have 

∥∇ 2 

> xz

Gh1,η 1 (z, x )v∗(x) − ∇ 2 

> xz

G(z∗(x), x )v∗(x)∥ ≤ L0,F L1,G 

μG

∥z − z∗(x)∥

+ L0,F 

μG

2L2,G 

3



h1(d + 5) 5/2 + h1(d + 3) 3/2

+ η31

h21

(d + 1)( p + 3) 3/2



.

Putting together all these bounds, we get the claim. Notice that in Algorithms 1 and 2, the search directions are constructed using unbiased estimators of the gradient and Hessian of the smoothed surrogates of the outer and inner objectives f and g. In the following appendices, we study such estimators and bound their approximation errors. In particular, in Appendix C.3 we study the approximation of the gradients and Hessians used in Algorithm 1, while in Appendix C.4 we study the surrogate constructed in Algorithm 2. 

C.3 Auxiliary results for Algorithm 1 

In this appendix, we analyze the approximation error of the finite-difference estimators and the search directions constructed in Algorithm 1. To simplify reading, for this section, we introduce some notation that we will use in the proofs. 

Notation. Let b = max( b1, b 2) and ℓ = max( ℓ1, ℓ 2). For every iteration k ∈ N, let (ξi,k )bi=1 and (ζi, k )bi=1 

be independent realizations of the random variables ξ and ζ sampled at iteration k. For every i = 1 , . . . , b ,let (w(i,j ) 

> k

)j = 1 ℓ and (u(i,j ) 

> k

)j = 1 ℓ be random vectors sampled i.i.d from N (0 , I p) and N (0 , I d), respectively, at iteration k ∈ N. We denote by ˆ∇z ghk (zk, x k, ξi, k ), ˆ∇z fhk (zk, x k, ζ i,k ), and ˆ∇xfhk (zk, x k, ζ i,k ) the finite-difference surrogates of the stochastic gradients of the inner and outer objectives g and f with respect to the variables z and x, computed at iteration k ∈ N using zk, xk, and ξi,k (or ζi,k ), as follows. 

ˆ∇z ghk (zk, x k, ξ i,k ) = 1

ℓ1

> ℓ1

X

> j=1

g(zk + hkw(i,j ) 

> k

, x k, ξ i,k ) − g(zk − hkw(i,j ) 

> k

, x k, ξ i,k )2hk

w(i,j )

> k

ˆ∇z fhk (zk, x k, ζ i,k ) = 1

ℓ2

> ℓ2

X

> j=1

f (zk + hkw(i,j ) 

> k

, x k, ζ i,k ) − f (zk, x k, ζ i,k )

hk

w(i,j )

> k

ˆ∇xfhk (zk, x k, ζ i,k ) = 1

ℓ2

> ℓ2

X

> j=1

f (zk, x k + hku(i,j ) 

> k

, ζ i,k ) − f (zk, x k, ζ i,k )

hk

u(i,j ) 

> k

.

(19) Let 

c(i,j ) 

> k

(zk, x k, ξ i,k ) = g(zk + hkw(i,j ) 

> k

, x k, ξ i,k ) + g(zk − hkw(i,j ) 

> k

, x k, ξ i,k ) − 2g(zk, x k, ξ i,k )2h2

> k

,s(i,j ) 

> k

(zk, x k, ξ i,k ) = g(zk + hkw(i,j ) 

> k

, x k + hku(i,j ) 

> k

, ξ i,k ) + g(zk − hkw(i,j ) 

> k

, x k − hku(i,j ) 

> k

, ξ i,k ) − 2g(zk, x k, ξ i,k )2h2

> k

We denote the finite-difference surrogates of Hessian block zz and xz of the inner target as follows. 

ˆ∇2 

> zz

ghk (zk, x k, ξ i,k ) = 1

ℓ1

> ℓ1

X

> j=1

c(i,j ) 

> k

(zk, x k, ξ i,k )( w(i,j ) 

> k

w(i,j )⊤ 

> k

− Ip).

ˆ∇2 

> xz

ghk (zk, x k, ξ i,k ) = 1

ℓ1

> ℓ1

X

> j=1

s(i,j ) 

> k

(zk, x k, ξ i,k )u(i,j ) 

> k

w(i,j )⊤ 

> k

.

(20) 30 Moreover, we denote by ∇z ghk , ∇z fhk , and ∇xfhk the gradients with respect to the variables z and x of the smooth surrogates (eq. (12)) of g and f at iteration k ∈ N. These are defined as indicated in Lemma C.5 e.g., 

∇z ghk (zk, x k, ξ i,k ) = 1

ℓ1

> ℓ1

X

> j=1

Ew(i,j )

> k

"

g(zk + hkw(i,j ) 

> k

, x k, ξi, k ) − g(zk − hkw(i,j ) 

> k

, x k, ξ i,k )2hk

w(i,j )

> k

#

= Ew

 g(zk + hkw, x k, ξi, k ) − g(zk − hkw, x k, ξ i,k )2hk

w



,

where the second equality follows from the fact that all w(i,j ) 

> k

are i.i.d. Analogously, ∇z fhk and ∇xfhk are defined. We begin by providing a lemma that bounds the expected norm of the surrogate gradient approximations. 

Lemma C.9 (Approximation Error Bounds) . Let Assumptions 3.1,3.2,3.3 hold. Let ˆ∇z ghk , ˆ∇z fhk and ˆ∇xfhk

be the stochastic gradient surrogates defined in eq. (19) . Let (zk)k∈N and (xk)k∈N be the sequences generated by Algorithm 1. Then, for every k ∈ NEk

 1

b1

> b1

X

> i=1

ˆ∇z ghk (zk, x k, ξ i,k )

> 2

 ≤ 2



2 + p + 2 

b1ℓ1



∥∇ z G(zk, x k)∥2

+

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G h2

> k

+ 2 



3 + p + 2 

ℓ1

 σ21,G 

b1

,

(21) 

Ek

 1

b2

> b2

X

> i=1

ˆ∇z fhk (zk, x k, ζ i,k )

> 2

 ≤ 2



1 + p + 2 

b2ℓ2



∥∇ z F (zk, x k)∥2

+

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2

> k

+ 2 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

,

and, 

Ek

 1

b2

> b2

X

> i=1

ˆ∇xfhk (zk, x k, ζ i,k )

> 2

 ≤ 2



1 + d + 2 

b2ℓ2



∥∇ xF (zk, x k)∥2

+

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F h2

> k

+ 2 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F .

Proof. We start proving eq. (21). Define 

g(i,j ) 

> k

= g(zk + hkw(i,j ) 

> k

, x k, ξ i,k ) − g(zk − hkw(i,j ) 

> k

, x, ξ i,k )2hk

w(i,j ) 

> k

,δ(i,j ) 

> k

= g(i,j ) 

> k

− ∇ z ghk (zk, x k, ξ i,k ), and δ(i) 

> k

=

> ℓ1

X

> j=1

δ(i,j ) 

> k

.

31 Adding and subtracting 1

b1ℓ1

b1P

i=1 

ℓ1P

j=1 

∇z ghk (zk, x k, ξ i,k ), we have 

1

b1

b1X

i=1 

ˆ∇z ghk (zk, x k, ξ i,k )

2

= 1

b21ℓ21

b1X

i=1 

ℓ1X

j=1 

δ(i,j )

k + ∇z ghk (zk, x k, ξ i,k )

2

= 1

b21ℓ21

b1X

i=1 

ℓ1X

j=1 

δ(i,j )

k

2

+ 1

b21

b1X

i=1 

∇z ghk (zk, x k, ξ i,k )

2

+ 2

b21ℓ21

* b1X

i=1 

ℓ1X

j=1 

δ(i,j )

k ,

b1X

i=1 

ℓ1X

j=1 

∇z ghk (zk, x k, ξ i,k )

+

.

Taking the expectation in w(i,j )

k for every i = 1 , · · · , b 1 and j = 1 , · · · , ℓ 1, we get 

EWk

 1

b1

b1X

i=1 

ˆ∇z ghk (zk, x k, ξ i,k )

2 = 1

b21ℓ21

EWk



b1X

i=1 

ℓ1X

j=1 

δ(i,j )

k

2 + 1

b21

b1X

i=1 

∇z ghk (zk, x k, ξ i,k )

2

+ 2

b21ℓ21

* b1X

i=1 

ℓ1X

j=1 

Ew(i,j )

> k

[δ(i,j )

k ],

b1X

i=1 

ℓ1X

j=1 

∇z ghk (zk, x k, ξ i,k )

+

.

By Lemma C.5, for every i and j, we have Ew(i,j )

> k

h

g(i,j )

k

i

= ∇z ghk (zk, x k, ξ i,k ). Thus, observing that 

Ew(i,j )

> k

h

δ(i,j )

k

i

= Ew(i,j )

> k

h

g(i,j )

k − ∇ z ghk (zk, x k, ξ i,k )

i

= Ew(i,j )

> k

h

g(i,j )

k

i

− ∇ z ghk (zk, x k, ξ i,k ) = 0 ,

we have, 

EWk

 1

b1

b1X

i=1 

ˆ∇z ghk (zk, x k, ξ i,k )

2 = 1

b21ℓ21

EWk



b1X

i=1 

ℓ1X

j=1 

δ(i,j )

k

2| {z }

A

+ 1

b21

b1X

i=1 

∇z ghk (z, x, ξ i)

2

| {z }

B

. (22) The term B can be bounded by adding and subtracting the full-gradient of the smoothing ∇z Ghk (zk, x k), we get 

1

b21

bX

i=1 

∇z ghk (zk, x k, ξ i,k )

2

≤ 2

b21

b1X

i=1 

∇z ghk (zk, x k, ξ i,k ) − ∇ z Ghk (zk, x k)

2

+ 2 ∥∇ z Ghk (zk, x k)∥2 .

Let ¯δi,k := ghk (zk, x k, ξ i,k ) − ∇ z Ghk (zk, x k). Then, taking the conditional expectation on ξi,k for every i =1, · · · , b 1, we have 

1

b21

Eξk



b1X

i=1 

∇z ghk (zk, x k, ξ i,k )

2 ≤ 2

b21

Eξk



b1X

i=1 

¯δi,k 

2 + 2 ∥∇ z Ghk (zk, x k)∥2

= 2

b21

 b1X

i=1 

Eξi,k 

h ¯δi,k 

2i

+

b1X

i=1 

X

t̸=i

Eξi,k 

 ¯δi,k , ¯δt,k 

 

+ 2 ∥∇ z Ghk (zk, x k)∥2 .

32 By Assumption 3.1 and, since for every i̸ = t, ξi,k and ξt,k are independent, we have 

1

b21

Eξk



> b1

X

> i=1

∇z ghk (zk, x k, ξ i,k )

> 2

 ≤ 2

b21

 b1X

> i=1

Eξi,k 

h ¯δi,k 

> 2

i

+

> b1

X

> i=1

X

> t̸=i

Eξi,k 

¯δi,k 

, Eξt,k 

¯δt,k 

| {z }

> =0



+ 2 ∥∇ z Ghk (zk, x k)∥2 .

Adding and subtracting ∇z g(zk, x k, ξ i,k ) and ∇z G(zk, x k), we get 

1

b21

Eξk



> b1

X

> i=1

∇z ghk (zk, x k, ξ i,k )

> 2

 ≤ 2

b21

> b1

X

> i=1



3Eξi,k 

h

∥∇ z ghk (zk, x k, ξ i,k ) − ∇ z g(zk, x k, ξ i,k )∥2i

+ 3 ∥∇ z G(zk, x k) − ∇ z Ghk (zk, x k)∥2

+ 3 Eξi,k 

∥∇ z g(zk, x k, ξ i,k ) − ∇ z G(zk, x k)∥2 

+ 2 ∥∇ z Ghk (zk, x k)∥2 .

By Lemma C.7 and Assumption 3.1, 

1

b21

Eξk



> b1

X

> i=1

∇z ghk (zk, x k, ξ i,k )

> 2

 ≤ 6

b1

 L21,G 

2 (p + 3) 3h2 

> k

+ σ21,G 



+ 2 ∥∇ z Ghk (zk, x k)∥2 .

Adding and subtracting ∇z G(zk, x k),

1

b21

Eξk



> b1

X

> i=1

∇z ghk (zk, x k, ξ i,k )

> 2

 ≤ 6

b1

 L21,G 

2 (p + 3) 3h2 

> k

+ σ21,G 



+ 2 ∥∇ z Ghk (zk, x k) − ∇ z G(zk, x k) + ∇z G(zk, x k)∥2

≤ 6

b1

 L21,G 

2 (p + 3) 3h2 

> k

+ σ21,G 



+ 4 ∥∇ z Ghk (zk, x k) − ∇ z G(zk, x k)∥2

+ 4 ∥∇ z G(zk, x k)∥2 .

By Lemma C.7, we get 

1

b21

Eξk



> b1

X

> i=1

∇z ghk (zk, x k, ξ i,k )

> 2

 ≤ 4 ∥∇ z G(zk, x k)∥2 +

 3

b1

+ 1 



L21,G (p + 3) 3h2 

> k

+ 6σ21,G 

b1

. (23) Now, we focus on bounding the term A. We have that 

1

b21ℓ21

EWk



> b1

X

> i=1
> ℓ1

X

> j=1

δ(i,j )

> k
> 2

 = 1

b21ℓ21



> b1

X

> i=1

EW (i)

> k

h

∥δ(i) 

> k

∥2i

+

> b1

X

> i=1

X

> t̸=i

EW (i)

> k

hD 

δ(i) 

> k

, δ (t)

> k

Ei  .

Notice that for i̸ = t, we have that for every j = 1 , · · · , ℓ 1, w(i,j ), w (t,j ) are all independent. Therefore, we have 

1

b21ℓ21

EWk



> b1

X

> i=1
> ℓ1

X

> j=1

δ(i,j )

> k
> 2

 = 1

b21ℓ21



> b1

X

> i=1

EW (i)

> k

h

∥δ(i) 

> k

∥2i

+

> b1

X

> i=1

X

> t̸=i

D

EW (i)

> k

[δ(i) 

> k

], EW (t)

> k

[δ(t) 

> k

]

E| {z }

> =0

 .

33 Similarly, we get 

1

b21ℓ21

EWk



b1X

i=1 

ℓ1X

j=1 

δ(i,j )

k

2 = 1

b21ℓ21

b1X

i=1 



ℓ1X

j=1 

Ew(i,j )

> k

h

∥δ(i,j )

k ∥2i

+

ℓ1X

j=1 

X

t̸=j

Ew(i,j )

> k

hD 

δ(i,j )

k , δ (i,t )

k

Ei  .

Again, observing that for j̸ = t, we have that w(i,j ), w (i,t ) are independent. Thus, we have 

1

b21ℓ21

EWk



b1X

i=1 

ℓ1X

j=1 

δ(i,j )

k

2 = 1

b21ℓ21

b1X

i=1 



ℓ1X

j=1 

Ew(i,j )

> k

h

∥δ(i,j )

k ∥2i

+

ℓ1X

j=1 

X

t̸=j

D

Ew(i,j )

> k

h

δ(i,j )

k

i

, Ew(i,t )

> k

h

δ(i,t )

k

iE | {z }

=0 



= 1

b21ℓ21

b1X

i=1 

ℓ1X

j=1 

Ew(i,j )

> k

h

∥δ(i,j )

k ∥2i

.

Developing the square, we get 

1

b21ℓ21

EWk



b1X

i=1 

ℓ1X

j=1 

δ(i,j )

k

2 = 1

b21ℓ21

b1X

i=1 

ℓ1X

j=1 

Ew(i,j )

> k

h

∥δ(i,j )

k ∥2i

= 1

b21ℓ21

b1X

i=1 

ℓ1X

j=1 



Ew(i,j )

> k



g(i,j )

k

2

+ ∥∇ z ghk (zk, x k, ξ i,k )∥2

− 2Ew(i,j )

> k

hD 

g(i,j )

k , ∇z ghk (zk, x k, ξ i,k )

Ei 

= 1

b21ℓ21

b1X

i=1 

ℓ1X

j=1 



Ew(i,j )

> k



g(i,j )

k

2

− ∥∇ z ghk (zk, x k, ξ i,k )∥2



≤ 1

b21ℓ21

b1X

i=1 

ℓ1X

j=1 

Ew(i,j )

> k



g(i,j )

k

2

.

(24) Therefore, taking the conditional expectation on ξi,k for every i in eq. (22) and using inequalities (24) and (23), we have 

Eξk

EWk

 1

b1

b1X

i=1 

ˆ∇z ghk (zk, x k, ξ i,k )

2 ≤ 1

b21ℓ21

b1X

i=1 

ℓ1X

j=1 

Eξi,k 



Ew(i,j )

> k



g(i,j )

k

2 | {z }

C

+4 ∥∇ z G(zk, x k)∥2

+

 3

b1

+ 1 



L21,G (p + 3) 3h2

k + 6σ21,G 

b1

.

(25) We focus on bounding C. Adding and subtracting g(zk, x k, ξ i,k ), we get 

Eξi,k 



Ew(i,j )

> k



g(i,j )

k

2 

≤ 12h2

k

Eξi,k 



Ew(i,j )

> k



g(zk + hkw(i,j )

k , x k, ξ i,k ) − g(zk, x k, ξ i,k )

2

∥w(i,j )

k ∥2

 

+ 12h2

k

Eξi,k 



Ew(i,j )

> k



g(zk, x k, ξ i,k ) − g(zk − hkw(i,j )

k , x k, ξ i,k )

2

∥w(i,j )

k ∥2

 

.

By symmetry, we have 

Eξi,k 



Ew(i,j )

> k



g(i,j )

k

2 

≤ 1

h2

k

Eξi,k 



Ew(i,j )

> k



g(zk + hkw(i,j )

k , x k, ξ i,k ) − g(zk, x k, ξ i,k )

2

∥w(i,j )

k ∥2

 

.

34 Adding and subtracting 

D

∇g(zk, x k, ξ i,k ), [hkw(i,j ) 

> k

, 0] 

E

where [hkw(i,j ) 

> k

, 0] ∈ Rp+d denotes the concatenation between hkw(i,j ) 

> k

and the vector of zeros of d entries. 

Eξi,k 



Ew(i,j )

> k



g(i,j )

> k
> 2

 

≤ 2

h2

> k

Eξi,k 



Ew(i,j )

> k

 

g(zk + hkw(i,j ) 

> k

, x k, ξ i,k ) − g(zk, x k, ξ i,k )

−

D

∇g(zk, x k, ξ i,k ), [hkw(i,j ) 

> k

, 0] 

E 2

∥w(i,j ) 

> k

∥2

 

+ 2

h2

> k

Eξi,k 



Ew(i,j )

> k

D 

∇g(zk, x k, ξ i,k ), [hkw(i,j ) 

> k

, 0] 

E 2

∥w(i,j ) 

> k

∥2

 

.

Notice that 

D

∇g(zk, x k, ξ i,k ), [hkw(i,j ) 

> k

, 0] 

E

=

D

∇z g(zk, x k, ξ i,k ), h kw(i,j )

> k

E

. By the Descent Lemma [39], 

Eξi,k 



Ew(i,j )

> k



g(i,j )

> k
> 2

 

≤ L21,G 

2 h2

> k

Ew(i,j )

> k



∥w(i,j ) 

> k

∥6

 

+ 2 Eξi,k 

h

∇z g(zk, x k, ξ i,k )⊤Ew(i,j )

> k

h

w(i,j ) 

> k

w(i,j )⊤ 

> k

w(i,j ) 

> k

w(i,j )⊤

> k

i

∇z g(zk, x k, ξ i,k )

i

.

By Lemma C.1, we have 

Eξi,k 



Ew(i,j )

> k



g(i,j )

> k
> 2

 

≤ L21,G 

2 (p + 6) 3h2 

> k

+ 2( p + 2) Eξi,k 

∥∇ z g(zk, x k, ξ i,k )∥2 .

Adding and subtracting ∇z G(zk, x k) and by Assumption 3.1, we get 

Eξi,k 



Ew(i,j )

> k



g(i,j )

> k
> 2

 

≤ L21,G 

2 (p + 6) 3h2

> k

+ 2( p + 2) Eξi,k 

∥∇ z g(zk, x k, ξ i,k ) − ∇ z G(zk, x k) + ∇z G(zk, x k)∥2

= L21,G 

2 (p + 6) 3h2 

> k

+ 2( p + 2) Eξi,k 

∥∇ z g(zk, x k, ξ i,k ) − ∇ z G(zk, x k)∥2

+ 2( p + 2) ∥∇ z G(zk, x k)∥2

+ 4( p + 2) Eξi,k [∇z g(zk, x k, ξ i,k ) − ∇ z G(zk, x k)] , ∇z G(zk, x k)

| {z }

> =0

≤ L21,G 

2 (p + 6) 3h2 

> k

+ 2( p + 2) σ21,G + 2( p + 2) ∥∇ z G(zk, x k)∥2.

Using this inequality in eq. (25), we get the first claim 

Eξk

EWk

 1

b1

> b1

X

> i=1

ˆ∇z ghk (zk, x k, ξ i,k )

> 2

 ≤ 2



2 + p + 2 

b1ℓ1



∥∇ z G(zk, x k)∥2

+

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G h2

> k

+ 2 



3 + p + 2 

ℓ1

 σ21,G 

b1

.

The proof of the second claim follows the same line. We report below the proof since the finite-difference surrogate used is different. Define 

f (i,j ) 

> k

= f (zk + hkw(i,j ) 

> k

, x k, ζ i,k ) − f (zk, x k, ζ i,k )

hk

w(i,j ) 

> k

,δ(i,j ) 

> f,k

= f (i,j ) 

> k

− ∇ z fhk (zk, x k, ζ i,k ), and δ(i) 

> f,k

=

> ℓ2

X

> j=1

δ(i,j ) 

> f,k

.

35 Following the same steps of the previous claim, we get 

Eζk

EWk

 1

b2

> b2

X

> i=1

ˆ∇z fhk (zk, x k, ζ i,k )

> 2

 ≤ 1

b22ℓ22

> b2

X

> i=1
> ℓ2

X

> j=1

Eζi,k 



Ew(i,j )

> k



f (i,j )

> k
> 2

 | {z }

> D

+4 ∥∇ z F (zk, x k)∥2

+

 3

b2

+ 1 



L21,F (p + 3) 3h2 

> k

+ 6σ21,F 

b2

.

(26) Similarly to the previous C term, we bound the term D. Adding and subtracting 

D

∇f (zk, x k, ζ i,k ), [hkw(i,j ) 

> k

, 0] 

E

,we have 

Eζi,k 



Ew(i,j )

> k



f (i,j )

> k
> 2

 

= 1

h2

> k

Eζi,k 



Ew(i,j )

> k



f (zk + hkw(i,j ) 

> k

, x k, ζ i,k ) − f (zk, x k)

2

∥w(i,j ) 

> k

∥2

 

≤ 2

h2

> k

Eζi,k 



Ew(i,j )

> k

 

f (zk + hkw(i,j ) 

> k

, x k, ζ i,k ) − f (zk, x k)

−

D

∇f (zk, x k, ζ i,k ), (hkw(i,j ) 

> k

, 0) 

E 2

∥w(i,j ) 

> k

∥2

 

+ 2

h2

> k

Eζi,k 



Ew(i,j )

> k

D 

∇f (zk, x k, ζ i,k ), (hkw(i,j ) 

> k

, 0) 

E 2

∥w(i,j ) 

> k

∥2

 

.

Notice that 

D

∇f (zk, x k, ζ i,k ), [hkw(i,j ) 

> k

, 0] 

E

=

D

∇z f (zk, x k, ξ i,k ), h kw(i,j )

> k

E

. By the Descent Lemma [39], 

Eζi,k 



Ew(i,j )

> k



f (i,j )

> k
> 2

 

≤ L21,F 

2 h2

> k

Ew(i,j )

> k



∥w(i,j ) 

> k

∥6

 

+ 2 Eζi,k 

h

∇z f (zk, x k, ζ i,k )⊤Ew(i,j )

> k

h

w(i,j ) 

> k

w(i,j )⊤ 

> k

w(i,j ) 

> k

w(i,j )⊤

> k

i

∇z f (zk, x k, ζ i,k )

i

.

By Lemma C.1, we have 

Eζi,k 



Ew(i,j )

> k



f (i,j )

> k
> 2

 

≤ L21,F 

2 (p + 6) 3h2 

> k

+ 2( p + 2) Eζi,k 

∥∇ z f (zk, x k, ζ i,k )∥2 .

Adding and subtracting ∇z F (zk, x k) and by Assumption 3.1, we get 

Eζi,k 



Ew(i,j )

> k



f (i,j )

> k
> 2

 

≤ L21,F 

2 (p + 6) 3h2

> k

+ 2( p + 2) Eζi,k 

∥∇ z f (zk, x k, ξ i,k ) − ∇ z F (zk, x k) + ∇z F (zk, x k)∥2

= L21,F 

2 (p + 6) 3h2 

> k

+ 2( p + 2) Eζi,k 

∥∇ z f (zk, x k, ζ i,k ) − ∇ z F (zk, x k)∥2

+ 2( p + 2) ∥∇ z F (zk, x k)∥2

+ 4( p + 2) Eξi,k [∇z f (zk, x k, ζ i,k ) − ∇ z F (zk, x k)] , ∇z F (zk, x k)

| {z }

> =0

≤ L21,F 

2 (p + 6) 3h2 

> k

+ 2( p + 2) σ21,F + 2( p + 2) ∥∇ z F (zk, x k)∥2.

36 Using this inequality in eq. (26), we get the second claim 

Eζk

EWk

 1

b2

> b2

X

> i=1

ˆ∇z fhk (zk, x k, ζ i,k )

> 2

 ≤ 2



1 + p + 2 

b2ℓ2



∥∇ z F (zk, x k)∥2

+

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2

> k

+ 2 

 p + 2 

b2ℓ2

+ 3

b2



σ21,F 

The proof of the last claim follows the same line. We now focus on deriving bounds on the expected norm of the product between a surrogate Hessian approxi-mation and a vector. We begin by stating an auxiliary proposition that bounds the expected norm of the product of a surrogate Hessian approximation (constructed using a single direction and a single sample) with a vector. This result is a special case of Proposition 2.5 in [ 1]; we report it here because it will be used and subsequently extended to derive bounds for our estimators. 

Proposition C.10 (Single direction Hessian estimator approximation error) . Let Assumption 3.3 holds. Let w, u 

be sampled i.i.d from N (0 , I p) and N (0 , I d), respectively. For h > 0, define 

ch(z, x, w, ξ ) = g(z + hw, x, ξ ) + g(z − hw, x, ξ ) − 2g(z, x, ξ )2h2

sh(z, x, w, ξ ) = g(z + hw, x + hu, ξ ) + g(z − hw, x − hu, ξ ) − 2g(z, x, ξ )2h2 .

Then, for every z, v ∈ Rp, x ∈ Rd, we have 

Eξ

Ew

∥ch(z, x, w, ξ )( ww ⊤ − Ip)v)∥2 ≤



4L22,G (p + 16) 4h2 + 15 2 (p + 6) 2Eξ

∥∇ 2 

> zz

g(z, x, ξ )∥2

> F

 

∥v∥2,

and, 

Eξ

Ew

∥sh(z, x, w, ξ )uw ⊤v)∥2 ≤



8L22,G h2  (p + 8) 4 + 2 p(d + 12) 3

+



6( p + 4)( p + 2) Eξ [∥∇ 2 

> zz

gh(z, x, ξ )∥2 

> F

]+ 36( p + 2) Eξ [∥∇ 2 

> xz

gh(z, x, ξ )∥2 

> F

]+ 30 p(d + 2) Eξ [∥∇ 2

> xx

gh(z, x, ξ )∥2 

> F

]

 

∥v∥2.

Proof. The first inequality is a special case of Proposition 2.5(a) in [ 1] with η = h and μ = 0 , while the second inequality is a special case of Proposition 2.5(b) in [1] with η = μ = h.Now, with the following lemma, we extend this result for hessian block estimators used in Algorithm 1. 

Lemma C.11 (Hessian Blocks approximation error) . Let Assumptions 3.3 holds. For every k ∈ N, let ˆ∇2 

> zz

ghk and 

ˆ∇2 

> xz

ghk be the Hessian block surrogates defined in eq. (20) . Let (xk)k∈N, (vk)k∈N and (zk)k∈N be the sequences generated by Algorithm 1. Let (ξi,k )b1 

> i=1

be realizations of independent copies of random variable ξ obtained at iteration k. Then, 

Eξk

EWk

 1

b1

> b1

X

> i=1

ˆ∇2 

> zz

ghk (zk, x k, ξ i,k )vk

> 2

 ≤

 4L22,G (p + 16) 4

b1ℓ1

h2 

> k

+



1 + 15( p + 6) 2p

2b1ℓ1



L21,G 



∥vk∥2,

(27) 37 and, 

Eξk

EWk ,U k

 1

b1

> b1

X

> i=1

ˆ∇2 

> xz

ghk (zk, x k, ξ i,k )vk

> 2

 ≤

 1

b1ℓ1



8L22,G h2

> k

 (p + 8) 4 + 2 p(d + 12) 3

+ 6( p + 4)( p + 2) p + 36( p + 2) min( p, d )+ 30 p(d + 2) dL 21,G 



+ L21,G 



∥vk∥2.

(28) 

Proof. We start by proving eq. (27). For i = 1 , · · · , b 1 and j = 1 , · · · , ℓ 1, we define 

c(i,j ) 

> k

= c(i,j ) 

> k

(zk, x k, ξ i,k ) = g(zk + hkw(i,j ) 

> k

, x k, ξ i,k ) + g(zk − hkw(i,j ) 

> k

, x k, ξ i,k ) − 2g(zk, x k, ξ i,k )2h2

> k

,δ(i,j ) 

> k

= c(i,j )

> k



w(i,j ) 

> k

w(i,j )⊤ 

> k

− Ip



vk − ∇ 2 

> zz

ghk (zk, x k, ξ i,k )vk,

∆(i) 

> k

=

> ℓ1

X

> j=1

δ(i,j ) 

> k

.

Adding and subtracting ∇2 

> zz

ghk (zk, x k, ξ i,k )vk and developing the square, we get 

1

b1

> b1

X

> i=1

ˆ∇2 

> zz

ghk (zk, x k, ξ i,k )vk

> 2

= 1

b21ℓ21

> b1

X

> i=1

∆(i)

> k
> 2

+ 1

b21

> b1

X

> i=1

∇2 

> zz

ghk (zk, x k, ξ i,k )vk

> 2

+ 2

b21ℓ21

* b1X

> i=1

∆(i) 

> k

,

> b1

X

> i=1
> ℓ1

X

> j=1

∇2 

> zz

ghk (zk, x k, ξ i,k )vk

+

.

Taking the expectation on w(i,j ) 

> k

for every i = 1 , · · · , b 1 and j = 1 , · · · , ℓ 1, and observing that by Lemma C.5, we have, 

EWk

" b1X

> i=1

∆(i)

> k

#

=

> b1

X

> i=1
> ℓ1

X

> j=1

Ew(i,j )

> k

h

c(i,j )

> k



w(i,j ) 

> k

w(i,j )⊤ 

> k

− Ip

i 

vk − ∇ 2 

> zz

ghk (zk, x k, ξ i,k )vk = 0 ,

we get 

EWk

 1

b1

> b1

X

> i=1

ˆ∇2 

> zz

ghk (zk, x k, ξ i,k )vk

> 2

 = 1

b21ℓ21

EWk



> b1

X

> i=1

∆(i)

> k
> 2

 + 1

b21

> b1

X

> i=1

∇2 

> zz

ghk (zk, x k, ξ i,k )vk

> 2

+ 2

b21ℓ21

*

EWk

" b1X

> i=1

∆(i)

> k

#| {z }

> =0

,

> b1

X

> i=1

∇2 

> zz

ghk (zk, x k, ξ i,k )vk

+

.

(29) 38 By independence, we have 

EWk



bX

i=1 

∆(i)

k

2 =

b1X

i=1 

EW (i)

> k

h

∥∆(i)

k ∥2i

+

b1X

i=1 

X

t̸=i

D

EW (i)

> k

h

∆(i)

k

i

, EW (t)

> k

h

∆(t)

k

iE | {z }

=0 

=

b1X

i=1 



ℓ1X

j=1 

Ew(i,j )

> k



δ(i,j )

k

2

+

ℓ1X

j=1 

X

t̸=j

D

Ew(i,j )

> k

h

δ(i,j )

k

i

, Ew(i,t )

> k

h

δ(i,t )

k

iE | {z }

=0 



=

b1X

i=1 

ℓ1X

j=1 

Ew(i,j )

> k

 

c(i,j )

k



w(i,j )

k w(i,j )⊤

k − Ip



− ∇ 2

zz ghk (zk, x k, ξ i,k )



vk

2

≤

b1X

i=1 

ℓ1X

j=1 

Ew(i,j )

> k



c(i,j )

k



w(i,j )

k w(i,j )⊤

k − Ip



vk

2

.

Here, the last inequality holds by the fact that Ew(i,j )

> k

h

c(i,j )

k



w(i,j )

k w(i,j )⊤

k − Ip



vk

i

= ∇2

zz ghk (zk, x k, ξ i,k )vk.Taking the expectation on ξi,k for every i = 1 , · · · , b 1, by Proposition C.10, 

Eξk

EWk



b1X

i=1 

∆(i)

k

2 ≤

b1X

i=1 

ℓ1X

j=1 

Eξi,k 



Ew(i,j )

> k



c(i,j )

k



w(i,j )

k w(i,j )⊤

k − Ip



vk

2 

≤

b1X

i=1 

ℓ1X

j=1 



4L22,G (p + 16) 4h2

k + 15 2 (p + 6) 2Eξi,k 

∥∇ 2

zz g(zk, x k, ξ i,k )∥2

F

 

∥vk∥2

≤

b1X

i=1 

ℓ1X

j=1 



4L22,G (p + 16) 4h2

k + 15 2 (p + 6) 2pEξi,k 

∥∇ 2

zz g(zk, x k, ξ i,k )∥2 

∥vk∥2.

Since for every ξ, we have that the function (z, x ) 7 → g(z, x, ξ ) is L1,G -smooth, we have that ∥∇ 2

zz g(z, x, ξ )∥ ≤ ∥∇ 2g(z, x, ξ )∥ ≤ L1,G . Therefore, we get 

Eξk

EWk



b1X

i=1 

∆(i)

k

2 ≤

b1X

i=1 

ℓ1X

j=1 



4L22,G (p + 16) 4h2

k + 15 2 (p + 6) 2pL 21,G 



∥vk∥2.

Using this inequality in eq. (29), we get 

Eξk

EWk

 1

b1

b1X

i=1 

ˆ∇2

zz ghk (zk, x k, ξ i,k )vk

2 ≤ 1

b1ℓ1



4L22,G (p + 16) 4h2

k + 15 2 (p + 6) 2pL 21,G 



∥vk∥2

+ 1

b21

Eξk



b1X

i=1 

∇2

zz ghk (zk, x k, ξ i,k )vk

2 .

By Proposition C.4, we have that ghk is L1,G -smooth. Thus ∥∇ 2

zz ghk (zk, x k, ξ i,k )∥ ≤ L1,G and, therefore, we have 

Eξk

EWk

 1

b1

b1X

i=1 

ˆ∇2

zz ghk (zk, x k, ξ i,k )vk

2 ≤

 4L22,G (p + 16) 4

b1ℓ1

h2

k +



1 + 15( p + 6) 2p

2b1ℓ1



L21,G 



∥vk∥2.

39 Now, we prove eq. (28). For i = 1 , · · · , b and j = 1 , · · · , ℓ we define 

s(i,j )

k = s(i,j )

k (zk, x k, ξ i,k ), ¯δ(i,j )

k = s(i,j )

k u(i,j )

k w(i,j )⊤

k vk − ∇ 2

xz ghk (zk, x k, ξ i,k )vk,

¯∆(i)

k =

ℓ1X

j=1 

¯δ(i,j )

k .

Adding and subtracting ∇2

xz ghk (zk, x k, ξ i,k )vk and developing the square, we get 

1

b1

b1X

i=1 

ˆ∇2

xz g(zk, x k, ξ i,k )vk

2

= 1

b21ℓ21

b1X

i=1 

¯∆(i)

k

2

+ 1

b21

b1X

i=1 

∇2

xz ghk (zk, x k, ξ i,k )vk

2

+ 2

b21ℓ21

* b1X

i=1 

¯∆(i)

k ,

b1X

i=1 

∇2

xz ghk (zk, x k, ξ i,k )vk

+

.

Taking the conditional expectation on w(i,j )

k , u (i,j )

k for every i = 1 , · · · , b 1 and j = 1 , · · · , ℓ 1, we have that since 

w(i,j )

k , u (i,j )

k , ξ i,k are independent, 

EWk ,U k

 1

b1

b1X

i=1 

ˆ∇2

xz g(zk, x k, ξ i,k )vk

2 = 1

b21ℓ21

EWk ,U k



b1X

i=1 

¯∆(i)

k

2| {z }

A

+ 1

b21

b1X

i=1 

∇2

xz ghk (zk, x k, ξ i,k )vk

2

| {z }

B

+ 2

b21ℓ21

* b1X

i=1 

EWk ,U k [ ¯∆(i)

k ]

| {z }

=0 

,

b1X

i=1 

∇2

xz ghk (zk, x k, ξ i,k )vk

+

.

(30) We bound the terms A and B separately. We start from A,

EWk ,U k



b1X

i=1 

¯∆(i)

k

2 =

b1X

i=1 

EW (i) 

> k

,U (i)

> k



¯∆(i)

k

2

+

b1X

i=1 

X

t̸=i

EWk ,U k

hD ¯∆(i)

k , ¯∆(t)

k

Ei 

.

Observing that, for every i̸ = t, w(i,j )

k , w (t,j )

k , u (i,j )

k , u (t,j )

k are independent, and that by Lemma C.5, we have 

EW (i) 

> k

,U (i)

> k

h ¯∆(i)

k

i

=

ℓ1X

j=1 

Ew(i,j ) 

> k

,u (i,j )

> k

h

s(i,j )

k u(i,j )

k w(i,j )⊤

k

i

vk − ∇ 2

xz ghk (zk, x k, ξ i,k )vk = 0 ,

we get 

EWk ,U k



b1X

i=1 

¯∆(i)

k

2 =

b1X

i=1 

EW (i) 

> k

,U (i)

> k



¯∆(i)

k

2

+

b1X

i=1 

X

t̸=i

D

EW (i) 

> k

,U (i)

> k

h ¯∆(i)

k

i

, EW (t) 

> k

,U (t)

> k

h ¯∆(t)

k

iE | {z }

=0 

.

Similarly, we have 

EWk ,U k



b1X

i=1 

¯∆(i)

k

2 =

b1X

i=1 

EW (i) 

> k

,U (i)

> k



¯∆(i)

k

2

=

b1X

i=1 

 ℓ1X

j=1 

Ew(i,j ) 

> k

,u (i,j )

> k



¯δ(i,j )

k

2

+

ℓ1X

j=1 

X

t̸=j

EW (i) 

> k

,U (i)

> k

hD ¯δ(i,j )

k , ¯δ(i,t )

k

Ei 

.

40 Again, since for every j̸ = t, w(i,j ) 

> k

, u (i,j ) 

> k

, w (i,t ) 

> k

, u (i,t ) 

> k

are independent and, by Lemma C.5, we have 

Ew(i,j ) 

> k,u (i,j )
> k

[¯δ(i,j ) 

> k

] = Ew(i,j ) 

> k,u (i,j )
> k

h

s(i,j ) 

> k

u(i,j ) 

> k

w(i,j )⊤

> k

i

vk − ∇ 2 

> xz

gk(zk, x k, ξ i,k )vk = 0 

we get 

EWk ,U k



> b1

X

> i=1

¯∆(i)

> k
> 2

 =

> b1

X

> i=1

 ℓ1X

> j=1

Ew(i,j ) 

> k,u (i,j )
> k



¯δ(i,j )

> k
> 2



+

> ℓ1

X

> j=1

X

> t̸=j

D

Ew(i,j ) 

> k,u (i,j )
> k

h¯δ(i,j )

> k

i

, Ew(i,t ) 

> k,u (i,t )
> k

h¯δ(i,t )

> k

iE | {z }

> =0



=

> b1

X

> i=1
> ℓ1

X

> j=1

Ew(i,j ) 

> k,u (i,j )
> k



s(i,j ) 

> k

u(i,j ) 

> k

w(i,j )⊤ 

> k

vk − ∇ 2 

> xz

ghk (zk, x k, ξ i,k )vk

> 2



≤

> b1

X

> i=1
> ℓ1

X

> j=1

Ew(i,j ) 

> k,u (i,j )
> k



s(i,j ) 

> k

u(i,j ) 

> k

w(i,j )⊤ 

> k

vk

> 2



.

Taking the expectation on ξk, by Proposition C.10, and since ∥A∥2 

> F

≤ rank (A)∥A∥2 where ∥A∥2 

> F

is the squared Frobenius norm of a matrix A, we have 

Eξk

EWk ,U k



> b1

X

> i=1

¯∆(i)

> k
> 2

 ≤

> b1

X

> i=1
> ℓ1

X

> j=1



8L22,G h2

> k

 (p + 8) 4 + 2 p(d + 12) 3

+



6( p + 4)( p + 2) pEξi,k [∥∇ 2 

> zz

ghk (zk, x k, ξ i,k )∥2]+ 36( p + 2) min( p, d )Eξi,k [∥∇ 2 

> xz

ghk (zk, x k, ξ i,k )∥2]+ 30 p(d + 2) dEξi,k [∥∇ 2

> xx

ghk (zk, x k, ξ i,k )∥2]

 

∥vk∥2.

Since, by Proposition C.4, the function ghk is L1,G -smooth, we have 

Eξk

EWk ,U k



> b1

X

> i=1

¯∆(i)

> k
> 2

 ≤

> b1

X

> i=1
> ℓ1

X

> j=1



8L22,G h2

> k

 (p + 8) 4 + 2 p(d + 12) 3

+



6( p + 4)( p + 2) p

+ 36( p + 2) min( p, d ) + 30 p(d + 2) d



L21,G 



∥vk∥2.

(31) Now, we bound the term B of eq. (30). By Cauchy-Schwartz, and since ghk is L1,G -smooth, we have 

> b1

X

> i=1

∇2 

> xz

ghk (zk, x k, ξ i,k )vk

> 2

≤ b1

> b1

X

> i=1

∥∇ 2 

> xz

ghk (zk, x k, ξ i,k )vk∥2

≤ b1

> b1

X

> i=1

∥∇ 2 

> xz

ghk (zk, x k, ξ i,k )∥2∥vk∥2

≤ b21L21,G ∥vk∥2.

Using this inequality and eq. (31) in eq. (30), we get the claim. In the following lemma, we provide bound on the norm of the search directions used in Algorithm 1. 41 Lemma C.12 (Bound on direction norm) . Let Assumptions 3.1,3.2,3.3 holds. Let Dkz , Dkv and Dkx be the search directions computed at iteration k ∈ N with eq. (4) , (6) and (8) . Let (zk)k∈N, (vk)k∈N and (xk)k∈N be the sequences generated by Algorithm 1. Then, for every k ∈ N, we have 

Ek

∥Dkz ∥2 ≤ 2



2 + p + 2 

b1ℓ1



L21,G ∥zk − z∗(xk)∥2

+

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G h2 

> k

+ 2 



3 + p + 2 

ℓ1

 σ21,G 

b1

.

(32) 

Moreover, let Ckv = 4 

 4L22,G (p+16) 4 

> b1ℓ1

h2 

> k

+



1 + 15( p+6) 2p

> 2b1ℓ1



L21,G 



. Then, for every k ∈ N,

Ek

∥Dkv ∥2 ≤ Ckv ∥vk − v∗(xk)∥2 + Ckv

L20,F 

μ2

> G

+ 4 



1 + p + 2 

b2ℓ2



L20,F + 4 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

+ 2 

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2

> k

,

(33) 

and, 

Ek

h

Dkx

> 2

i

≤ 2Ckx ∥vk − v∗(xk)∥2 + 2 Ckx

L20,F 

μ2

> G

+ 2 

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F h2

> k

+ 4 



1 + d + 2 

b2ℓ2



L20,F + 4 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F ,

(34) 

with Ckx = 2 



> 1
> b1ℓ1



C(1)  

> d

h2 

> k

+ C(2) 

> d



+ L21,G 



where 

C(1)  

> d

= 8 L22,G 

 (p + 8) 4 + 2 p(d + 12) 3

C(2)  

> d

= 6( p + 4)( p + 2) p + 36( p + 2) min( p, d ) + 30 p(d + 2) dL 21,G 

Proof. We start by proving eq. (32). By eq. (4) and Lemma C.9, 

Ek

∥Dkz ∥2 ≤ 2



2 + p + 2 

b1ℓ1



∥∇ z G(zk, x k)∥2

+

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G h2

> k

+ 2 



3 + p + 2 

ℓ1

 σ21,G 

b1

.

Since ∇z G(z∗(xk), x k) = 0 , by L1,G -smoothness, we have 

Ek

∥Dkz ∥2 ≤ 2



2 + p + 2 

b1ℓ1



L21,G ∥zk − z∗(xk)∥2

+

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G h2

> k

+ 2 



3 + p + 2 

ℓ1

 σ21,G 

b1

.

42 Now we prove eq. (33). By eq. (6), we have 

Ek

∥Dkv ∥2 = Ek

 1

b1

> b1

X

> i=1

ˆ∇2 

> zz

ghk (zk, x k, ξ i,k , W i,k )

!

vk + 1

b2

> b2

X

> i=1

ˆ∇z fhk (zk, x k, ζ i,k , ¯Wi,k )

> 2



≤ 2Ek

 1

b1

> b1

X

> i=1

ˆ∇2 

> zz

ghk (zk, x k, ξ i,k , W i,k )

!

vk

> 2



+ 2 Ek

 1

b2

> b2

X

> i=1

ˆ∇z fhk (zk, x k, ζ i,k , ¯Wi,k )

> 2

 .

By Lemma C.9 and Lemma C.11, we get 

Ek

∥Dkv ∥2 ≤ 2

 4L22,G (p + 16) 4

b1ℓ1

h2 

> k

+



1 + 15( p + 6) 2p

2b1ℓ1



L21,G 



∥vk∥2

+ 4 



1 + p + 2 

b2ℓ2



∥∇ z F (zk, x k)∥2

+ 2 

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2 

> k

+ 4 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

.

Since F is L0,F -Lipschitz continuous, we have 

Ek

∥Dkv ∥2 ≤ 2

 4L22,G (p + 16) 4

b1ℓ1

h2 

> k

+



1 + 15( p + 6) 2p

2b1ℓ1



L21,G 



∥vk∥2

+ 4 



1 + p + 2 

b2ℓ2



L20,F 

+ 2 

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2 

> k

+ 4 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

.

Adding and subtracting v∗(xk), we get 

Ek

∥Dkv ∥2 ≤ 4

 4L22,G (p + 16) 4

b1ℓ1

h2 

> k

+



1 + 15( p + 6) 2p

2b1ℓ1



L21,G 



∥vk − v∗(xk)∥2

+ 4 

 4L22,G (p + 16) 4

b1ℓ1

h2 

> k

+



1 + 15( p + 6) 2p

2b1ℓ1



L21,G 



∥v∗(xk)∥2

+ 4 



1 + p + 2 

b2ℓ2



L20,F 

+ 2 

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2 

> k

+ 4 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

.

Let Ckv = 4 

 4L22,G (p+16) 4 

> b1ℓ1

h2 

> k

+



1 + 15( p+6) 2p

> 2b1ℓ1



L21,G 



. By Lemma C.3, we have 

Ek

∥Dkv ∥2 ≤ Ckv ∥vk − v∗(xk)∥2 + Ckv

L20,F 

μ2

> G

+ 4 



1 + p + 2 

b2ℓ2



L20,F + 4 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

+ 2 

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2

> k

.

43 Finally, we prove eq. (34). By eq. (8), 

Ek

h

Dkx

> 2

i

= Ek

 1

b1

> b1

X

> i=1

ˆ∇2 

> xz

gk(zk, x k, ξ i,k )

!

vk + 1

b2

> b2

X

> i=1

ˆ∇xfk(zk, x k, ζ i,k )

> 2



≤ 2Ek

 1

b1

> b1

X

> i=1

ˆ∇2 

> xz

gk(zk, x k, ξ i,k )

!

vk

> 2



+ 2 Ek

 1

b2

> b2

X

> i=1

ˆ∇xfk(zk, x k, ζ i,k )

> 2

 .

By Lemma C.9 and Lemma C.11, we have 

Ek

h

Dkx

> 2

i

≤ 2

 1

b1ℓ1



8L22,G h2

> k

 (p + 8) 4 + p(d + 12) 3

+ 6( p + 4)( p + 2) p + 36( p + 2) min( p, d )+ 30 p(d + 2) dL 21,G 



+ L21,G 



∥vk∥2

+ 4 



1 + d + 2 

b2ℓ2



∥∇ xF (zk, x k)∥2

+ 2 

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F h2

> k

+ 4 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F .

By L0,F -Lipschitz continuity of F ,

Ek

h

Dkx

> 2

i

≤ 2

 1

b1ℓ1



8L22,G h2

> k

 (p + 8) 4 + 2 p(d + 12) 3

+ 6( p + 4)( p + 2) p + 36( p + 2) min( p, d )+ 30 p(d + 2) dL 21,G 



+ L21,G 



∥vk∥2

+ 2 

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F h2

> k

+ 4 



1 + d + 2 

b2ℓ2



L20,F + 4 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F .

Let Ckx = 2 



> 1
> b1ℓ1



8L22,G h2

> k

 (p + 8) 4 + 2 p(d + 12) 3 + 6( p + 4)( p + 2) p + 36( p + 2) min( p, d ) + 30 p(d + 2) dL 21,G 



+

L21,G 



. Adding and subtracting v∗(xk), by Lemma C.3, we get 

Ek

h

Dkx

> 2

i

≤ 2Ckx ∥vk − v∗(xk)∥2

+ 2 Ckx

L20,F 

μ2

> G

+ 2 

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F h2

> k

+ 4 



1 + d + 2 

b2ℓ2



L20,F + 4 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F .

44 Now, in the next lemma, we provides bounds on the error of the sequences zk, v k generated by Algorithm 1 relative to the corresponding optimal solutions z∗(xk), v ∗(xk) at each iterate xk.

Lemma C.13 (Bound of sequences) . Let Assumptions 3.2, 3.3 holds. Let (zk)k∈N, (vk)k∈N be the sequences generated by Algorithm 1. Let ωzk =



1 + 4

> μGρk



, the for every k ∈ NEk

∥zk+1 − z∗(xk+1 )∥2 ≤



1 − μGρk

2



∥zk − z∗(xk)∥2 + 2 ρ2

> k

Ek

h

Dkz

> 2

i

+ 2 ωzk L2

> ∗

γ2 

> k

Ek

h

Dkx

> 2

i

+ L21,G 

4μG

(d + 3) 3ρkh2

> k

.

(35) 

Moreover, let ωv, 1 =



L1,F + L0,F L2,G 

> μG



, ωv, 2 = 16 L22,G 

> 9

 (d + 5) 5/2 + ( d + 3) 3/22, ωvk =



1 + 2

> μGρk



and hk ≤ 

> μG
> 4√ωv, 2

. Then, we have 

Ek

∥vk+1 − v∗(xk+1 )∥2 ≤ (1 − μGρk) ∥vk − v∗(xk)∥2 + 4 ω2

> v, 1

μG

ρk∥zk − z∗(xk)∥2

+ 2 ρ2

> k

Ek

∥Dkv ∥2 + 2 ωvk L2

> ∗

γ2 

> k

Ek

∥Dkx∥2

+ 2 L21,G (d + 3) 3

μG

+ 4 ωv, 2L20,F 

μ3

> G

!

ρkh2

> k

.

(36) 

Proof. We start by proving eq. (35). Adding and subtracting z∗(xk),

∥zk+1 − z∗(xk+1 )∥2 = ∥zk+1 − z∗(xk) + z∗(xk) − z∗(xk+1 )∥2

= ∥zk+1 − z∗(xk)∥2 + ∥z∗(xk+1 ) − z∗(xk)∥2 + 2 ⟨zk+1 − z∗(xk), z ∗(xk) − z∗(xk+1 )⟩

= ∥zk+1 − z∗(xk)∥2

| {z }

> A

+ ∥z∗(xk+1 ) − z∗(xk)∥2

| {z }

> B

−2 ⟨zk+1 − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩

| {z }

> C

. (37) We start by bounding A. By eq. (5), we have 

∥zk+1 − z∗(xk)∥2 = ∥zk − z∗(xk)∥2 + ρ2 

> k

Dkz 

> 2

− 2ρk Dkz , z k − z∗(xk) .

Taking the conditional expectation, by eq. (4), 

Ek

∥zk+1 − z∗(xk)∥2 = ∥zk − z∗(xk)∥2 + ρ2

> k

Ek

h

Dkz

> 2

i

− 2ρk ⟨∇ z Ghk (zk, x k), z k − z∗(xk)⟩ .

Adding and subtracting ∇z G(zk, x k), we get 

Ek

∥zk+1 − z∗(xk)∥2 = ∥zk − z∗(xk)∥2 + ρ2

> k

Ek

h

Dkz

> 2

i

− 2ρk ⟨∇ z G(zk, x k), z k − z∗(xk)⟩− 2ρk ⟨∇ z Ghk (zk, x k) − ∇ z G(zk, x k), z k − z∗(xk)⟩ .

By μG-strong convexity of G, we have 

Ek

∥zk+1 − z∗(xk)∥2 = (1 − 2μGρk)∥zk − z∗(xk)∥2 + ρ2

> k

Ek

h

Dkz

> 2

i

− 2ρk ⟨∇ z Ghk (zk, x k) − ∇ z G(zk, x k), z k − z∗(xk)⟩ .

By Cauchy-Schwartz and Lemma C.7, 

Ek

∥zk+1 − z∗(xk)∥2 = (1 − 2μGρk)∥zk − z∗(xk)∥2 + ρ2

> k

Ek

h

Dkz

> 2

i

+ 2 ρk

 L1,G 

2 (d + 3) 3/2hk



∥zk − z∗(xk)∥.

45 By Young’s Lemma with parameter ν1 > 0,

Ek

∥zk+1 − z∗(xk)∥2 = (1 − 2μGρk)∥zk − z∗(xk)∥2 + ρ2

> k

Ek

h

Dkz

> 2

i

+ ρk

ν1

 L1,G 

2 (d + 3) 3/2hk

2

+ ν1ρk∥zk − z∗(xk)∥2.

Let ν1 = μG,

Ek

∥zk+1 − z∗(xk)∥2 = (1 − μGρk)∥zk − z∗(xk)∥2 + ρ2

> k

Ek

h

Dkz

> 2

i

+ L21,G 

4μG

(d + 3) 3ρkh2

> k

.

(38) Now we bound the term B. By Lemma C.2 and eq. (9), we have 

∥z∗(xk+1 ) − z∗(xk)∥2 ≤ L2

> ∗

∥xk+1 − xk∥2 = L2

> ∗

γ2 

> k

Dkx 

> 2

. (39) Finally, we bound the term C. By eq. (9) and Cauchy-Schwartz, 

−2 ⟨zk+1 − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩ = −2 ⟨zk − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩

+ 2 ρk Dkz , z ∗(xk+1 ) − z∗(xk)

≤ − 2 ⟨zk − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩

+ 2 ρk∥Dkz ∥∥ z∗(xk+1 ) − z∗(xk)∥.

By Lemma C.2, z∗ is L∗-Lipschitz. Thus, 

−2 ⟨zk+1 − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩ ≤ − 2 ⟨zk − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩

+ 2 L∗ρk∥Dkz ∥∥ xk+1 − xk∥.

By Young’s inequality and eq. (9), 

−2 ⟨zk+1 − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩ ≤ − 2 ⟨zk − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩

+ ρ2 

> k

Dkz 

> 2

+ L2

> ∗

γ2 

> k

Dkx 

> 2

.

By Cauchy-Schwartz and Lemma C.2, 

−2 ⟨zk+1 − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩ ≤ 2L∗∥zk − z∗(xk)∥∥ xk+1 − xk∥

+ ρ2 

> k

Dkz 

> 2

+ L2

> ∗

γ2 

> k

Dkx 

> 2

.

Let ν2 > 0, by ab ≤ ν2a2 + b2

> ν2

−2 ⟨zk+1 − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩ ≤ 2ν2∥zk − z∗(xk)∥2 + 2 L2

> ∗

ν2

∥xk+1 − xk∥2

+ ρ2 

> k

Dkz 

> 2

+ L2

> ∗

γ2 

> k

Dkx 

> 2

.

Let ν2 = μGρk 

> 4

,

−2 ⟨zk+1 − z∗(xk), z ∗(xk+1 ) − z∗(xk)⟩ ≤ μG

2 ρk∥zk − z∗(xk)∥2 + 8L2

> ∗

γ2

> k

μGρk

∥Dkx∥2

+ ρ2 

> k

Dkz 

> 2

+ L2

> ∗

γ2 

> k

Dkx 

> 2

.

(40) Restarting from eq. (37), taking the conditional expectation and using equations (38), (39) and (40), we get 

Ek

∥zk+1 − z∗(xk+1 )∥2 ≤



1 − μGρk

2



∥zk − z∗(xk)∥2 + 2 ρ2

> k

Ek

h

Dkz

> 2

i

+ 2 



1 + 4

μGρk



L2

> ∗

γ2 

> k

Ek

h

Dkx

> 2

i

+ L21,G 

4μG

(d + 3) 3ρkh2

> k

46 Now we prove eq. (36). Adding and subtracting v∗(xk), we have 

∥vk+1 − v∗(xk+1 )∥2 = ∥vk+1 − v∗(xk) + v∗(xk) − v∗(xk+1 )∥2

= ∥vk+1 − v∗(xk)∥2 + ∥v∗(xk+1 ) − v∗(xk)∥2 + 2 ⟨vk+1 − v∗(xk), v ∗(xk) − v∗(xk+1 )⟩

= ∥vk+1 − v∗(xk)∥2

| {z }

> A

+ ∥v∗(xk+1 ) − v∗(xk)∥2

| {z }

> B

−2 ⟨vk+1 − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩

| {z }

> C

. (41) We start bounding the term A. By eq. (7) we have 

∥vk+1 − v∗(xk)∥2 = ∥vk − ρkDkv − v∗(xk)∥2

= ∥vk − v∗(xk)∥2 + ρ2

> k

∥Dkv ∥2 − 2ρk Dkv , v k − v∗(xk) .

Let ˆDkv = ∇zz Ghk (zk, x k)vk + ∇z Fhk (zk, x k). Observing that Ek[Dkv ] = ˆDkv , taking the conditional expectation, we have 

Ek

∥vk+1 − v∗(xk)∥2 = ∥vk − v∗(xk)∥2 + ρ2

> k

Ek

∥Dkv ∥2 − 2ρk

D ˆDkv , v k − v∗(xk)

E

.

Adding and subtracting ¯Dkv = ∇2 

> zz

G(zk, x k)vk + ∇z F (zk, x k),

Ek

∥vk+1 − v∗(xk)∥2 = ∥vk − v∗(xk)∥2 + ρ2

> k

Ek

∥Dkv ∥2

− 2ρk

D ˆDkv − ¯Dkv , v k − v∗(xk)

E

−2ρk ¯Dkv , v k − v∗(xk)

| {z }

> A1

. (42) We focus on A1. Let ¯Dkv, ∗ = ∇2 

> zz

G(z∗(xk), x k)v∗(xk) + ∇z F (z∗(xk), x k) = 0 . Thus, we have 

−2ρk ¯Dkv , v k − v∗(xk) = −2ρk ¯Dkv − ¯Dkv, ∗, v k − v∗(xk)

= −2ρk ∇2 

> zz

G(zk, x k)vk − ∇ 2 

> zz

G(z∗(xk), x k)v∗(xk), v k − v∗(xk)

− 2ρk ⟨∇ z F (zk, x k) − ∇ z F (z∗(xk), x k), v k − v∗(xk)⟩ .

Adding and subtracting ∇2 

> zz

G(zk, x k)v∗(xk),

−2ρk ¯Dkv , v k − v∗(xk) = −2ρk ∇2 

> zz

G(zk, x k)vk − ∇ 2 

> zz

G(zk, x k)v∗(xk), v k − v∗(xk)

− 2ρk ∇2 

> zz

G(zk, x k)v∗(xk) − ∇ 2 

> zz

G(z∗(xk), x k)v∗(xk), v k − v∗(xk)

− 2ρk ⟨∇ z F (zk, x k) − ∇ z F (z∗(xk), x k), v k − v∗(xk)⟩ .

By strong-convexity of G, Cauchy-Schwartz and Lemma C.3, we get 

−2ρk ¯Dkv , v k − v∗(xk) ≤ − 2ρkμG∥vk − v∗(xk)∥2

+ 2L0,F 

μG

ρk∥∇ 2 

> zz

G(zk, x k) − ∇ 2 

> zz

G(z∗(xk), x k)∥∥ vk − v∗(xk)∥

+ 2 ρk∥∇ z F (zk, x k) − ∇ z F (z∗(xk), x k)∥∥ vk − v∗(xk)∥.

Since hessian of G are L2,G -Lipschitz and F is L1,F -smooth, we have 

−2ρk ¯Dkv , v k − v∗(xk) ≤ − 2ρkμG∥vk − v∗(xk)∥2

+ 2L0,F 

μG

L2,G ρk∥zk − z∗(xk)∥∥ vk − v∗(xk)∥

+ 2 L1,F ρk∥zk − z∗(xk)∥∥ vk − v∗(xk)∥.

Let ωv, 1 =



L1,F + L0,F L2,G 

> μG



, we get 

−2ρk ¯Dkv , v k − v∗(xk) ≤ − 2ρkμG∥vk − v∗(xk)∥2 + 2 ρkωv, 1∥zk − z∗(xk)∥∥ vk − v∗(xk)∥.

47 By Young’s inequality with parameter ν1 > 0, we have 

−2ρk ¯Dkv , v k − v∗(xk) ≤ − 2ρkμG∥vk − v∗(xk)∥2 + 2 ρk

ω2

> v, 1

2ν1

∥zk − z∗(xk)∥2 + ν1

2 ∥vk − v∗(xk)∥2

!

= −(2 μG − ν1)ρk∥vk − v∗(xk)∥2 + ω2

> v, 1

ν1

ρk∥zk − z∗(xk)∥2.

Using this inequality in eq. (42), we get 

Ek

∥vk+1 − v∗(xk)∥2 = (1 − 2μGρk + ν1ρk) ∥vk − v∗(xk)∥2 + ρ2

> k

Ek

∥Dkv ∥2

− 2ρk

D ˆDkv − ¯Dkv , v k − v∗(xk)

E

+ ω2

> v, 1

ν1

ρk∥zk − z∗(xk)∥2.

By Cauchy-Schwartz and Young’s inequality with parameter ν2 > 0, we have 

Ek

∥vk+1 − v∗(xk)∥2 = (1 − 2μGρk + ν1ρk + ν2ρk) ∥vk − v∗(xk)∥2 + ρ2

> k

Ek

∥Dkv ∥2

+ ρk

ν2

∥ ˆDkv − ¯Dkv ∥2

| {z }

> A2

+ ω2

> v, 1

ν1

ρk∥zk − z∗(xk)∥2. (43) Now we bound the term A2.

∥ ˆDkv − ¯Dkv ∥2 = ∥∇ 2 

> zz

Ghk (zk, x k)vk + ∇z Fhk (zk, x k) − ∇ 2 

> zz

G(zk, x k)vk − ∇ z F (zk, x k)∥2

≤ 2∥  ∇2 

> zz

Ghk (zk, x k) − ∇ 2 

> zz

G(zk, x k) vk∥2 + 2 ∥∇ z Fhk (zk, x k) − ∇ z F (zk, x k)∥2

≤ 2∥∇ 2 

> zz

Ghk (zk, x k) − ∇ 2 

> zz

G(zk, x k)∥2∥vk∥2 + 2 ∥∇ z Fhk (zk, x k) − ∇ z F (zk, x k)∥2.

By Lemma C.7, we have 

∥ ˆDkv − ¯Dkv ∥2 ≤ 8L22,G 

9



(d + 5) 5/2 + ( d + 3) 3/22

h2

> k

∥vk∥2

+ L21,G 

2 (d + 3) 3h2

> k

.

Adding and subtracting v∗(xk), we get 

∥ ˆDkv − ¯Dkv ∥2 ≤ 16 L22,G 

9



(d + 5) 5/2 + ( d + 3) 3/22

h2

> k

∥vk − v∗(xk)∥2

+ 16 L22,G 

9



(d + 5) 5/2 + ( d + 3) 3/22

h2

> k

∥v∗(xk)∥2

+ L21,G 

2 (d + 3) 3h2

> k

.

Again by Lemma C.3, we have 

∥ ˆDkv − ¯Dkv ∥2 ≤ 16 L22,G 

9



(d + 5) 5/2 + ( d + 3) 3/22

h2

> k

∥vk − v∗(xk)∥2

+ 16 L22,G 

9



(d + 5) 5/2 + ( d + 3) 3/22 L20,F 

μ2

> G

h2

> k

+ L21,G 

2 (d + 3) 3h2

> k

.

48 Let ωv, 2 = 16 L22,G 

> 9

 (d + 5) 5/2 + ( d + 3) 3/22. Using this inequality in eq. (43), we get 

Ek

∥vk+1 − v∗(xk)∥2 ≤



1 − 2μGρk + ν1ρk + ν2ρk + ωv, 2h2

> k

ρk

ν2



∥vk − v∗(xk)∥2

+ ρ2

> k

Ek

∥Dkv ∥2 + ω2

> v, 1

ν1

ρk∥zk − z∗(xk)∥2

+ L21,G 

2 (d + 3) 3 ρk

ν2

h2 

> k

+ ωv, 2

L20,F 

μ2

> G

ρk

ν2

h2

> k

.

Setting ν1 = ν2 = μG 

> 4

and since h2 

> k

≤ μ2 

> G
> 16 ωv, 2

, we have 

Ek

∥vk+1 − v∗(xk)∥2 ≤



1 − 5μG

4 ρk



∥vk − v∗(xk)∥2

+ ρ2

> k

Ek

∥Dkv ∥2 + 4 ω2

> v, 1

μG

ρk∥zk − z∗(xk)∥2

+ 2 L21,G (d + 3) 3

μG

ρkh2 

> k

+ 4 ωv, 2L20,F 

μ3

> G

ρkh2

> k

.

(44) Now, we bound the term B of eq. (41) . By Lemma C.2, v∗ is L∗-Lipschitz continuous. Thus, by eq. (9) , we have 

∥v∗(xk+1 ) − v∗(xk)∥2 ≤ L2

> ∗

∥xk+1 − xk∥2 = L2

> ∗

γ2 

> k

∥Dkx∥2. (45) Finally, we bound the term C of eq. (41). By eq. (7), we have 

−2 ⟨vk+1 − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩ = −2 ⟨vk − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩

+ 2 ρk Dkv , v ∗(xk+1 ) − v∗(xk) .

By Cauchy-Schwartz and Lemma C.2, we have 

−2 ⟨vk+1 − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩ ≤ − 2 ⟨vk − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩

+ 2 L∗ρk∥Dkv ∥∥ xk+1 − xk∥.

By Young’s inequality with ν3 > 0 and eq. (9), we get 

−2 ⟨vk+1 − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩ ≤ − 2 ⟨vk − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩

+ ρ2

> k

ν3∥Dkv ∥2 + L2

> ∗

ν3

γ2 

> k

∥Dkx∥2.

Similarly, by Cauchy-Schwartz, Lemma C.2 (i.e. L∗-Lipschitz of v∗) and by Young’s inequality with parameter 

ν4 > 0,

−2 ⟨vk+1 − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩ ≤ 2L∗∥vk − v∗(xk)∥∥ xk+1 − xk∥

+ ρ2

> k

ν3∥Dkv ∥2 + L2

> ∗

ν3

γ2 

> k

∥Dkx∥2

≤ ν4∥vk − v∗(xk)∥2 + 1

ν4

L2

> ∗

∥xk+1 − xk∥2

+ ρ2

> k

ν3∥Dkv ∥2 + L2

> ∗

ν3

γ2 

> k

∥Dkx∥2.

Let ν3 = 1 and ν4 = μGρk 

> 4

, we have 

−2 ⟨vk+1 − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩ ≤ μG

4 ρk∥vk − v∗(xk)∥2 + 4

μGρk

L2

> ∗

γ2 

> k

∥Dkx∥2

+ ρ2

> k

∥Dkv ∥2 + L2

> ∗

γ2 

> k

∥Dkx∥2.

(46) 49 Let ωvk =



1 + 2

> μGρk



. Restarting from eq. (41) , taking the conditional expectation and by equations (44) ,(45),(46), we get the claim 

Ek

∥vk+1 − v∗(xk+1 )∥2 ≤ (1 − μGρk) ∥vk − v∗(xk)∥2 + 4 ω2

> v, 1

μG

ρk∥zk − z∗(xk)∥2

+ 2 ρ2

> k

Ek

∥Dkv ∥2 + 2 ωvk L2

> ∗

γ2 

> k

Ek

∥Dkx∥2

+ 2 L21,G (d + 3) 3

μG

ρkh2 

> k

+ 4 ωv, 2L20,F 

μ3

> G

ρkh2

> k

.

The following lemma characterizes the descent of the value function Ψ along the sequence of the iterates xk

generated by Algorithm 1. 

Lemma C.14 (Value Function sequence bound) . Under Assumption 3.2, 3.3. Let (xk)k∈N be the sequence generated by Algorithm 1. Let ωv, 1 =



L1,F + L0,F L2,G 

> μG



. Then, 

Ek [Ψ( xk+1 )] ≤ Ψ( xk) − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2 + LΨ

2 γ2 

> k

Ek

∥Dkx∥2

+ 2 γkω2

> v, 1

∥zk − z∗(xk)∥2 + 2 γkL21,G ∥vk − v∗(xk)∥2

+ C21 γkh2

> k

,

where C1 =



2 L0,F L2,G 

> 3μG

 (d + 5) 5/2 + ( d + 3) 3/2 + ( d + 1)( p + 3) 3/2 + L1,F  

> 2

(d + 3) 3/2



.Proof. By Lemma C.2, Ψ is LΨ-smooth. Thus, by the Descent Lemma [39] we have 

Ψ( xk+1 ) ≤ Ψ( xk) + ⟨∇ Ψ( xk), x k+1 − xk⟩ + LΨ

2 ∥xk+1 − xk∥2.

By eq. (9), we have 

Ψ( xk+1 ) ≤ Ψ( xk) − γk ∇Ψ( xk), D kx + LΨ

2 γ2 

> k

∥Dkx∥2.

Let ˆDkx = ∇2 

> xz

Ghk (zk, x k)vk + ∇xFhk (zk, x k). Taking the conditional expectation, we get 

Ek[Ψ( xk+1 )] ≤ Ψ( xk) − γk

D

∇Ψ( xk), ˆDkx

E

+ LΨ

2 γ2 

> k

Ek

∥Dkx∥2 .

By ⟨a, b ⟩ = 12

 ∥a∥2 + ∥b∥2 − ∥ a − b∥2, we have 

Ek [Ψ( xk+1 )] ≤ Ψ( xk) − γk

2



∥∇ Ψ( xk)∥2 + ∥ ˆDkx∥2 − ∥ ˆDkx − ∇ Ψ( xk)∥2

+ LΨ

2 γ2 

> k

Ek

∥Dkx∥2

= Ψ( xk) − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2

+ γk

2 ∥ ˆDkx − ∇ Ψ( xk)∥2 + LΨ

2 γ2 

> k

Ek

∥Dkx∥2 .

50 Let ωv, 1 =



L1,F + L0,F L2,G 

> μG



. By Lemma C.8 with h1 = hk, η1 = hk, h2 = 0 and η2 = hk, we get 

Ek [Ψ( xk+1 )] ≤ Ψ( xk) − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2

+ LΨ

2 γ2 

> k

Ek

∥Dkx∥2 + γk

2



ωv, 1∥zk − z∗(xk)∥ + L1,G ∥vk − v∗(xk)∥

+ 2 L0,F L2,G 

3μG



hk(d + 5) 5/2 + hk(d + 3) 3/2 + hk(d + 1)( p + 3) 3/2



+ L1,F 

2 (d + 3) 3/2h2

> k

2

Let C1 := 



2 L0,F L2,G 

> 3μG



(d + 5) 5/2 + ( d + 3) 3/2 + ( d + 1)( p + 3) 3/2



+ L1,F  

> 2

(d + 3) 3/2



hk.

Ek [Ψ( xk+1 )] ≤ Ψ( xk) − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2

+ LΨ

2 γ2 

> k

Ek

∥Dkx∥2 + γk

2



ωv, 1∥zk − z∗(xk)∥ + L1,G ∥vk − v∗(xk)∥

+ C1hk

2

.

By (a + b)2 ≤ 2a2 + 2 b2, we get 

Ek [Ψ( xk+1 )] ≤ Ψ( xk) − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2

+ LΨ

2 γ2 

> k

Ek

∥Dkx∥2

+ γk



ωv, 1∥zk − z∗(xk)∥ + L1,G ∥vk − v∗(xk)∥

2

+ C21 γkh2

> k

≤ Ψ( xk) − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2 + LΨ

2 γ2 

> k

Ek

∥Dkx∥2

+ 2 γkω2

> v, 1

∥zk − z∗(xk)∥2 + 2 γkL21,G ∥vk − v∗(xk)∥2

+ C21 γkh2

> k

.

C.4 Auxiliary results for Algorithm 2 

In this appendix, we collect all lemmas required for the analysis of Algorithm 2. Unlike Algorithm 1, which relies on unbiased estimators of the Hessian of the smoothing, Algorithm 2 directly approximates Hessian–vector products using first-order finite differences, and then replaces the resulting gradients with zeroth-order surro-gates. We begin by introducing a lemma that bounds the approximation error of the first-order finite-difference for a twice-differentiable function with Lipschitz-continuous Hessians. 

Lemma C.15 (First-order approximation of Hessian-vector products) . Let f : Rp × Rd → R be a twice-differentiable function with L2,G -Lipschitz continuous hessian. Then, for every z, v ∈ Rp, x ∈ Rd and ¯h > 0,

1¯h

 ∇z f (z + ¯hv, x ) − ∇ z f (z, x ) − ∇ 2 

> zz

f (z, x )v ≤ L2,G 

2 ¯h∥v∥2,

1¯h

 ∇xf (z + ¯hv, x ) − ∇ xf (z, x ) − ∇ 2 

> xz

f (z, x )v ≤ L2,G 

2 ¯h∥v∥2.

51 Proof. The proof of both inequalities follows the same line of Lemma 1.2.4 of [ 33 ]. For completeness, we include the proof of the first inequality (the second follows analogously). Since f is twice differentiable, we have 

∇z f (z + ¯hv, x ) = ∇z f (z, x ) + 

Z 10

∇2 

> zz

f (z + t¯hv, x ) ¯hv dt. 

Adding and subtracting ∇2 

> zz

f (z, x ) ¯hv 

∇z f (z + ¯hv, x ) = ∇z f (z, x ) + ∇2 

> zz

f (z, x ) ¯hv +

Z 10

(∇2 

> zz

f (z + t¯hv, x ) − ∇ 2 

> zz

f (z, x )) ¯hv dt. 

Therefore, we have 

1¯h

 ∇z f (z + ¯hv, x ) − ∇ z f (z, x ) − ∇ 2 

> zz

f (z, x )v = 1¯h

Z 10

(∇2 

> zz

f (z + t¯hv, x ) − ∇ 2 

> zz

f (z, x )) ¯hv dt 

≤ 1¯h

Z 10

(∇2 

> zz

f (z + t¯hv, x ) − ∇ 2 

> zz

f (z, x )) ¯hv dt 

≤ 1¯h

Z 10

∇2 

> zz

f (z + t¯hv, x ) − ∇ 2 

> zz

f (z, x ) ∥¯hv ∥ dt 

By L2,G -Lipschitz continuity of the Hessian, we get the claim 

1¯h

 ∇z f (z + ¯hv, x ) − ∇ z f (z, x ) − ∇ 2 

> zz

f (z, x )v ≤ L2,G ¯h∥v∥2

Z 10

t dt = L2,G 

2 ¯h∥v∥2.

In the following lemma, we derive bounds on the norm of the finite-difference approximations of the Hessian-block–vector products used in Algorithm 2. 

Lemma C.16 (Bound on surrogate difference) . Let Assumptions 3.2 and 3.3 hold. Let zk, v k and xk be the sequences generated by Algorithm 2. Let hk, ¯hk > 0. Then, for every k ∈ NEk



ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k) 2

≤ C1L21,G ¯h2

> k

∥vk − v∗(xk)∥2 + C1

L21,G L20,F 

μ2

> G

¯h2 

> k

+ 3L21,G (p + 6) 3

b1ℓ1

h2

> k

,

Ek



ˆ∇xgk(zk + ¯hkvk, x k) − ˆ∇xgk(zk, x k) 2

≤ C2L21,G ¯h2

> k

∥vk − v∗(xk)∥2 + C2

L21,G L20,F 

μ2

> G

¯h2 

> k

+ 3L21,G (d + 6) 3

b1ℓ1

h2

> k

.

where C1 = 4 



1 + 3( p+2) 

> b1ℓ1



and C2 = 4 



1 + 3( d+2) 

> b1ℓ1



.Proof. We start by proving the first inequality. To simplify the reading we will use the following notation. 

g(i,j ) 

> v,k

= g(zk + ¯hkvk + hkw(i,j ) 

> k

, x k, ξ i,k ) − g(zk + ¯hkvk, x k, ξ i,k )

hk

w(i,j ) 

> k

,g(i,j ) 

> k

= g(zk + hkw(i,j ) 

> k

, x k, ξ i,k ) − g(zk, x k, ξ i,k )

hk

w(i,j ) 

> k

,δ(i,j ) 

> k

= g(i,j ) 

> v,k

− g(i,j ) 

> k

− (∇z ghk (zk + ¯hkvk, x k, ξ i,k ) − ∇ z ghk (zk, x k, ξ i,k )) ,δ(i) 

> k

=

> ℓ1

X

> j=1

δ(i,j ) 

> k

.

52 Adding and subtracting 1

> b1ℓ1
> b1

P

> i=1
> ℓ1

P

> j=1

∇z ghk (zk + ¯hkvk, x k, ξ i,k ) − ∇ z ghk (zk, x k, ξ i,k ), we get 

Ek



ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k) 2

≤ 2Ek

 1

b1ℓ1

> b1

X

> i=1

δ(i)

> k
> 2



+ 2 Ek

h

∇z ghk (zk + ¯hkvk, x k, ξ i,k ) − ∇ z ghk (zk, x k, ξ i,k ) 2i

.

By L1,G -smoothness of g, we get 

Ek



ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k) 2

≤ 2Ek

 1

b1ℓ1

> b1

X

> i=1

δ(i)

> k
> 2



+ 2 L21,G ¯h2 

> k

∥vk∥2 .

Adding and subtracting v∗(xk) and by Lemma C.3, we get 

Ek



ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k) 2

≤ 2Ek

 1

b1ℓ1

> b1

X

> i=1

δ(i)

> k
> 2



+ 4 L21,G ¯h2 

> k

∥vk − v∗(xk)∥2 + 4 L21,G sL 20,F 

μ2

> G

¯h2

> k

Since for every i = 1 , · · · , b 1 and t = 1 , · · · , b 1 with i̸ = t, w(i,j ) 

> k

and w(t,j ) 

> k

are independent for every 

j = 1 , · · · , ℓ 1, we get 

Ek



ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k) 2

≤ 2

b21ℓ21



> b1

X

> i=1

Ek



δ(i)

> k
> 2



+

> b1

X

> i=1

X

> t̸=i

D

Ek[δ(i) 

> k

], Ek[δ(t) 

> k

]

E

+ 4 L21,G ¯h2 

> k

∥vk − v∗(xk)∥2 + 4 L21,G sL 20,F 

μ2

> G

¯h2

> k

.

Observing that, by Lemma C.5, we have 

Ek[δ(i) 

> k

] = 

> ℓ1

X

> j=1

Ew(i,j )

> k

[g(i,j ) 

> v,k

− g(i,j ) 

> k

] − (∇z ghk (zk + ¯hkvk, x k, ξ i,k ) − ∇ z ghk (zk, x k, ξ i,k )) = 0 ,

we get 

Ek



ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k) 2

≤ 2

b21ℓ21

> b1

X

> i=1

Ek



δ(i)

> k
> 2



+ 4 L21,G ¯h2 

> k

∥vk − v∗(xk)∥2 + 4 L21,G sL 20,F 

μ2

> G

¯h2

> k

.

Similarly, recalling that δ(i) 

> k

= ℓ1P

> j=1

δ(i,j ) 

> k

, we have 

Ek



ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k) 2

≤ 2

b21ℓ21

> b1

X

> i=1



> ℓ1

X

> j=1

Ek



δ(i,j )

> k
> 2



+

> ℓ1

X

> j=1

X

> t̸=j

Ek

hD 

δ(i,j ) 

> k

, δ (i,t )

> k

Ei 

+ 4 L21,G ¯h2 

> k

∥vk − v∗(xk)∥2 + 4 L21,G sL 20,F 

μ2

> G

¯h2

> k

.

53 For j = 1 , · · · , ℓ 1 and t = 1 , · · · , ℓ 1 with j̸ = t, w(i,j ) 

> k

and w(i,t ) 

> k

are independent for every i = 1 , · · · , b 1. Thus, we get 

Ek



ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k) 2

≤ 2

b21ℓ21

> b1

X

> i=1
> ℓ1

X

> j=1

Ek



δ(i,j )

> k
> 2



+ 4 L21,G ¯h2 

> k

∥vk − v∗(xk)∥2 + 4 L21,G sL 20,F 

μ2

> G

¯h2

> k

≤ 2

b21ℓ21

> b1

X

> i=1
> ℓ1

X

> j=1

Ek



g(i,j ) 

> v,k

− g(i,j )

> k
> 2

| {z }

> A

+ 4 L21,G ¯h2 

> k

∥vk − v∗(xk)∥2 + 4 L21,G sL 20,F 

μ2

> G

¯h2

> k

.

(47) Now, we focus on bounding A. Adding and subtracting 

D

∇g(zk + ¯hkvk, x k, ξ i,k ), [w(i,j ) 

> k

, 0] 

E

and 

D

∇g(zk, x k, ξ i,k ), [w(i,j ) 

> k

, 0] 

E

,we have 

Ek



g(i,j ) 

> v,k

− g(i,j )

> k
> 2



= 1

h2

> k

Ek



(g(zk + ¯hkvk + hkw(i,j ) 

> k

, x k, ξ i,k ) − g(zk + ¯hkvk, x k, ξ i,k )

− (g(zk + hkw(i,j ) 

> k

, x k, ξ i,k ) − g(zk, x k, ξ i,k ))) 2 w(i,j )

> k
> 2



≤ 3

h2

> k

Ek



(g(zk + ¯hkvk + hkw(i,j ) 

> k

, x k, ξ i,k ) − g(zk + ¯hkvk, x k, ξ i,k )

−

D

∇g(zk + ¯hkvk, x k, ξ i,k ), [hkw(i,j ) 

> k

, 0] 

E

)2∥w(i,j ) 

> k

∥2



+ 3

h2

> k

Ek



(g(zk + hkw(i,j ) 

> k

, x k, ξ i,k ) − g(zk, x k, ξ i,k )

−

D

∇g(zk, x k, ξ i,k ), [hkw(i,j ) 

> k

, 0] 

E

)2∥w(i,j ) 

> k

∥2



+ 3

h2

> k

Ek



(

D

∇g(zk + ¯hkvk, x k, ξ i,k ) − ∇ g(zk, x k, ξ i,k ), [hkw(i,j ) 

> k

, 0] 

E

)2∥w(i,j ) 

> k

∥2



.

Since g is L1,G -smooth, by the Descent Lemma [39], 

Ek



g(i,j ) 

> v,k

− g(i,j )

> k
> 2



≤ 3L21,G 

2 h2

> k

Ek[∥w(i,j ) 

> k

∥6]+ 3

h2

> k

Ek



(

D

∇g(zk + ¯hkvk, x k, ξ i,k ) − ∇ g(zk, x k, ξ i,k ), [hkw(i,j ) 

> k

, 0] 

E

)2∥w(i,j ) 

> k

∥2



= 3L21,G 

2 h2

> k

Ek[∥w(i,j ) 

> k

∥6]+ 3 Eξi,k 



(∇z g(zk + ¯hkvk, x k, ξ i,k ) − ∇ z g(zk, x k, ξ i,k )) ⊤

Ew(i,k )

> k

[w(i,j ) 

> k

w(i,j )⊤ 

> k

w(i,j ) 

> k

w(i,j )⊤ 

> k

]( ∇z g(zk + ¯hkvk, x k, ξ i,k ) − ∇ z g(zk, x k, ξ i,k )) 



.

By Proposition C.1, we have 

Ek



g(i,j ) 

> v,k

− g(i,j )

> k
> 2



≤ 3L21,G 

2 (p + 6) 3h2 

> k

+ 3( p + 2) Eξi,k 



∥∇ z g(zk + ¯hkvk, x k, ξ i,k ) − ∇ z g(zk, x k, ξ i,k )∥2



.

54 Again by L1,G -smoothness of g, we have 

Ek



g(i,j ) 

> v,k

− g(i,j )

> k
> 2



≤ 3L21,G 

2 (p + 6) 3h2 

> k

+ 3( p + 2) L21,G ¯h2

> k

∥vk∥2.

Adding and subtracting v∗(xk), by Lemma C.3, we get 

Ek



g(i,j ) 

> v,k

− g(i,j )

> k
> 2



≤ 3L21,G 

2 (p + 6) 3h2 

> k

+ 6( p + 2) L21,G ¯h2

> k

∥vk − v∗(xk)∥2

+ 6( p + 2) L21,G L20,F 

μ2

> G

¯h2

> k

.

Using this bound in eq. (47), we get the first claim 

Ek



ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k) 2

≤ 4



1 + 3( p + 2) 

b1ℓ1



L21,G ¯h2

> k

∥vk − v∗(xk)∥2

+ 4 



1 + 3( p + 2) 

b1ℓ1

 L21,G L20,F 

μ2

> G

¯h2 

> k

+ 3L21,G (p + 6) 3

b1ℓ1

h2

> k

.

The proof of the second inequality follows the same line. In the next lemma, we provide bounds on the norm of the search directions computed in Algorithm 2. 

Lemma C.17 (Bound on search directions) . Let Assumptions 3.2,3.1 and 3.3 hold. Let ˆDkz , ˆDkv and ˆDkx be the search directions defined in eq. (10) . Let zk, v k and xk be the sequences generated by Algorithm 2. Then, for every 

k ∈ N,

Ek

h

∥ ˆDkz ∥2i

≤ 2



2 + p + 2 

b1ℓ1



L21,G ∥zk − z∗(xk)∥2

+

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G h2 

> k

+ 2 



3 + p + 2 

ℓ1

 σ21,G 

b1

.

Let C1 = 4 



1 + 3( p+2) 

> b1ℓ1



, B1 = 2 



1 + p+2 

> b2ℓ2



, B2 =

 (p+6) 3 

> 2b2ℓ2

+

 3 

> b2

+ 1 



(p + 3) 3



L21,F and B3 = 2 

 p+2  

> ℓ2

+ 3 

 σ21,F  

> b2

.Then for every k ∈ N,

Ek



ˆDkv

> 2



≤ 2C1L21,G ∥vk − v∗(xk)∥2 + 2 C1

L21,G L20,F 

μ2

> G

+ 6L21,G (p + 6) 3

b1ℓ1

h2

> k

¯h2

> k

+ 2 B1L20,F + 2 B2h2 

> k

+ 2 B3.

Moreover, let C2 = 4 



1 + 3( d+2) 

> b1ℓ1



, B4 = 2 



1 + d+2 

> b2ℓ2



, B5 =

 (d+6) 3 

> 2b2ℓ2

+

 3 

> b2

+ 1 



(d + 3) 3



L21,F and B6 =2

 d+2  

> b2ℓ2

+ 3

> b2



σ21,F . Then, 

Ek



ˆDkx

> 2



≤ 2C2L21,G ∥vk − v∗(xk)∥2 + 2 C2

L21,G L20,F 

μ2

> G

+ 6L21,G (d + 6) 3

b1ℓ1

h2

> k

¯h2

> k

+ 2 B4L20,F + 2 B5h2 

> k

+ 2 B6.

Proof. The proof of the first inequality follows the same line of the proof of eq. (32) . We focus on proving the second inequality. By ∥a + b∥2 ≤ 2∥a∥2 + 2 ∥b∥2,

Ek



ˆDkv

> 2



= Ek

"

1¯hk

 ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k)



+ ˆ∇z fk(zk, x k)

> 2

#

≤ 2 Ek

"

1¯hk

 ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k)

 2#| {z }

> A

+2 Ek



ˆ∇z fk(zk, x k) 2| {z }

> B

.

55 We bound the B term by Lemma C.9, 

Ek



ˆ∇z fk(zk, x k) 2

≤ 2



1 + p + 2 

b2ℓ2

| {z }

> B1

∥∇ z F (zk, x k)∥2

+

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F 

| {z }

> B2

h2 

> k

+ 2 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

| {z }

> B3

.

We bound the A term by Lemma C.16. Let C1 = 4 



1 + 3( p+2) 

> b1ℓ1



. Then, 

2¯h2

> k

Ek[∥ ˆ∇z gk(zk + ¯hkvk, x k) − ˆ∇z gk(zk, x k)∥2] ≤ C1L21,G ∥vk − v∗(xk)∥2 + C1

L21,G L20,F 

μ2

> G

+ 3L21,G (p + 6) 3

b1ℓ1

h2

> k

¯h2

> k

.

Thus, by these bounds, we have 

Ek



ˆDkv

> 2



≤ 2C1L21,G ∥vk − v∗(xk)∥2 + 2 C1

L21,G L20,F 

μ2

> G

+ 6L21,G (p + 6) 3

b1ℓ1

h2

> k

¯h2

> k

+ 2 B1 ∥∇ z F (zk, x k)∥2

+ 2 B2h2 

> k

+ 2 B3.

Since F is L0,F -Lipschitz continuous, we get the first claim 

Ek



ˆDkv

> 2



≤ 2C1L21,G ∥vk − v∗(xk)∥2 + 2 C1

L21,G L20,F 

μ2

> G

+ 6L21,G (p + 6) 3

b1ℓ1

h2

> k

¯h2

> k

+ 2 B1L20,F + 2 B2h2 

> k

+ 2 B3.

The proof of the last inequality follows the same line. 

Ek



ˆDkx

> 2



= Ek

"

1¯hk

 ˆ∇xgk(zk + ¯hkvk, x k) − ˆ∇xgk(zk, x k)



+ ˆ∇xfk(zk, x k)

> 2

#

≤ 2 Ek

"

1¯hk

 ˆ∇xgk(zk + ¯hkvk, x k) − ˆ∇xgk(zk, x k)

 2#| {z }

> A

+2 Ek



ˆ∇xfk(zk, x k) 2| {z }

> B

.

We bound the B term by Lemma C.9, 

Ek



ˆ∇xfk(zk, x k) 2

≤ 2



1 + d + 2 

b2ℓ2

| {z }

> B4

∥∇ xF (zk, x k)∥2

+

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F 

| {z }

> B5

h2

> k

+ 2 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F 

| {z }

> B6

.

56 We bound the A term by Lemma C.16. Let C2 = 4 



1 + 3( d+2) 

> b1ℓ1



. Then, 

2¯h2

> k

Ek[∥ ˆ∇xgk(zk + ¯hkvk, x k) − ˆ∇xgk(zk, x k)∥2] ≤ C2L21,G ∥vk − v∗(xk)∥2 + C2

L21,G L20,F 

μ2

> G

+ 3L21,G (d + 6) 3

b1ℓ1

h2

> k

¯h2

> k

.

Thus, by these bounds, we have 

Ek



ˆDkx

> 2



≤ 2C2L21,G ∥vk − v∗(xk)∥2 + 2 C2

L21,G L20,F 

μ2

> G

+ 6L21,G (d + 6) 3

b1ℓ1

h2

> k

¯h2

> k

+ 2 B4 ∥∇ z F (zk, x k)∥2

+ 2 B5h2 

> k

+ 2 B6.

Since F is L0,F -Lipschitz continuous, we get the first claim 

Ek



ˆDkx

> 2



≤ 2C2L21,G ∥vk − v∗(xk)∥2 + 2 C2

L21,G L20,F 

μ2

> G

+ 6L21,G (d + 6) 3

b1ℓ1

h2

> k

¯h2

> k

+ 2 B4L20,F + 2 B5h2 

> k

+ 2 B6.

In the next lemma, we bound the error of the sequence (vk)k∈N generated by Algorithm 2 relative to the corresponding optimal solutions v∗(xk) at each iterate xk. We do not need to bound the error of the sequence 

(zk)k∈N, since it differs from that of ZOBA only in the construction of the gradient estimator (as no Hessian is involved in the zk update of ZOBA). Indeed, in Algorithm 2 this surrogate is constructed using forward finite differences instead of central differences. As shown in Lemma C.9, this change yields the same bound on the surrogate norm, analogously to the bound obtained for the gradient surrogate of the outer objective f in ZOBA. 

Lemma C.18 (Bound on vk iteration) . Let Assumptions 3.2 and 3.3 hold. Let (vk)k∈N be the sequence generated in Algorithm 2 and let ¯hk =

( ˆhk  

> ∥vk∥

if ∥vk∦ = 0 ˆhk

with ˆhk ≤ μG√8L2,G 

. Let ωv, 1 =



L1,F + L0,F L2,G 

> μG



, ωv, 2 =

> 16 L22,G
> 9

 (d + 5) 5/2 + ( d + 3) 3/22 and ωvk =



1 + 2

> μGρk



. Let hk ≤ μG√16 ωv, 2

. Then, for every k ∈ N,

Ek

∥vk+1 − v∗(xk+1 )∥2 ≤



1 − μG

2 ρk



∥vk − v∗(xk)∥2 + 4 ω2

> v, 1

μG

ρk∥zk − z∗(xk)∥2

+ 2 ρ2

> k

Ek[∥ ˆDkv ∥2] + 2 ωvk L2

> ∗

γ2 

> k

Ek[∥ ˆDkx∥2]+ 2 L21,G (d + 3) 2

μG

ρkh2 

> k

+ 4 ωv, 2L20,F 

μ3

> G

ρkh2

> k

+ 8

μG

L22,G 

4

L20,F 

μ2

> G

ρk ˆh2

> k

.

Proof. The proof follows the same line of Lemma C.13. Adding and subtracting v∗(xk), we have 

∥vk+1 − v∗(xk+1 )∥2 = ∥vk+1 − v∗(xk)∥2

| {z }

> A

+ ∥v∗(xk+1 ) − v∗(xk)∥2

| {z }

> B

−2 ⟨vk+1 − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩

| {z }

> C

.

(48) 57 We start by bounding A. By Algorithm 2 and taking the conditional expectation 

Ek[∥vk+1 − v∗(xk)∥2] = ∥vk − v∗(xk)∥2 + ρ2

> k

Ek[∥ ˆDkv ∥2]

− 2ρk

 1¯hk

(∇z Ghk (zk + ¯hkvk, x k) − ∇ z G(zk, x k)) + ∇z Fhk (zk, x k), v k − v∗(xk)



.

Let ˜Dkv = ∇2 

> zz

Ghk (zk, x k)vk + ∇z Fhk (zk, x k). Adding and subtracting ∇zz Ghk (zk, x k)vk, we get 

Ek[∥vk+1 − v∗(xk)∥2] = ∥vk − v∗(xk)∥2 + ρ2

> k

Ek[∥ ˆDkv ∥2]

− 2ρk

D ˜Dkv , v k − v∗(xk)

E

− 2ρk

 1¯hk

(∇z Ghk (zk + ¯hkvk, x k) − ∇ z G(zk, x k)) − ∇ 2 

> zz

G(zk, x k)vk, v k − v∗(xk)



.

Let ωv, 1 =



L1,F + L0,F L2,G 

> μG



, ωv, 2 = 16 L22,G 

> 9

 (d + 5) 5/2 + ( d + 3) 3/22. For h2 

> k

≤ μ2 

> G
> 16 ωv, 2

, following the same line of the proof of Lemma C.13, we get 

Ek[∥vk+1 − v∗(xk)∥2] = 



1 − 5μG

4 ρk



∥vk − v∗(xk)∥2

+ ρ2

> k

Ek[∥ ˆDkv ∥2] + 4 ω2

> v, 1

μG

ρk∥zk − z∗(xk)∥2

+ 2 L21,G (d + 3) 2

μG

ρkh2 

> k

+ 4 ωv, 2L20,F 

μ3

> G

ρkh2

> k

− 2ρk

 1¯hk

(∇z Ghk (zk + ¯hkvk, x k) − ∇ z G(zk, x k)) − ∇ 2 

> zz

G(zk, x k)vk, v k − v∗(xk)



.

Let 

∆k = 1¯hk

(∇z Ghk (zk + ¯hkvk, x k) − ∇ z G(zk, x k)) − ∇ 2 

> zz

G(zk, x k)vk.

By Cauchy-Schwartz and Young’s inequality with parameter μG 

> 4

, we have 

−2ρk ⟨∆k, v k − v∗(xk)⟩ ≤ 4ρk

μG

∥∆k∥2 + μG

4 ρk∥vk − v∗(xk)∥2.

By Lemma C.15 and by the choice, 

¯hk =

( ˆhk  

> ∥vk∥

∥vk∦ = 0 ˆhk

,

we have 

−2ρk ⟨∆k, v k − v∗(xk)⟩ ≤ 4ρk

μG

L22,G 

4 ¯h2

> k

∥vk∥4 + μG

4 ρk∥vk − v∗(xk)∥2

= 4ρk

μG

L22,G 

4 ˆh2

> k

∥vk∥2 + μG

4 ρk∥vk − v∗(xk)∥2.

Adding and subtracting v∗(xk) and by Lemma C.3, 

−2ρk ⟨∆k, v k − v∗(xk)⟩ ≤ 8ρk

μG

L22,G 

4 ˆh2

> k

∥vk − v∗(xk)∥2 + μG

4 ρk∥vk − v∗(xk)∥2 + 8ρk

μG

L22,G 

4 ˆh2

> k

∥v∗(xk)∥2

≤ 8

μG

L22,G 

4 ˆh2 

> k

+ μG

4

!

ρk∥vk − v∗(xk)∥2 + 8ρk

μG

L22,G 

4

L20,F 

μ2

> G

ˆh2

> k

.

58 Since ˆhk ≤ μG√8L2,G 

= q μG

> 4
> μG
> 84
> L22,G

,

−2ρk ⟨∆k, v k − v∗(xk)⟩ ≤ μG

2 ρk∥vk − v∗(xk)∥2 + 8

μG

L22,G 

4

L20,F 

μ2

> G

ρk ˆh2

> k

.

Thus, we conclude the bound of term A obtaining 

Ek[∥vk+1 − v∗(xk)∥2] ≤



1 − 5μG

4 ρk + μG

2 ρk



∥vk − v∗(xk)∥2

+ ρ2

> k

Ek[∥ ˆDkv ∥2] + 4 ω2

> v, 1

μG

ρk∥zk − z∗(xk)∥2

+ 2 L21,G (d + 3) 2

μG

ρkh2 

> k

+ 4 ωv, 2L20,F 

μ3

> G

ρkh2

> k

+ 8

μG

L22,G 

4

L20,F 

μ2

> G

ρk ˆh2

> k

.

We bound the B term by Lemma C.2 and Algorithm 2, 

∥v∗(xk+1 ) − v∗(xk)∥2 ≤ L2

> ∗

∥xk+1 − xk∥2 = L2

> ∗

γ2 

> k

∥ ˆDkx∥2.

The bound on term C follows the same line of the analogous bound in the proof of Lemma C.13, 

−2 ⟨vk+1 − v∗(xk), v ∗(xk+1 ) − v∗(xk)⟩ ≤ μG

4 ρk∥vk − v∗(xk)∥2 + 4

μGρk

L2

> ∗

γ2 

> k

∥ ˆDkx∥2

+ ρ2

> k

∥ ˆDkv ∥2 + L2

> ∗

γ2 

> k

∥ ˆDkx∥2.

Let ωvk =



1 + 2

> μGρk



. Restarting from eq. (48) , taking the conditional expectation and using the bounds on the terms A, B and C, we get the claim 

Ek[∥vk+1 − v∗(xk+1 )∥2] ≤



1 − μG

2 ρk



∥vk − v∗(xk)∥2 + 4 ω2

> v, 1

μG

ρk∥zk − z∗(xk)∥2

+ 2 ρ2

> k

Ek[∥ ˆDkv ∥2] + 2 ωvk L2

> ∗

γ2 

> k

Ek[∥ ˆDkx∥2]+ 2 L21,G (d + 3) 2

μG

ρkh2 

> k

+ 4 ωv, 2L20,F 

μ3

> G

ρkh2

> k

+ 8

μG

L22,G 

4

L20,F 

μ2

> G

ρk ˆh2

> k

.

In the following lemma, we derive the bound on the value function Ψ along the sequence (xk)k∈N generated by Algorithm 2. 

Lemma C.19 (Function value decrease) . Let Assumptions 3.2, 3.1, 3.3 hold. Let (xk)k∈N be the sequence generated by Algorithm 2. Then for every k ∈ N,

Ek[Ψ( xk+1 )] ≤ Ψ( xk) − γk

2 ∥∇ Ψ( xk)∥2 + LΨ

2 γ2 

> k

Ek[∥ ˆDkx∥2]+ 4 C21,Ψγk∥zk − z∗(xk)∥2 + C4,Ψγk∥vk − v∗(xk)∥2

+ 2 C22,Ψγkh2 

> k

+ C3,Ψˆh2

> k

γk,

59 where 

C1,Ψ = L1,F + L0,F L1,G 

μG

,C2,Ψ = 2L2,G L0,F 

3μG



(d + 5) 5/2 + ( d + 3) 3/2 + ( d + 1)( p + 3) 3/2

+ L1,F d√p

2

C3,Ψ = L22,G L20,F 

2μ2

> G

and C4,Ψ = 4 L21,G + μ2

> G

16 .

Proof. The proof follows the same line of Lemma C.14. By Lemma C.2, Ψ is LΨ-smooth. Thus, by the Descent Lemma [39] and Algorithm 2, we have 

Ψ( xk+1 ) ≤ Ψ( xk) − γk

D

∇Ψ( xk), ˆDkx

E

+ LΨ

2 γ2 

> k

∥ ˆDkx∥2.

Taking the conditional expectation, 

Ek[Ψ( xk+1 )] ≤ Ψ( xk)

− γk



∇Ψ( xk), 1¯hk

(∇xGhk (zk + ¯hkvk, x k) − ∇ xGhk (zk, x k)) + ∇xFhk (zk, x k)



+ LΨ

2 γ2 

> k

Ek[∥ ˆDkx∥2].

By ⟨a, b ⟩ = 12 (∥a∥2 + ∥b∥2 − ∥ a − b∥2), we get 

Ek[Ψ( xk+1 )] ≤ Ψ( xk)

− γk

2



∥∇ Ψ( xk)∥2

+ ∥ 1¯hk

(∇xGhk (zk + ¯hkvk, x k) − ∇ xGhk (zk, x k)) + ∇xFhk (zk, x k)∥2

− ∥ 1¯hk

(∇xGhk (zk + ¯hkvk, x k) − ∇ xGhk (zk, x k)) + ∇xFhk (zk, x k) − ∇ Ψ( xk)∥2



+ LΨ

2 γ2 

> k

Ek[∥ ˆDkx∥2]

≤ Ψ( xk) − γk

2 ∥∇ Ψ( xk)∥2 + LΨ

2 γ2 

> k

Ek[∥ ˆDkx∥2]+ γk

2 ∥ 1¯hk

(∇xGhk (zk + ¯hkvk, x k) − ∇ xGhk (zk, x k)) + ∇xFhk (zk, x k) − ∇ Ψ( xk)∥2

| {z }

> A

.

(49) Now we bound A. Let ¯∆k = 1¯hk

(∇xGhk (zk + ¯hkvk, x k)−∇ xGhk (zk, x k))+ ∇xFhk (zk, x k), ∆k = 1¯hk

(∇xGhk (zk +¯hkvk, x k) − ∇ xGhk (zk, x k)) − ∇ 2 

> xz

Ghk (zk, x k)vk and ˜Dkx = ∇2 

> xz

Ghk (zk, x k)vk + ∇xFhk (zk, x k). Adding and subtracting ∇2 

> xz

Ghk (zk, x k)vk and by ∥a + b∥2 ≤ 2∥a∥2 + 2 ∥b∥2,

γk

2 ∥ ¯∆k − ∇ Ψ( xk)∥2 ≤ γk∥∆k∥2 + γk∥ ˜Dkx − ∇ Ψ( xk)∥2.

60 By Lemma C.15 and Lemma C.8 with h1 = η1 = η2 = hk, we have 

γk

2 ∥ ¯∆k − ∇ Ψ( xk)∥2 ≤ L22,G 

4 ¯h2

> k

γk∥vk∥4

+ γk

  

L1,F + L0,F L1,G 

μG

| {z }

> C1,Ψ

∥zk − z∗(xk)∥ + L1,G ∥vk − v∗(xk)∥

+

 L0,F 

μG

2L2,G 

3



(d + 5) 5/2 + ( d + 3) 3/2 + ( d + 1)( p + 3) 3/2

+ L1,F d√p

2

| {z }

> =: C2,Ψ

hk

2

≤ L22,G 

4 ¯h2

> k

γk∥vk∥4

+ 4 C21,Ψγk∥zk − z∗(xk)∥2 + 4 L21,G γk∥vk − v∗(xk)∥2 + 2 C22,Ψγkh2

> k

.

By the choice 

¯hk =

( ˆhk  

> ∥vk∥

∥vk∦ = 0 ˆhk

,

we have 

γk

2 ∥ ¯∆k − ∇ Ψ( xk)∥2 ≤ L22,G 

4 ˆh2

> k

γk∥vk∥2

+ 4 C21,Ψγk∥zk − z∗(xk)∥2 + 4 L21,G γk∥vk − v∗(xk)∥2 + 2 C22,Ψγkh2

> k

.

Adding and subtracting v∗(xk) and by Lemma C.3, we get 

γk

2 ∥ ¯∆k − ∇ Ψ( xk)∥2 ≤ L22,G 

2

L20,F 

μ2

> G

| {z }

> =: C3,Ψ

ˆh2

> k

γk

+ 4 C21,Ψγk∥zk − z∗(xk)∥2 + 4L21,G + L22,G 

2 ˆh2

> k

!

γk∥vk − v∗(xk)∥2 + 2 C22,Ψγkh2

> k

.

Since ˆhk ≤ μG√8L2,G 

, we have 

γk

2 ∥ ¯∆k − ∇ Ψ( xk)∥2 ≤ 4C21,Ψγk∥zk − z∗(xk)∥2 +



4L21,G + μ2

> G

16 

| {z }

> =: C4,Ψ

γk∥vk − v∗(xk)∥2

+ 2 C22,Ψγkh2 

> k

+ C3,Ψˆh2

> k

γk.

Using this inequality in eq. (49), we get 

Ek[Ψ( xk+1 )] ≤ Ψ( xk) − γk

2 ∥∇ Ψ( xk)∥2 + LΨ

2 γ2 

> k

Ek[∥ ˆDkx∥2]+ 4 C21,Ψγk∥zk − z∗(xk)∥2 + C4,Ψγk∥vk − v∗(xk)∥2

+ 2 C22,Ψγkh2 

> k

+ C3,Ψˆh2

> k

γk.

61 D Proofs of Main Results 

Proof of Theorem 3.4 

Let δzk = ∥zk − z∗(xk)∥2 and δvk = ∥vk − v∗(xk)∥2. For ϕkz , ϕ kv > 0 s.t. ϕk+1  

> v

≤ ϕkv and ϕk+1  

> z

≤ ϕkz , define 

Lk = Ψ( xk) + ϕkz δzk + ϕkv δvk .

Consider 

Lk+1 − Lk = Ψ( xk+1 ) − Ψ( xk) + ϕk+1  

> z

δzk+1 − ϕkz δzk + ϕk+1  

> v

δvk+1 − ϕkv δvk

≤ Ψ( xk+1 ) − Ψ( xk) + ϕkz (δzk+1 − δzk ) + ϕkv (δvk+1 − δvk ).

Taking the conditional expectation, we have 

Ek [Lk+1 ] − Lk ≤ Ek [Ψ( xk+1 )] − Ψ( xk) + ϕkz (Ek

δzk+1 

 − δzk ) + ϕkv (Ek

δvk+1 

 − δvk ).

By Lemma C.14, we get 

Ek [Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2 + LΨ

2 γ2 

> k

Ek

∥Dkx∥2

+ 2 γkω2

> v, 1

δzk + 2 γkL21,G δvk

+ ϕkz (Ek

δzk+1 

 − δzk ) + ϕkv (Ek

δvk+1 

 − δvk )+ C21 γkh2

> k

.

By Lemma C.13, we have 

Ek [Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2

+ 2 ϕkv ρ2

> k

Ek

∥Dkv ∥2 + 2 ϕkz ρ2

> k

Ek

h

Dkz

> 2

i

−



μGϕkv ρk − 2L21,G γk



δvk

−

 μG

2 ϕkz ρk − 4 ω2

> v, 1

μG

ϕkv ρk − 2ω2

> v, 1

γk



δzk

+

 LΨ

2 + 2 wzkL2

> ∗

ϕkz + 2 ωvk L2

> ∗

ϕkv



γ2 

> k

Ek

∥Dkx∥2

+ C21 γkh2

> k

+ L21,G 

4μG

(d + 3) 3ϕkz + 2 L21,G (d + 3) 3

μG

ϕkv + 4 ωv, 2L20,F 

μ3

> G

ϕkv

!

ρkh2

> k

.

62 Let ω3 = 2 



2 + p+2 

> b1ℓ1



L21,G and ωkdx =

 LΨ 

> 2

+ 2 wzkL2

> ∗

ϕkz + 2 ωvk L2

> ∗

ϕkv



. By Lemma C.12, we get 

Ek [Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2

−



μGϕkv ρk − 2Ckv ϕkv ρ2 

> k

− 2L21,G γk − ωkdx 2Ckx γ2

> k



δvk

−

 μG

2 ϕkz ρk − 4 ω2

> v, 1

μG

ϕkv ρk − 2ω3ϕkz ρ2 

> k

− 2ω2

> v, 1

γk



δzk

+ C21 γkh2

> k

+ L21,G 

4μG

(d + 3) 3ϕkz + 2 L21,G (d + 3) 3

μG

ϕkv + 4 ωv, 2L20,F 

μ3

> G

ϕkv

!

ρkh2

> k

+ 2 

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G ϕkz ρ2

> k

h2

> k

+ 4 



3 + p + 2 

ℓ1

 σ21,G 

b1

ϕkz ρ2

> k

+ 2 ϕkv ρ2

> k



Ckv

L20,F 

μ2

> G

+ 4 



1 + p + 2 

b2ℓ2



L20,F + 4 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

+ 2 

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2

> k



+ ωkdx γ2

> k



2Ckx

L20,F 

μ2

> G

+ 2 

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F h2

> k

+ 4 



1 + d + 2 

b2ℓ2



L20,F + 4 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F 



.

Let 

Ck 

> 2

:= L21,G 

4μG

(d + 3) 3ϕkz + 2 L21,G (d + 3) 3

μG

ϕkv + 4 ωv, 2L20,F 

μ3

> G

ϕkv

!

C3 := 4 



3 + p + 2 

ℓ1

 σ21,G 

b1

C4 := 2 

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G 

Ck 

> 5

(hk) := 



2Ckx

L20,F 

μ2

> G

+ 2 

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F h2

> k

+ 4 



1 + d + 2 

b2ℓ2



L20,F + 4 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F 



Ck 

> 6

(hk) := 



Ckv

L20,F 

μ2

> G

+ 4 



1 + p + 2 

b2ℓ2



L20,F + 4 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

+ 2 

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2

> k



.

63 Thus, we have 

Ek [Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2 − γk

2 ∥ ˆDkx∥2

−



μGϕkv ρk − 2Ckv ϕkv ρ2 

> k

− 2L21,G γk − ωkdx 2Ckx γ2

> k



δvk

−

 μG

2 ϕkz ρk − 4 ω2

> v, 1

μG

ϕkv ρk − 2ω3ϕkz ρ2 

> k

− 2ω2

> v, 1

γk



δzk

+ C21 γkh2 

> k

+ Ck 

> 2

ρkh2 

> k

+ C4ϕkz ρ2

> k

h2

> k

+ C3ϕkz ρ2 

> k

+ 2 ϕkv Ck 

> 6

(hk)ρ2 

> k

+ ωkdx Ck 

> 5

(hk)γ2 

> k

.

Let ¯Ck = C21 γkh2 

> k

+Ck 

> 2

ρkh2 

> k

+C4ϕkz ρ2

> k

h2 

> k

+C3ϕkz ρ2 

> k

+2 ϕkv Ck 

> 6

(hk)ρ2 

> k

+ωkdx Ck 

> 5

(hk)γ2 

> k

. Observing that − γk 

> 2

∥ ˆDkx∥2 ≤ 0,we have 

Ek [Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

−



μGϕkv ρk − 2Ckv ϕkv ρ2 

> k

− 2L21,G γk − ωkdx 2Ckx γ2

> k



δvk

−

 μG

2 ϕkz ρk − 4 ω2

> v, 1

μG

ϕkv ρk − 2ω3ϕkz ρ2 

> k

− 2ω2

> v, 1

γk



δzk

+ ¯Ck.

Choosing hk such that h2 

> k

≤ min 

 b1ℓ1

> 4L22,G (p+16) 4



1 + 15( p+6) 2p

> 2b1ℓ1



L21,G , μ2 

> G
> 16 ωv, 2

, 116 L22,G (( p+8) 4+p(d+12) 3)



, we have, 

Ckv = 8 



1 + 15( p + 6) 2p

2b1ℓ1



L21,G =: ¯Cv .

Therefore, we get 

Ek [Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

−



μGϕkv ρk − 2 ¯Cv ϕkv ρ2 

> k

− 2L21,G γk − ωkdx 2Ckx γ2

> k



δvk

−

 μG

2 ϕkz ρk − 4 ω2

> v, 1

μG

ϕkv ρk − 2ω3ϕkz ρ2 

> k

− 2ω2

> v, 1

γk



δzk

+ ¯Ck.

Recalling that 1

ωzk

= μGρk

μGρk + 4 and 1

ωvk

= μGρk

μGρk + 2 .

Let 

ϕkz = ¯ϕz

ωzk

and ϕkv = ¯ϕv

ωvk

.

Notice that for ρk decreasing sequence or constant ϕk+1  

> z

≤ ϕz and ϕk+1  

> v

≤ ϕz . Then, we have 

ωkdx = LΨ

2 + 2 L2

> ∗

( ¯ϕz + ¯ϕv ) =: ωdx .

Moreover, for h2 

> k

≤ 116 L22,G (( p+8) 4+p(d+12) 3) , we get 

Ckx = 2 

 1

b1ℓ1

 12 + 6( p + 4)( p + 2) p + 36( p + 2) min( p, d ) + 30 p(d + 2) dL 21,G 



+ L21,G 



=: ¯Cx.

64 Therefore, we have 

Ek [Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

−

 μG

ωvk

¯ϕv ρk − 2 ¯Cv

ωvk

¯ϕv ρ2 

> k

− 2L21,G γk − ωdx 2 ¯Cxγ2

> k



δvk

−

 μG

2ωzk

¯ϕz ρk − 4 ω2

> v, 1

μGωvk

¯ϕv ρk − 2 ω3

ωzk

¯ϕz ρ2 

> k

− 2ω2

> v, 1

γk



δzk

+ ¯Ck.

Notice that since ρk < min(1 , μG 

> 2 ¯Cv

), we have 

−

 μG

2ωvk

¯ϕv ρk − 2L21,G γk − ωdx 2 ¯Cxγ2

> k



≤ − 

 μ2

> G

2( μG + 2) ¯ϕv

| {z }

> A

ρ2 

> k

− 2L21,G γk − ωdx 2 ¯Cxγ2

> k



.

Thus for γk < −2L21,G +√4L41,G +8 ωdx ¯CxAρ 2  

> k
> 4ωdx ¯Cx

, we have 

−



Aρ 2 

> k

− 2L21,G γk − ωdx 2 ¯Cxγ2

> k



< 0.

Moreover, since for ρk ≤ min(1 , μG 

> 2 ¯Cv

), we have 

μGρk

4 > 1

wzk

= μGρk

μGρk + 4 > μGρk

μG + 4 ,μGρk

2 > 1

wvk

= μGρk

μGρk + 2 > μGρk

μG + 2 .

Let Tk = μG

> 2ωzk

¯ϕz ρk − 4 ω2

> v, 1
> μGωvk

¯ϕv ρk − 2 ω3

> ωzk

¯ϕz ρ2

> k

. Thus, we get 

Tk ≥ μ2

> G

2( μG + 4) ¯ϕz ρ2 

> k

− 2ω2 

> v, 1

¯ϕv ρ2 

> k

− ω3μG

2 ¯ϕz ρ3

> k

=

 μ2

> G

2( μG + 4) ¯ϕz − 2ω2 

> v, 1

¯ϕv − ω3μG

2 ¯ϕz ρk



ρ2

> k

.

Setting ¯ϕv = μ2

> G
> 8( μG+4) ω2
> v, 1

¯ϕz , we have 

Tk ≥

 μ2

> G

2( μG + 4) ¯ϕz − 2ω2 

> v, 1

¯ϕv − ω3μG

2 ¯ϕz ρk



ρ2

> k

=

 μ2

> G

4( μG + 4) ¯ϕz − ω3μG

2 ¯ϕz ρk



ρ2

> k

.

Notice that for ρk < μG 

> 2ω3(μG+4)

,

Tk >

 μ2

> G

4( μG + 4) ¯ϕz − ω3μG

2 ¯ϕz ρk



ρ2

> k

>

 μ2

> G

4( μG + 4) ¯ϕz − μ2

> G

8( μG + 4) ¯ϕz



ρ2

> k

= μ2

> G

8( μG + 4) ¯ϕz ρ2

> k

| {z }

> =: ¯Tk

> 0.

65 Thus, for γk < μ2 

> G
> 16 ω2
> v, 1(μG+4)

¯ϕz ρ2 

> k

we have 

Tk − 2ω2

> v, 1

γk > ¯Tk − 2ω2

> v, 1

γk > 0.

This implies that 

−



Tk − 2ω2

> v, 1

γk



≤ 0.

Thus, for 

ρk < min 



1, μG

2ω3(μG + 4) , μG

2 ¯Cv



,γk < min 

 μ2

> G

16 ω2

> v, 1

(μG + 4) ¯ϕz ρ2

> k

,

−2L21,G +

q

4L41,G + 8 ωdx ¯CxAρ 2

> k

4ωdx ¯Cx

 ,h2 

> k

≤ min b1ℓ1

4L22,G (p + 16) 4



1 + 15( p + 6) 2p

2b1ℓ1



L21,G , μ2

> G

16 ωv, 2

, 116 L22,G (( p + 8) 4 + p(d + 12) 3)

!

,

we have 

Ek [Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2 + ¯Ck.

Notice that, for ρk ≤ ¯ρ < min 



1, μG 

> 2ω3(μG+4)

, μG

> 2 ¯Cv



, taking ¯ϕz = O( 1¯ρ ), there exists cγ > 0 such that γk ≤ cγ ρk <

min 

 μ2 

> G
> 16 ω2
> v, 1(μG+4)

¯ϕz ρ2

> k

, −2L21,G +√4L41,G +8 ωdx ¯CxAρ 2 

> k
> 4ωdx ¯Cx



. Taking the full expectation and summing for k = 1 , · · · , K ,we get 

> K

X

> k=0

γk

2 E

h

∥∇ Ψ( xk)∥2i

≤ E[L0 − LK+1 ] + 

> K

X

> k=0

¯Ck

= E[Ψ( x0) + ϕ0 

> z

δz0 + ϕ0 

> v

δv0 −Ψ( xK+1 )

| {z } 

> ≤− min Ψ

−ϕK+1  

> z

δzK+1 − ϕK+1  

> v

δvK+1 

| {z }

> ≤0

] + 

> K

X

> k=0

¯Ck

≤ [Ψ( x0) − min Ψ] + ϕ0 

> z

δz0 + ϕ0 

> v

δv0 +

> K

X

> k=0

¯Ck.

Dividing both sides by KP

> k=0

γk, we get the claim i.e. 

1

> K

P

> k=0

γkKX

> k=0

γk

2 E

h

∥∇ Ψ( xk)∥2i

≤ [Ψ( x0) − min Ψ] 

> K

P

> k=0

γk

+ ϕ0 

> z

δz0 + ϕ0 

> v

δv0

> K

P

> k=0

γk

+

> K

P

> k=0

¯CkKP

> k=0

γk

.

Proof of Corollary 3.5 

By Theorem 3.4, we have 

1

> K

P

> k=0

γkKX

> k=0

γk

2 E

h

∥∇ Ψ( xk)∥2i

≤ [Ψ( x0) − min Ψ] 

> K

P

> k=0

γk

+ ϕ0 

> z

δz0 + ϕ0 

> v

δv0

> K

P

> k=0

γk

+

> K

P

> k=0

¯CkKP

> k=0

γk

.

66 Recalling that 

¯Ck = C21 γkh2 

> k

+ Ck 

> 2

ρkh2 

> k

+ C4ϕkz ρ2

> k

h2 

> k

+ C3ϕkz ρ2 

> k

+ 2 ϕkv Ck 

> 6

(hk)ρ2 

> k

+ ωkdx Ck 

> 5

(hk)γ2 

> k

.

where, 

Ck 

> 2

:= L21,G 

4μG

(d + 3) 3ϕkz + 2 L21,G (d + 3) 3

μG

ϕkv + 4 ωv, 2L20,F 

μ3

> G

ϕkv

!

C3 := 4 



3 + p + 2 

ℓ1

 σ21,G 

b1

C4 := 2 

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G 

Ck 

> 5

(hk) := 



2Ckx

L20,F 

μ2

> G

+ 2 

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F h2

> k

+ 4 



1 + d + 2 

b2ℓ2



L20,F + 4 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F 



Ck 

> 6

(hk) := 



Ckv

L20,F 

μ2

> G

+ 4 



1 + p + 2 

b2ℓ2



L20,F + 4 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

+ 2 

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F h2

> k



.

(50) Recalling that μGρk

4 > 1

ωzk

= μGρk

μGρk + 4 and μGρk

2 > 1

ωvk

= μGρk

μGρk + 2 .

Let 

ϕkz = ¯ϕz

ωzk

and ϕkv = ¯ϕv

ωvk

.

By the choice of ϕkv , ϕ kz we have 

ωkdx = LΨ

2 + 2 L2

> ∗

( ¯ϕz + ¯ϕv ) =: ωdx .

Since h2 

> k

≤ min( 116 L22,G (( p+8) 4+p(d+12) 3) , b1ℓ1

> 4L22,G (p+16) 4



1 + 15( p+6) 2p

> 2b1ℓ1



L21,G , μ2 

> G
> 16 ωv, 2

), we have, 

Ckv = 8 



1 + 15( p + 6) 2p

2b1ℓ1



L21,G =: ¯Cv ,

and 

Ckx = 2 

 1

b1ℓ1

 12 + 6( p + 4)( p + 2) p + 36( p + 2) min( p, d ) + 30 p(d + 2) dL 21,G 



+ L21,G 



=: ¯Cx.

67 By the choice of parameters (I), we bound the terms in eq. (50) as 

C21 γkh2 

> k

= C21 γh 2, C3ϕkz ρ2 

> k

< C 3

μG

4 ¯ϕz

| {z }

> =: C3,1

ρ3, C4ϕkz ρ2

> k

h2 

> k

< C 4 ¯ϕz

μG

4| {z }

> =: C4,1

ρ3h2

Ck 

> 2

ρkh2 

> k

< L21,G 

16 (d + 3) 3 ¯ϕz + L21,G (d + 3) 3 ¯ϕv + 2ωv, 2L20,F 

μ2

> G

¯ϕv

!| {z }

> =: C2

ρh 2

C5(hk) < 2 ¯Cx

L20,F 

μ2

> G

ωdx γ2 + 2 

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F ωdx h2γ2

+ 4 



1 + d + 2 

b2ℓ2



L20,F ωdx γ2 + 4 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F ωdx γ2

=



2 ¯Cx

L20,F 

μ2

> G

+ 4 



1 + d + 2 

b2ℓ2



L20,F + 4 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F 



ωdx 

| {z }

> C5,1

γ2

+ 2 

 (d + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F ωdx 

| {z }

> C5,2

h2γ2

C6(hk) <



¯Cv

L20,F 

μG

+ 4 



1 + p + 2 

b2ℓ2



L20,F μG + 4 

 p + 2 

ℓ2

+ 3 

 μGσ21,F 

b2

| {z }

> C6,1

ρ3

+ 2 μG

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F 

| {z }

> C6,2

h2ρ3

Using these bound, we get the claim 

1

K + 1 

> K

X

> k=0

E

h

∥∇ Ψ( xk)∥2i

≤ [Ψ( x0) − min Ψ] + ϕ0 

> z

δz0 + ϕ0 

> v

δv0

γ(K + 1) +



C21 + C2

ργ



h2

+  C3,1 + C4,1h2 + C6,1 + C6,2h2 ρ3

γ +  C5,1 + C5,2h2 γ. 

By the choice of parameters (II), we have γk = γ(k + 1) −α1 , ρk = ρ(k + 1) −α2 and hk = h(k + 1) −α3 . Thus, we bound the terms in eq. (50) as 

C21 γkh2 

> k

= C21 γh 2(k + 1) −(α1+2 α3), C3ϕkz ρ2 

> k

≤ C3μG

4 ¯ϕz ρ3(k + 1) −3α2

C4ϕkz ρ2

> k

h2 

> k

≤ C4μG

4 ¯ϕz ρ3h2(k + 1) −(3 α1+2 α3), Ck 

> 2

ρkh2 

> k

≤ C2ρh 2(k + 1) −(α2+2 α3)

C5(hk)ωkdx γ2 

> k

≤ C5,1γ2(k + 1) −2α1 + C5,2h2γ2(k + 1) −2( α1+α3)

2ϕkv C6(hk)ρ2 

> k

≤ C6,1ρ3(k + 1) −3α2 + C6,2h2ρ3(k + 1) −(3 α2+2 α3).

Since α1, α 2 ∈ (1 /2, 1) and α3 > 1, we have 

> K

X

> k=0

γk ≤ (K + 1) 1−α1 − 11 − α1

γ and 

> K

X

> k=0

ρk ≤ (K + 1) 1−α2 − 11 − α2

ρ. 

68 Moreover, we get 

> K

X

> k=0

C21 γkh2 

> k

= C21 γ2h2

> K

X

> k=0

(k + 1) −(α1+2 α3) ≤ C21 γ2h2 α1 + 2 α3

α1 + 2 α3 − 1 ,

> K

X

> k=0

Ck 

> 2

ρkh2 

> k

≤ C2ρh 2

> K

X

> k=0

(k + 1) −(α2+2 α3) ≤ C2ρh 2 α2 + 2 α3

α2 + 2 α3 − 1 ,

> K

X

> k=0

C3ϕkz ρ2 

> k

≤ C3μG

4 ¯ϕzKX

> k=0

ρ3 

> k

≤ C3μG

4 ¯ϕz ρ3 3α2

3α2 − 1 ,

> K

X

> k=0

C4ϕkz ρ2

> k

h2 

> k

≤ C4μG

4 ¯ϕz ρ3h2

> K

X

> k=0

(k + 1) −(3 α1+2 α3) ≤ C4μG

4 ¯ϕz ρ3h2 3α1 + 2 α3

3α1 + 2 α3 − 1 ,

> K

X

> k=0

C5(hk)ωkdx γ2 

> k

≤ C5,1γ2

> K

X

> k=0

(k + 1) −2α1 + C5,2h2γ2

> K

X

> k=0

(k + 1) −2( α1+α3)

≤ C5,1γ2 2α1

2α1 − 1 + C5,2h2γ2 2( α1 + α3)2( α1 + α3) − 12

> k

X

> k=0

ϕkv C6(hk)ρ2 

> k

≤ C6,1ρ3

> k

X

> k=0

(k + 1) −3α2 + C6,2h2ρ3

> k

X

> k=0

(k + 1) −(3 α2+2 α3)

≤ C6,1ρ3 3α2

3α2 − 1 + C6,2h2ρ3 3α2 + 2 α3

3α2 + 2 α3 − 1 .

Therefore, using these bounds, we get the claim 

1

> K

P

> k=0

γkKX

> k=0

γkE

h

∥∇ Ψ( xk)∥2i

≤ [Ψ( x0) − min Ψ] + ϕ0 

> z

δz0 + ϕ0 

> v

δv0

γ(K + 1) 1−α1 + O

 1(K + 1) 1−α1



.

Finally, to prove the last claim, by choosing γk = γ, ρk = ρ and hk = h for every k ∈ N, we have 

1

K + 1 

> K

X

> k=0

E

h

∥∇ Ψ( xk)∥2i

≤ [Ψ( x0) − min Ψ] + ϕ0 

> z

δz0 + ϕ0 

> v

δv0

γ(K + 1) +



C21 + C2

ργ



h2

+  C3 + C4h2 μG ¯ϕz

ρ3

γ +  C5,1 + C5,2h2 γ +  C6,1 + C6,2h2 ρ2

γ .

Rearranging the terms and by ϕkv = ¯ϕv

> ωvk

, ϕkz = ¯ϕz

> ωzk

and ¯ϕv = μ2

> G
> 8( μG+4) ω2
> v, 1

¯ϕz we have 

1

K + 1 

> K

X

> k=0

E

h

∥∇ Ψ( xk)∥2i

≤ 1

γ(K + 1) [Ψ( x0) − min Ψ] + ¯ϕz

ωz

> 0

δz0 + μ2

> G

8( μG + 4) ω2

> v, 1

ωv

> 0

¯ϕz δv0

!

+



C21 + C2

ργ + C5,2ωdx γ + C4μG ¯ϕz

ρ3

γ + C6,2

ρ2

γ



h2

+ C3μG ¯ϕz

ρ3

γ + C5,1γ + C6,1

ρ2

γ .

Due to the conditions on ρk, we have 

μG

4 > 1

ωzk

= μGρk

μGρk + 4 and μG

2 > 1

ωvk

= μGρk

μGρk + 2 .

69 Then, we get 

1

K + 1 

> K

X

> k=0

E

h

∥∇ Ψ( xk)∥2i

≤ 1

γ(K + 1) [Ψ( x0) − min Ψ] + ¯ϕz μ2

> G

4 δz0 + μ3

> G

16( μG + 4) ω2

> v, 1

¯ϕz δv0

!| {z }

> =:∆ 1

+



C21 + C2

ργ + C5,2ωdx γ + C4μG ¯ϕz

ρ3

γ + C6,2

ρ2

γ



h2

+ C3μG ¯ϕz

ρ3

γ + C5,1γ + C6,1

ρ2

γ .

Let ε ∈ (0 , 1) . Consider the following inequality, 

∆1

γ(K + 1) + C(γ, ρ )h2 +



C3μG ¯ϕz

ρ3

γ + C5,1γ + C6,1

ρ2

γ



≤ ε. 

Since the term with C(γ, ρ ) can be controlled with h, we choose γ by minimizing ∆1

> γ(K+1)



C3μG ¯ϕz ρ3 

> γ

+ C5,1γ + C6,1 ρ2

> γ



i.e. 

γ =

s

1

C5,1

 ∆1

K + 1 + C3μG ¯ϕz ρ3 + C6,1ρ2



.

Thus, we get 

2

s

C5,1

 ∆1

(K + 1) + C3μG ¯ϕz ρ3 + C6,1ρ2



+ C(γ, ρ )h2 ≤ ε. 

Let h = q ε 

> 2C(γ,ρ )

, we get 

2

s

C5,1

 ∆1

(K + 1) + C3μG ¯ϕz ρ3 + C6,1ρ2



≤ ε

2 .

Multplying both sides by 1/2, we have 

∆1

(K + 1) ≤ ε2

4C5,1

− (C3μG ¯ϕz ρ + C6,1)ρ2.

Choosing ρ = ε√16 C5,1C6,1

and ¯ϕz = C6,1

√16 C5,1C6,1 

> C3μGε

= C6,1 

> C3μGρ

, we get 

∆1

(K + 1) ≤ ε2

8C5,1

.

Therefore, choosing 

(K + 1) ≥ 8C5,1

∆1

ε−2,

we get 1

> K+1
> K

P

> k=0

E

h

∥∇ Ψ( xk)∥2i

≤ ε. Choosing b1ℓ1 = ⌊O (p(d + p)2/c 1)⌋ and b2ℓ2 = ⌊O (p(p + d)2/c 2)⌋ for 

c1, c 2 ∈ R+, the dependence on the dimensions p, d inside the constants (and in the condition on the stepsizes 

ρ, γ simplifies, yielding the claim (III) i.e. the complexity is 

O  p(d + p)2ε−2 .

70 D.1 Proof of Theorem 3.6 

Let δzk = ∥zk − z∗(xk)∥2 and δvk = ∥vk − v∗(xk)∥2. For ϕkz , ϕ kv > 0 s.t. ϕk+1  

> v

≤ ϕkv and ϕk+1  

> z

≤ ϕkz , define 

Lk = Ψ( xk) + ϕkz δzk + ϕkv δvk .

Thus, we have 

Lk+1 − Lk ≤ Ψ( xk+1 ) − Ψ( xk) + ϕkz

 δzk+1 − δzk

 + ϕkv

 δvk+1 − δvk

 .

Taking the conditional expectation, 

Ek[Lk+1 ] − Lk ≤ Ek[Ψ( xk+1 )] − Ψ( xk) + ϕkz

 Ek[δzk+1 ] − δzk

 + ϕkv

 Ek[δvk+1 ] − δvk

 .

Let 

C1,Ψ = L1,F + L0,F L1,G 

μG

,C2,Ψ = 2L2,G L0,F 

3μG



(d + 5) 5/2 + ( d + 3) 3/2 + ( d + 1)( p + 3) 3/2

+ L1,F d√p

2

C3,Ψ = L22,G L20,F 

2μ2

> G

and C4,Ψ = 4 L21,G + μ2

> G

16 .

By Lemma C.19, 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2 + LΨ

2 γ2 

> k

Ek[∥ ˆDkx∥2]+ 4 C21,Ψγkδzk + C4,Ψγkδvk

+ ϕkz

 Ek[δzk+1 ] − δzk

 + ϕkv

 Ek[δvk+1 ] − δvk



+ 2 C22,Ψγkh2 

> k

+ C3,Ψˆh2

> k

γk.

Notice that the bounds on δzk of Lemma C.13 holds also for Algorithm 2. Let 

C1 = 2 L21,G (d + 3) 2

μG

+ 4 ωv, 2L20,F 

μ3

> G

and C2 = 8

μG

L22,G 

4

L20,F 

μ2

> G

,C3 = L21,G 

4μG

(d + 3) 3.

Thus, by Lemma C.13 and Lemma C.18, we get 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

−

 μG

2 ϕkv ρk − C4,Ψγk



δvk

− μG

2 ϕkz ρk − 4 ω2

> v, 1

μG

ϕkv ρk − 4C21,Ψγk

!

δzk

+ 2 ϕkz ρ2

> k

Ek



ˆDkz

> 2



+ 2 ϕkv ρ2

> k

Ek[∥ ˆDkv ∥2]+



2ωvk L2

> ∗

ϕkv + LΨ

2 + 2 ωzk L2

> ∗

ϕkz



γ2 

> k

Ek[∥ ˆDkx∥2]+ ( C1h2 

> k

+ C2ˆh2

> k

)ϕkv ρk + 2 C22,Ψγkh2 

> k

+ C3,Ψˆh2

> k

γk

+ C3ϕkz ρkh2

> k

.

71 Observing that by Lemma C.17, we have 

Ek



ˆDkz

> 2



≤ 2



2 + p + 2 

b1ℓ1

| {z }

> =: CA, 1

∥∇ z G(zk, x k)∥2

+

 (p + 6) 3

2b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G 

| {z }

> =: CA, 2

h2

> k

+ 2 



3 + p + 2 

ℓ1

 σ21,G 

b1

| {z }

> CA, 3

Since ∇z G(z∗(xk), x k) = 0 , by L1,G -smoothness we have 

Ek



ˆDkz

> 2



≤ CA, 1 ∥∇ z G(zk, x k) − ∇ z G(z∗(xk), x k)∥2 + CA, 2h2 

> k

+ CA, 3

≤ CA, 1L21,G δzk + CA, 2h2 

> k

+ CA, 3.

Thus, we have 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

−

 μG

2 ϕkv ρk − C4,Ψγk



δvk

− μG

2 ϕkz ρk − 4 ω2

> v, 1

μG

ϕkv ρk − 4C21,Ψγk − 2ϕkz ρ2

> k

CA, 1L21,G 

!

δzk

+ 2 ϕkv ρ2

> k

Ek[∥ ˆDkv ∥2]+



2ωvk L2

> ∗

ϕkv + LΨ

2 + 2 ωzk L2

> ∗

ϕkz



γ2 

> k

Ek[∥ ˆDkx∥2]+ ( C1h2 

> k

+ C2ˆh2

> k

)ϕkv ρk + 2 C22,Ψγkh2 

> k

+ C3,Ψˆh2

> k

γk

+ C3ϕkz ρkh2 

> k

+ 2 CA, 2ϕkz ρ2

> k

h2

> k

+ 2 ϕkz ρ2

> k

CA, 3.

Let CB, 1 = 4 



1 + 3( p+2) 

> b1ℓ1



, CB, 2 = 2 



1 + p+2 

> b2ℓ2



, CB, 3 =

 (p+6) 3 

> 2b2ℓ2

+

 3 

> b2

+ 1 



(p + 3) 3



L21,F and CB, 4 =2

 p+2  

> ℓ2

+ 3 

 σ21,F  

> b2

. Let CC, 1 = 4 



1 + 3( d+2) 

> b1ℓ1



, CC, 2 = 2 



1 + d+2 

> b2ℓ2



, CC, 3 =

 (d+6) 3 

> 2b2ℓ2

+

 3 

> b2

+ 1 



(d + 3) 3



L21,F 

and CC, 4 = 2 

 d+2  

> b2ℓ2

+ 3

> b2



σ21,F . Let 

Sk = 2 ωvk L2

> ∗

ϕkv + LΨ

2 + 2 ωzk L2

> ∗

ϕkz .

72 By Lemma C.17, we have 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

−

 μG

2 ϕkv ρk − C4,Ψγk − 4CB, 1L21,G ϕkv ρ2 

> k

− 2CC, 1L21,G Skγ2

> k



δvk

− μG

2 ϕkz ρk − 4 ω2

> v, 1

μG

ϕkv ρk − 4C21,Ψγk − 2ϕkz ρ2

> k

CA, 1L21,G 

!

δzk

+ ( C1h2 

> k

+ C2ˆh2

> k

)ϕkv ρk + 2 C22,Ψγkh2 

> k

+ C3,Ψˆh2

> k

γk

+ 2 CB, 1

L21,G L20,F 

μ2

> G

ϕkv ρ2

> k

+ C3ϕkz ρkh2 

> k

+ 2 CA, 2ϕkz ρ2

> k

h2

> k

+ 2 CC, 1

L21,G L20,F 

μ2

> G

Skγ2

> k

+ 2 ϕkz ρ2

> k

CA, 3

+

 12 L21,G (p + 6) 3

b1ℓ1

ϕkv ρ2 

> k

+ 6L21,G (d + 6) 3

b1ℓ1

Skγ2

> k

| {z }

> CL,k

h2

> k

¯h2

> k

+ (2 CC, 2L20,F + 2 CC, 3h2 

> k

+ 2 CC, 4)Skγ2

> k

+ 2 ϕkv ρ2

> k

(2 CB, 2L20,F + 2 CB, 3h2 

> k

+ 2 CB, 4).

Recall that 

¯hk =

( ˆhk  

> ∥vk∥

if ∥vk∦ = 0 ˆhk

.

Therefore, for every k ∈ N, for which ∥vk∦ = 0 , we have 

CL,k 

h2

> k

¯h2

> k

= CL,k 

h2

> k

ˆh2

> k

∥vk∥2

≤ 2CL,k 

h2

> k

ˆh2

> k

∥vk − v∗(xk)∥2 + 2 CL,k 

h2

> k

ˆh2

> k

∥v∗(xk)∥2

≤ 2CL,k 

h2

> k

ˆh2

> k

∥vk − v∗(xk)∥2 + 2L20,F 

μ2

> G

CL,k 

h2

> k

ˆh2

> k

,

where the last inequality follows by Lemma C.3. For every k ∈ N for which ∥vk∥ = 0 , we have instead 

CL,k 

h2

> k

¯h2

> k

= CL,k 

h2

> k

ˆh2

> k

.

Let θk =

( 2L20,F 

> μ2
> G

if ∥vk∦ = 0 1 < 1 + 2L20,F  

> μG

=: θ. Thus, for every k ∈ N we have 

CL,k 

h2

> k

¯h2

> k

≤ 2CL,k 

h2

> k

ˆh2

> k

∥vk − v∗(xk)∥21∥vk ∦ =0 + θkCL,k 

h2

> k

ˆh2

> k

≤ 2CL,k 

h2

> k

ˆh2

> k

∥vk − v∗(xk)∥2 + θC L,k 

h2

> k

ˆh2

> k

,

73 where the last inequality follows from the fact that 2CL,k h2

> k

ˆh2

> k

∥vk − v∗(xk)∥2 ≥ 0. Therefore, we get 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

− μG

2 ϕkv ρk − C4,Ψγk − 4CB, 1L21,G ϕkv ρ2

k − 2CC, 1L21,G Skγ2

k − 2CL,k 

h2

k

ˆh2

k

!

δvk

− μG

2 ϕkz ρk − 4 ω2

v, 1

μG

ϕkv ρk − 4C21,Ψγk − 2ϕkz ρ2

kCA, 1L21,G 

!

δzk

+ ( C1h2

k + C2ˆh2

k)ϕkv ρk + 2 C22,Ψγkh2

k + C3,Ψˆh2

kγk

+ 2 CB, 1

L21,G L20,F 

μ2

G

ϕkv ρ2

k

+ C3ϕkz ρkh2

k + 2 CA, 2ϕkz ρ2

kh2

k

+ 2 CC, 1

L21,G L20,F 

μ2

G

Skγ2

k

+ θC L,k 

h2

k

ˆh2

k

+ (2 CC, 2L20,F + 2 CC, 3h2

k + 2 CC, 4)Skγ2

k

+ 2 ϕkv ρ2

k(2 CB, 2L20,F + 2 CB, 3h2

k + 2 CB, 4).

Let ˆCk = ( C1h2

k + C2ˆh2

k)ϕkv ρk + 2 C22,Ψγkh2

k + C3,Ψˆh2

kγk

+ 2 CB, 1

L21,G L20,F 

μ2

G

ϕkv ρ2

k + C3ϕkz ρkh2

k + 2 CA, 2ϕkz ρ2

kh2

k + 2 CC, 1

L21,G L20,F 

μ2

G

Skγ2

k

+ 2 ϕkz ρ2

kCA, 3 + 12 L21,G 

b1ℓ1

ϕkv ρ2

k + 6L21,G 

b1ℓ1

Skγ2

k + θC L,k 

h2

k

ˆh2

k

+ (2 CC, 2L20,F + 2 CC, 3h2

k + 2 CC, 4)Skγ2

k + 2 ϕkv ρ2

k(2 CB, 2L20,F + 2 CB, 3h2

k + 2 CB, 4).

(51) Then, 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

− μG

2 ϕkv ρk − C4,Ψγk − 4CB, 1L21,G ϕkv ρ2

k − 2CC, 1L21,G Skγ2

k − 2CL,k 

h2

k

ˆh2

k

!

δvk

− μG

2 ϕkz ρk − 4 ω2

v, 1

μG

ϕkv ρk − 4C21,Ψγk − 2ϕkz ρ2

kCA, 1L21,G 

!

δzk

+ ˆCk.

Since ρk ≤ min(1 , 116 CB, 1L21,G 

, 14CA, 1L21,G 

), we have 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

− μG

4 ϕkv ρk − C4,Ψγk − 2CC, 1L21,G Skγ2

k − 2CL,k 

h2

k

ˆh2

k

!

δvk

− μG

4 ϕkz − 4 ω2

v, 1

μG

ϕkv

!

ρk − 4C21,Ψγk

!

δzk

+ ˆCk.

74 Let 

ˆS := LΨ + 4( ¯ϕv + ¯ϕz )L2

> ∗

2 .

Choosing ϕkz = ¯ϕz /ω zk and ϕkv = ¯ϕv /ω vk . we have 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

− μG

4¯ϕv

ωvk

ρk − C4,Ψγk − 2CC, 1L21,G ˆSγ 2 

> k

− 2CL,k 

h2

> k

ˆh2

> k

!

δvk

− μG

4¯ϕz

ωzk

− 4 ω2

> v, 1

μG

¯ϕv

ωvk

!

ρk − 4C21,Ψγk

!

δzk

+ ˆCk.

Let 

γ(1)  

> max

= −C4,Ψ +

q

C24,Ψ + CC, 1L21,G ˆS μ2 

> G
> μG+2

¯ϕv ρ2

> k

4CC, 1L21,G ˆS ,γ(2)  

> max

= μ2

> G

32 C21,Ψ(μG + 4) ¯ϕz ρ2

> k

.

Notice that since ρk ≤ ¯ρ < min(1 , 116 CB, 1L21,G 

, 14CA, 1L21,G 

), and γk ≤ ¯γ < min( γ(1) 

> max

, γ (2) 

> max

), we have that for every 

k ∈ N, μGρk

4 > 1

ωzk

= μGρk

μGρk + 4 > μGρk

μG + 4 

μGρk

2 > 1

ωvk

= μGρk

μGρk + 2 > μGρk

μG + 2 ,

and 

CL,k =

 12 L21,G (p + 6) 3

b1ℓ1

ϕkv ρ2 

> k

+ 6L21,G (d + 6) 3

b1ℓ1

Skγ2

> k



≤

 6L21,G (p + 6) 3

b1ℓ1

¯ϕv μG ¯ρ2 + 6L21,G (d + 6) 3

b1ℓ1

ˆS¯γ2



=: ¯CL

Therefore, we have 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

− μ2

> G

4( μG + 2) ¯ϕv ρ2 

> k

− C4,Ψγk − 2CC, 1L21,G ˆSγ 2 

> k

− 2 ¯CL

h2

> k

ˆh2

> k

!

δvk

−

 μ2

> G

4( μG + 4) ¯ϕz − 2ω2 

> v, 1

¯ϕv



ρ2 

> k

− 4C21,Ψγk



δzk

+ ˆCk.

Since hk ≤ μG

> 4

√(μG+2) ¯CL

p ¯ϕv ˆhk, we get 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

−

 μ2

> G

8( μG + 2) ¯ϕv ρ2 

> k

− C4,Ψγk − 2CC, 1L21,G ˆSγ 2

> k



δvk

−

 μ2

> G

4( μG + 4) ¯ϕz − 2ω2 

> v, 1

¯ϕv



ρ2 

> k

− 4C21,Ψγk



δzk

+ ˆCk.

75 Notice that for μ2

> G

8ω2

> v, 1

(μG + 4) ¯ϕz > ¯ϕv ,

we have that  μ2

> G

4( μG + 4) ¯ϕz − 2ω2 

> v, 1

¯ϕv



> 0.

Thus, due to the choice ¯ϕv = μ2 

> G
> 16 ω2
> v, 1(μG+4)

¯ϕz , we have 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2

−

 μ2

> G

8( μG + 2) ¯ϕv ρ2 

> k

− C4,Ψγk − 2CC, 1L21,G ˆSγ 2

> k



δvk

−

 μ2

> G

8( μG + 4) ¯ϕz ρ2 

> k

− 4C21,Ψγk



δzk

+ ˆCk.

For γk < γ (2) 

> max

, we have 

−

 μ2

> G

8( μG + 4) ¯ϕz ρ2 

> k

− 4C21,Ψγk



< 0.

For γk < γ (1) 

> max

, we have 

−

 μ2

> G

8( μG + 2) ¯ϕv ρ2 

> k

− C4,Ψγk − 2CC, 1L21,G ˆSγ 2

> k



< 0.

Thus, for γk < min( γ(1) 

> max

, γ (2) 

> max

) and ρk < min(1 , 116 CB, 1L21,G 

, 14CA, 1L21,G 

), we have 

Ek[Lk+1 ] − Lk ≤ − γk

2 ∥∇ Ψ( xk)∥2 + ˆCk.

Notice that, denoting ¯ρ such that ρk ≤ ¯ρ < min(1 , 116 CB, 1L21,G 

, 14CA, 1L21,G 

) and taking ¯ϕz = O( 1¯ρ ) then there exists 

¯cγ > 0 s.t. γk < ¯cγ ρk. Taking the full expectation and summing for k = 0 , · · · , K , we get 

E[LK+1 ] − L0 ≤ − 

> K

X

> k=0

γk

2 E ∥∇ Ψ( xk)∥2 +

> K

X

> k=0

ˆCk.

Rearranging the terms, and observing that − min Ψ ≥ − E[Ψ( xK+1 )] , we get 

> K

X

> k=0

γk

2 E ∥∇ Ψ( xk)∥2 ≤ L0 − E[LK+1 ] + 

> K

X

> k=0

ˆCk

= Ψ( x0) −E[Ψ( xK+1 )] 

| {z } 

> ≤− min Ψ

+ϕ0 

> z

∥z0 − z∗(x0)∥2 + ϕ0 

> v

∥v0 − v∗(x0)∥2

−ϕ0 

> z

E ∥zK+1 − z∗(xK+1 )∥2 − ϕ0 

> v

E ∥vK+1 − v∗(xK+1 )∥2| {z }

> ≤0

+

> K

X

> k=0

ˆCk.

Let 

Rinit = Ψ( x0) − min Ψ + ϕ0 

> z

∥z0 − z∗(x0)∥2 + ϕ0 

> v

∥v0 − v∗(x0)∥2.

76 Multiplying by 2 and dividing by KP

> k=0

γk in both sides, we get the claim 

1

> K

P

> k=0

γkKX

> k=0

γkE ∥∇ Ψ( xk)∥2 ≤ 2

> K

P

> k=0

γk

Rinit +

> K

X

> k=0

ˆCk

!

.

D.2 Proof of Corollary 3.7 

By Theorem 3.6, we have 

1

> K

P

> k=0

γkKX

> k=0

γk

2 E ∥∇ Ψ( xk)∥2 ≤ 1

> K

P

> k=0

γk

Rinit +

> K

X

> k=0

ˆCk

!

.

Recall that 

ˆCk = ( C1h2 

> k

+ C2ˆh2

> k

)ϕkv ρk + 2 C22,Ψγkh2 

> k

+ C3,Ψˆh2

> k

γk

+ 2 CB, 1

L21,G L20,F 

μ2

> G

ϕkv ρ2 

> k

+ C3ϕkz ρkh2 

> k

+ 2 CA, 2ϕkz ρ2

> k

h2 

> k

+ 2 CC, 1

L21,G L20,F 

μ2

> G

Skγ2

> k

+ 2 ϕkz ρ2

> k

CA, 3 + 12 L21,G 

b1ℓ1

ϕkv ρ2 

> k

+ 6L21,G 

b1ℓ1

Skγ2 

> k

+ θC L,k 

h2

> k

ˆh2

> k

+ (2 CC, 2L20,F + 2 CC, 3h2 

> k

+ 2 CC, 4)Skγ2 

> k

+ 2 ϕkv ρ2

> k

(2 CB, 2L20,F + 2 CB, 3h2 

> k

+ 2 CB, 4).

where θ = 1 + 2L20,F 

> μ2
> G

and 

C1 = 2 L21,G (d + 3) 2

μG

+ 4 ωv, 2L20,F 

μ3

> G

, C2 = 8

μG

L22,G 

4

L20,F 

μ2

> G

,C3 = L21,G 

4μG

(d + 3) 3, CA, 1 = 2 



2 + p + 2 

b1ℓ1



, CA, 2 =

 12b1ℓ1

+

 3

b1

+ 1 



(p + 3) 3



L21,G 

CA, 3 = 2 



3 + p + 2 

ℓ1

 σ21,G 

b1

, CB, 1 = 4 



1 + 3( p + 2) 

b1ℓ1



, CB, 2 = 2 



1 + p + 2 

b2ℓ2



CB, 3 =

 (p + 6) 3

2b2ℓ2

+

 3

b2

+ 1 



(p + 3) 3



L21,F , CB, 4 = 2 

 p + 2 

ℓ2

+ 3 

 σ21,F 

b2

CC, 1 = 4 



1 + 3( d + 2) 

b1ℓ1



, CC, 2 = 2 



1 + d + 2 

b2ℓ2



, CC, 3 =

 12b2ℓ2

+

 3

b2

+ 1 



(d + 3) 3



L21,F 

CC, 4 = 2 

 d + 2 

b2ℓ2

+ 3

b2



σ21,F , CL,k =

 12 L21,G (p + 6) 3

b1ℓ1

ϕkv ρ2 

> k

+ 6L21,G (d + 6) 3

b1ℓ1

Skγ2

> k



With the choice ϕkz = ¯ϕz /ω zk and ϕkv = ¯ϕv /ω vk , we have 

Sk = LΨ + 4( ¯ϕv + ¯ϕz )L2

> ∗

2 = ˆS. 

Moreover, due to the conditions on ρk, we have for every k ∈ N

μG

4 > μGρk

4 > 1

ωzk

= μGρk

μGρk + 4 > μGρk

μG + 4 

μG

2 > μGρk

2 > 1

ωvk

= μGρk

μGρk + 2 > μGρk

μG + 2 ,

77 and, thus, 

CL,k ≤ 6

 L21,G (p + 6) 3μG

b1ℓ1

¯ϕv ρ2 

> k

+ L21,G (d + 6) 3

b1ℓ1

ˆSγ 2

> k



Replacing γk, ρ k, h k and ˆhk with the parameter choice (I), we have 

1

K + 1 

> K

X

> k=0

E ∥∇ Ψ( xk)∥2 ≤ 2

γ(K + 1) Rinit +

> K

X

> k=0

ˆC

!

, (52) with 

ˆC = ( C2 ¯ϕv ρ2 + C3,Ψγ + 2 CC, 3 ˆSγ 2)ˆh2 + ( C1 ¯ϕv ρ2 + C3 ¯ϕz ρ2 + 2 C22,Ψγ + 2 CA, 2 ¯ϕz ρ3 + 2 CB, 3 ¯ϕv ρ3)h2

+ 2 CB, 1

L21,G L20,F 

μ2

> G

¯ϕv ρ3 + 2 CA, 3 ¯ϕz ρ3 + 12 L21,G 

b1ℓ1

¯ϕv ρ3

+ 6 θ

 L21,G (p + 6) 3μG

b1ℓ1

¯ϕv ρ2 + L21,G (d + 6) 3

b1ℓ1

ˆSγ 2

 h2

ˆh2

+ 2 CC, 1

L21,G L20,F 

μ2

> G

ˆSγ 2 + 6L21,G 

b1ℓ1

ˆSγ 2 + (2 CC, 2L20,F + 2 CC, 4) ˆSγ 2 + 2 ¯ϕv ρ3(2 CB, 2L20,F + 2 CB, 4).

Rearranging the terms, we get the first claim. The proof of the point (II) follows the same line of point (II) of proof of Corollary 3.5. Now, we prove the point (III). Recalling that ¯ϕv = μ2 

> G
> 16 ω2
> v, 1(μG+4)

¯ϕz , we have 

ˆC = ( C2 ¯ϕv ρ2 + C3,Ψγ + 2 CC, 3 ˆSγ 2)

| {z }

> C1(γ,ρ )

ˆh2 + ( C1 ¯ϕv ρ2 + C3 ¯ϕz ρ2 + 2 C22,Ψγ + 2 CA, 2 ¯ϕz ρ3 + 2 CB, 3 ¯ϕv ρ3)

| {z }

> C2(γ,ρ )

h2

+ 6 θ

 L21,G (p + 6) 3μG

b1ℓ1

¯ϕv ρ2 + L21,G (d + 6) 3

b1ℓ1

ˆSγ 2

| {z }

> C3(γ,ρ )

h2

ˆh2

+ 2CC, 1

L21,G L20,F 

μ2

> G

+ 6L21,G 

b1ℓ1

+ (2 CC, 2L20,F + 2 CC, 4)

!

ˆS

| {z }

> ˜C1

γ2

CB, 1

L21,G L20,F 

8ω2

> v, 1

(μG + 4) + 2 CA, 3 + 12 L21,G μ2

> G

16 ω2

> v, 1

(μG + 4) b1ℓ1

+ μ2

> G

(2 CB, 2L20,F + 2 CB, 4)8ω2

> v, 1

(μG + 4) 

!| {z }

> ˜C2

¯ϕz ρ3.

Therefore, plugging ˆC in eq. (52), we get 

1

K + 1 

> K

X

> k=0

E ∥∇ Ψ( xk)∥2 ≤ 2Rinit 

γ(K + 1) + ˜C1γ + ˜C2 ¯ϕz

ρ3

γ + C1(γ, ρ )

γ ˆh2 + C2(γ, ρ )

γ h2 + C3(γ, ρ )

γh2

ˆh2 .

By the choice h = ˆh2 and since ˆh < 1, we have 

1

K + 1 

> K

X

> k=0

E ∥∇ Ψ( xk)∥2 ≤ 2Rinit 

γ(K + 1) + ˜C1γ + ˜C2 ¯ϕz

ρ3

γ + C1(γ, ρ )

γ ˆh2 + C2(γ, ρ )

γ ˆh4 + C3(γ, ρ )

γ ˆh2

< 2Rinit 

γ(K + 1) + ˜C1γ + ˜C2 ¯ϕz

ρ3

γ + (C1(γ, ρ ) + C2(γ, ρ ) + C3(γ, ρ )) 

γ ˆh2.

Since ρ < ¯ρ < min(1 , 116 CB, 1L21,G 

, 116 CA, 1L21,G 

), and by the choice γ = ˆ cγ ρ, we have 

1

K + 1 

> K

X

> k=0

E ∥∇ Ψ( xk)∥2 < 2Rinit 

ˆcγ ρ(K + 1) + ˜C1 ˆcγ + ˜C2

ˆcγ

!

ρ + (C1(γ, ρ ) + C2(γ, ρ ) + C3(γ, ρ )) ˆcγ ρ ˆh2.

78 Moreover, we have 

C1(γ, ρ ) < C2

μ2

> G

16 ω2

> v, 1

(μG + 4) + C3,Ψ ˆcγ + 2 CC, 3 ˆSˆc2 

> γ

¯ρ

!| {z }

> ¯∆1

ρC2(γ, ρ ) < C1

μ2

> G

16 ω2

> v, 1

(μG + 4) + C3 + 2 C22,Ψ ˆcγ + 2 CA, 2 ¯ρ + CB, 3μ2

> G

8ω2

> v, 1

(μG + 4) ¯ρ

!| {z }

> ¯∆2

ρC3(γ, ρ ) < 6θ L21,G (p + 6) 3μ3

> G

16 ω2

> v, 1

(μG + 4) b1ℓ1

+ L21,G (d + 3) 3

b1ℓ1

ˆSˆcγ ¯ρ

!| {z }

> ¯∆3

ρ

Thus, we have 

1

K + 1 

> K

X

> k=0

E ∥∇ Ψ( xk)∥2 < 2Rinit 

ˆcγ ρ(K + 1) + ˜C1 ˆcγ + ˜C2

ˆcγ

!

ρ +

  ¯∆1 + ¯∆2 + ¯∆3



ˆcγ

ˆh2.

Let ε ∈ (0 , 1) . By the choice ˆh ≤ min(1 ,

r ˆcγ ε

> 2

( ¯∆1+ ¯∆2+ ¯∆3) ), we get 

1

K + 1 

> K

X

> k=0

E ∥∇ Ψ( xk)∥2 < 2Rinit 

ˆcγ ρ(K + 1) + ˜C1 ˆcγ + ˜C2

ˆcγ

!

ρ + ε

2 .

Consider the following inequality 

2Rinit 

ˆcγ ρ(K + 1) + ˜C1 ˆcγ + ˜C2

ˆcγ

!

ρ ≤ ε

2 .

We choose ρ by minimizing the left handside i.e. 

ρ =

s

2Rinit 

¯cγ ¯∆4(K + 1) 

Thus, we get 

2

s

2Rinit ¯∆4

ˆcγ (K + 1) < ε

2 .

Hence, for 

K + 1 > 32 Rinit ¯∆4

ˆcγ

ε−2,

we have 1

> K+1
> k

P

> k=0

E ∥∇ Ψ( xk)∥2 < ε . Therefore, choosing b1ℓ1 = O(⌊ d+pc′

> 1

⌋) and b2ℓ2 = O(⌊ d+pc′

> 2

⌋) with 

c′

> 1

, c ′ 

> 2

∈ R+, we get that the complexity is 

O  (d + p)ε−2 .

79