Title: MAR: Efficient Large Language Models via Module-aware Architecture Refinement

URL Source: https://arxiv.org/pdf/2601.21503v1

Published Time: Fri, 30 Jan 2026 01:46:25 GMT

Number of Pages: 5

Markdown Content:
# MAR: EFFICIENT LARGE LANGUAGE MODELS VIA MODULE-AWARE ARCHITECTURE REFINEMENT 

# Junhong Cai 1, Guiqin Wang 2,3 , Kejie Zhao 1, Jianxiong Tang 4, Xiang Wang 5,Luziwei Leng 5, Ran Cheng 6, Yuxin Ma *1 , Qinghai Guo *5 

1Department of CSE, Southern University of Science and Technology , Shenzhen, China 

2School of Computer Science and Technology, Xi’an Jiaotong University , Xi’an, China 

3National Engineering Laboratory for Big Data Analytics, Xi’an Jiaotong University , Xi’an, China 

4Department of Computer Science, City University of Hong Kong , Hong Kong, China 

5ACS Laboratory, Huawei Technologies Co., Ltd. , Shenzhen, China 

6Department of Data Science and Artificial Intelligence, Department of Computing, Hong Kong Polytechnic University , Hong Kong, China 

ABSTRACT 

Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information den-sity and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neu-ron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consump-tion. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs. 

Index Terms — Efficient Large Language Models, Spiking neu-ral networks, Knowledge distillation, Linear attention 

1. INTRODUCTION 

In recent years, Large Language Models (LLMs) [1, 2, 3] have shown remarkable generalization and adaptability across diverse domains [4, 5]. However, their massive parameter scales and high computational costs hinder both development and deployment. To mitigate these issues, research has pursued two main direc-tions: (1) model compression techniques, such as quantization and knowledge distillation, to reduce energy consumption without alter-ing architectures [6, 7, 8]; and (2) architectural innovations to lower sequence modeling complexity, including FlashAttention variants [9, 10] and State Space Models (SSMs) [11, 12]. Yet, most efforts target the quadratic cost of attention [13], while in non-extremely long sequences, Feed-Forward Networks (FFNs) often dominate en-ergy consumption (see Figure 1). Thus, optimizing FFNs is equally critical for efficiency. 

> *

The corresponding authors are Yuxin Ma (mayx@sustech.edu.cn) and Qinghai Guo (guoqinghai@huawei.com). 0 2000 4000 6000 8000 10000 

> Sequence Length (L)
> 0
> 5
> 10
> 15
> 20
> 25
> 30
> 35
> 40
> Total Energy Consumption (J)
> LLaMA-Attention
> LLaMA-FFN
> Llamba-Discrete Mamba2
> Crossover at L=2470
> Crossover at L=9330

Fig. 1 . Energy consumption of LLaMA-3.2 Attention/FFN versus Llamba’s [14] Discrete Mamba-2. Crossovers occur at sequence lengths 2470 and 9330. Within architectural optimization, bio-inspired Spiking Neural Networks (SNNs) [15, 16] have emerged as a promising solution to alleviate the computational burden of linear layers and reduce FFN energy consumption. Prior studies have shown that incorporat-ing SNNs into Transformer architectures to sparsify activations can achieve encouraging results [17]. However, extending this approach to SSMs remains non-trivial, primarily due to two challenges: (1) a temporal mismatch between the continuous dynamics of SSMs and the discrete nature of spike events, and (2) reduced information den-sity, as the number of activations per timestep is notably lower than in conventional ANNs [18]. To overcome these limitations, we propose Module-aware Ar-chitecture Refinement (MAR), a two-stage framework that jointly linearizes attention mechanisms and reduces FFN computational cost. We further introduce the Spike-aware Bidirectional Distil-lation Strategy (SBDS) to preserve performance under resource constraints. The main contributions are as follows: Firstly, to jointly optimize quadratic attention and FFNs, we pro-pose the two-stage MAR. In the first stage, SSMs are introduced to achieve linear-time sequence modeling. In the second stage, spiking neurons are employed to sparsify activations, thereby substantially  

> arXiv:2601.21503v1 [cs.AI] 29 Jan 2026 FFN
> Discrete
> Mamba-2
> Transform
> FFN
> Discrete
> Mamba-2
> FFN
> Self-
> Attention
> Spiking module Adaptive ternary multi-step neuron
> Transform
> Distillation

Fig. 2 . Overview of the MAR framework. This figure illustrates how a dense attention-based model is transformed into a sparse and effi-cient linear sequence model through a two-stage optimization pro-cess. reducing the computational cost of linear layers. Secondly, to address temporal mismatch and low information density, we propose Adaptive Ternary Multi-step Neuron (ATMN) and SBDS. ATMN increases information capacity to mitigate sparsity-induced loss, while SBDS uses reverse-KL compensation with Pre-Norm alignment to mitigate temporal mismatch. Thirdly, extensive experimental results validate the effectiveness of our approach. Our linear spiking model successfully restores the performance of teacher model on multiple commonsense reasoning and question answering benchmarks while significantly reducing in-ference energy consumption. At the same time, it surpasses larger spiking LLM as well as other efficient models of comparable scale. 

2. METHOD 2.1. Module-aware Architecture Refinement 

To address the computational bottlenecks of attention mechanisms and FFNs, we propose Module-aware Architecture Refinement (MAR), a two-stage optimization framework that restructures LLMs through a module-aware strategy to improve the efficiency of both sequence modeling and FFN computation. An overview of the framework is shown in Figure 2. In the first stage, we replace the attention mechanism with an SSM to achieve linear-time sequence processing. Given the maturity of prior work in this direction, we directly adopt Llamba [14] as the baseline in the second stage. Llamba is an attention-free variant of LLaMA-3.2, built upon discrete multi-head Mamba-2 modules. In the second stage, MAR targets the high cost of FFNs and other fully connected layers. Spiking neurons (ATMNs) are inserted before fully connected units, sparsifying activations by replacing multiply–accumulate (MAC) with accumulate (AC) operations. In Llamba’s decoder, spiking neurons are placed at four positions per layer, before the input and output projections of both Mamba-2 and FFN modules (Figures 3, 4). 

2.2. Adaptive Ternary Multi-step Neuron 

Traditional spiking neurons typically encode information using bi-nary spike sequences with values in {0, 1 }. However, this represen-tation discards negative signals, thereby limiting the representational capacity of individual neurons and reducing information density. To overcome this limitation while preserving sparsity, we draw inspira-tion from the design of [19] and propose the Adaptive Ternary Multi-step Neuron (ATMN), which enhances the information-carrying ca-pacity of spiking neurons. Its working mechanism can be described by the following iterative equations: Feed Forward       

> RMS Norm
> Discrete
> Mamba-2
> RMS Norm
> Feed Forward
> RMS Norm
> Discrete
> Mamba-2
> RMS Norm
> x N x N
> Embedding Embedding
> LM Head LM Head
> Teacher Model Student Model
> Spiking module Adaptive ternary multi-step neuron Alignment

Fig. 3 . Overview of the SBDS: the student model (right) learns from the teacher model (left) through feature-level and logit-level losses. 

ht = It · δt, 0 + 1

τ · ut−1, (1) 

st =



1 if ht ≥ Vadaptive 

−1 if ht ≤ − Vadaptive 

0 otherwise ,

(2) 

ut = ht − st · Vadaptive , (3) 

Vadaptive = ea, (4) where It denotes the input current at time step t, τ is the membrane time constant, and ht, ut represent the membrane potentials before and after spike reset, respectively. st indicates the spike output, and 

Vadaptive is a neuron-wise adaptive threshold constrained to be non-negative via an exponential function with trainable parameter a. The Kronecker delta function δt, 0 equals 1 only at t = 0 , allowing exter-nal input injection at the initial step. Unlike conventional binary spiking neurons, ATMN emits a neg-ative spike when the membrane potential falls below −Vadaptive ,thereby mimicking biological inhibitory signals. This bidirectional spiking mechanism effectively mitigates the low information density issue caused by sparse activations and enhances representational ca-pacity. In addition, we inject external input only at the initial step, while subsequent updates rely solely on the residual membrane po-tential, simplifying the temporal update process. During reset, the membrane potential moves toward the resting state if a spike occurs; otherwise, the accumulated value is retained. 

2.3. Spike-aware Bidirectional Distillation Strategy 

To mitigate temporal mismatch and restore the performance of spik-ing models under resource constraints, we propose the Spike-aware Bidirectional Distillation Strategy (SBDS). This strategy provides multi-granularity supervision to guide the student model in effec-tively learning linguistic capabilities from the teacher model. The overall process is illustrated in Figure 3. Gate Proj Up Proj     

> σ
> Down Proj
> X
> Mamba-2
> Linear
> X
> Conv
> Linear Linear
> σ
> Adaptive ternary multi-step neuron
> XElement-wise multiplication
> σSiLU activation
> ABXC

Fig. 4 . Illustration of spiking integration in the Discrete Mamba-2 (left) and FFN (right). The left and right sides correspond to the Dis-crete Mamba-2 and the Feed Forward Network in the student model shown in Figure 3, respectively. Specifically, we adopt Llamba as the teacher model, while the student model is a spiking variant constructed using the MAR frame-work. At the output level, we extend the standard KL loss with a reverse KL divergence term to form a bidirectional distillation loss. While the standard KL emphasizes alignment over the full output distribution, the reverse KL encourages the student model to focus more on high-confidence predictions, making the formulation better suited to the sparse and bursty nature of spiking activations. The corresponding logit alignment loss is defined as follows: 

L1(p|| q) = 

> D−1

X

> k=0

[αp (k) − βq (k)] · [log p(k) − log q(k)] , (5) where p and q denote the teacher and student distributions, re-spectively, and D is the size of the discrete space, typically the vo-cabulary size. The parameters α and β are tunable hyperparameters that control the trade-off between capturing the overall distribution and emphasizing high-confidence regions, making the loss function better aligned with the sparse characteristics of spiking outputs. At the feature level, we further design a pre-normalization state alignment loss, which compares the representations of the teacher and student models immediately after the first Root Mean Square Normalization (RMSNorm) module in each layer, thereby facilitat-ing more effective alignment of semantic representations. The for-mal definition of the loss is given as follows: 

L2(hj , h k ) = || PreNorm (hj ) − PreNorm (hs)|| 2, (6) where, hj and hk denote the input features at the corresponding layer of the teacher and student models, respectively, and PreNorm (·) rep-resents the standard pre-layer normalization operation. Accordingly, the final total loss function is defined as follows: 

Ldistill = 1

T M  

> T−1

X 

> t=0
> M−1

X

> m=0

L1(pt,m || qt,m )+ 1

T L  

> T−1

X

> t=0
> L−1

X

> l=0

L2(hjt,l , h kt,l ),

(7) where T is the number of time steps, M is the sequence length, and 

L is the number of layers. L1 denotes the logit-level loss between teacher logits pt,m and student logits qt,m , while L2 is the feature-level loss between their pre-normalized hidden states hjt,l and hkt,l .This SBDS guides the student model to focus on high-confidence outputs and align semantic representations across layers, accel-erating convergence and enhancing performance under resource constraints. 

3. EXPERIMENT 3.1. Data and experimental settings 

We use Llamba-1B, the Mamba variant of LLaMA-3.2-1B, as the starting point for the second stage of the MAR framework. For training, we employ the GenQA [20], OpenHermes 2.5 [21], and InfinityInstruct [22] datasets, which together contain approximately 7 billion tokens, and train for one epoch. 

3.2. Main results 

To comprehensively assess both the effectiveness and efficiency of our proposed MAR framework, we evaluate the model on two fronts: accuracy performance across zero-shot reasoning benchmarks, and energy consumption during inference. We conduct zero-shot evaluations on six commonsense rea-soning and question answering datasets: Physical Interaction QA (PIQA) [26], Boolean Questions (BoolQ) [27], Winogrande (WG) [28], HellaSwag (HS) [29], ARC Challenge (ARC-C), and ARC Easy (ARC-E)[30]. As shown in Table 1, our method achieves an average accuracy of 57.93% across six tasks. Compared with the teacher model Llamba (61.88%), its performance is largely re-stored; compared with the much larger SpikeLLM (7B, 52.48%), our model attains a 5.45 percentage point improvement using only 1.4B parameters, demonstrating superior parameter efficiency. In addition, it also surpasses other efficiency-oriented models, such as Bi-Mamba (49.38%), SmoothQuant (54.93%), and TinyLLaMA (55.91%), confirming that the proposed approach can effectively recover performance under limited resource conditions. To further quantify the efficiency advantages of the proposed method, we compare the inference energy consumption of the MAR model with its dense baseline. Following the energy metrics re-ported by [31], we assume that each MAC, multiplication, and AC operation consumes 4.6, 3.7, and 0.9 pJ, respectively. On the WG dataset, we measure the average firing rate of spiking neurons in the decoder and estimate the total energy consumption at different se-quence lengths. As shown in Figure 5, compared with same-sized Llamba and LLaMA models, MAR exhibits a substantially slower growth in energy consumption. Moreover, MAR consistently incurs lower inference energy across all sequence lengths, and this advan-tage becomes more pronounced as the sequence length increases. These results demonstrate the superiority of MAR in effectively re-ducing the computational energy cost of FFNs and other linear lay-ers. 

3.3. Ablation study 

To assess the individual contributions of each component within the MAR framework, we conduct a series of systematic ablation studies. Each subsection provides a focused analysis of how different mod-ules influence overall model performance, with the average accuracy across six benchmark tasks used as the evaluation metric. As shown in Table 2, we progressively introduce ATMN, the re-verse KL divergence compensatory term, and the pre-normalization Model Size SNN Accuracy (%) PIQA BoolQ WG HS ARC-e ARC-c Average LLaMA 1.3B no 74.32 69.54 59.67 60.76 68.52 38.05 61.80 Llamba 1.4B no 73.78 68.62 60.69 61.31 69.57 37.32 61.88 Bi-Mamba[23] 1.3B no 69.20 62.00 53.70 43.10 43.90 24.40 49.38 SmoothQuant[7] 1.3B no 71.60 57.77 60.06 53.68 57.03 29.44 54.93 TinyLLaMA[24] 1.3B no 73.30 57.80 59.10 59.90 55.30 30.10 55.91 SpikeLLM[25] 7B yes 65.45 64.37 54.3 56.59 41.67 32.51 52.48 Ours 1.4B yes 70.35 68.17 55.33 50.91 67.21 35.58 57.20 

Table 1 . Comparison of zero-shot performance on commonsense reasoning and question answering benchmarks across various models. For ARC-E and HS, we use normalized logits’ results. 0 2000 4000 6000 8000 10000 

> Sequence Length (L)
> 0
> 10
> 20
> 30
> 40
> 50
> 60
> 70
> 80
> Total Energy Consumption (J)
> LLaMA-Decoder
> Llamba-Decoder
> Ours-Decoder

Fig. 5 . Total energy consumption of different decoders at varying sequence lengths. alignment loss on a shared baseline to evaluate the contribution of each module to model performance. The baseline, which contains only traditional binary neurons with standard KL-based distillation, achieves an average accuracy of 46.28%. Replacing the binary neurons with ATMN increases accuracy to 55.20%, demonstrat-ing its effectiveness in alleviating the issue of reduced information density. Adding the reverse KL term further improves accuracy to 55.46%, indicating that this compensatory component helps address the sparse and bursty nature of spiking outputs. With the addi-tional introduction of pre-normalization alignment loss, accuracy rises to 57.20%, highlighting enhanced representational consistency between teacher and student models. These results collectively validate the effectiveness of the proposed optimization strategy. ATMN Reserve KL PN loss Average acc. (%) - - - 46.28 

✓ - - 55.20 

✓ ✓ - 55.46 

✓ ✓ ✓ 57.20 

Table 2 . Ablation study of core components. Each column shows whether a module is enabled. Reserve KL and PN loss denote the re-verse KL divergence term and the pre-normalization alignment loss. Next, we compare three feature-level alignment strategies to examine their role in the distillation process: using only pre-normalization alignment, using only post-normalization alignment, and combining both. The baseline configuration includes only logit-level distillation loss, and all strategies are evaluated on this basis. As shown in Table 3, pre-normalization alignment achieves the best performance, while post-normalization alignment alone or the combination of both yields relatively lower accuracy. This indicates that pre-normalization features provide more stable and transferable representations for guiding the student model. Therefore, we adopt pre-normalization alignment as the default feature-level distillation strategy in our final framework. Pre-norm Post-norm Average acc. (%) - - 55.46 

✓ - 57.20 - ✓ 56.75 

✓ ✓ 56.08 

Table 3 . Comparison of feature-level alignment strategies. Pre-Norm aligns features after the first RMSNorm in each layer, whereas Post-Norm aligns features after the final normalization. In addition, we conducted a sensitivity analysis of the hyper-parameters α and β to examine the effect of weighting configu-rations in SBDS. As shown in Table 4, using only standard KL (α = 1 , β = 0 , 55.20%) or only reverse KL ( α = 0 , β = 1 ,55.92%) failed to achieve optimal performance. By contrast, com-bining the two led to significant improvements, with the best result obtained at α = 0 .2, β = 0 .7, reaching an average accuracy of 56.40%. These results confirm the effectiveness of the bidirectional distillation loss. 

α β Average acc.(%) 0 1 55.92 1 0 55.20 0.7 0.2 56.39 0.2 0.7 56.40 

Table 4 . Sensitivity analysis of hyperparameters α and β in SBDS. 

4. CONCLUSION 

This paper proposes a two-stage MAR framework to jointly optimize attention mechanisms and FFNs in LLMs. In the first stage, SSMs are introduced to achieve linear-time sequence modeling; in the sec-ond, activation spiking is applied to reduce the computational cost of FFNs. In addition, ATMN is designed to mitigate the issue of low information density, while SBDS addresses temporal mismatches and restores performance. Experimental results show that the pro-posed method effectively recovers teacher-level performance under constrained resources, substantially reduces inference energy con-sumption, and outperforms not only models of comparable scale but also larger efficient models, demonstrating its potential to achieve a favorable balance between accuracy and efficiency. 5. REFERENCES 

[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al., “LLaMA: Open and efficient foundation language models,” 2023, arXiv:2302.13971 .[2] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al., “Qwen3 technical report,” 2025, 

arXiv:2505.09388 .[3] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al., “Deepseek-v3 technical report,” 2024, 

arXiv:2412.19437 .[4] Quzhe Huang, Mingxu Tao, Chen Zhang, Zhenwei An, Cong Jiang, Zhibin Chen, Zirui Wu, and Yansong Feng, “Lawyer LLaMA technical report,” 2023, arXiv:2305.15062 .[5] Tiedong Liu and Bryan Kian Hsiang Low, “Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic tasks,” 2023, 

arXiv:2305.14201 .[6] Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, and Tri Dao, “The Mamba in the Llama: Distilling and accelerating hybrid models,” in NeurIPS , 2024, vol. 37, pp. 62432–62457. [7] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien De-mouth, and Song Han, “SmoothQuant: Accurate and effi-cient post-training quantization for large language models,” in 

ICML , 2023, vol. 202, pp. 38087–38099. [8] Guanghui Wang, Zhiyong Yang, Zitai Wang, Shi Wang, Qian-qian Xu, and Qingming Huang, “ABKD: Pursuing a proper allocation of the probability mass in knowledge distillation via 

α-β-divergence,” in ICML , 2025, vol. 267, pp. 65167–65212. [9] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´ e, “FlashAttention: Fast and memory-efficient exact attention with IO-awareness,” in NeurIPS , 2022, vol. 35, pp. 16344– 16359. [10] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma, “Linformer: Self-attention with linear complexity,” 2020, arXiv:2006.04768 .[11] Albert Gu, Karan Goel, and Christopher Re, “Efficiently mod-eling long sequences with structured state spaces,” in ICLR ,2022. [12] Albert Gu and Tri Dao, “Mamba: Linear-time sequence mod-eling with selective state spaces,” 2023, arXiv:2312.00752 .[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polo-sukhin, “Attention is all you need,” in NeurIPS , 2017, vol. 30, pp. 6000 – 6010. [14] Aviv Bick, Tobias Katsch, Nimit Sohoni, Arjun Desai, and Al-bert Gu, “Llamba: Scaling distilled recurrent models for effi-cient language processing,” 2025, arXiv:2502.14458 .[15] Wolfgang Maass, “Networks of spiking neurons: the third gen-eration of neural network models,” Neural networks , vol. 10, no. 9, pp. 1659–1671, 1997. [16] Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kher-adpisheh, Timoth´ ee Masquelier, and Anthony Maida, “Deep learning in spiking neural networks,” Neural networks , vol. 111, pp. 47–63, 2019. [17] Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng YAN, Yonghong Tian, and Li Yuan, “Spikformer: When spiking neural network meets transformer,” in ICLR ,2023. [18] Yan Zhong, Ruoyu Zhao, Chao Wang, Qinghai Guo, Jian-guo Zhang, Zhichao Lu, and Luziwei Leng, “SPikE-SSM: A sparse, precise, and efficient spiking state space model for long sequences learning,” 2024, arXiv:2410.17268 .[19] Yufei Guo, Yuanpei Chen, Xiaode Liu, Weihang Peng, Yuhan Zhang, Xuhui Huang, and Zhe Ma, “Ternary Spike: Learning ternary spikes for spiking neural networks,” in AAAI , 2024, vol. 38, pp. 12244–12252. [20] Jiuhai Chen, Rifaa Qadri, Yuxin Wen, Neel Jain, John Kirchen-bauer, Tianyi Zhou, and Tom Goldstein, “GenQA: Generat-ing millions of instructions from a handful of prompts,” 2024, 

arXiv:2406.10323 .[21] Teknium, “Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,” 2023, 

https://huggingface.co/datasets/teknium/OpenHermes-2.5 .[22] Jijie Li, Li Du, Hanyu Zhao, Bo-wen Zhang, Liangdong Wang, Boyan Gao, Guang Liu, and Yonghua Lin, “Infinity Instruct: Scaling instruction selection and synthesis to enhance language models,” 2025, arXiv:2506.11116 .[23] Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, and Zhiqiang Shen, “Bi-mamba: Towards accurate 1-bit state space models,” 2024, arXiv:2411.11843 .[24] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu, “TinyLlama: An open-source small language model,” 2024, 

arXiv:2401.02385 .[25] Xingrun Xing, Boyan Gao, Zheng Liu, David A. Clifton, Shi-tao Xiao, Wanpeng Zhang, Li Du, Zheng Zhang, Guoqi Li, and Jiajun Zhang, “SpikeLLM: Scaling up spiking neural network to large language models via saliency-based spiking,” in ICLR ,2025. [26] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al., “PIQA: Reasoning about physical commonsense in natural lan-guage,” in AAAI , 2020, vol. 34, pp. 7432–7439. [27] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova, “BoolQ: Exploring the surprising difficulty of natural yes/no questions,” in NAACL , 2019, pp. 2924–2936. [28] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi, “WinoGrande: An adversarial winograd schema challenge at scale,” in AAAI , 2020, vol. 34, pp. 8732–8740. [29] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi, “HellaSwag: Can a machine really finish your sentence?,” in ACL , 2019, pp. 4791–4800. [30] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord, “Think you have solved question answering? try arc, the ai2 reasoning challenge,” 2018, arXiv:1803.05457 .[31] Mark Horowitz, “1.1 computing’s energy problem (and what we can do about it),” in ISSCC , 2014, pp. 10–14.