# SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization
# SMOG：面向多目标贝叶斯优化的可扩展元学习

**Authors**: Leonard Papenmeier, Petru Tighineanu \\
**Date**: 2026-01-29 \\
**PDF**: https://arxiv.org/pdf/2601.22131v1 \\
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 6.0 \\
**Evidence**: Meta-learning for multi-objective optimization acceleration \\

---

## Abstract
Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.

## 摘要
多目标优化旨在解决具有竞争性

---

## 速览摘要（自动生成）

**问题**：如何利用历史数据加速具有多个竞争目标的黑盒优化任务。

**