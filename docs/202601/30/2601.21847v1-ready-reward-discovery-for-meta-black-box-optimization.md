# READY: Reward Discovery for Meta-Black-Box Optimization
# READY：元黑盒优化的奖励发现

**Authors**: Zechuan Huang, Zhiguang Cao, Hongshu Guo, Yue-Jiao Gong, Zeyuan Ma \\
**Date**: 2026-01-29 \\
**PDF**: https://arxiv.org/pdf/2601.21847v1 \\
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 10.0 \\
**Evidence**: uses evolution of heuristics and LLMs for automated algorithm design \\

---

## Abstract
Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY's project at https://anonymous.4open.science/r/ICML_READY-747F.

## 摘要
元黑盒优化（MetaBBO）是优化领域中一个新兴

---

## 论文详细总结（自动生成）

这篇论文提出了 **READY**（**REA**ward **D**iscovery for Meta-BBO），这是首个利用大语言模型（LLM）为元黑盒优化（MetaBBO）自动发现奖励框架的研究。

以下是对该论文的深度结构化总结：

### 1. 核心问题与整体含义（研究动机）
*   **背景**：元黑盒优化（MetaBBO）通过强化学习（RL）训练元策略来指导优化器，而**奖励函数（Reward Function）**是连接优化环境与策略学习的关键信号。
*   **核心痛点**：
    1.  **人工依赖**：目前的奖励函数多由专家手工设计，存在设计偏差和“奖励作弊（Reward Hacking）”风险。
    2.  **扩展性差**：为不同的 MetaBBO 架构（如参数控制、算子选择）设计奖励需要耗费大量人力。
    3.  **性能瓶颈**：手工设计的简单启发式奖励往往无法充分发挥神经网络策略的潜力。
*   **研究目标**：利用 LLM 的代码生成与推理能力，自动化地为多种 MetaBBO 任务搜索高效、可解释且通用的奖励函数。

### 2. 方法论：READY 框架
READY 采用基于 LLM 的**多任务程序进化（Multi-task Program Evolution）**范式：
*   **多任务生态位架构（Niche-based Architecture）**：为每个 MetaBBO 任务维护一个独立的种群（Niche），支持并行搜索。
*   **初始化（贪婪采样）**：
    *   **专家锚定**：将原始手工奖励作为初始个体。
    *   **上下文生成**：LLM 根据任务元数据（算法逻辑、代码接口）生成多样化候选。
    *   **拒绝采样**：仅保留性能优于手工基准的个体。
*   **精细化进化算子（5 种反射算子）**：
    *   **M1 局部反射变异**：分析失败案例（Bad Cases）并针对性修复代码。
    *   **M2 历史反射变异**：回顾进化轨迹，识别导致性能提升的代码模式并外推。
    *   **M3 全局反射变异**：从所有任务的淘汰存档中总结通用设计原则。
    *   **C1 剥削性交叉**：结合局部最优与全局最优个体的逻辑。
    *   **C2 探索性交叉**：跨任务借鉴独特的算法特征。
*   **显式知识转移（Knowledge Transfer）**：LLM 分析不同 MetaBBO 任务间的相似性，将源任务的成功奖励逻辑“翻译”并移植到目标任务中，加速收敛。

### 3. 实验设计
*   **实验场景（3 种异构 MetaBBO 任务）**：
    1.  **RLDAS**：算法选择任务（PPO 代理切换不同的差分进化算法）。
    2.  **RLEPSO**：参数控制任务（PPO 动态调整粒子群算法的参数）。
    3.  **DEDQN**：算子选择任务（DQN 选择变异算子）。
*   **测试集**：BBOB 测试套件（8 个训练函数，16 个零样本测试函数）。
*   **Benchmark（对比方法）**：
    *   **手工设计奖励**（各算法原始论文中的奖励）。
    *   **自动化基准**：Eureka（专门的奖励发现方法）、EoH 和 ReEvo（通用的 LLM 启发式搜索框架）。

### 4. 资源与算力
*   **LLM 后端**：主要使用 **DeepSeek-V3.2**，消融实验中也测试了 Qwen-3-Max 和 Gemini-3-Flash。
*   **训练时长**：
    *   READY 在 **7 小时**内完成了 3 个任务的并行奖励发现。
    *   相比之下，基线方法（如 Eureka, EoH）需要独立运行，总耗时在 14 到 30 小时之间。
*   **并行框架**：使用 **Ray** 框架加速适应度评估（并行运行 MetaBBO 训练）。

### 5. 实验数量与充分性
*   **实验规模**：
    *   每个发现的奖励在测试集上进行了 **51 次独立运行**，以确保统计显著性。
    *   涵盖了 16 个不同特征的测试函数（单峰、多峰、病态等）。
*   **消融实验**：针对进化算子、知识转移模块、LLM 模型能力做了详细的消融分析。
*   **泛化性测试**：进行了**零样本转移（Zero-shot Transfer）**实验，将为 DEDQN 发现的奖励直接应用于未见的 RLDEAFL 算法，验证了奖励逻辑的通用性。
*   **评价**：实验设计非常充分，通过多任务并行对比和跨算法转移，客观地证明了该方法的优越性。

### 6. 主要结论与发现
*   **性能提升**：READY 发现的奖励在 48 个测试场景中的 36 个优于手工设计，且在大多数情况下击败了 Eureka 等自动化基线。
*   **效率优势**：通过多任务知识共享，READY 的搜索速度比独立搜索快 2-4 倍。
*   **零样本泛化**：发现的奖励捕捉到了优化的本质特征（如探索与利用的平衡），能够直接提升未参与