Title: Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model

URL Source: https://arxiv.org/pdf/2601.21877v1

Published Time: Fri, 30 Jan 2026 02:15:12 GMT

Number of Pages: 19

Markdown Content:
## Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

Chen Wang * 1 Sijie Ma * 1 Zeyuan Ma * 1 Yue-Jiao Gong 1

Abstract 

Benchmark Design in Black-Box Optimiza-tion (BBO) is a fundamental yet open-ended topic. Early BBO benchmarks are predominantly human-crafted, introducing expert bias and con-straining diversity. Automating this design pro-cess can relieve the human-in-the-loop burden while enhancing diversity and objectivity. We propose Evolution of Benchmark (EoB), an au-tomated BBO benchmark designer empowered by the large language model (LLM) and its pro-gram evolution capability. Specifically, we formu-late benchmark design as a bi-objective optimiza-tion problem towards maximizing (i) landscape diversity and (ii) algorithm-differentiation abil-ity across a portfolio of BBO solvers. Under this paradigm, EoB iteratively prompts LLM to evolve a population of benchmark programs and employs a reflection-based scheme to co-evolve the land-scape and its corresponding program. Comprehen-sive experiments validate our EoB is a competitive candidate in multi-dimensional usages: 1) Bench-marking BBO algorithms; 2) Training and testing learning-assisted BBO algorithms; 3) Extending proxy for expensive real-world problems. 

1. Introduction 

Black-Box Optimization (BBO) problems are inherently hard (Jones et al., 1998) and widely encountered across var-ious real-world scenarios (Gen & Cheng, 1999; Terayama et al., 2021; Ma et al., 2025e). During the past two decades, a broad spectrum of BBO algorithms (Holland, 1992; Storn & Price, 1997; Beyer & Schwefel, 2002; Kennedy & Eber-hart, 1995; Snoek et al., 2012; Byrd et al., 2016; Akiba et al., 2019; Golovin et al., 2017) have been proposed to address BBO problems (Holland, 1992; Storn & Price, 1997; Beyer & Schwefel, 2002; Kennedy & Eberhart, 1995; Snoek et al., 

> 1

South China University of Technology. Correspondence to: Yue-Jiao Gong <gongyuejiao@gmail.com >.

Preprint. January 30, 2026. Benchmark Design by Human (Costly )

Benchmark Design by LLM (Ours, Automatic )        

> target task analyse formulate implement benchmark
> target task benchmark ev aluate & evolve
> analyse formulate implement

Figure 1. Paradigm shift from human-crafted benchmark design toward LLM-based automation. 

2012; Byrd et al., 2016). However, compared to the expo-nential growth in algorithm development, the advancement of BBO benchmarks has lagged significantly behind. A well-developed BBO benchmark is curcial for the ad-vancement of the BBO field, as it helps ensure both fair com-parison and performance coherence on out-of-benchmark tasks. Existing benchmarks such as CoCo-BBOB (Hansen et al., 2010) and IEEE CEC series (Liang et al., 2015a) are commonly used within the community, where the synthetic functions are carefully designed to provide various land-scape challenges (Mersmann et al., 2011) for evaluating BBO algorithms. However, their limited testing instances hinder them from keeping pace with the swift emergence of novel BBO tasks in this fast-evolving era. Besides, given the rise of learning-assisted BBO techniques more recently (Ma et al., 2025d; van Stein & B ¨ack, 2024b; Li et al., 2024b), training them requires a large number of diversified BBO instances to ensure robust generalization. This poses signif-icant challenges for hand-crafted benchmark design since this paradigm needs deep expertise and massive labor input. To address these fundamental limitations, we propose Evo-lution of Benchmark (EoB) as a compact, self-contained and fully automated BBO benchmark designer. As illus-trated in Fig. 1, EoB is empowered by recent advances of LLMs for program evolution (Guo et al., 2023; Liu et al., 2024; van Stein & B ¨ack, 2024a; Novikov et al., 2025) to achieve both benchmark quality and automated design ef-ficiency. In such a paradigm, a population of feasible pro-grams is maintained and undergoes an iterative code-level 1

> arXiv:2601.21877v1 [cs.NE] 29 Jan 2026 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model

reproduction process. Those programs with better fitness are selected and kept until the end of evolution. However, unlike the heuristic search focused in prior works, BBO benchmark design presents unique challenges that require specialized solutions: 1) Complex design objectives align-ment : We introduce a multi-objective program evolution paradigm that simultaneously optimizes the landscape simi-larity and algorithm distinguishing capability of benchmarks. 2) Program-landscape co-evolution : We propose a novel reflection-based evolution strategy that leverages LLM’s deep understanding of mathematical structures to achieve program-landscape co-evolution. By analyzing the causal relationship between program patterns and performance, this strategy enables the automatic synthesis of complex benchmark functions without explicit landscape editing. We summarize the contributions of this paper in threefold: • EoB represents the first paradigm shift from human-crafted to LLM-automated BBO benchmark design, delivering orders of magnitude efficiency improvement while maintaining benchmarking effectiveness. • Our key innovations include (i) the bi-objective pro-gram search modeling, (ii) the tailored LLM-based program evolution paradigm, and (iii) the reflective landscape editing scheme, which ensure the automa-tion and performance of the search process. • EoB undergoes rigorous empirical validation across different perspectives: classic BBO algorithm bench-marking, modern learning-assisted optimizer training, and proxy modeling for expensive real-world prob-lems, which underscores its universal applicability and practical value for the BBO community. 

2. Related Works 

2.1. BBO Benchmarks 

Compared to the exponential development scale of BBO algorithms, exploration and in-depth analysis of BBO benchmarks are still far from sufficient. Initial efforts focus on synthetic functions with closed forms, such as CoCo-BBOB (Hansen et al., 2010), IEEE CEC Benchmark suites (Mohamed et al., 2021) and IOHProfiler (Doerr et al., 2019). They are simple and easy to analyze, hence being commonly used in the community. Further efforts include category extensions (Tian et al., 2017; Wu et al., 2017; Da et al., 2017; Zhong et al., 2024), reproducible base-lines (Duan et al., 2024), and efficiency optimization (Huang et al., 2024). However, recent advances in learning-assisted BBO algorithms (Ma et al., 2023b; 2025c) reveal these synthetic benchmarks show limited performance evaluation coherence due to the gap between realistic tasks and these synthetic toy problems. Prior attempts to address this in-clude hand-crafted extensions (Hwang et al., 2010; Arango et al., 2021; Shehadeh & Kudela, 2025; Xue et al., 2025) or genetic programming-based search (Mu ˜noz & Smith-Miles, 2020; Long et al., 2023; Tian et al., 2020; Wang et al., 2025). However, the former is both time- and resource-consuming, while the latter depends on deep expertise in domain-specific language. In contrast, Our EoB uses LLM-based program evolution to design benchmarks automati-cally and efficiently with minimal need for deep expertise. 

2.2. Program Evolution by LLMs 

We briefly review representative LLM-based program evo-lution methods. The core idea is that: by integrating LLMs with an evaluator to provide proper performance feedback, they might gain strength in open discovery through iterative in-context learning (Min et al., 2022). The early efforts in the field includes FunSearch (Romera-Paredes et al., 2024) for heuristic discovery, EvoPrompting (Chen et al., 2023) for neural architecture search and EvoPromp (Guo et al., 2023) for automatic prompt optimization. They share an evolution-ary paradigm where a population of feasible programs are maintained and go through tailored reproduce-then-select loop until an optimal program is evolved. Following these initial examples, a wide array of researches are emerged (Liu et al., 2024; Ye et al., 2024; van Stein & B ¨ack, 2024a; Yao et al., 2025; Liu et al., 2025b; Novikov et al., 2025; Ma et al., 2023a). Given its prosperity, this paper presents the first exploration on using LLM-based program evolution for BBO benchmark design. 

3. Methodology 

3.1. Overview 

Given M design requirements (objectives) O1, ..., OM ,such as the landscape diversity (Mu ˜noz et al., 2014) and algorithm distinguishing capability (Kostovska et al., 2023), we formulate the benchmark design problem as a multi-objective optimization problem: 

min   

> f∈F

(O1(f ), ..., OM (f )) , (1) where F spans a comprehensive function space, which in this paper are all feasible programs that could be used as objec-tive functions for optimization problems. The M design ob-jectives are probably conflict with each other, hence we ob-tain a Pareto front set {f ∗ 

> 1

, ..., f ∗ 

> K

} with K non-dominated program instances, which can be regarded as a benchmark suite with multiple diverse testing instances. For the basics of Pareto and domination, refer to (Deb et al., 2016). 2Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

3.2. EoB 

EoB treats each benchmark function as an evolving program, searching through the vast space of possible mathematical formulations using a multi-objective evolutionary strategy in MOEA/D (Zhang & Li, 2007) manner. It operates through five interconnected stages: initialization creates a diverse population of candidate functions; evaluation measures each function’s landscape characteristics and algorithm discrimi-nation power; evolution employs LLM-driven reflection to generate improved variants; selection maintains evolution-ary pressure through Pareto-based comparison; and archive management preserves the best discoveries. 3.2.1. I NITIALIZATION 

The initialization in EoB resembles that of MOEA/D. First, we generate a group of reference vectors uniformly spread in the bi-objective space. The reference vectors are N 2-dimensional ones: ⃗λi = ( iN − 1 , N − 1 − iN − 1 ), i = 0 , 1, 2, ..., N − 1. (2) We then randomly sample a function program f t=0  

> i

from LLM for each ⃗ λi, resulting in an initial solution popula-tion of size N , where t denotes the evolutionary generation. To sample the program solutions from LLM, we repeat-edly call an existing LLM using a carefully constructed 

Init Prompt (see Appendix A). The aim of this prompt is, on the one hand, to help LLM grasp the basic context informa-tion, such as its role (an optimization expert and benchmark designer) and its task (to generate a program of an objec-tive function). On the other hand, it should steer the LLM with sufficient expert-level specifications, which include two major parts. 

Generation Preference. Given the bi-objective setting in EoB, we add an additional paragraph in Init Prompt to in-form the LLM which reference vector ⃗ λi it generates a func-tion program for. For example, when ⃗ λi = (1 , 0) , the LLM is instructed to generate a function program that considers maximizing its similarity with the target problem rather than its algorithm distinguishing capability. 

Population Diversity. If we do not explicitly ask the LLM to generate function programs with different landscape char-acteristics, it tends to generate highly similar instances, which greatly harms the diversity in the initial population. To this end, we have summarized seven kinds of landscape construction knowledge from existing literature (Yazdani et al., 2025; Cenikj et al., 2026; Seiler et al., 2025; Mers-mann et al., 2011), and added them into our prompt to steer the initialization. We list three examples here and provide details of all seven types of knowledge in Appendix A.2: 1) Asymmetric masking: LLMs can use masking func-tions (e.g., np.where (x ≤ b, ·, ·)) to construct asymmetric objective landscapes; 2) Nonlinearity composition: LLMs can use nested nonlinear operations (e.g., 1 

> abs (tanh (x))+1

) to create highly twisted landscapes; 3) Polynomial periodic-ity: LLMs can insert polynomial terms into periodic func-tions (e.g., sin (2 πx 3)) to create non-stationary periodic os-cillations. During the initialization phase, we randomly se-lect one of the seven types of landscape construction knowl-edge and add the selected one into the Init Prompt , ensuring the initialized population comprises programs with diverse landscape characteristics. 3.2.2. E VALUATION 

While our EoB supports flexible design requirements (as shown in Eq. 1), we address two major objectives that are es-sential for BBO benchmark design in this paper: O1, Land-scape Similarity Indicator (LSI), which indicates whether a searched program shows landscape similarity with the target hard-to-model BBO tasks; and O2, Algorithm Distinguish-ing Capability (ADC), which measures how effectively the searched program differentiates various BBO algorithms. Both objectives should be maximized. 

LSI. Given a group of H optimization tasks of interest 

Ftarget : {p1, ..., p H }, a primary objective is to design a group of K synthetic testing instances F : {f1, ..., f K } that exhibit similar landscape features to problems in Ftarget .Thus, we derive a similarity checking metric, LSI, to mea-sure how similar a testing program f ∈ F designed by EoB is to Ftarget . Specifically, we first compute the landscape features of either problems in Ftarget or the testing program 

f through exploratory landscape analysis (ELA) (Mersmann et al., 2011), which samples a set of solutions and corre-sponding objective values to compute diverse landscape features, such as convexity, for an optimization problem. In-stead of directly using the computationally inefficient ELA features, we adopt NeurELA (Ma et al., 2025b), a recently proposed neural network-based landscape profiling system. For each p ∈ Ftarget , we use NeurELA to obtain their 16-dimensional feature vector zp. For f , we obtain zf . Then, the LSI of f under Ftarget is formulated as: 

O1(f |Ftarget ) = 11 + 1 

> σ2

min p∈Ftarget || zf − zp|| 2

(3) where σ is a normalization factor to control the sensitivity of landscape feature similarity. A larger O1 indicates that the program f designed by LLM is at least similar to a problem instance p ∈ Ftarget . This metric, by encourag-ing generated functions to align with various established landscape features, partly contributes to the overall diversity of the evolved benchmark suite. Further, by additionally considering the following ADC metric, EoB explores a set of solutions (benchmark programs) along the Pareto front, which exhibit good diversity. 3Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

ADC. The second benchmark design objective we consider in EoB is its algorithm distinguishing capability. Suppose we have a testing program f designed by EoB and a BBO algorithm pool Λ : {A1, ..., A L} with L BBO algorithms. ADC measures performance variation across these BBO algorithms on f by first running the L algorithms on f for 

B random seed-controlled independent runs. We record a 

B × L-dimensional objective matrix Obj where each entry 

Obj i,j records the best-so-far objective value achieved by 

Aj while optimizing f at the i-th independent run. Then the ADC of f over the algorithm pool Λ is formulated as: 

O2(f |Λ) = std ( 1

B

> B

X

> i=1

⃗Obj i, :), (4) where ⃗ Obj i, : denotes the best-so-far objective values achieved by all BBO algorithms in Λ at the i-th run. We also note that to address the variation in objective values across different f designed by EoB, we apply min-max normaliza-tion on the Obj matrix. A larger ADC indicates that f is sufficiently good and challenging for distinguishing diverse BBO algorithms in terms of their performances. 

PBI. For each individual program fi in the initialized popu-lation and subsequent evolving populations, we first com-pute its LSI O1(f ) and ADC O2(f ). After objective com-putation, we further compute Penalty-based Boundary In-tersection (PBI), a scalarization trick to turn multi-objective space into measurable scalar performance. PBI receives three inputs: O1(fi), O2(fi), and the reference vector ⃗ λi.It then outputs a scalar score that reflects the aggregated performance of fi between the tradeoff of LSI ( O1) and ADC ( O2). The PBI score is used as reference information for subsequent LLM-based program evolution, including both the reproduction (Sec. 3.2.3) and selection (Sec. 3.2.4) phases. For details of PBI’s computation, refer to the origi-nal MOEA/D paper (Zhang & Li, 2007). 3.2.3. E VOLUTION 

Given a population of candidate programs {f ti }Ni=1 at the 

t-th generation ( t = 0 is the initialized population), we propose a two-stage reproduction scheme to fully leverage the LLM’s outstanding reasoning capability for code-level understanding and program search. 

Reflection stage. At the first stage, we iteratively prompt the LLM with pairs of candidate programs and instruct it to compare the objective scores of these program pairs. The LLM then engages in comparison and performs re-flective thinking, resulting in valuable insights on how to improve the candidate programs. We provide a detailed prompt for this stage in Appendix A.3.1. Specifically, sim-ilar to MOEA/D, we first construct the neighborhood set for each f ti , which comprises 5 candidate programs with the closest Euclidean distance to f ti in terms of the corre-sponding reference vectors. Then, we randomly select two candidates f ′ and f ′′ from the neighborhood set. For each candidate program, the LLM is provided with three inputs: (i) Source code, (ii) scores of LSI, ADC, and PBI, and (iii) reference vectors. Considering that the candidate program is generated by the LLM, it may encounter syntax errors and other invalid issues; therefore, EoB steers the LLM to output comprehensive analyzes on a case-by-case basis: • Aggressive Mutation: If f ′ and f ′′ are both invalid, the LLM is directed to analyze the reasons behind them and output corresponding suggestions to rectify the invalid issues or facilitate a complete regeneration. • Conservative Mutation: If one of them is invalid, the LLM is directed to ignore the invalid one and focus on the valid one to analyze why it works in LSI/ADC/PBI aspects and subsequently propose the corresponding improvement suggestions. • Reflection-Based Crossover: If both of them are valid, the LLM is directed to conduct a three-fold in-depth reflection when comparing f ′ with f ′′ (where the can-didate with the higher PBI is designated as the winner 

and the other as the loser ): (i) Successful Pattern Iden-tification: recognize the major code-level differences between winner and loser , summarize knowledge and its relationship with landscape characteristics. (ii) Ben-eficial Gene Preservation: if the loser shows a higher score in either LSI or ADC, the LLM is asked to an-alyze the reason behind it and preserve such valuable knowledge. (iii) Bi-objective Tradeoff Analysis: rec-ognize and analyze the positive/negative impacts of certain parts of the programs’ codes on LSI and ADC, with a special focus on the conflict case such as adding a mathematical operation increase LSI but harm ADC. In the end of this stage, all intermediate analyzes and thoughts are summarized by the LLM and organized in a JSON-like response, including a summary of the overall rea-soning process, condensed analysis results for the aforemen-tioned case-by-case reflections, and suggestions/instructions for generating the next generation offspring program. 

Reproduction stage. Given the JSON-like reflection ob-tained in the Reflection stage, the LLM is directed to refer to and follow these useful analysis or improvement sug-gestions to produce a single new program candidate f t+1  

> i

.By repeatedly calling this two-stage evolution scheme N

times, we could obtain an updated candidate population. We provide detailed prompts for this stage at Appendix A.3.2 Such comparison-based reflection and construction could facilitate fine-grained and effective benchmark function dis-covery through the co-evolution of both code-level programs and corresponding landscape-level characteristics. 4Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

Algorithm 1 Evolution of Benchmark (EoB) 

Input : Large Language Model LLM , Target BBO tasks 

Ftarget , BBO algorithm pool Λ, #Reference vectors N ,#Evolution generations T , Pareto set P F = ∅.

Output : Finally obtained benchmark  

> 1:

Let t = 0 , initialize population {f ti }Ni=1 (Sec. 3.2.1);  

> 2:

Evaluate each f ti (Sec. 3.2.2), update P F (Sec. 3.2.5);  

> 3:

while t < T do  

> 4:

Reproduce offspring {f t+1  

> i

}Ni=1 (Sec. 3.2.3);  

> 5:

Evaluate each f t+1  

> i

(Sec. 3.2.2);  

> 6:

Select elites from {f ti }Ni=1 ∪ { f t+1  

> i

}Ni=1 (Sec. 3.2.4);  

> 7:

Update P F (Sec. 3.2.5) with elites, t ← t + 1 ; 

> 8:

end while  

> 9:

return P F 

3.2.4. S ELECTION 

Given the parent population {f ti }Ni=1 and the newly repro-duced offspring population {f t+1  

> i

}Ni=1 , we first compare the PBI scores between f ti and f t+1  

> i

; the one with the higher score survives. To further enhance exploration and exploita-tion in the evolutionary process, we additionally allow com-parison between f t+1  

> i

and the neighbors of f ti . A neighbor-ing parent program will be replaced by f t+1  

> i

if its PBI is lower. We restrict such neighboring selection operations to 2 times per individual to avoid premature convergence. 3.2.5. P ARETO FRONT MAINTENANCE 

EoB maintains the up-to-date Pareto front set along the itera-tive program evolution. In this work, we define Pareto dom-inance between two programs fi and fj as: fi dominates 

fj if and only if O1(fi) ≥ O 1(fj ) and O2(fi) ≥ O 2(fj ),and vise versa. A program is regarded as a Pareto front if all other programs in the population cannot dominate it. Initially, an empty Pareto front set P F is established. After initialization, P F is updated by the non-dominated programs in the population. At subsequent generations, P F 

is updated by first being concatenated with the selected non-dominated elite programs (from the union of parents and offspring), and then retaining those non-dominated ones. After all designs have been introduced above, we present the overall workflow of EoB in Alg. 1, which shows a simple pipeline. After the evolution, each program in the Pareto front set can be regarded as a good benchmark function. The whole set can hence be regarded as a complete benchmark, robustly providing both desired problem landscape diversity and effective algorithm distinguishing capabilities.. 

4. Experimental Results 

In this section, we provide empirical validation to demon-strate 1) EoB provides an automated way to address multi-dimensional benchmarking needs in BBO community; 2) Each design proposed for EoB is effective. In the rest of this section, we first summarize basic experimental settings in Sec. 4.1 and then dive into the above two aspects in Sec. 4.2, Sec. 4.3 and Sec. 4.4. 

4.1. Basic Setup EoB. The base LLM used in EoB is Deepseek-v3.2 (Liu et al., 2025a) with its default setting. For all of the follow-ing experiments, we set the number of reference vectors 

N = 32 (which is also the population size), the number of evolution generations T = 10 . As for the BBO algo-rithm pool Λ, we select from related literature 10 represen-tative BBO algorithms: Vanilla DE (Storn & Price, 1997), JDE21 (Brest et al., 2021), MadDE (Biswas et al., 2021), NL-SHADE-LBC (Stanovov et al., 2022), PSO (Kennedy & Eberhart, 1995), GLPSO (Gong et al., 2015), sDMS-PSO (Liang et al., 2015b), SAHLPSO (Tao et al., 2021), CMA-ES (Hansen et al., 2003), Sep-CMA-ES (Ros & Hansen, 2008). They cover diverse categories and show diverse optimization capabilities in our preliminary study, hence beneficial for measuring ADC score. We use these 

L = 10 algorithms and B = 10 independent runs to com-pute ADC scores. The weighted sum strength factor θ in PBI score computation is set to 5. At last, we note that the choice of target BBO tasks Ftarget varies in different experiments to demonstrate multi-dimensional usages of EoB, which we will detail in each experiment section. 

Involved Benchmarks. Several benchmarks are involved to serve as both the compared baselines and the target BBO tasks Ftarget : 1) CoCo-BBOB (Hansen et al., 2010; 2021): which includes 24 hand-crafted synthetic functions with di-verse landscape characteristics. It is the most representative BBO benchmark used by optimization researchers to eval-uate and compare their BBO algorithms. 2) MetaBox (Ma et al., 2023b; 2025c), a benchmark platform proposed re-cently for learning-based BBO algorithms (Ma et al., 2025d). It contains a wide array of realistic benchmarks such as UAV path planning (Shehadeh & Kudela, 2025) (56 testing cases), hyper-parameter optimization (Arango et al., 2021) (935 testing cases). We use UAV, HPO to denote them. All ex-periments are conducted on a platform equipped with an RTX 2080Ti 11GB GPU, an Intel Xeon E5-2680 v4 @ 56x 3.3GHz CPU, and 128GB RAM. We provide our project at 

https://anonymous.4open.science/r/EoB .

4.2. Benchmarking BBO algorithms 

One of the primary usages of EoB is to extend (construct) from representative benchmarks to enhance their bench-marking capability, such as the algorithm distinguishing capability. To this end, we set the target BBO tasks Ftarget 

as CoCo-BBOB benchmark, and run EoB (Alg. 1) multiple 5Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model Insight      

> Global asymmetry and hard
> splits improve ADC but
> destroyed LSI.
> Strategy
> Discard asymmetry term to
> restore the symmetric core,
> then apply a Gaussian
> Envelope to confine valley.
> Adding Valley
> Insight
> Valley’s magnitude (0.9) is
> too disruptive for the global
> landscape.
> Strategy
> Fine -tune gain to 0.2, which
> adds subtle ADC without
> overwhelming LSI.
> Smoothing Valley
> Insight
> Excessive rotational
> symmetry limits ADC.
> Strategy
> Inject dimension -specific
> asymmetry to break
> invariance while preserving
> the global structure.
> Break Symmetry
> Gen 0 : Symmetric Oscillator Gen 8 : Enveloped Valley Gen 10 : Smooth Valley Gen 0 : Symmetric Oscillator Gen 4 : Asymmetric Hybrid Gen 8 : Enveloped Valley Gen 10 : Smooth Valley

(ADC Move)  (LSI Move)                                                                                             

> def func (self ,x):
> phase_shift =np.sin(x) +
> np.cos(x[:, :: -1])
> amplitude =np.tanh(np.exp( -
> np.abs(phase_shift)))
> modulated =
> np.log(np.abs(amplitude) +1)*
> phase_shift
> perturbed =np.cos(modulated) *
> np.sin(modulated[:, :: -1]+x)
> entangled =np.exp( -
> np.abs(perturbed)) *np.tanh(x +
> perturbed)
> res =
> np.mean(np.sqrt(np.abs(entangled) +
> 1e -08 )*np.cos(entangled), axis =1)
> return res
> def func (self ,x):
> phase_shift =np.sin(x) +
> np.cos(x[:, :: -1])
> amplitude =np.tanh(np.exp( -
> np.abs(phase_shift)))
> asymmetric =np.sin(x[:, : 3]*
> x[:, -3:]) *amplitude[:, : 3]
> perturbed =np.cos(amplitude *
> phase_shift) *
> np.sin(phase_shift[:, :: -1]+
> asymmetric)
> entangled =... (same as Gen 0)
> res =... (same as Gen 0)
> return res
> def func (self ,x):
> phase_shift, amplitude,
> modulated, perturbed, entangled
> =... (same as Gen 0)
> mask =np.cos(x) >0
> envelope =np.exp( -
> np.power (x, 2))
> trap_perturbation =envelope *
> np.where (mask, np.sin( np.pi *
> np.abs(x)), np.tanh(x) /
> (np.abs(x) +1.5 ))
> entangled =entangled *(1+0.9 *
> trap_perturbation )
> res =... (same as Gen 4)
> return res
> def func (self ,x):
> phase_shift, amplitude,
> modulated, perturbed, entangled
> =... (same as Gen 0)
> mask,envelop,trap_perturbation
> =... (same as Gen 8)
> entangled =entangled *(1+0.2 *
> trap_perturbation )
> res =... (same as Gen 8)
> return res
> DeepSeek -V3.2
> Evolution of benchmark program
> Evolution of landscape by reflection
> LSI

(b) 

(c) (a)  The evolution of Pareto programs   

> (b)
> (c)
> ADC
> Landscape Similarity Indicator
> Algorithm Distinguishing Capability
> 2D landscape visualization

0 .1 0 20. .3 0 .4 0

1-LSI 

0.1 0.2 0.3 0.4 0.5 

> 1-ADC

(a) 

(ADC Move) 

Figure 2. Visualization of EoB on the co-evolution of the benchmark program and corresponding landscape characteristics. 

times to obtain 24 10-dimensional function programs 1 (the same size as CoCo-BBOB), which we name the searched benchmark as EoB-BBOB. We leave the detailed function programs in EoB-BBOB at our project. We provide follow-ing interesting results to present how well EoB performs. 4.2.1. E VOLUTION EFFECTIVENESS 

We first illustrate the evolution path of EoB’s program evo-lution process in Fig. 2. In the left of this figure, we plot the pareto front improvement during the evolution, which shows that the programs evolved by EoB continuously improve both the LSI and ADC scores along the program evolu-tion, clearly demonstrating the effectiveness of EoB. We further delve into the details of such effective evolution in the right of this figure, where we showcase the concrete evo-lution path of a program individual (from its initialization to the end of search). The results there validate a key design motivation we mentioned before: by using LLM-based pro-gram evolution, EoB is capable of evolving the landscape of benchmark functions and its code-level implementation simultaneously. Driven by our proposed LLM-based bi-objective evolution, the function program is evolved step by step with clear and interpretable design thoughts. This significantly mitigates the expert-level knowledge and labor input required to design a benchmark. 4.2.2. B ENCHMARK DIVERSITY COMPARISON 

We further illustrate the instance-level diversity (more uni-form is better) of CoCo-BBOB and the searched EoB-BBOB in Fig. 3. The results there are obtained by first testing the 10 algorithms in Λ on each of the 24 testing instances in a benchmark for 10 independent runs. For i-th testing instance and j-th run, we record the standard devia-tion of best-so-far objective values (min-max normalized) 

1One EoB run can not ensure the finally obtained P F contains 24 non-dominated programs, multiple runs (2-3) are needed. -0.1 -0.05 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 

Difficulty Level 

0510 15 20 

> Percentage (%)

CoCo-BBOB EoB-BBOB 

Figure 3. Benchmark diversity of CoCo-BBOB and EoB-BBOB. 

found by the 10 algorithms. A larger standard deviation indicates the testing instance is more challenging and hence easier to distinguish different BBO algorithms. Then, we obtain 240 standard deviation values which we use to plot the distribution density in Fig. 3. The results may indi-cate that: 1) Almost all hand-crafted synthetic functions in CoCo-BBOB show low algorithm distinguishing capability and difficulty diversity; 2) EoB could automatically design a more uniform and diversified benchmark by LLM-based program evolution, which is promising. 

4.3. Training Learnable Optimizer 

In this section we explore whether EoB could also address the new benchmarking needs in our community. More re-cently, learning-based BBO algorithms (Li et al., 2024a; Yang et al., 2025; Ma et al., 2025d; Kimiaei & Kungurtsev, 2025) emerge as an active and promising research avenue. They address the potential limitations in human-crafted BBO algorithms such as the adaptability issue and limited generalization by meta-learning (Finn et al., 2017; Thrun & Pratt, 1998) effective algorithm control policy that boosts classical BBO algorithms across a problem distribution. The finally learned BBO algorithms hence show preferred gener-6Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 0.4 0.5 0.6 0.7 0.8              

> Performance
> LDE
> CoCo-BBOB EoB-BBOB EoB-UAV EoB-HPO 0.5 0.6 0.7 0.8
> GLEET
> 0.7 0.8
> SYMBOL
> 0.5 0.6 0.7 0.8
> RLDEAFL
> 010 20 30
> Step
> 0.1 0.2 0.3 0.4
> Performance
> 0510 15
> Step
> 0.1 0.2 0.3 0.4 0.5 0510 15
> Step
> 0.1 0.2 0.3 0.4 0.5 0510 15
> Step
> 0.1 0.2 0.3 0.4 0.5
> UAV HPO

Figure 4. Zero-shot generalization performances of four learnable optimizers when trained by different benchmarks. Testing perfor-mance curves are presented. 

alization towards unseen BBO tasks. Such paradigm gives birth to a large array of learnable optimizers (Guo et al., 2024; Zhao et al., 2024; Lange et al., 2023; Wu et al., 2025; Ma et al., 2025a; Gao et al., 2025; Du et al., 2025; Tian et al., 2025; Lu et al., 2025; Han et al., 2025; Pei et al., 2025). In this experiment, to ensure fast implementation and rigorous experimental protocol, we use four MetaBBO baselines well-maintained in MetaBox (Ma et al., 2023b; 2025c): LDE (Sun et al., 2021), GLEET (Ma et al., 2024), SYMBOL (Chen et al., 2024) and RLDEAFL (Guo et al., 2025). 4.3.1. G ENERALIZATION ADVANTAGE 

The most important aspect cared about by the users of learn-able optimizers is its true generalization potential. To vali-date EoB’s advantage on this aspect, we establish two vali-dation scenarios: 1) A user trains learnable optimizer on a benchmark and then expects the trained model generalizes well on UAV tasks; 2) The same scenario yet the target tasks are HPO tasks. The benchmark baselines used for training in 1) are CoCo-BBOB, EoB-BBOB we searched in Sec 4.2 and EoB-UAV, which is searched by EoB using UAV-train-set 2 as the Ftarget in Alg. 1. And the baselines for 2) are CoCo-BBOB, EoB-BBOB and EoB-HPO (HPO-train-set as Ftarget ). For the four learnable optimizer baselines LDE, GLEET, SYMBOL and RLDEAFL, we train them on the two scenarios by their default settings and the three bench-mark baselines in the two scenarios respectively, and test the trained models on UAV-train-set and HPO-train-set for 10 independent optimization episodes. We plot the opti-mization curves in terms of generalization performances in Fig. 4, where the results clearly underscore that: 1) Through the bi-objective program evolution, the benchmark opti-mized by EoB could provide high-quality training source for learnable optimizers; 2) CoCo-BBOB, while regarded as a golden standard for evaluating classic BBO algorithms, may not be a proper choice for learning-assisted methods 

> 2

Benchmarks in MetaBox are split into a train/test set to support learnable optimizers. 

Table 1. Evaluation efficiency comparison.                                            

> Algorithm Scenario UAV HPO
> Real EoB Real EoB LDE Normal 30.38 min 3.124 s 23.72 min 2.923 s
> Large ≈7 h 28.32 s ≈4 h 27.43 s
> GLEET Normal 26.23 s 0.256 s 14.68 s 0.231 s
> Large 284.2 s 0.287 s 146.6 s 0.274 s
> SYMBOL Normal 43.68 s 0.423 s 190.8 s 0.412 s
> Large 78.95 s 0.528 s 312.1 s 0.547 s
> RLDEAFL Normal 98.24 s 0.921 s 15.98 s 0.852 s
> Large 22.91 min 1.442 s 205.7 s 1.352 s

Table 2. GLHF Performance                

> Scenario CoCo-BBOB EoB-BBOB EoB-Scenario
> UAV 8.380E-01 7.508E-01 8.484E-01 ↑±5.193E-02 ±1.149E-01 ±3.854E-02
> HPO 5.144E-01 5.214E-01 6.580E-01 ↑±2.113E-01 ±1.769E-01 ±1.933E-01

due to its biased human design. However, up to now, it is still the first choice in many learnable optimizers. The searched programs in EoB-UAV/HPO are at our project. We also want to emphasize the advantage of EoB in the efficiency side. Realistic BBO task such as UAV and HPO we discussed above are often quite expensive (either time-consuming or resource-consuming). However, training an advanced learnable optimizer require repeated interplay with the target BBO tasks. If the target BBO tasks are expensive, training on them is impractical. To demonstrate this aspect, we present at Table 1 the average time consumed by the four learnable optimizer baselines per learning step when they use UAV/HPO as training set directly or use EoB-UAV/EoB-HPO instead. A significant efficiency leap could be observed from the results, which highlights an additional usage of EoB: searching for proxy benchmark for those expensive and hard-to-model BBO tasks. 4.3.2. G RADIENT -DEPENDENT CASE 

Another interesting usage of EoB is when we consider a spe-cific kind of learnable optimizer that requires the gradient information of the training problems to supervise its neural network model. An example is GLHF (Li et al., 2024b). This specific feature hinders such learnable optimizers from direct training on realistic BBO tasks where gradient infor-mation is not accessible, e.g., UAV/HPO. Fortunately, EoB could address this issue by searching executable function programs that show both high similarity (LSI) with the tar-get tasks and substantial difficulty diversity (ADC), e.g., EoB-UAV/HPO. More importantly, they can be easily trans-formed into any autograd framework such as torch and jax ,by LLM. The results in Table 2 validate EoB’s advantage for GLHF-like learnable optimizers. 7Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

Table 3. Performance Consistency                         

> Scenario CoCo-BBOB EoB-BBOB EoB-Scenario
> UAV 0.003 0.396 0.453 ↑
> HPO 0.694 0.702 0.761 ↑0.2 0.4 0.6 0.8 1
> 1 - LSI
> 00.1 0.2 0.3 0.4 0.5
> Ours Nneighbor =0
> Nneighbor =10
> 0.2 0.4 0.6 0.8 1
> 1 - LSI
> 00.1 0.2 0.3 0.4
> Ours No reflection No knowledge
> 0.2 0.4 0.6 0.8 1
> 1 - LSI
> 00.2 0.4 0.6 0.8
> 1 - ADC
> DeepSeek-V3.2 Qwen Plus GPT-4.1 Mini Gemini-2.5 Flash
> θ=1
> =10 θ

Figure 5. The ablation results shown as the pareto front set P F 

found by different baselines. 

4.3.3. K EY REASON BEHIND 

At last, we give a simple yet intuitive interpretation on the reason behind EoB’s strength. Specifically, we first define a simple metric C(Bsource , B target ) that measures the performance consistency between a source benchmark 

Bsource and a target benchmark Btarget . Specifically, we first test the 10 BBO algorithms in our proposed algorithm pool Λ on Bsource to obtain a 10-dimensional performance vector ⃗ Ssource : { 1 

> rank i

}10 

> i=1

, where rank i is the average rank of i-th algorithm in Λ. We use the same logic to obtain ⃗Starget . Then the performance consistency is computed as: 

C(Bsource , B target ) = 1 − W (⃗ Ssource ,⃗ S target ), (5) where W(·, ·) denotes the Wasserstein distance. A larger consistency score indicates that the performance of BBO algorithms on the source benchmark is more consistent with the performance on the target task. Such performance pre-dictivity may support the generalization of learnable opti-mizers. To validate this, we present in Table 3 the perfor-mance consistency between {CoCo-BBOB, EoB-BBOB, EoB-UAV, EoB-HPO } and {UAV, HPO }. The results there, combined with the results in Fig. 4 and Table 2 cross-validate what we have assumed. 

4.4. Ablation Studies 

To validate the contribution of each component within EoB, we conducted three ablation studies: 1) LLM Backbone Capability, assessing the impact of the underlying model on evolutionary efficiency; 2) Hyperparameter Sensitivity, analyzing the effects of neighborhood size ( Nneighbor ) and PBI penalty ( θ) on multi-objective optimization; and 3) Key Component Analysis, removing the Reflection Stage and Landscape Initialization Knowledge. Using the UAV-train set as the target task, the resulting Pareto fronts (Fig. 5) reveal that: 1) Deepseek-v3.2 achieves the most balanced performance among the backbones. 2) Diversity-oriented settings ( θ = 10 , N neighbor = 10 ) enhance LSI but com-promise the objective trade-off, whereas exploitation-heavy settings ( θ = 1 , N neighbor = 1 ) suffer from low efficiency due to restricted information exchange. 3) The absence of Landscape Knowledge causes LSI stagnation, while re-moving the Reflection Stage degrades overall efficiency by severing the feedback loop of comparative insights. 

5. Discussion 

Takeaways. 1) If you are seeking a more challenging syn-thetic benchmark to test your BBO algorithms, feel free to use our EoB-BBOB; 2) If you are developing a learning-assisted BBO algorithm to solve some kind of expensive realistic tasks, run EoB to attain cheap synthetic benchmark to save your time and money; 3) You may consider using EoB to assist your analysis on some hard-to-model BBO tasks by extending their proxy. 

Future Directions. We would like to discuss a few promis-ing directions. The first is extending EoB towards diverse optimization categories, e.g., constrained optimization (Wu et al., 2017), multi-objective optimization (Zhang et al., 2008) and dynamic optimization (Zhong et al., 2024), for which EoB provides a universal and automated pipeline. Another important direction is to make the benchmark de-sign objectives more fine-grained. While two design ob-jectives we considered in this paper more or less provide an effective way to ensure comprehensive benchmarking, additional design objectives such as computational complex-ity (Huang et al., 2024) and surrogate difficulty (Ma et al., 2025f) could definitely help benchmarking in complex opti-mization tasks. At last, one may also consider using EoB’s paradigm for research fields such as AutoML (Gijsbers et al., 2024), AI4Science (Wei et al., 2025) and Benchmark of Benchmark (Qian et al., 2026), however, more efforts and certain expertise are still needed for adaption. 

6. Conclusion 

In this work, we address the bottleneck in benchmark design for the BBO field by replacing the laborious hand-crafted design process with an automatic paradigm. To this end, we propose EoB, the first systematic framework to facilitate the evolution of high-quality benchmarks. EoB operates under a bi-objective LLM-based program search, aiming to discover evaluation programs (BBO instances) that si-multaneously exhibit problem landscape diversity and al-gorithm distinguishing capabilities. EoB is fully automatic through iterative multi-turn LLM conversations, ensuring minimal human-level design bias. Beyond its automation, EoB’s end-to-end paradigm supports diverse needs, includ-ing synthetic benchmark construction, training set design for learning-based BBO algorithms and proxy extension for hard-to-model BBO tasks. Utimately, this work not only 8Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

introduces a powerful new tool for automatic benchmark generation, but it also highlights the indispensable role of thoughtfully designed evaluation landscapes in advancing BBO research. 

Impact Statement 

We claim a significant impact of our work on both research development in the optimization community and, more im-portantly, its foundational impact on how researchers and engineers approach the evaluation, refinement, and redesign of optimization algorithms. By automating the benchmark design process, the substantial labor resources and special-ized expertise costs could be significantly reduced. Such an automated pipeline also helps mitigate human-based bias to assure benchmarking objectivity. This paradigm shift will significantly transform the evaluation and refinement of op-timization algorithms, enabling more rigorous and objective comparisons that accelerate advancements across BBO and its diverse applications 

References 

Akiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , pp. 2623–2631, 2019. Arango, S. P., Jomaa, H. S., Wistuba, M., and Grabocka, J. Hpo-b: A large-scale reproducible benchmark for black-box hpo based on openml. arXiv preprint arXiv:2106.06257 , 2021. Beyer, H.-G. and Schwefel, H.-P. Evolution strategies–a comprehensive introduction. Natural Computing , 2002. Biswas, S., Saha, D., De, S., Cobb, A. D., Das, S., and Jalaian, B. A. Improving differential evolution through bayesian hyperparameter optimization. In CEC , 2021. Brest, J., Mau ˇcec, M. S., and Bo ˇskovi ´c, B. Self-adaptive dif-ferential evolution algorithm with population size reduc-tion for single objective bound-constrained optimization: Algorithm j21. In CEC , 2021. Byrd, R. H., Hansen, S. L., Nocedal, J., and Singer, Y. A stochastic quasi-newton method for large-scale optimiza-tion. SIAM Journal on Optimization , 26(2):1008–1031, 2016. Cenikj, G., Nikolikj, A., Petelin, G., van Stein, N., Do-err, C., and Eftimov, T. A survey of features used for representing black-box single-objective continuous opti-mization. Swarm and Evolutionary Computation , 101: 102288, 2026. Chen, A., Dohan, D., and So, D. Evoprompting: Lan-guage models for code-level neural architecture search. 

Advances in neural information processing systems , 36: 7787–7817, 2023. Chen, J., Ma, Z., Guo, H., Ma, Y., Zhang, J., and Gong, Y.-j. Symbol: Generating flexible black-box optimizers through symbolic equation learning. In ICLR , 2024. Da, B., Ong, Y.-S., Feng, L., Qin, A. K., Gupta, A., Zhu, Z., Ting, C.-K., Tang, K., and Yao, X. Evolutionary multitasking for single-objective continuous optimization: Benchmark problems, performance metric, and baseline results. arXiv preprint arXiv:1706.03470 , 2017. Deb, K., Sindhya, K., and Hakanen, J. Multi-objective optimization. In Decision sciences , pp. 161–200. CRC Press, 2016. Doerr, C., Ye, F., Horesh, N., Wang, H., Shir, O. M., and B ¨ack, T. Benchmarking discrete optimization heuristics with iohprofiler. In GECCO , 2019. Du, Y., Yu, H., Xie, X., Zheng, Y., Zhan, L., Du, Y., Hu, C., Wang, B., and Jiang, J. Meta-black-box optimization with bi-space landscape analysis and dual-control mechanism for saea. arXiv preprint arXiv:2511.15551 , 2025. Duan, Q., Zhou, G., Shao, C., Wang, Z., Feng, M., Huang, Y., Tan, Y., Yang, Y., Zhao, Q., and Shi, Y. Pypop7: A pure-python library for population-based black-box optimization. Journal of Machine Learning Research ,2024. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML ,2017. Gao, J., Liu, Y., Cheng, R., and Tan, K. C. Learning to evolve with convergence guarantee via neural unrolling. 

arXiv preprint arXiv:2512.11453 , 2025. Gen, M. and Cheng, R. Genetic algorithms and engineering optimization . John Wiley & Sons, 1999. Gijsbers, P., Bueno, M. L., Coors, S., LeDell, E., Poirier, S., Thomas, J., Bischl, B., and Vanschoren, J. Amlb: an automl benchmark. Journal of Machine Learning Research , 25(101):1–65, 2024. Golovin, D., Solnik, B., Moitra, S., Kochanski, G., Karro, J., and Sculley, D. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining , pp. 1487–1495, 2017. Gong, Y.-J., Li, J.-J., Zhou, Y., Li, Y., Chung, H. S.-H., Shi, Y.-H., and Zhang, J. Genetic learning particle swarm optimization. IEEE TC , 2015. 9Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

Guo, H., Ma, Y., Ma, Z., Chen, J., Zhang, X., Cao, Z., Zhang, J., and Gong, Y.-J. Deep reinforcement learning for dynamic algorithm selection: A proof-of-principle study on differential evolution. TSMC , 2024. Guo, H., Ma, S., Huang, Z., Hu, Y., Ma, Z., Zhang, X., and Gong, Y.-J. Reinforcement learning-based self-adaptive differential evolution through automated landscape fea-ture learning. arXiv preprint arXiv:2503.18061 , 2025. Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. Connecting large language mod-els with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532 , 2023. Han, M., Li, X., Wu, K., Zhang, X., and Wang, H. En-hancing zero-shot black-box optimization via pretrained models with efficient population modeling, interaction, and stable gradient approximation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. Hansen, N., M ¨uller, S. D., and Koumoutsakos, P. Reduc-ing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es). Evo-lutionary computation , 2003. Hansen, N., Auger, A., Finck, S., and Ros, R. Real-parameter black-box optimization benchmarking 2010: Experimental setup . PhD thesis, INRIA, 2010. Hansen, N., Auger, A., Ros, R., Mersmann, O., Tu ˇsar, T., and Brockhoff, D. Coco: A platform for comparing con-tinuous optimizers in a black-box setting. Optimization Methods and Software , 2021. Holland, J. H. Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence . MIT press, 1992. Huang, B., Cheng, R., Li, Z., Jin, Y., and Tan, K. C. Evox: A distributed gpu-accelerated framework for scalable evolu-tionary computation. IEEE Transactions on Evolutionary Computation , 2024. Hwang, H., Vreven, T., Janin, J., and Weng, Z. Protein– protein docking benchmark version 4.0. Proteins: Struc-ture, Function, and Bioinformatics , 2010. Jones, D. R., Schonlau, M., and Welch, W. J. Efficient global optimization of expensive black-box functions. Journal of Global optimization , 13(4):455–492, 1998. Kennedy, J. and Eberhart, R. Particle swarm optimization. In Proceedings of ICNN’95-International Conference on Neural Networks , 1995. Kimiaei, M. and Kungurtsev, V. Machine learning algo-rithms for improving black box optimization solvers. 

arXiv preprint arXiv:2509.25592 , 2025. Kostovska, A., Jankovic, A., Vermetten, D., D ˇzeroski, S., Eftimov, T., and Doerr, C. Comparing algorithm selec-tion approaches on black-box optimization problems. In 

Proceedings of the Companion Conference on Genetic and Evolutionary Computation , pp. 495–498, 2023. Lange, R. T., Schaul, T., Chen, Y., Zahavy, T., Dalibard, V., Lu, C., Singh, S., and Flennerhag, S. Discovering evolution strategies via meta-black-box optimization. In 

The Eleventh International Conference on Learning Rep-resentations , 2023. Li, P., Hao, J., Tang, H., Fu, X., Zhen, Y., and Tang, K. Bridging evolutionary algorithms and reinforcement learning: A comprehensive survey on hybrid algorithms. 

IEEE Transactions on evolutionary computation , 2024a. Li, X., Wu, K., Zhang, X., Wang, H., Liu, J., et al. Pre-trained optimization model for zero-shot black box op-timization. Advances in Neural Information Processing Systems , 2024b. Liang, J., Qu, B., Suganthan, P., and Chen, Q. Problem definitions and evaluation criteria for the cec 2015 com-petition on learning-based real-parameter single objective optimization. In Proceedings of the 2015 IEEE Congress on Evolutionary Computation , 2015a. Liang, J. J., Guo, L., Liu, R., and Qu, B.-Y. A self-adaptive dynamic particle swarm optimizer. In 2015 IEEE Congress on Evolutionary Computation (CEC) , pp. 3206–3213. IEEE, 2015b. Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. Deepseek-v3. 2: Pushing the frontier of open large language models. 

arXiv preprint arXiv:2512.02556 , 2025a. Liu, F., Tong, X., Yuan, M., Lin, X., Luo, F., Wang, Z., Lu, Z., and Zhang, Q. Evolution of heuristics: Towards efficient automatic algorithm design using large language model. arXiv preprint arXiv:2401.02051 , 2024. Liu, F., Lin, X., Yao, S., Wang, Z., Tong, X., Yuan, M., and Zhang, Q. Large language model for multiobjective evolutionary optimization. In International Conference on Evolutionary Multi-Criterion Optimization , pp. 178– 191. Springer, 2025b. Long, F. X., Vermetten, D., Kononova, A. V., Kalkreuth, R., Yang, K., B ¨ack, T., and van Stein, N. Challenges of ela-guided function evolution using genetic programming. In IJCCI , 2023. 10 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

Lu, C., Xue, K., Yuan, L., Wang, Y., Wang, Y., Sheng, F., and Qian, C. Sequential multi-agent dynamic algorithm configuration. In The Thirty-ninth Annual Conference on Neural Information Processing Systems , 2025. Ma, Y. J., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A. Eureka: Human-level reward design via coding large lan-guage models. arXiv preprint arXiv:2310.12931 , 2023a. Ma, Z., Guo, H., Chen, J., Li, Z., Peng, G., Gong, Y.-J., Ma, Y., and Cao, Z. Metabox: A benchmark platform for meta-black-box optimization with reinforcement learning. In NeurIPS , 2023b. Ma, Z., Chen, J., Guo, H., Ma, Y., and Gong, Y.-J. Auto-configuring exploration-exploitation tradeoff in evolu-tionary computation via deep reinforcement learning. In 

GECCO , 2024. Ma, Z., Cao, Z., Jiang, Z., Guo, H., and Gong, Y.-J. Meta-black-box-optimization through offline q-function learn-ing. In ICML , 2025a. Ma, Z., Chen, J., Guo, H., and Gong, Y.-J. Neu-ral exploratory landscape analysis for meta-black-box-optimization. In ICLR , 2025b. Ma, Z., Gong, Y.-J., Guo, H., Qiu, W., Ma, S., Lian, H., Zhan, J., Chen, K., Wang, C., Huang, Z., et al. Metabox-v2: A unified benchmark platform for meta-black-box optimization. arXiv preprint arXiv:2505.17745 , 2025c. Ma, Z., Guo, H., Gong, Y.-J., Zhang, J., and Tan, K. C. Toward automated algorithm design: A survey and practi-cal guide to meta-black-box-optimization. IEEE TEVC ,2025d. Ma, Z., Huang, W., Song, G.-H., Guo, H., Ma, S., Cao, Z., and Gong, Y.-J. Evolutionary system 2 reasoning: An empirical proof. arXiv preprint arXiv:2512.05760 ,2025e. Ma, Z., Huang, Z., Chen, J., Cao, Z., and Gong, Y.-J. Surro-gate learning in meta-black-box optimization: A prelimi-nary study. arXiv preprint arXiv:2503.18060 , 2025f. Mersmann, O., Bischl, B., Trautmann, H., Preuss, M., Weihs, C., and Rudolph, G. Exploratory landscape anal-ysis. In Proceedings of the 13th annual conference on Genetic and evolutionary computation , 2011. Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? 

arXiv preprint arXiv:2202.12837 , 2022. Mohamed, A. W., Hadi, A. A., Mohamed, A. K., Agrawal, P., Kumar, A., and Suganthan, P. N. Problem definitions and evaluation criteria for the CEC 2021 special session and competition on single objective bound constrained numerical optimization. Technical report, 2021. Mu ˜noz, M. A. and Smith-Miles, K. Generating new space-filling test instances for continuous black-box optimiza-tion. Evolutionary computation , 2020. Mu ˜noz, M. A., Kirley, M., and Halgamuge, S. K. Ex-ploratory landscape analysis of continuous space opti-mization problems using information content. IEEE trans-actions on evolutionary computation , 19(1):74–87, 2014. Novikov, A., V ˜u, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J., Mehrabian, A., et al. Alphaevolve: A coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131 , 2025. Pei, J., Mei, Y., Liu, J., and Zhang, M. Libog: Lifelong learn-ing for black-box optimizer generation. arXiv preprint arXiv:2505.13025 , 2025. Qian, Q., Huang, C., Xu, J., Lv, C., Wu, M., Liu, W., Wang, X., Wang, Z., Huang, Z., Tian, M., et al. Benchmarkˆ 2: Systematic evaluation of llm benchmarks. arXiv preprint arXiv:2601.03986 , 2026. Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S., Wang, P., Fawzi, O., et al. Mathematical discoveries from program search with large language models. Nature , 625 (7995):468–475, 2024. Ros, R. and Hansen, N. A simple modification in cma-es achieving linear time and space complexity. In Inter-national conference on parallel problem solving from nature , 2008. Seiler, M. V., Kerschke, P., and Trautmann, H. Deep-ela: Deep exploratory landscape analysis with self-supervised pretrained transformers for single-and multi-objective continuous optimization problems. Evolutionary Compu-tation , pp. 1–27, 2025. Shehadeh, M. A. and Kudela, J. Benchmarking global optimization techniques for unmanned aerial vehicle path planning, 2025. URL https://arxiv.org/abs/ 2501.14503 .Snoek, J., Larochelle, H., and Adams, R. P. Practical bayesian optimization of machine learning algorithms. 

Advances in neural information processing systems , 25, 2012. 11 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

Stanovov, V., Akhmedova, S., and Semenkin, E. Nl-shade-lbc algorithm with linear parameter adaptation bias change for cec 2022 numerical optimization. In 2022 IEEE CEC , 2022. Storn, R. and Price, K. Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. Journal of Global Optimization , 1997. Sun, J., Liu, X., B ¨ack, T., and Xu, Z. Learning adaptive differential evolution algorithm from optimization experi-ences by policy gradient. IEEE TEVC , 2021. Tao, X., Li, X., Chen, W., Liang, T., Li, Y., Guo, J., and Qi, L. Self-adaptive two roles hybrid learning strategies-based particle swarm optimization. Information Sciences ,578:457–481, 2021. Terayama, K., Sumita, M., Tamura, R., and Tsuda, K. Black-box optimization for automated discovery. Accounts of Chemical Research , 54(6):1334–1346, 2021. Thrun, S. and Pratt, L. Learning to learn: Introduction and overview. In Learning to learn . 1998. Tian, Y., Cheng, R., Zhang, X., and Jin, Y. Platemo: A mat-lab platform for evolutionary multi-objective optimization [educational forum]. IEEE Computational Intelligence Magazine , 12(4):73–87, 2017. Tian, Y., Peng, S., Zhang, X., Rodemann, T., Tan, K. C., and Jin, Y. A recommender system for metaheuristic algorithms for continuous optimization based on deep recurrent neural networks. IEEE TAI , 2020. Tian, Y., Qi, X., Yang, S., He, C., Tan, K. C., Jin, Y., and Zhang, X. A universal framework for automatically gener-ating single-and multi-objective evolutionary algorithms. 

IEEE Transactions on Evolutionary Computation , 2025. van Stein, N. and B ¨ack, T. Llamea: A large language model evolutionary algorithm for automatically generat-ing metaheuristics. IEEE Transactions on Evolutionary Computation , 2024a. van Stein, N. and B ¨ack, T. Llamea: A large language model evolutionary algorithm for automatically generating meta-heuristics. IEEE TEVC , 2024b. Wang, C., Gong, Y.-J., Cao, Z., and Ma, Z. Instance genera-tion for meta-black-box optimization through latent space reverse engineering. arXiv preprint arXiv:2509.15810 ,2025. Wei, J., Yang, Y., Zhang, X., Chen, Y., Zhuang, X., Gao, Z., Zhou, D., Wang, G., Gao, Z., Cao, J., et al. From ai for science to agentic science: A survey on autonomous scientific discovery. arXiv preprint arXiv:2508.14111 ,2025. Wu, G., Mallipeddi, R., and Suganthan, P. N. Problem defini-tions and evaluation criteria for the cec 2017 competition on constrained real-parameter optimization. National University of Defense Technology, Changsha, Hunan, PR China and Kyungpook National University, Daegu, South Korea and Nanyang Technological University, Singapore, Technical Report , 9:2017, 2017. Wu, S.-H., Huang, Y., Wu, X., Feng, L., Zhan, Z.-H., and Tan, K. C. Learning to transfer for evolutionary multi-tasking. IEEE Transactions on Cybernetics , 2025. Xue, K., Chen, R.-T., Tan, R.-X., Lin, X., Shi, Y., Xu, S., Yuan, M., and Qian, C. Bboplace-bench: Benchmark-ing black-box optimization for chip placement. arXiv preprint arXiv:2510.23472 , 2025. Yang, X., Wang, R., Li, K., and Ishibuchi, H. Meta-black-box optimization for evolutionary algorithms: Review and perspective. Swarm and Evolutionary Computation ,93:101838, 2025. Yao, S., Liu, F., Lin, X., Lu, Z., Wang, Z., and Zhang, Q. Multi-objective evolution of heuristic using large language model. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 39, pp. 27144–27152, 2025. Yazdani, D., Peng, M., Yazdani, D., Yazdi, S. F., Omid-var, M. N., Sun, Y., Nguyen, T. T., Li, C., and Li, X. Portal: Controllable landscape generator for continu-ous optimization-part i: Framework. arXiv preprint arXiv:2512.00288 , 2025. Ye, H., Wang, J., Cao, Z., Berto, F., Hua, C., Kim, H., Park, J., and Song, G. Reevo: Large language models as hyper-heuristics with reflective evolution. Advances in neural information processing systems , 37:43571–43608, 2024. Zhang, Q. and Li, H. Moea/d: A multiobjective evolutionary algorithm based on decomposition. IEEE Transactions on evolutionary computation , 11(6):712–731, 2007. Zhang, Q., Zhou, A., Zhao, S., Suganthan, P. N., Liu, W., Ti-wari, S., et al. Multiobjective optimization test instances for the cec 2009 special session and competition. 2008. Zhao, Q., Liu, T., Yan, B., Duan, Q., Yang, J., and Shi, Y. Automated metaheuristic algorithm design with autore-gressive learning. IEEE TEVC , 2024. Zhong, Y., Wang, X., Sun, Y., and Gong, Y.-J. Sddobench: A benchmark for streaming data-driven optimization with concept drift. In Proceedings of the Genetic and Evolu-tionary Computation Conference , pp. 59–67, 2024. 12 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model You are an expert Python/Numpy programmer specializing in symbolic mathematical function                 

> construction.
> Your goal is to generate concise, vectorized, and syntactically correct Python code.
> ### Behavioral Guidelines:
> - Ensure the generated code runs without syntax errors.
> - Avoid imports beyond `import numpy as np`.
> - Use pure numpy-based vectorized operations only.
> - Follow the following given structural constraints strictly (e.g., class names, method signatures,
> etc.):
> ```python
> class GeneratedFunction:
> def __init__(self, dim = {self.dim}, ub = 5., lb = -5.):
> self.dim = dim
> self.ub = ub
> self.lb = lb
> def func(self, x):
> # vectorized expression here
> res = ...
> return res
> ```
> - Always ensure the generated class name is `GeneratedFunction`.
> - `__init__(self, dim, ub, lb)` —stores the dimensionality and x's bounds.
> - `func(self, x)` —accepts `x` as a numpy array of shape (num_of_x, dim).
> - Output a numpy array `res` of shape (num_of_x,).
> - Avoid mixing scalar and row vectors directly (e.g., x[:, 0] + np.sin(x) is invalid).
> Always reduce across columns using np.sum(..., axis=1) or np.mean(..., axis=1) when combining
> components.
> - Avoid mixing (n,) and (n,d) shapes directly.
> - 'axis = 0' is FORBIDDEN in this context.
> - Always output ONLY valid Python code —no explanations or comments.
> ...(The content below is in Part 2)
> ### Code Style Constraints (Strictly
> Enforced)
> 1. **Parsimony Principle **:The best solution
> is the simplest one. Do NOT create "spaghetti
> code" by stacking random operators.
> 2. **Functional Composition **:Prefer nested
> functional calls over defining many
> intermediate variables.
> -**Bad **:
> ```python
> a = np.sin(x)
> b = np.tanh(a)
> c = np.exp(b)
> ```
> -**Good **:
> ```python
> c = np.exp(np.tanh(np.sin(x)))
> ```
> 3. **Variable Limit **:You are allowed a
> MAXIMUM of 5-8 intermediate variables.
> Variables should only be defined if they
> represent a distinct, reusable physical concept
> (e.g., `amplitude`, `frequency`, `decay`).
> 4. **Vectorization **:Use NumPy
> vectorization efficiently. Avoid creating
> variables `A`, `B`, `C` just to sum them up at
> the end.

System Prompt (Part 1) System Prompt (Part 2) 

Figure 6. Code Generation Guideline (System Prompt) 

A. Prompts in EoB 

A.1. Code Generation Guideline (System Prompt) 

This prompt serves as the foundational rule set for the LLM, strictly regulating the format, interface, and stylistic quality of the generated Python code. The detailed design of the prompts is shown in fig 6. Each part of the prompt and its description will be highlighted in three different colors, as described below: • Interface Standardization: To ensure the generation of executable and mathematically coherent benchmarks, this prompt conditions the LLM as a specialized NumPy expert, enforcing a strict schema where outputs must be encapsulated within a ‘GeneratedFunction‘ class supporting vectorized inputs. Crucially, the prompt imposes rigorous syntactic and stylistic regularization to guarantee solvability and readability: it mandates shape consistency (e.g., prohibiting reduction along the batch dimension to preserve sample independence). • Parsimony Principle: To prevent the generated function code from becoming ”spaghetti-like,” our prompt design requires the LLM to avoid defining a large number of intermediate variables when generating nested structured functions. Instead, it should construct them by directly combining functions. • Concise, Efficient, and Meaningful: We restrict the number of intermediate variables generated by the LLM during the code generation process, requiring each intermediate variable to correspond to a specific physical concept to ensure meaningfulness. Additionally, we mandate the efficient use of NumPy operations during code generation to improve performance. 

A.2. Initialization 

In the strategy described in Section 3.2.1, we employ a carefully constructed Init Prompt to help LLM grasps the essential context information, such as its role (as an optimization expert and benchmark designer) and its specific task (generating program code for the objective function). The knowledge regarding the code generation of the objective function in the 13 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model You are an expert in *benchmark function synthesis 

for continuous optimization research *.

You need to generate functions that align with the 

goal in multi-objective framework guided by lambda 

vectors: [1, 0] , where: 

1 : Emphasize topological alignment with the target 

problem distribution manifold (LSI). 

0 : Emphasize discriminative hardness across the 

10-algorithm portfolio (ADC). 

The values in each dimension of the lambda vector 

indicate the importance of the corresponding 

objective, higher values mean greater emphasis on 

that objective. 

Design the function to strongly exhibit the 

characteristics emphasized by the lambda vector, 

allowing ignoring less important aspects. 

（Here is one randomly selected style template from 

the seven available styles of knowledge. ）

Only output the PYTHON code defined. No 

explanation and comment.                                    

> Asymmetric Masking
> Pattern spirit: Define **idx = x < k **or
> more nonlinear mask conditions (k is a
> random float or vector in [self.lb,
> self.ub], **set fixed seed = 42 **if randomly
> generated), and then use **idx **to
> construct the function structure for partial
> decision variables.
> Example:
> y = x.copy()
> pos_mask = (x > k) # boolean mask
> if np.any(pos_mask):
> y[pos_mask] = ...
> if np.any(~pos_mask):
> y[~pos_mask] = ...
> Nested Activation
> Pattern spirit: activation →abs →inverse/log
> Example:
> R = np.tanh(np.abs(x *c1)); A = 1 / (np.abs(R) + 1)
> Polynomially Warped Periodicity
> Pattern spirit: frequency/amplitude controlled
> by x nonlinearly
> Example: P = np.sin(x **2 + 0.3 *x**3)
> Chained Multi-Stage Perturbation
> Pattern spirit: A →B→C→back to A or x,
> with no dominant single stage
> Example: A = np.exp(-np.abs(x + b))
> B = np.log(np.abs(A) + 1)
> C = np.cos(B) *B
> Nonlinear Operator Switching
> Pattern spirit: symbolic triggers (sign or magnitude)
> modify operator flow
> Example: T = np.sign(np.sin(x) + 0.1 *x)
> Recursive Symbolic Feedback
> Pattern spirit: intermediate stage re-enters
> later symbolic stage
> Example: Z = np.sin(A) + A *np.cos(A)
> W = np.exp(-np.abs(Z)) + np.log(np.abs(Z)+1)
> Cross-Dimensional Operator Interaction
> Pattern spirit: NOT simple additive
> separability —geometry arises from mixing
> coordinates.
> Examples:
> X1 = y[:, :3] *y[:, -3:]
> X2 = y[:, 0:3] *y[:, 1:4] # sliding-window coupling
> I = np.sum(X2, axis=1) # →reduces to (n,)

User Prompt Seven Types of Knowledge 

Figure 7. Initialization Prompt (User) 

Init Prompt is provided by the System Prompt in Appendix A.1, while the knowledge related to the roles of the LLM as a benchmark designer and optimization expert is provided by the User Prompt . The content of the User Prompt is shown in Fig 7. The User Prompt consists of two major parts, which are highlighted in different colors in the following content and in Fig 7 : • Generation Preference. This part drives the generation of the initial individual by explicitly defining the multi-objective optimization context. The prompt explicitly maps the reference vector ⃗ λi = [ λi, 1, λ i, 2] dimensions to semantic objectives: λi, 1: Emphasizes topological alignment with the target problem distribution (LSI); λi, 2: Emphasizes discriminative hardness, encouraging the creation of ”strategy-breaker” landscapes that differentiate between algo-rithms (ADC). Guided by this part, LLM can design the function that strongly exhibits the characteristics emphasized by ⃗ λi, while ignoring less important aspects. • Population Diversity. To address the issue of LLM generating a large number of highly similar function instances due to a lack of landscape knowledge, which leads to a loss of diversity in the initial population, we constructed a knowledge base comprising seven types of landscape styles (as shown in the right part of Fig 7). By injecting random landscape style knowledge into the prompts used to generate initial individuals, we enhance both the code diversity and landscape diversity of the initial population. The seven types of landscape style knowledge are introduced as follows: 

– Asymmetric Masking. The Asymmetric Masking style explicitly disrupts global symmetry by applying distinct symbolic operations to sub-regions of the search space defined by boolean conditions (e.g., x < k ). By forcing the LLM to use np.where or boolean indexing, this style creates landscapes with sharp regional shifts and discontinuities, challenging algorithms that assume global rotational invariance. 

– Nested Activation. The Nested Activation style (often implemented via rational composition) creates landscapes with steep, controlled basins. It enforces a specific operator pipeline, typically composing activation functions with rational transformations (e.g., f (x) = 1

| tanh( x)|+1 ) to generate narrow valleys that test algorithmic stability without causing numerical overflow. 

– Polynomially Warped Periodicity. The Polynomially Warped Periodicity style modulates the frequency or amplitude of periodic functions using nonlinear polynomial terms. Unlike standard Rastrigin-like functions, this style produces ”chirp-like” structures (e.g., sin( x2 + 0 .3x3)) where the gradient frequency changes dynamically across the search space, challenging step-size adaptation mechanisms. 14 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model ### Role & Objective 

You are the **"Reflector" **, a critical component of a Multi-Objective Evolutionary 

Algorithm (MOEA/D) system driven by LLMs. 

Your goal is to perform **Meta-Learning **: you analyze the execution results of generated 

Python benchmark functions to extract "Design Patterns" that map mathematical 

structures to specific optimization objectives. 

...(Here is Expert Domain Knowledge Context) 

**PBI (MO Metric): ** Used to evaluate the performance under above 2 objectives. 

**Lower is better. **

### Deep Analysis Rules (The Chain of Thought) 

1. **Intent Awareness: ** Acknowledge that Function B evolved under `lambda_B`. If B has 

high scores in objectives relevant to `lambda_B`, its structure is *valid *, even if it lost the 

PBI duel against A. 

Do not just describe *what * happened. Explain *why * it happened based on the code logic 

(e.g., "The nested `np.sin` caused high frequency oscillation, improving Novelty"). 

2. **The "Loser's Treasure": ** You MUST identify if the Loser outperformed the Winner 

in ANY single objective (Obj 1, or 2). If so, that specific code pattern is a "Valuable Gene." 

3. **Trade-off Logic: ** Explain *why * a high score in one objective caused a poor PBI. (e.g., 

"B's extreme oscillation boosted Novelty but ruined Convergence"). Explicitly identify when 

a mathematical structure improves one objective but degrades another. 

### Handling Invalid/Placeholder Functions 

You may encounter functions that are **Placeholders ** (indicated by class names like 

`GeneratedFunction_Invalid_...` or returning `np.nan`). These represent failed 

executions. 

**Treat them according to the following STRICT rules: **

**SCENARIO A: One Valid Function vs. One Placeholder **

- **Winner: ** The Valid function is ALWAYS the Winner (regardless of PBI). 

- **Loser: ** The Placeholder is the Loser. **IGNORE IT. ** Do not analyze its 

patterns. 

- **Strategy: ** Focus solely on improving the Valid function based on the Target 

Lambda. 

**SCENARIO B: Both are Placeholders (Double Failure) **

- **Analysis: ** Stop all pattern analysis. 

- **Strategy: ** **RANDOM RESTART. ** Ignore previous history. Propose a 

completely new, mathematically robust and STRUCTURAL INNOVATION function 

that theoretically aligns with the current Target Lambda. 

### Strict Output Format Rules 

1. **JSON Only: ** Your output must be a valid, parseable JSON object. 

2. **No Markdown: ** Do NOT use ```json code blocks or markdown formatting. 

Start directly with `{` and end with `}`. 

3. **No Commentary: ** Do not output any conversational text before or after the 

JSON. 

## Domain Knowledge Base 

You must interpret performance based on these definitions: 

### Technical Context: NeurELA (Neural Exploratory Landscape Analysis) 

You are using a state-of-the-art Deep Learning Scorer called **NeurELA **. You must 

understand how it evaluates functions: 

* **Mechanism: ** NeurELA utilizes a **Dual-Attention Transformer ** architecture. 

* **Input: ** It takes a sampled population of points $(X, Y)$ (e.g., 500 points) from the 

generated function. 

* **Process: ** It uses **Self-Attention ** to capture interactions between dimensions and 

individuals. It learns the "context" of the landscape —understanding. 

* **Output: ** It aggregates the entire population into a compact **16-dimensional Latent 

Embedding ** (a "Landscape Fingerprint"). 

* **The Score: ** The metric provided (0.0 to 1.0) is a **Cauchy Kernel Distance Score ** (1/(1 + 

(d_{min} / \sigma)^2)) between the generated function's embedding and the target's embedding 

in this high-dimensional latent space. 

* It is **NOT ** a simple statistics check (like mean/variance). 

* It represents **Topological Similarity **. A high score means the generated function shares 

the same fundamental mathematical structure and complexity as the target. 

1. **LSI score (Obj 1 - Cluster Densification): **

* **Context: ** We want to densify existing clusters of specific problems. The scorer measures 

the NeurELA Latent Embedding distance to the **nearest ** existing specific problem. 

* **High Score: ** **"Valid Cluster Member" **. The function successfully falls into the 

"gravitational pull" of an existing problem family. 

* **Advice: ** Don't try to be too unique. Pick a specific "style" of difficulty (e.g., multimodal 

traps) and reinforce it to ensure the function lands firmly within a known cluster. 

2. **ADC score (Obj 2 - Portfolio Diversity): **

* **Context: ** The function is evaluated on a diverse portfolio of **10 

algorithms **, ranging from classics to modern CEC-winners: 

* *Classics: * **PSO ** ('95), **DE ** ('97), **CMA-ES ** ('01). 

* *Advanced CMA-ES: * **SEPCMAES **.

* *Advanced PSOs: * **GL-PSO **, **sDMS-PSO **, **SAHL-PSO **.

* *Adaptive DEs (SOTA): * **jDE21 **, **MadDE **, **NL-SHADE-LBC **.

* **Goal: ** **"Hierarchical Hardness" **. A high score (Normalized Std Dev) 

means the function exposes specific limitations in these varying search logics. 

It shouldn't just be "hard for everyone." 

* **High Score: ** **"Strategy Breaker" **. The function successfully 

separates the algorithms. Common patterns include: 

* *Rotation Trap: * **CMA-ES ** solves it (rotation invariant), but **PSOs **

and **Classic DE ** fail (coordinate dependent). 

* *Adaptation Deception: * **Classic DE ** might work by luck, but 

**SHADE/MadDE ** fail because the landscape "tricks" their parameter 

adaptation history (e.g., deceptive gradients leading to bad $F/CR$ values). 

* *Multimodal Trap: * **sDMS-PSO ** (sub-swarms) succeeds, but **CMA-

ES ** (single Gaussian) gets stuck in a local optimum. 

* **Low Score: ** **"Indiscriminate Landscape" **. The function is likely either: 

* *Too Easy: * Even basic PSO solves it. 

* *Pure Noise: * Even NL-SHADE-LBC fails, performing no better than Random 

Search. 

Expert Domain Knowledge 

Reflection System Prompt Part 1 Reflection System Prompt Part 2 

Figure 8. Reflection Stage System Prompt 

– Chained Multi-stage Perturbation. The Chained Multi-stage Perturbation style fosters feature mixing by enforcing a cyclical composition of disparate mathematical operators (e.g., exponential decay feeding into logarithmic scaling, then into trigonometric oscillation). This ensures that no single operator dominates the global trend, resulting in hybrid topologies that are difficult to classify under standard BBOB categories. 

– Recursive Symbolic Feedback. The Recursive Symbolic Feedback style generates fractal-like micro-structures by allowing intermediate symbolic variables to re-enter calculations later in the pipeline (e.g., Z = sin( A) + 

A · cos( A)). This recursion creates deep variable dependencies where small input changes propagate into high-frequency local optima. 

– Nonlinear Operator Switching. The Nonlinear Operator Switching style utilizes symbolic triggers (such as magnitude or sign) to modify operator flow, creating ”manifold switching” effects where the landscape topology changes smoothly but drastically. 

– Cross-Dimensional Operator Interaction The Cross-dimensional Operator Interaction module mandates coordi-nate mixing (e.g., sliding window coupling or product interactions like xi · xj and x[: k] ∗ x[−k :] ). Uniquely, this style permits temporary dimensional reduction (e.g., np.sum along the feature axis) to create interaction terms, which are then broadcast back to the original shape, thereby invalidating algorithms that rely on independent coordinate search. 15 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model # Context: The Duel 

We are comparing two neighbor functions generated in the current generation. 

## Task Parameters 

- **Target Objective Vector (lambda): ** : (Here is the reference vector) 

- **Comparison Metric: ** PBI (Penalty-based Boundary Intersection). 

## Intent Analysis (The Setup) 

- **Function A ** was aiming for Lambda_A: (f ’ ‘s reference vector) 

- **Function B ** was aiming for Lambda_B: {f ’’ ‘s reference vector} 

*(Note: If Lambda_B prioritizes different objectives, B might look different. Do 

not penalize B solely for being different.) *

## Function A (Focus: Target lambda_A) - {'Winner'if pbi_a < pbi_b else'Loser'} 

**Performance: **

{Here is f ’ ‘s LSI score and ADC score} 

- **PBI Score: ** {f ’ ‘s PBI score} 

**Code: **

(Here is f ’ ‘s code) 

## Function B (Focus: Neighbor lambda_B) - {'Winner'if pbi_b < pbi_a else'Loser'} 

**Performance: **

{Here is f ’’ ‘s LSI score and ADC score} 

- **PBI Score: ** {f ’’ ‘s PBI score} 

**Code: **

(Here is f ’’ ‘s code) 

# Analysis Task 

Check if Function A or B are **Placeholders ** (Class name contains 'Invalid'). 

1. **If One is Valid and One is Invalid: **

- The Valid one is the source of truth. Analyze why it worked. 

- Ignore the Invalid one completely. 

- **Strategy: ** Propose an innovation to further improve the Valid function. 

2. **If BOTH are Invalid: **

- **Strategy: ** **Random Exploration. ** Disregard both codes. Suggest a 

random STRUCTURAL INNOVATION function weighted by lambda_A. 

- Set `successful_patterns` and `valuable_patterns_in_loser` to empty lists 

`[]`. 

3. **If BOTH are Valid: **

- Perform standard comparison (Attribution, Cross-Learning, Trade-offs). 

Perform the structural analysis based on your Domain Knowledge: 

1. **Attribution: ** Why did the Winner satisfy lambda_A better and Why did 

the Loser lose PBI? Link math operators to performance. 

2. **Cross-Learning: ** valid patterns in the Loser (especially for objectives 

where lambda_A has low weight). Did the Loser beat the Winner in NeurELA or 

Algo Discrim? 

- If YES: Extract the specific math snippet responsible. This is a gene to 

preserve. 

- If NO: The Loser might be a failed mutation. 

3. **Trade-offs: ** Conflicting impacts of specific structures. 

# Output Format (JSON) 

Provide your response in the following JSON format, no comments, every perspective of reflection must be BRIEF AND CONCISE: 

{{ 

"reasoning_summary": "Analysis of the comparison", 

"successful_patterns": ["Math structures in Winner that helped minimize PBI"], 

"valuable_patterns_in_loser": ["Math structures in Loser that achieved high single-objective scores (if any)"], 

"trade_off_insight": "Observation on conflicting objectives, e.g., 'Operator X helps Obj1 but hurts Obj2'.", 

"innovation_strategy": "Specific advice for the next Constructor to combine the best of both worlds." 

}} 

Reflection User Prompt 

Figure 9. Reflection User Prompt 

In conclusion, the Init Prompt relies on a synergistic prompting strategy that integrates the System Prompt and the 

User Prompt . The System Prompt acts as a strict Code Generation Guideline, imposing strict syntactic constraints, interface standardization, and stylistic regularization to guarantee the executability and numerical stability of the generated code. The User Prompt injects the multi-objective reference vectors ( ⃗ λ) and specific Landscape Construction Knowledge that randomly selected from the seven landscape styles detailed above. This dual-prompt framework effectively conditions the LLM to function as a domain expert, ensuring that the resulting initial population comprises objective function programs that are not only computationally compliant but also exhibit rich and distinct diversity, laying a robust foundation for the subsequent evolutionary process. 

A.3. Reflection & Reproduction 

As mentioned in Section 3.2.3, the reflection and reproduction stages serve as the cognitive core of our evolutionary framework, simulating the crossover and mutation mechanisms of traditional evolutionary algorithms but operating within the semantic space of code analysis. The core objective is to convert numerical multi-objective performance into actionable coding insights. A.3.1. R EFLECTION STAGE 

To enable the LLM to perform high-level reasoning and comparative analysis, we designed a two-part Reflection Prompt 

that explicitly instructs the LLM to play the role of a senior optimization and landscape design expert. This enables LLM to interpret complex code structures based on the two objectives defined in Section 3.2.2, as well as knowledge of landscape theory and optimization principles. The Reflection Prompt consists of the following two components: • System Prompt. The system prompt serves as the foundational rule set for the Reflection stage. It transforms raw execution program into actionable design intelligence by analyzing ”Winner vs. Loser” pairs to extract causal links between mathematical structures and multi-objective metrics. The prompt consists of four streamlined modules which 16 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

highlighted with different color in Fig 8 and following content : 

– Role & Obejective (with Domain Knowledge). This module establishes the role of an expert ”Reflector” and injects a specialized Domain Knowledge Base. It provides theoretical context for NeurELA’s structure and mechanism, LSI score and ADC score, ensuring all analysis is technically grounded. 

– Deep Analysis Rules. This module requires the LLM to conduct a three-fold in-depth reflection (when comparing functions f ′ and f ′′ , a candidate is labeled as the winner if its PBI is higher, or as the loser otherwise): 1) Successful Pattern: Identify the major code-level differences between the winner and the loser, summarize such knowledge, and relate it to landscape characteristics. 2) Gene Preservation: If the loser scores higher in either LSI or ADC, the LLM is asked to analyze the reasons behind it and preserve such valuable knowledge. 3) Bi-objective Tradeoff: Recognize and analyze the positive/negative impacts of certain parts of the program’s code on LSI and ADC, with a special focus on conflict cases—for example, adding a mathematical operation may increase LSI but harm ADC. Such cases serve as the basis for crossover operations. 

– Handling Invalid Functions. This module ensures robustness in runtime errors by defining strict branching logic. In the scenario of ”one valid item and one placeholder item,” it focuses solely on improving the valid function. In the scenario of ”dual failure,” it triggers a ”random restart,” instructing the model to abandon the current path and propose entirely new structural innovations. Such cases serve as the basis for mutation operations. 

– Output Formatting Constraints. To facilitate automated processing, this module enforces a rigid JSON-only output format. It strictly forbids Markdown formatting or conversational fillers, ensuring the extracted insights and improvement strategies can be directly parsed and utilized by the subsequent Reproduction stage. • User Prompt. The user prompt serves as a dynamic context injector, incorporating details about the selected two candidates, f ′ and f ′′ , from the neighborhood set into a templated prompt. It dynamically constructs a task prompt for each function evolution duel. The user prompt can be divided into the following three parts, with the content of each section highlighted in different colors in the text below and in Fig 9: 

– Contextual Instantiation (The Duel Setup). This module provides the raw material for analysis by injecting the source codes, LSI, ADC, and PBI Scores of the paired candidates f ′ and f ′′ . Crucially, it explicitly provides the specific reference vector ( ⃗λ′ 

> f

) and the neighbor’s vector ( ⃗ λ′′  

> f

). This enables the LLM to integrate the preferences of its reference vector when conducting comparative analysis of a pair of functions, thereby extracting insights and experience. This prevents the LLM from relying solely on absolute LSI or ADC scores and drawing extreme evolutionary insights. 

– Scenario-Based Evolution. This module explicitly marks the validity status of each function (identifying the ”Invalid” category), thereby requiring the LLM to provide evolutionary insights in different directions. The design of this process is similar to the corresponding section in the reflection system prompt mentioned above. It instructs the LLM to perform the following operations: conduct standard comparative analysis when both functions are valid; execute ”innovative improvement” guided by the reference vector when only a single valid function remains; or trigger ”random exploration” in cases of dual failure. 

– Strategy Output Format. This module requires mapping the final analysis to the specified JSON output fields. The content of the JSON fields is largely consistent with the analytical rules of the reflection system prompt. It instructs the LLM to dissect the ”winner” to extract successful patterns (i.e., structures that minimize the PBI), output as ”successful patterns”; to mine valuable patterns from the ”loser”, specifically, certain code snippets that may score high on a single objective (such as high ADC) but perform poorly overall in terms of PBI, output as ”valuable patterns in loser”; to analyze code snippets where potential improvement conflicts exist, output as ”trade off insight”; and finally, to output the summarized insights as ”innovation strategy”. After completing the design of the aforementioned system prompt and user prompt, during the evolution process, the reflector dynamically injects into the user prompt the source code, LSI, ADC, and PBI scores required for reflection, along with the corresponding reference vector. It then combines the two prompts and calls the LLM to perform all intermediate analyses and reasoning, organizing the response content in a JSON-like format. This includes a summary of the overall reasoning process, condensed analysis results for the above case-by-case reflection, and suggestions/instructions for generating the next generation offspring program. 17 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model # Evolutionary Task:                        

> ## 1. Optimization Objectives
> Target Vector lambda: {lambda_vec}
> (Goal: Imporve LSI score and ADC score guided by Target Vector.)
> ## 2. Reflector Strategy (Mutation Recipe)
> Apply the following strategic changes to the code:
> -**Reasoning: **{reasoning_summary}
> -**KEEP (From Parent A): **{successful_patterns_text}
> -**INCORPORATE (From Parent B): **{valuable_patterns_in_loser_text}
> -**AVOID (Trade-off Risks): **{trade_off_insight}
> -**INNOVATION ACTION: **{innovation_strategy}
> ## 3. Genetic Material (Parent Codes)
> [Parent A]
> {fa}
> [Parent B]
> {fb}
> ## 4. Coding Instruction
> Write the new `class GeneratedFunction`.
> - Combine the parents based on the Reflector Strategy.
> - Ensure `func(self, x)` handles `x` with shape (n, dim) and returns (n,).
> ## 5. Aggressive Refactoring (The Critical Step)
> Review your draft and apply these simplifications:
> -**Collapse Variables **: Merge linear chains like `A=...; B=tanh(A); C=sin(B)` into
> `C=sin(tanh(...))`.
> -**Remove Redundancy **: If a variable is used only once, remove it and inline the
> expression.
> -**Limit Intermediates **: You are allowed a MAXIMUM of 5-8 intermediate
> variables. They must represent distinct physical concepts (e.g., `oscillation`,
> `bias`), not just `temp1`, `temp2`.
> ### Evolutionary Context (The Mission)
> You are acting as the "Crossover & Mutation" operator in
> an Evolutionary Algorithm.
> - You will be provided with **Two Parent Codes **and a
> **Reflector Strategy **.
> -**Inherit: **Keep the successful mathematical
> patterns from Parent A (Winner).
> -**Transfer: **Incorporate valuable patterns from
> Parent B (Neighbor) as requested.
> -**Innovate: **Apply the specific strategy to modify
> the function landscape.
> -**Synthesize: **The result must be a cohesive
> `GeneratedFunction` class, not just a concatenation of
> lines.

Code Generation Guideline 

Reproduction Stage System Prompt 

Reproduction Stage User Prompt 

Figure 10. Reproduction Stage Prompt 

A.3.2. R EPRODUCTION STAGE 

Based on the JSON-like reflections obtained in the reflection stage, the reproduction stage requires the LLM to refer to and follow these useful analyses or improvement suggestions to generate new program candidates. In this stage, a new role must be defined for the LLM, and the JSON-formatted reflections generated in the reflection stage need to be unpacked and injected into a prompt template that includes the reference vector and the codes of the two parent. Similarly, the prompt for this stage also consists of two parts, as illustrated in Fig 10 and described below: • System Prompt. In addition to strictly adhering to the code generation guidelines defined in Appendix A.1, the system prompt for the reproduction stage clearly defines its role as an evolutionary operator (crossover and mutation). This module, guided by a specific reflection strategy, enables the LLM to process two parent codes and synthesize a new offspring. The generation process follows a strict four-step protocol: inheritance (retain the robust mathematical structure of the winner), transfer (assimilate beneficial ”genes” from the neighbor), innovation (apply strategic perturbations), and synthesis. Crucially, the prompt requires the final output to be a unified, mathematically coherent ’GeneratedFunction’ class, explicitly prohibiting the mechanical concatenation of unrelated code segments. • User Prompt. The user prompt for the reproduction stage is dynamically constructed by parsing and unpacking the structured JSON insights derived from the Reflection stage, while simultaneously injecting the source codes of the two parent candidates and target reference vector. This prompt is structurally organized into three distinct operational modules highlighted in different colors in Fig 10 and following content: 

– Optimization Context and Genetic Material. This module establishes the boundary conditions for the new generation. It explicitly defines the Optimization Objectives (including the target reference vector ⃗ λ and specific 18 Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model 

landscape goals) and provides the raw Genetic Material by embedding the full source codes of two parent ( fA

and fB ). This ensures the LLM has direct access to both the specific directional goal and the exact syntax of the predecessor functions. 

– The Reflector Strategy. Acting as the core evolutionary directive, this module translates the insights extracted from the previous stage into actionable instructions. It presents a structured ”Evolution Recipe” that explicitly dictates: 1) Keep: Which successful patterns from Parent A must be preserved. 2) Incorporate: Which specific ”genes” (code snippets) from Parent B should be transferred. 3) Avoid: Which trade-off risks to mitigate. 4) Innovation: The specific landscape modification strategy to apply. This ensures that the crossover is not random, but a directed synthesis based on the Reflector’s analysis. 

– Coding Instruction and Aggressive Refactoring. This module enforces strict implementation standards to maintain code quality. The rules for code writing and refactoring in the prompt are similar to those in the code generation guidelines of the system prompt and will not be elaborated on further here. 19