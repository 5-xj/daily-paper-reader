# Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model
# 基准测试的演化：利用大语言模型进行黑盒优化基准测试设计

**Authors**: Chen Wang, Sijie Ma, Zeyuan Ma, Yue-Jiao Gong \\
**Date**: 2026-01-29 \\
**PDF**: https://arxiv.org/pdf/2601.21877v1 \\
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-pink">keyword:EOH</span> <span class="tag-label tag-pink">keyword:EAA</span> \\
**Score**: 9.0 \\
**Evidence**: automated benchmark design using program evolution and optimization solvers \\

---

## Abstract
Benchmark Design in Black-Box Optimization (BBO) is a fundamental yet open-ended topic. Early BBO benchmarks are predominantly human-crafted, introducing expert bias and constraining diversity. Automating this design process can relieve the human-in-the-loop burden while enhancing diversity and objectivity. We propose Evolution of Benchmark (EoB), an automated BBO benchmark designer empowered by the large language model (LLM) and its program evolution capability. Specifically, we formulate benchmark design as a bi-objective optimization problem towards maximizing (i) landscape diversity and (ii) algorithm-differentiation ability across a portfolio of BBO solvers. Under this paradigm, EoB iteratively prompts LLM to evolve a population of benchmark programs and employs a reflection-based scheme to co-evolve the landscape and its corresponding program. Comprehensive experiments validate our EoB is a competitive candidate in multi-dimensional usages: 1) Benchmarking BBO algorithms; 2) Training and testing learning-assisted BBO algorithms; 3) Extending proxy for expensive real-world problems.

## 摘要
黑盒优化（

---

## 论文详细总结（自动生成）

这篇论文提出了 **Evolution of Benchmark (EoB)**，这是首个利用大语言模型（LLM）和程序演化技术自动设计黑盒优化（BBO）基准测试的框架。以下是对该论文的深度结构化总结：

### 1. 核心问题与整体含义（研究动机和背景）
*   **核心问题**：传统的黑盒优化基准测试（如 CoCo-BBOB, IEEE CEC 系列）主要由专家手工设计。这种模式存在三个痛点：
    1.  **专家偏见与多样性受限**：手工设计的函数往往遵循特定的数学模式，难以覆盖现实世界复杂多变的景观（Landscape）。
    2.  **人力成本高昂**：设计高质量、具有区分度的基准测试需要深厚的领域知识和大量调试。
    3.  **学习型算法的需求**：近年来兴起的学习辅助 BBO 算法（如 MetaBBO）需要海量、多样化的训练实例，现有基准测试规模太小，难以支持其泛化性训练。
*   **整体含义**：论文旨在实现从“人工设计”到“LLM 自动化设计”的范式转变，通过程序演化自动生成既符合目标特征又能有效区分算法性能的基准测试函数。

### 2. 核心方法论
EoB 将基准测试设计建模为一个**双目标优化问题**，并结合 **MOEA/D（基于分解的多目标进化算法）** 框架与 LLM 进行程序演化。

*   **双目标定义**：
    1.  **景观相似度指标 (LSI)**：衡量生成的程序在景观特征（如凸性、多峰性等）上与目标现实问题（如 UAV 路径规划）的接近程度。
    2.  **算法区分能力 (ADC)**：衡量该基准函数能否拉开不同优化算法（如 DE, PSO, CMA-ES）之间的性能差距。
*   **关键技术细节**：
    1.  **初始化**：引入 7 种景观构造知识（如非对称掩码、非线性组合、多项式周期性等）引导 LLM 生成多样化的初始代码。
    2.  **基于反思的演化 (Reflection-based Evolution)**：
        *   **反思阶段**：LLM 比较“胜者”和“败者”代码，分析哪些数学算子导致了高 LSI 或 ADC，识别“成功模式”和“败者中的宝贵基因”。
        *   **繁殖阶段**：LLM 根据反思建议，通过交叉和变异生成下一代函数程序。
    3.  **帕累托维护**：维护一个非支配解集（Pareto Front），最终输出一组兼顾相似性和区分度的基准测试集。

### 3. 实验设计
*   **实验场景**：
    1.  **算法基准测试**：生成 EoB-BBOB，对比其与经典 BBOB 的多样性。
    2.  **学习型优化器训练**：在生成的基准上训练 MetaBBO 算法（如 LDE, GLEET），测试其在现实任务（UAV, HPO）上的零样本泛化能力。
    3.  **昂贵问题的代理建模**：为计算昂贵的现实问题生成廉价的替代基准。
*   **对比基准 (Benchmark)**：CoCo-BBOB, MetaBox (包含 UAV 路径规划和 HPO 超参数优化)。
*   **涉及算法**：使用了 10 种代表性 BBO 算法（DE, JDE21, MadDE, NL-SHADE-LBC, PSO, GLPSO, sDMS-PSO, SAHLPSO, CMA-ES, Sep-CMA-ES）作为评估 ADC 的工具池。

### 4. 资源与算力
*   **基础模型**：默认使用 **DeepSeek-v3**。消融实验中对比了 Qwen Plus, GPT-4o Mini, Gemini-1.5 Flash。
*   **硬件环境**：
    *   GPU: NVIDIA RTX 2080Ti (11GB)。
    *   CPU: Intel Xeon E5-2680 v4 @ 56x 3.3GHz。
    *   RAM: 128GB。
*   **训练时长**：文中未明确给出总演化时长，但提到在代理建模实验中，使用 EoB 生成的基准训练 MetaBBO 算法，每步耗时从分钟级/小时级缩短至秒级（例如从 7 小时缩短至 28 秒）。

### 5. 实验数量与充分性
*   **实验规模**：
    *   设置了 32 个参考向量（种群规模），演化 10 代。
    *   针对 4 种不同的学习型优化器进行了泛化性验证。
    *   进行了详细的消融实验（LLM 主干模型、超参数 $\theta$ 和邻域大小、反思机制与景观知识的有效性）。
*   **充分性评价**：实验设计较为充分，涵盖了从传统算法评估到现代深度学习优化器训练的多个维度。通过可视化景观演化路径和帕累托前沿，客观地展示了演化过程的有效性。

### 6. 主要结论与发现
1.  **自动化优势**：EoB 能够自动发现比人工设计更具挑战性、分布更均匀的基准函数。