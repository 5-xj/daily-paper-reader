{
  "mode": "standard",
  "generated_at": "2026-02-13T04:35:45.540237+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 1,
    "deep_cap": 8,
    "deep_selected": 1,
    "quick_candidates": 5,
    "quick_skim_target": 13,
    "quick_selected": 5
  },
  "deep_dive": [
    {
      "id": "2602.11917v1",
      "title": "AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution",
      "abstract": "Extracting signals through alpha factor mining is a fundamental challenge in quantitative finance. Existing automated methods primarily follow two paradigms: Decoupled Factor Generation, which treats factor discovery as isolated events, and Iterative Factor Evolution, which focuses on local parent-child refinements. However, both paradigms lack a global structural view, often treating factor pools as unstructured collections or fragmented chains, which leads to redundant search and limited diversity. To address these limitations, we introduce AlphaPROBE (Alpha Mining via Principled Retrieval and On-graph Biased Evolution), a framework that reframes alpha mining as the strategic navigation of a Directed Acyclic Graph (DAG). By modeling factors as nodes and evolutionary links as edges, AlphaPROBE treats the factor pool as a dynamic, interconnected ecosystem. The framework consists of two core components: a Bayesian Factor Retriever that identifies high-potential seeds by balancing exploitation and exploration through a posterior probability model, and a DAG-aware Factor Generator that leverages the full ancestral trace of factors to produce context-aware, nonredundant optimizations. Extensive experiments on three major Chinese stock market datasets against 8 competitive baselines demonstrate that AlphaPROBE significantly gains enhanced performance in predictive accuracy, return stability and training efficiency. Our results confirm that leveraging global evolutionary topology is essential for efficient and robust automated alpha discovery. We have open-sourced our implementation at https://github.com/gta0804/AlphaPROBE.",
      "authors": [
        "Taian Guo",
        "Haiyang Shen",
        "Junyu Luo",
        "Binqi Chen",
        "Hongjun Ding",
        "Jinsheng Huang",
        "Luchen Liu",
        "Yun Ma",
        "Ming Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 13:14:58+00:00",
      "link": "https://arxiv.org/pdf/2602.11917v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Automated heuristic discovery via on-graph biased evolution for alpha mining",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.12164v1",
      "title": "Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision",
      "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.",
      "authors": [
        "Xiaohan He",
        "Shiyang Feng",
        "Songtao Huang",
        "Lei Bai",
        "Bin Wang",
        "Bo Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-12 16:46:00+00:00",
      "link": "https://arxiv.org/pdf/2602.12164v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Co-evolving framework for self-evolution aligns with evolution of heuristics",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.11573v1",
      "title": "Fast Tuning the Index Construction Parameters of Proximity Graphs in Vector Databases",
      "abstract": "k-approximate nearest neighbor search (k-ANNS) in high-dimensional vector spaces is a fundamental problem across many fields. With the advent of vector databases and retrieval-augmented generation, k-ANNS has garnered increasing attention. Among existing methods, proximity graphs (PG) based approaches are the state-of-the-art (SOTA) methods. However, the construction parameters of PGs significantly impact their search performance. Before constructing a PG for a given dataset, it is essential to tune these parameters, which first recommends a set of promising parameters and then estimates the quality of each parameter by building the corresponding PG and then testing its k-ANNS performance. Given that the construction complexity of PGs is superlinear, building and evaluating graph indexes accounts for the primary cost of parameter tuning. Unfortunately, there is currently no method considered and optimized this process.In this paper, we introduce FastPGT, an efficient framework for tuning the PG construction parameters. FastPGT accelerates parameter estimation by building multiple PGs simultaneously, thereby reducing repeated computations. Moreover, we modify the SOTA tuning model to recommend multiple parameters at once, which can be efficiently estimated using our method of building multiple PGs simultaneously. Through extensive experiments on real-world datasets, we demonstrate that FastPGT achieves up to 2.37x speedup over the SOTA method VDTuner, without compromising tuning quality.",
      "authors": [
        "Wenyang Zhou",
        "Jiadong Xie",
        "Yingfan Liu",
        "Zhihao Yin",
        "Jeffrey Xu Yu",
        "Hui Li",
        "Zhangqian Mu",
        "Xiaotian Qiao",
        "Jiangtao Cui"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-02-12 04:45:43+00:00",
      "link": "https://arxiv.org/pdf/2602.11573v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Fast tuning of construction parameters is an efficient automatic algorithm task",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.11770v1",
      "title": "An objective-function-free algorithm for general smooth constrained optimization",
      "abstract": "A new algorithm for smooth constrained optimization is proposed that never computes the value of the problem's objective function and that handles both equality and inequality constraints. The algorithm uses an adaptive switching strategy between a normal step aiming at reducing constraint's infeasibility and a tangential step improving dual optimality, the latter being inspired by the AdaGrad-norm method. Its worst-case iteration complexity is analyzed, showing that the norm of the gradients generated converges to zero like O(1/\\sqrt{k+1}) for problems with full-rank Jacobians. Numerical experiments show that the algorithm's performance is remarkably insensitive to noise in the objective function's gradient.",
      "authors": [
        "S. Bellavia",
        "S. Gratton",
        "B. Morini",
        "Ph. L. Toint"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-12 09:50:32+00:00",
      "link": "https://arxiv.org/pdf/2602.11770v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "efficient automatic algorithm for smooth constrained optimization",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.11872v1",
      "title": "A High-Performance Parallel Algorithm for Multi-Objective Integer Optimization",
      "abstract": "Multi-objective integer optimization problems are hard to solve, mainly because the number of nondominated images is often extremely large. We present the first exact algorithm, called PEA, that fully utilizes the multicore architecture of modern hardware. By exploiting the structure of the parameter set of the underlying scalarization, PEA can use a high number of threads while avoiding the usual pitfalls of parallel computing. It is highly scalable and easy to implement. As a result, PEA can solve much larger instances than previous state-of-the-art algorithms. Besides, PEA has a sound theoretical foundation. Unlike other existing parallel algorithms, it always solves the same number of scalarization problems as comparable sequential algorithms. We demonstrate the potential of PEA in a computational study.",
      "authors": [
        "Kathrin Prinz",
        "Levin Nemesch",
        "Stefan Ruzika"
      ],
      "primary_category": "math.OC",
      "categories": [
        "math.OC"
      ],
      "published": "2026-02-12 12:21:36+00:00",
      "link": "https://arxiv.org/pdf/2602.11872v1",
      "tags": [
        "keyword:EAA"
      ],
      "llm_score": 6.0,
      "llm_evidence": "high-performance parallel algorithm for integer optimization",
      "llm_tags": [
        "keyword:EAA",
        "keyword:LNS"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.11937v1",
      "title": "Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration",
      "abstract": "Reasoning-focused LLMs improve answer quality by generating longer reasoning traces, but the additional tokens dramatically increase serving cost, motivating inference optimization. We extend and apply Puzzle, a post-training neural architecture search (NAS) framework, to gpt-oss-120B to produce gpt-oss-puzzle-88B, a deployment-optimized derivative. Our approach combines heterogeneous MoE expert pruning, selective replacement of full-context attention with window attention, FP8 KV-cache quantization with calibrated scales, and post-training reinforcement learning to recover accuracy, while maintaining low generation length. In terms of per-token speeds, on an 8XH100 node we achieve 1.63X and 1.22X throughput speedups in long-context and short-context settings, respectively. gpt-oss-puzzle-88B also delivers throughput speedups of 2.82X on a single NVIDIA H100 GPU. However, because token counts can change with reasoning effort and model variants, per-token throughput (tok/s) and latency (ms/token) do not necessarily lead to end-to-end speedups: a 2X throughput gain is erased if traces grow 2X. Conversely, throughput gains can be spent on more reasoning tokens to improve accuracy; we therefore advocate request-level efficiency metrics that normalize throughput by tokens generated and trace an accuracy--speed frontier across reasoning efforts. We show that gpt-oss-puzzle-88B improves over gpt-oss-120B along the entire frontier, delivering up to 1.29X higher request-level efficiency. Across various benchmarks, gpt-oss-puzzle-88B matches or slightly exceeds the parent on suite-average accuracy across reasoning efforts, with retention ranging from 100.8% (high) to 108.2% (low), showing that post-training architecture search can substantially reduce inference costs without sacrificing quality.",
      "authors": [
        "Akhiad Bercovich",
        "Nir Ailon",
        "Vladimir Anisimov",
        "Tomer Asida",
        "Nave Assaf",
        "Mohammad Dabbah",
        "Ido Galil",
        "Amnon Geifman",
        "Yonatan Geifman",
        "Izhak Golan",
        "Roi Koren",
        "Itay Levy",
        "Zach Moshe",
        "Pavlo Molchanov",
        "Najeeb Nabwani",
        "Mostofa Patwari",
        "Omri Puny",
        "Tomer Ronen",
        "Itamar Schen",
        "Elad Segal",
        "Ido Shahaf",
        "Oren Tropp",
        "Ran Zilberstein",
        "Ran El-Yaniv"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-12 13:36:19+00:00",
      "link": "https://arxiv.org/pdf/2602.11937v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "neural architecture search and inference optimization for efficiency",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    }
  ]
}