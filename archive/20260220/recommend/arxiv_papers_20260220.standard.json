{
  "mode": "standard",
  "generated_at": "2026-02-20T04:28:49.017239+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 0,
    "deep_cap": 8,
    "deep_selected": 0,
    "quick_candidates": 6,
    "quick_skim_target": 13,
    "quick_selected": 6
  },
  "deep_dive": [],
  "quick_skim": [
    {
      "id": "2602.17497v1",
      "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
      "abstract": "Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the environment for temporal credit assignment. Extended evaluation on four BabyAI scenarios show that RICOL achieves comparable convergent performance with traditional online RL algorithms with significantly higher sample efficiency. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.",
      "authors": [
        "Wen-Tse Chen",
        "Jiayu Chen",
        "Fahim Tajwar",
        "Hao Zhu",
        "Xintong Duan",
        "Ruslan Salakhutdinov",
        "Jeff Schneider"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-19 16:13:28+00:00",
      "link": "https://arxiv.org/pdf/2602.17497v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence": "discusses self-evolving agents and iterative refinement of heuristics",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.17100v1",
      "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
      "abstract": "Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively refine the topology within an instance using execution feedback, which leads to redundant communication and performance bottlenecks. To address these issues, we propose AgentConductor: a reinforcement learning-optimized MAS with an LLM-based orchestrator agent as its core, which enables end-to-end feedback-driven dynamic generation of interaction topologies. For each query, AgentConductor infers agent roles and task difficulty, then constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology, underpinned by two key innovations. First, we design a novel topological density function that captures communication-aware mathematical characterizations of multi-agent interactions. Second, we adopt difficulty interval partitioning to avoid excessive pruning for precise topological density upper bound measurement per difficulty level and finer-grained control. Empirically, across three competition-level and two foundational code datasets, AgentConductor achieves state-of-the-art accuracy, outperforming the strongest baseline by up to 14.6% in pass@1 accuracy, 13% in density reduction, and 68% in token cost reduction.",
      "authors": [
        "Siyu Wang",
        "Ruotian Lu",
        "Zhihao Yang",
        "Yuchao Wang",
        "Yanzhou Zhang",
        "Lei Xu",
        "Qimin Xu",
        "Guojun Yin",
        "Cailian Chen",
        "Xinping Guan"
      ],
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA"
      ],
      "published": "2026-02-19 05:51:55+00:00",
      "link": "https://arxiv.org/pdf/2602.17100v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Iterative refinement of interaction topologies for code generation aligns with automated heuristic evolution",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.17641v1",
      "title": "FAMOSE: A ReAct Approach to Automated Feature Discovery",
      "abstract": "Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE's strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.",
      "authors": [
        "Keith Burghardt",
        "Jienan Liu",
        "Sadman Sakib",
        "Yuning Hao",
        "Bo Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-19 18:53:15+00:00",
      "link": "https://arxiv.org/pdf/2602.17641v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence": "autonomous agent for feature discovery in large spaces aligns with efficient automatic algorithms",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.17130v1",
      "title": "Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances",
      "abstract": "We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.",
      "authors": [
        "Victor Kondratiev",
        "Irina Gribanova",
        "Alexander Semenov"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-19 07:05:48+00:00",
      "link": "https://arxiv.org/pdf/2602.17130v1",
      "tags": [
        "keyword:EAA",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "efficient parallel algorithm for hard combinatorial instances",
      "llm_tags": [
        "keyword:EAA",
        "keyword:LNS"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.17151v1",
      "title": "ARCANE: Scalable high-degree cubature formulae for simulating SDEs without Monte Carlo error",
      "abstract": "Monte Carlo sampling is the standard approach for estimating properties of solutions to stochastic differential equations (SDEs), but accurate estimates require huge sample sizes. Lyons and Victoir (2004) proposed replacing independently sampled Brownian driving paths with \"cubature formulae\", deterministic weighted sets of paths that match Brownian \"signature moments\" up to some degree $D$. They prove that cubature formulae exist for arbitrary $D$, but explicit constructions are difficult and have only reached $D=7$, too small for practical use. We present ARCANE, an algorithm that efficiently and automatically constructs cubature formulae of arbitrary degree. It reproduces the state of the art in seconds and reaches $\\boldsymbol{D=19}$ within hours on modest hardware. In simulations across multiple different SDEs and error metrics, our cubature formulae robustly achieve an error orders of magnitude smaller than Monte Carlo with the same number of paths.",
      "authors": [
        "Peter Koepernik",
        "Thomas Coxon",
        "James Foster"
      ],
      "primary_category": "math.NA",
      "categories": [
        "math.NA",
        "cs.MS",
        "math.PR"
      ],
      "published": "2026-02-19 07:51:48+00:00",
      "link": "https://arxiv.org/pdf/2602.17151v1",
      "tags": [
        "keyword:EAA"
      ],
      "llm_score": 6.0,
      "llm_evidence": "efficient and automatic construction of cubature formulae",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.17607v1",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
      "abstract": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \\texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.",
      "authors": [
        "Jianda Du",
        "Youran Sun",
        "Haizhao Yang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-19 18:31:52+00:00",
      "link": "https://arxiv.org/pdf/2602.17607v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "autonomous multi-agent pipeline for designing numerical solvers aligns with efficient automatic algorithms",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    }
  ]
}