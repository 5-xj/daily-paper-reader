{
  "mode": "standard",
  "generated_at": "2026-02-05T04:29:14.699269+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 2,
    "deep_cap": 8,
    "deep_selected": 2,
    "quick_candidates": 7,
    "quick_skim_target": 13,
    "quick_selected": 7
  },
  "deep_dive": [
    {
      "id": "2602.04529v1",
      "title": "Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization",
      "abstract": "The advent of Large Language Models (LLMs) has opened new frontiers in automated algorithm design, giving rise to numerous powerful methods. However, these approaches retain critical limitations: they require extensive evaluation of the target problem to guide the search process, making them impractical for real-world optimization tasks, where each evaluation consumes substantial computational resources. This research proposes an innovative and efficient framework that decouples algorithm discovery from high-cost evaluation. Our core innovation lies in combining a Genetic Programming (GP) function generator with an LLM-driven evolutionary algorithm designer. The evolutionary direction of the GP-based function generator is guided by the similarity between the landscape characteristics of generated proxy functions and those of real-world problems, ensuring that algorithms discovered via proxy functions exhibit comparable performance on real-world problems. Our method enables deep exploration of the algorithmic space before final validation while avoiding costly real-world evaluations. We validated the framework's efficacy across multiple real-world problems, demonstrating its ability to discover high-performance algorithms while substantially reducing expensive evaluations. This approach shows a path to apply LLM-based automated algorithm design to computationally intensive real-world optimization challenges.",
      "authors": [
        "Haoran Yin",
        "Shuaiqun Pan",
        "Zhao Wei",
        "Jian Cheng Wong",
        "Yew-Soon Ong",
        "Anna V. Kononova",
        "Thomas Bäck",
        "Niki van Stein"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-04 13:18:45+00:00",
      "link": "https://arxiv.org/pdf/2602.04529v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 10.0,
      "llm_evidence": "LLM-driven evolutionary algorithm designer for efficient optimization",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    },
    {
      "id": "2602.04807v1",
      "title": "Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning",
      "abstract": "We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.",
      "authors": [
        "Wolfgang Maass",
        "Sabine Janzen",
        "Prajvi Saxena",
        "Sach Mukherjee"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-04 17:53:28+00:00",
      "link": "https://arxiv.org/pdf/2602.04807v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Uses evolutionary optimization in an outer loop to discover efficient learning architectures",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.04248v1",
      "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
      "abstract": "Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.",
      "authors": [
        "Hao Lu",
        "Haoyuan Huang",
        "Yulin Zhou",
        "Chen Li",
        "Ningxin Zhu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-04 06:14:55+00:00",
      "link": "https://arxiv.org/pdf/2602.04248v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Proposes a dual-loop framework for continuous agent evolution and memory optimization",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.04265v1",
      "title": "Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for enhancing reasoning in Large Language Models (LLMs). However, it frequently encounters challenges such as entropy collapse, excessive verbosity, and insufficient exploration for hard problems. Crucially, existing reward schemes fail to distinguish between the need for extensive search during problem-solving and the efficiency required for mastered knowledge. In this work, we introduce T2T(Thickening-to-Thinning), a dynamic reward framework inspired by human learning processes. Specifically, it implements a dual-phase mechanism: (1) On incorrect attempts, T2T incentivizes \"thickening\" (longer trajectories) to broaden the search space and explore novel solution paths; (2) Upon achieving correctness, it shifts to \"thinning\", imposing length penalties to discourage redundancy, thereby fostering model confidence and crystallizing reasoning capabilities. Extensive experiments on mathematical benchmarks (MATH-500, AIME, AMC) across Qwen-series and Deepseek models demonstrate that T2T significantly outperforms standard GRPO and recent baselines, achieving superior performance.",
      "authors": [
        "Wenze Lin",
        "Zhen Yang",
        "Xitai Jiang",
        "Pony Ma",
        "Gao Huang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-04 06:55:58+00:00",
      "link": "https://arxiv.org/pdf/2602.04265v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Dynamic reward framework to broaden search space during problem solving",
      "llm_tags": [
        "keyword:LNS",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.04745v1",
      "title": "Impact of diversity on bounded archives for multi-objective local search",
      "abstract": "This work tackles two critical challenges related to the development of metaheuristics for Multi-Objective Optimization Problems (MOOPs): the exponential growth of non-dominated solutions and the tendency of metaheuristics to disproportionately concentrate their search on a subset of the Pareto Front. To counteract the first, bounded archives are employed as a strategic mechanism for effectively managing the increasing number of non-dominated solutions. Addressing the second challenge involves an in-depth exploration of solution diversity algorithms found in existing literature. Upon recognizing that current approaches predominantly center on diversity within the objective space, this research introduces innovative methods specifically designed to enhance diversity in the solution space. Results demonstrate the efficacy of the Hamming Distance Archiving Algorithm, one of the newly proposed algorithms for multi-objective local search, surpassing the performance of the Adaptive Grid Archiving and the Hypervolume Archiving, both drawn from the literature. This outcome suggests a promising avenue for enhancing the overall efficiency of metaheuristics employed for solving MOOPs.",
      "authors": [
        "Amadeu A. Coco",
        "Cyprien Borée",
        "Julien Baste",
        "Laetitia Jourdan",
        "Lucien Mousin"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-04 16:47:14+00:00",
      "link": "https://arxiv.org/pdf/2602.04745v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Discusses multi-objective local search and metaheuristics which are foundational to large neighborhood search",
      "llm_tags": [
        "keyword:LNS"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.04411v1",
      "title": "Self-evolving Embodied AI",
      "abstract": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence.",
      "authors": [
        "Tongtong Feng",
        "Xin Wang",
        "Wenwu Zhu"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET",
        "cs.CV"
      ],
      "published": "2026-02-04 10:40:34+00:00",
      "link": "https://arxiv.org/pdf/2602.04411v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Discusses model self-evolution and adaptive agents in dynamic environments",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.04837v1",
      "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
      "abstract": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.",
      "authors": [
        "Zhaotian Weng",
        "Antonis Antoniades",
        "Deepak Nathani",
        "Zhen Zhang",
        "Xiao Pu",
        "Xin Eric Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-04 18:29:36+00:00",
      "link": "https://arxiv.org/pdf/2602.04837v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Focuses on open-ended self-improvement and evolution of agent architectures",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.04492v1",
      "title": "Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish",
      "abstract": "Constructing mechanistic models of neural circuits is a fundamental goal of neuroscience, yet verifying such models is limited by the lack of ground truth. To rigorously test model discovery, we establish an in silico testbed using neuromechanical simulations of a larval zebrafish as a transparent ground truth. We find that LLM-based tree search autonomously discovers predictive models that significantly outperform established forecasting baselines. Conditioning on sensory drive is necessary but not sufficient for faithful system identification, as models exploit statistical shortcuts. Structural priors prove essential for enabling robust out-of-distribution generalization and recovery of interpretable mechanistic models. Our insights provide guidance for modeling real-world neural recordings and offer a broader template for AI-driven scientific discovery.",
      "authors": [
        "Jan-Matthis Lueckmann",
        "Viren Jain",
        "Michał Januszewski"
      ],
      "primary_category": "q-bio.NC",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-04 12:33:29+00:00",
      "link": "https://arxiv.org/pdf/2602.04492v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "LLM-based tree search for autonomous discovery of mechanistic models",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.04811v1",
      "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
      "abstract": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.",
      "authors": [
        "Jiarui Yuan",
        "Tailin Jin",
        "Weize Chen",
        "Zeyuan Liu",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-04 17:58:32+00:00",
      "link": "https://arxiv.org/pdf/2602.04811v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Explores self-evolution of agents and knowledge internalization which relates to evolution of heuristics",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "6"
    }
  ]
}