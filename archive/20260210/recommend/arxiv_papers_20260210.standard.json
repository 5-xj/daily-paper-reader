{
  "mode": "standard",
  "generated_at": "2026-02-10T04:46:10.016834+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 2,
    "deep_cap": 8,
    "deep_selected": 2,
    "quick_candidates": 7,
    "quick_skim_target": 13,
    "quick_selected": 7
  },
  "deep_dive": [
    {
      "id": "2602.08513v1",
      "title": "A Multi-objective Evolutionary Algorithm Based on Bi-population with Uniform Sampling for Neural Architecture Search",
      "abstract": "Neural architecture search (NAS) automates neural network design, improving efficiency over manual approaches. However, efficiently discovering high-performance neural network architectures that simultaneously optimize multiple objectives remains a significant challenge in NAS. Existing methods often suffer from limited population diversity and inadequate exploration of the search space, particularly in regions with extreme complexity values. To address these challenges, we propose MOEA-BUS, an innovative multi-objective evolutionary algorithm based on bi-population with uniform sampling for neural architecture search, aimed at simultaneously optimizing both accuracy and network complexity. In MOEA-BUS, a novel uniform sampling method is proposed to initialize the population, ensuring that architectures are distributed uniformly across the objective space. Furthermore, to enhance exploration, we deploy a bi-population framework where two populations evolve synergistically, facilitating comprehensive search space coverage. Experiments on CIFAR-10 and ImageNet demonstrate MOEA-BUS's superiority, achieving top-1 accuracies of 98.39% on CIFAR-10, and 80.03% on ImageNet. Notably, it achieves 78.28% accuracy on ImageNet with only 446M MAdds. Ablation studies confirm that both uniform sampling and bi-population mechanisms enhance population diversity and performance. Additionally, in terms of the Kendall's tau coefficient, the SVM achieves an improvement of at least 0.035 compared to the other three commonly used machine learning models, and uniform sampling provided an enhancement of approximately 0.07.",
      "authors": [
        "Yu Xue",
        "Pengcheng Jiang",
        "Chenchen Zhu",
        "Yong Zhang",
        "Ran Cheng",
        "Kaizhou Gao",
        "Dunwei Gong"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-09 11:04:07+00:00",
      "link": "https://arxiv.org/pdf/2602.08513v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Evolutionary algorithm for automated architecture search aligns with evolution of heuristics and efficient automatic algorithms",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    },
    {
      "id": "2602.08619v1",
      "title": "Enhancing Genetic Algorithms with Graph Neural Networks: A Timetabling Case Study",
      "abstract": "This paper investigates the impact of hybridizing a multi-modal Genetic Algorithm with a Graph Neural Network for timetabling optimization. The Graph Neural Network is designed to encapsulate general domain knowledge to improve schedule quality, while the Genetic Algorithm explores different regions of the search space and integrates the deep learning model as an enhancement operator to guide the solution search towards optimality. Initially, both components of the hybrid technique were designed, developed, and optimized independently to solve the tackled task. Multiple experiments were conducted on Staff Rostering, a well-known timetabling problem, to compare the proposed hybridization with the standalone optimized versions of the Genetic Algorithm and Graph Neural Network. The experimental results demonstrate that the proposed hybridization brings statistically significant improvements in both the time efficiency and solution quality metrics, compared to the standalone methods. To the best of our knowledge, this work proposes the first hybridization of a Genetic Algorithm with a Graph Neural Network for solving timetabling problems.",
      "authors": [
        "Laura-Maria Cornei",
        "Mihaela-Elena BreabÄƒn"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-09 13:10:16+00:00",
      "link": "https://arxiv.org/pdf/2602.08619v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 8.0,
      "llm_evidence": "hybridizes genetic algorithms with neural networks for optimization search",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.08616v1",
      "title": "Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces",
      "abstract": "Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.",
      "authors": [
        "Heiko Hoppe",
        "Fabian Akkerman",
        "Wouter van Heeswijk",
        "Maximilian Schiffer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09 13:05:07+00:00",
      "link": "https://arxiv.org/pdf/2602.08616v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 7.0,
      "llm_evidence": "addresses large discrete action spaces using neighborhood-based exploration",
      "llm_tags": [
        "keyword:LNS",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.08354v1",
      "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?",
      "abstract": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.",
      "authors": [
        "Zixuan Huang",
        "Xin Xia",
        "Yuxi Ren",
        "Jianbin Zheng",
        "Xuanda Wang",
        "Zhixia Zhang",
        "Hongyan Xie",
        "Songshi Liang",
        "Zehao Chen",
        "Xuefeng Xiao",
        "Fuzhen Zhuang",
        "Jianxin Li",
        "Yikun Ban",
        "Deqing Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-09 07:38:22+00:00",
      "link": "https://arxiv.org/pdf/2602.08354v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "guided efficient reasoning and computational efficiency in search processes",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08745v1",
      "title": "On the Expressive Power of GNNs for Boolean Satisfiability",
      "abstract": "Machine learning approaches to solving Boolean Satisfiability (SAT) aim to replace handcrafted heuristics with learning-based models. Graph Neural Networks have emerged as the main architecture for SAT solving, due to the natural graph representation of Boolean formulas. We analyze the expressive power of GNNs for SAT solving through the lens of the Weisfeiler-Leman (WL) test. As our main result, we prove that the full WL hierarchy cannot, in general, distinguish between satisfiable and unsatisfiable instances. We show that indistinguishability under higher-order WL carries over to practical limitations for WL-bounded solvers that set variables sequentially. We further study the expressivity required for several important families of SAT instances, including regular, random and planar instances. To quantify expressivity needs in practice, we conduct experiments on random instances from the G4SAT benchmark and industrial instances from the International SAT Competition. Our results suggest that while random instances are largely distinguishable, industrial instances often require more expressivity to predict a satisfying assignment.",
      "authors": [
        "Saku Peltonen",
        "Roger Wattenhofer"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-02-09 14:48:16+00:00",
      "link": "https://arxiv.org/pdf/2602.08745v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Replacing handcrafted heuristics with learning-based models for SAT solving",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.08528v1",
      "title": "Automatic regularization parameter choice for tomography using a double model approach",
      "abstract": "Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.",
      "authors": [
        "Chuyang Wu",
        "Samuli Siltanen"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "math.OC"
      ],
      "published": "2026-02-09 11:23:57+00:00",
      "link": "https://arxiv.org/pdf/2602.08528v1",
      "tags": [
        "keyword:EAA"
      ],
      "llm_score": 6.0,
      "llm_evidence": "automatic parameter choice using feedback control algorithm",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08952v1",
      "title": "Clique-Based Deletion-Correcting Codes via Penalty-Guided Clique Search",
      "abstract": "We study the construction of $d$-deletion-correcting binary codes by formulating the problem as a Maximum Clique Problem (MCP). In this formulation, vertices represent candidate codewords and edges connect pairs whose longest common subsequence (LCS) distance guarantees correction of up to $d$ deletions. A valid codebook corresponds to a clique in the resulting graph, and finding the largest codebook is equivalent to identifying a maximum clique. While MCP-based formulations for deletion-correcting codes have previously been explored, we demonstrate that applying Penalty-Guided Clique Search (PGCS), a lightweight stochastic clique-search heuristic inspired by Dynamic Local Search (DLS), consistently yields larger codebooks than existing graph-based heuristics, including minimum-degree and coloring methods, for block lengths $n = 8,9,\\dots,14$ and deletion parameters $d = 1,2,3$. In several finite-length regimes, the resulting codebooks match known optimal sizes and outperform classical constructions such as Helberg codes. For decoding under segmented reception, where codeword boundaries are known, we propose an optimized LCS-based decoder that exploits symbol-count filtering and early termination to substantially reduce the number of LCS evaluations while preserving exact decoding guarantees. These optimizations lead to significantly lower average-case decoding complexity than the baseline $O(|C| n^2)$ approach.",
      "authors": [
        "Aniruddh Pandav",
        "Rajshekhar V Bhat"
      ],
      "primary_category": "cs.IT",
      "categories": [
        "cs.IT"
      ],
      "published": "2026-02-09 17:47:11+00:00",
      "link": "https://arxiv.org/pdf/2602.08952v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence": "stochastic clique-search heuristic inspired by local search",
      "llm_tags": [
        "keyword:LNS",
        "keyword:EOH"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.08603v1",
      "title": "OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval",
      "abstract": "Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.",
      "authors": [
        "Teng Wang",
        "Rong Shan",
        "Jianghao Lin",
        "Junjie Wu",
        "Tianyi Xu",
        "Jianping Zhang",
        "Wenteng Chen",
        "Changwang Zhang",
        "Zhaoxiang Wang",
        "Weinan Zhang",
        "Jun Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-09 12:44:56+00:00",
      "link": "https://arxiv.org/pdf/2602.08603v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "reformulates heuristic search into principled trajectory optimization",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08990v1",
      "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
      "abstract": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
      "authors": [
        "Shiyang Feng",
        "Runmin Ma",
        "Xiangchao Yan",
        "Yue Fan",
        "Yusong Hu",
        "Songtao Huang",
        "Shuaiyu Zhang",
        "Zongsheng Cao",
        "Tianshuo Peng",
        "Jiakang Yuan",
        "Zijie Guo",
        "Zhijie Zhong",
        "Shangheng Du",
        "Weida Wang",
        "Jinxin Shi",
        "Yuhao Zhou",
        "Xiaohan He",
        "Zhiyin Yu",
        "Fangchen Yu",
        "Qihao Zheng",
        "Jiamin Wu",
        "Mianxin Liu",
        "Chi Zhang",
        "Shaowei Hou",
        "Shuya Li",
        "Yankai Jiang",
        "Wenjie Lou",
        "Lilong Wang",
        "Zifu Wang",
        "Jiong Wang",
        "Wanghan Xu",
        "Yue Deng",
        "Dongrui Liu",
        "Yiheng Wang",
        "Wenlong Zhang",
        "Fenghua Ling",
        "Shufei Zhang",
        "Xiaosong Wang",
        "Shuangjia Zheng",
        "Xun Huang",
        "Siqi Sun",
        "Shuyue Hu",
        "Peng Ye",
        "Chunfeng Song",
        "Bin Wang",
        "Conghui He",
        "Yihao Liu",
        "Xin Li",
        "Qibin Hou",
        "Tao Chen",
        "Xiangyu Yue",
        "Bin Wang",
        "Liang He",
        "Dahua Lin",
        "Bowen Zhou",
        "Bo Zhang",
        "Lei Bai"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-09 18:36:06+00:00",
      "link": "https://arxiv.org/pdf/2602.08990v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 7.0,
      "llm_evidence": "subsystems for solution optimization and evolution align with heuristic evolution",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "7"
    }
  ]
}