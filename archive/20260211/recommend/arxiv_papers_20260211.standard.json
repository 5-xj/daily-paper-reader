{
  "mode": "standard",
  "generated_at": "2026-02-11T04:51:39.859160+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 1,
    "deep_cap": 8,
    "deep_selected": 1,
    "quick_candidates": 6,
    "quick_skim_target": 13,
    "quick_selected": 6
  },
  "deep_dive": [
    {
      "id": "2602.10085v1",
      "title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs",
      "abstract": "Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\\href{https://sites.google.com/view/code-sharp/homepage}{here}$.",
      "authors": [
        "Richard Bornemann",
        "Pierluigi Vito Amadori",
        "Antoine Cully"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-10 18:51:39+00:00",
      "link": "https://arxiv.org/pdf/2602.10085v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 8.0,
      "llm_evidence": "open-ended discovery and evolution of skills as reward programs",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.09530v1",
      "title": "Learning to Discover Iterative Spectral Algorithms",
      "abstract": "We introduce AutoSpec, a neural network framework for discovering iterative spectral algorithms for large-scale numerical linear algebra and numerical optimization. Our self-supervised models adapt to input operators using coarse spectral information (e.g., eigenvalue estimates and residual norms), and they predict recurrence coefficients for computing or applying a matrix polynomial tailored to a downstream task. The effectiveness of AutoSpec relies on three ingredients: an architecture whose inference pass implements short, executable numerical linear algebra recurrences; efficient training on small synthetic problems with transfer to large-scale real-world operators; and task-defined objectives that enforce the desired approximation or preconditioning behavior across the range of spectral profiles represented in the training set. We apply AutoSpec to discovering algorithms for representative numerical linear algebra tasks: accelerating matrix-function approximation; accelerating sparse linear solvers; and spectral filtering/preconditioning for eigenvalue computations. On real-world matrices, the learned procedures deliver orders-of-magnitude improvements in accuracy and/or reductions in iteration count, relative to basic baselines. We also find clear connections to classical theory: the induced polynomials often exhibit near-equiripple, near-minimax behavior characteristic of Chebyshev polynomials.",
      "authors": [
        "Zihang Liu",
        "Oleg Balabanov",
        "Yaoqing Yang",
        "Michael W. Mahoney"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "published": "2026-02-10 08:39:59+00:00",
      "link": "https://arxiv.org/pdf/2602.09530v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence": "framework for discovering iterative spectral algorithms for optimization",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.09424v1",
      "title": "Reward-Guided Discrete Diffusion via Clean-Sample Markov Chain for Molecule and Biological Sequence Design",
      "abstract": "Discrete diffusion models have recently emerged as a powerful class of generative models for chemistry and biology data. In these fields, the goal is to generate various samples with high rewards (e.g., drug-likeness in molecules), making reward-based guidance crucial. Most existing methods are based on guiding the diffusion model using intermediate rewards but tend to underperform since intermediate rewards are noisy due to the non-smooth nature of reward functions used in scientific domains. To address this, we propose Clean-Sample Markov Chain (CSMC) Sampler, a method that performs effective test-time reward-guided sampling for discrete diffusion models, enabling local search without relying on intermediate rewards. CSMC constructs a Markov chain of clean samples using the Metropolis-Hastings algorithm such that its stationary distribution is the target distribution. We design a proposal distribution by sequentially applying the forward and backward diffusion processes, making the acceptance probability tractable. Experiments on molecule and biological sequence generation with various reward functions demonstrate that our method consistently outperforms prior approaches that rely on intermediate rewards.",
      "authors": [
        "Prin Phunyaphibarn",
        "Minhyuk Sung"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "published": "2026-02-10 05:39:48+00:00",
      "link": "https://arxiv.org/pdf/2602.09424v1",
      "tags": [
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "local search and reward-guided sampling for discrete sequence design",
      "llm_tags": [
        "keyword:LNS",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.09574v1",
      "title": "Aligning Tree-Search Policies with Fixed Token Budgets in Test-Time Scaling of LLMs",
      "abstract": "Tree-search decoding is an effective form of test-time scaling for large language models (LLMs), but real-world deployment imposes a fixed per-query token budget that varies across settings. Existing tree-search policies are largely budget-agnostic, treating the budget as a termination condition, which can lead to late-stage over-branching or premature termination. We propose {Budget-Guided MCTS} (BG-MCTS), a tree-search decoding algorithm that aligns its search policy with the remaining token budget: it starts with broad exploration, then prioritizes refinement and answer completion as the budget depletes while reducing late-stage branching from shallow nodes. BG-MCTS consistently outperforms budget-agnostic tree-search baselines across different budgets on MATH500 and AIME24/25 with open-weight LLMs.",
      "authors": [
        "Sora Miyamoto",
        "Daisuke Oba",
        "Naoaki Okazaki"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-10 09:23:26+00:00",
      "link": "https://arxiv.org/pdf/2602.09574v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Budget-guided tree search is a form of efficient automatic algorithm for decoding",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.09851v1",
      "title": "CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization",
      "abstract": "Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy \"FE-then-HPO\" workflows that cannot capture strong FE-HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian HPO for robust end-to-end AutoML. CoFEH uses an LLM-driven FE optimizer powered by Tree of Thought (ToT) to explore flexible FE pipelines, a Bayesian optimization (BO) module to solve HPO, and a dynamic optimizer selector that realizes interleaved optimization by adaptively scheduling FE and HPO steps. Crucially, we introduce a mutual conditioning mechanism that shares context between LLM and BO, enabling mutually informed decisions. Experiments show that CoFEH not only outperforms traditional and LLM-based FE baselines, but also achieves superior end-to-end performance under joint optimization.",
      "authors": [
        "Beicheng Xu",
        "Keyao Ding",
        "Wei Liu",
        "Yupeng Lu",
        "Bin Cui"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-10 14:54:17+00:00",
      "link": "https://arxiv.org/pdf/2602.09851v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "LLM-driven optimization of search spaces and pipelines aligns with evolution of heuristics",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.09662v1",
      "title": "TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution",
      "abstract": "Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\\emph{i.e.}, trajectory difficulty) and breadth (\\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.",
      "authors": [
        "Deyang Jiang",
        "Jing Huang",
        "Xuanle Zhao",
        "Lei Chen",
        "Liming Zheng",
        "Fanfan Liu",
        "Haibo Qiu",
        "Peng Shi",
        "Zhixiong Zeng"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-02-10 11:16:57+00:00",
      "link": "https://arxiv.org/pdf/2602.09662v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Verifiable evolution and tree-structured planning for efficient automation",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.09877v1",
      "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies",
      "abstract": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.",
      "authors": [
        "Chenxu Wang",
        "Chaozhuo Li",
        "Songyang Liu",
        "Zejian Chen",
        "Jinyu Hou",
        "Ji Qi",
        "Rui Li",
        "Litian Zhang",
        "Qiwei Ye",
        "Zheng Liu",
        "Xu Chen",
        "Xi Zhang",
        "Philip S. Yu"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-02-10 15:18:19+00:00",
      "link": "https://arxiv.org/pdf/2602.09877v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Self-evolving AI societies relate to the evolution of heuristics and improvement loops",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "6"
    }
  ]
}