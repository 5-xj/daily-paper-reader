{
  "mode": "standard",
  "generated_at": "2026-02-03T04:28:20.460400+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 2,
    "deep_cap": 8,
    "deep_selected": 2,
    "quick_candidates": 9,
    "quick_skim_target": 13,
    "quick_selected": 9
  },
  "deep_dive": [
    {
      "id": "2602.02139v1",
      "title": "EvoMU: Evolutionary Machine Unlearning",
      "abstract": "Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.",
      "authors": [
        "Pawel Batorski",
        "Paul Swoboda"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-02-02 14:19:13+00:00",
      "link": "https://arxiv.org/pdf/2602.02139v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Evolutionary search for task-specific loss functions aligns with evolution of heuristics",
      "llm_tags": [
        "keyword:EOH"
      ]
    },
    {
      "id": "2602.02311v1",
      "title": "Introns and Templates Matter: Rethinking Linkage in GP-GOMEA",
      "abstract": "GP-GOMEA is among the state-of-the-art for symbolic regression, especially when it comes to finding small and potentially interpretable solutions. A key mechanism employed in any GOMEA variant is the exploitation of linkage, the dependencies between variables, to ensure efficient evolution. In GP-GOMEA, mutual information between node positions in GP trees has so far been used to learn linkage. For this, a fixed expression template is used. This however leads to introns for expressions smaller than the full template. As introns have no impact on fitness, their occurrences are not directly linked to selection. Consequently, introns can adversely affect the extent to which mutual information captures dependencies between tree nodes. To overcome this, we propose two new measures for linkage learning, one that explicitly considers introns in mutual information estimates, and one that revisits linkage learning in GP-GOMEA from a grey-box perspective, yielding a measure that needs not to be learned from the population but is derived directly from the template. Across five standard symbolic regression problems, GP-GOMEA achieves substantial improvements using both measures. We also find that the newly learned linkage structure closely reflects the template linkage structure, and that explicitly using the template structure yields the best performance overall.",
      "authors": [
        "Johannes Koch",
        "Tanja Alderliesten",
        "Peter A. N. Bosman"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-02-02 16:42:30+00:00",
      "link": "https://arxiv.org/pdf/2602.02311v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 8.0,
      "llm_evidence": "discusses linkage and efficient evolution in GP-GOMEA",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.01935v1",
      "title": "COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation",
      "abstract": "Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.",
      "authors": [
        "Annabelle Sujun Tang",
        "Christopher Priebe",
        "Lianhui Qin",
        "Hadi Esmaeilzadeh"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PL"
      ],
      "published": "2026-02-02 10:37:05+00:00",
      "link": "https://arxiv.org/pdf/2602.01935v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence": "MCTS reasoning for compiler optimization search aligns with heuristic evolution and efficient algorithms",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.01842v1",
      "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models",
      "abstract": "Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.",
      "authors": [
        "Jinbin Bai",
        "Yixuan Li",
        "Yuchen Zhu",
        "Yi Xin",
        "Qingyu Shi",
        "Aosong Feng",
        "Xiaohong Liu",
        "Molei Tao",
        "Jianru Xue",
        "Xiangtai Li",
        "Ming-Hsuan Yang"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-02 09:14:51+00:00",
      "link": "https://arxiv.org/pdf/2602.01842v1",
      "tags": [
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Hierarchical search and pruning for efficient algorithm scaling",
      "llm_tags": [
        "keyword:LNS",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.02025v1",
      "title": "Hippasus: Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data",
      "abstract": "Machine learning models depend critically on feature quality, yet useful features are often scattered across multiple relational tables. Feature augmentation enriches a base table by discovering and integrating features from related tables through join operations. However, scaling this process to complex schemas with many tables and multi-hop paths remains challenging. Feature augmentation must address three core tasks: identify promising join paths that connect the base table to candidate tables, execute these joins to materialize augmented data, and select the most informative features from the results. Existing approaches face a fundamental tradeoff between effectiveness and efficiency: achieving high accuracy requires exploring many candidate paths, but exhaustive exploration is computationally prohibitive. Some methods compromise by considering only immediate neighbors, limiting their effectiveness, while others employ neural models that require expensive training data and introduce scalability limitations. We present Hippasus, a modular framework that achieves both goals through three key contributions. First, we combine lightweight statistical signals with semantic reasoning from Large Language Models to prune unpromising join paths before execution, focusing computational resources on high-quality candidates. Second, we employ optimized multi-way join algorithms and consolidate features from multiple paths, substantially reducing execution time. Third, we integrate LLM-based semantic understanding with statistical measures to select features that are both semantically meaningful and empirically predictive. Our experimental evaluation on publicly available datasets shows that Hippasus substantially improves feature augmentation accuracy by up to 26.8% over state-of-the-art baselines while also offering high runtime performance.",
      "authors": [
        "Serafeim Papadias",
        "Kostas Patroumpas",
        "Dimitrios Skoutas"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.LG"
      ],
      "published": "2026-02-02 12:21:24+00:00",
      "link": "https://arxiv.org/pdf/2602.02025v1",
      "tags": [
        "keyword:EAA"
      ],
      "llm_score": 7.0,
      "llm_evidence": "efficient automatic feature augmentation framework",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2602.01966v1",
      "title": "Self-Consolidation for Self-Evolving Agents",
      "abstract": "While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.",
      "authors": [
        "Hongzhuo Yu",
        "Fei Zhu",
        "Guo-Sen Xie",
        "Ling Shao"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-02 11:16:07+00:00",
      "link": "https://arxiv.org/pdf/2602.01966v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Self-evolving agents align with the evolution of heuristics and automatic algorithm design",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.01983v1",
      "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning",
      "abstract": "Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\\uparrow$ and +23.04%$\\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.",
      "authors": [
        "Xintian Shen",
        "Jiawei Chen",
        "Lihao Zheng",
        "Hao Ma",
        "Tao Wei",
        "Kun Zhan"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-02-02 11:37:45+00:00",
      "link": "https://arxiv.org/pdf/2602.01983v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "evolving tool creation via experience reuse",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.02206v1",
      "title": "Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning",
      "abstract": "The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.",
      "authors": [
        "Tong Yang",
        "Yemin Wang",
        "Chaoning Zhang",
        "Aming Wu"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-02 15:12:13+00:00",
      "link": "https://arxiv.org/pdf/2602.02206v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Textual Strategy Evolution module aligns with Evolution of Heuristics",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.02369v1",
      "title": "Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback",
      "abstract": "Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \\emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \\textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \\textsc{Live-Evo} decouples \\emph{what happened} from \\emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \\textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \\textit{Prophet Arena} benchmark over a 10-week horizon, \\textsc{Live-Evo} improves Brier score by 20.8\\% and increases market returns by 12.9\\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.",
      "authors": [
        "Yaolun Zhang",
        "Yiran Wu",
        "Yijiong Yu",
        "Qingyun Wu",
        "Huazheng Wang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-02 17:34:50+00:00",
      "link": "https://arxiv.org/pdf/2602.02369v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Online self-evolving memory system for agents aligns with evolution of heuristics",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.02474v1",
      "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
      "abstract": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.",
      "authors": [
        "Haozhen Zhang",
        "Quanyu Long",
        "Jianzhu Bao",
        "Tao Feng",
        "Weizhi Zhang",
        "Haodong Yue",
        "Wenya Wang"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-02-02 18:53:28+00:00",
      "link": "https://arxiv.org/pdf/2602.02474v1",
      "tags": [
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Evolving memory skills for agents relates to Evolution of Heuristics",
      "llm_tags": [
        "keyword:EOH"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.02486v1",
      "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
      "abstract": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.",
      "authors": [
        "Jialiang Zhu",
        "Gongrui Zhang",
        "Xiaolong Ma",
        "Lin Xu",
        "Miaosen Zhang",
        "Ruiqi Yang",
        "Song Wang",
        "Kai Qiu",
        "Zhirong Wu",
        "Qi Dai",
        "Ruichun Ma",
        "Bei Liu",
        "Yifan Yang",
        "Chong Luo",
        "Zhengyuan Yang",
        "Linjie Li",
        "Lijuan Wang",
        "Weizhu Chen",
        "Xin Geng",
        "Baining Guo"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-02-02 18:58:07+00:00",
      "link": "https://arxiv.org/pdf/2602.02486v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Agentic framework for efficient search and iterative reflection",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    }
  ]
}