{
  "mode": "standard",
  "generated_at": "2026-01-23T03:57:04.175059+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 6,
    "deep_divecandidates": 10,
    "deep_cap": 11,
    "deep_selected": 10,
    "quick_candidates": 20,
    "quick_skim_target": 16,
    "quick_selected": 16
  },
  "deep_dive": [
    {
      "id": "2601.15628v1",
      "title": "CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models",
      "abstract": "Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.",
      "authors": [
        "Haibo Tong",
        "Zeyang Yue",
        "Feifei Zhao",
        "Erliang Lin",
        "Lu Jia",
        "Ruolin Chen",
        "Yinqian Sun",
        "Qian Zhang",
        "Yi Zeng"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 03:59:19+00:00",
      "link": "https://arxiv.org/pdf/2601.15628v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "comprehensive benchmark for frontier LLMs like GPT and Qwen",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.16199v1",
      "title": "PAL*M: Property Attestation for Large Generative Models",
      "abstract": "Machine learning property attestations allow provers (e.g., model providers or owners) to attest properties of their models/datasets to verifiers (e.g., regulators, customers), enabling accountability towards regulations and policies. But, current approaches do not support generative models or large datasets. We present PAL*M, a property attestation framework for large generative models, illustrated using large language models. PAL*M defines properties across training and inference, leverages confidential virtual machines with security-aware GPUs for coverage of CPU-GPU operations, and proposes using incremental multiset hashing over memory-mapped datasets to efficiently track their integrity. We implement PAL*M on Intel TDX and NVIDIA H100, showing it is efficient, scalable, versatile, and secure.",
      "authors": [
        "Prach Chantasantitam",
        "Adam Ilyas Caulfield",
        "Vasisht Duddu",
        "Lachlan J. Gunn",
        "N. Asokan"
      ],
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR"
      ],
      "published": "2026-01-22 18:51:13+00:00",
      "link": "https://arxiv.org/pdf/2601.16199v1",
      "tags": [
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 9.0,
      "llm_evidence": "framework for property attestation in large language models aligns with LLM evaluation query",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.15625v1",
      "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors",
      "abstract": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.",
      "authors": [
        "Zhiwei Zhang",
        "Fei Zhao",
        "Rui Wang",
        "Zezhong Wang",
        "Bin Liang",
        "Jiakang Wang",
        "Yao Hu",
        "Shaosheng Cao",
        "Kam-Fai Wong"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 03:57:35+00:00",
      "link": "https://arxiv.org/pdf/2601.15625v1",
      "tags": [
        "keyword:RL",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Addresses LLM tool use and error recovery using reinforcement learning",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.15706v1",
      "title": "Improving Methodologies for LLM Evaluations Across Global Languages",
      "abstract": "As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.   The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.",
      "authors": [
        "Akriti Vij",
        "Benjamin Chua",
        "Darshini Ramiah",
        "En Qi Ng",
        "Mahran Morsidi",
        "Naga Nikshith Gangarapu",
        "Sharmini Johnson",
        "Vanessa Wilfred",
        "Vikneswaran Kumaran",
        "Wan Sie Lee",
        "Wenzhuo Yang",
        "Yongsen Zheng",
        "Bill Black",
        "Boming Xia",
        "Frank Sun",
        "Hao Zhang",
        "Qinghua Lu",
        "Suyu Ma",
        "Yue Liu",
        "Chi-kiu Lo",
        "Fatemeh Azadi",
        "Isar Nejadgholi",
        "Sowmya Vajjala",
        "Agnes Delaborde",
        "Nicolas Rolin",
        "Tom Seimandi",
        "Akiko Murakami",
        "Haruto Ishi",
        "Satoshi Sekine",
        "Takayuki Semitsu",
        "Tasuku Sasaki",
        "Angela Kinuthia",
        "Jean Wangari",
        "Michael Michie",
        "Stephanie Kasaon",
        "Hankyul Baek",
        "Jaewon Noh",
        "Kihyuk Nam",
        "Sang Seo",
        "Sungpil Shin",
        "Taewhi Lee",
        "Yongsu Kim",
        "Daisy Newbold-Harrop",
        "Jessica Wang",
        "Mahmoud Ghanem",
        "Vy Hong"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 07:18:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15706v1",
      "tags": [
        "keyword:ppo",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Technical evaluation of frontier AI models across global languages and safeguards",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.15708v1",
      "title": "Persona Switch: Mixing Distinct Perspectives in Decoding Time",
      "abstract": "Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.",
      "authors": [
        "Junseok Kim",
        "Nakyeong Yang",
        "Kyomin Jung"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 07:30:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15708v1",
      "tags": [
        "keyword:ppo"
      ],
      "llm_score": 8.0,
      "llm_evidence": "describes decoding methodologies and evaluation of widely-used large language models",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.15761v1",
      "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning",
      "abstract": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.",
      "authors": [
        "Xiefeng Wu",
        "Mingyu Hu",
        "Shu Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 08:51:16+00:00",
      "link": "https://arxiv.org/pdf/2601.15761v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Proposes a new off-policy actor-critic method for reinforcement learning",
      "llm_tags": [
        "keyword:RL"
      ]
    },
    {
      "id": "2601.15953v1",
      "title": "Decoupling Return-to-Go for Efficient Decision Transformer",
      "abstract": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.",
      "authors": [
        "Yongyi Wang",
        "Hanyu Liu",
        "Lingfeng Li",
        "Bozhou Chen",
        "Ang Li",
        "Qirui Zheng",
        "Xionghui Yang",
        "Wenxin Li"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 13:42:08+00:00",
      "link": "https://arxiv.org/pdf/2601.15953v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Improves Decision Transformer efficiency for offline reinforcement learning",
      "llm_tags": [
        "keyword:RL"
      ]
    },
    {
      "id": "2601.16007v1",
      "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models",
      "abstract": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.",
      "authors": [
        "Chak-Wing Mak",
        "Guanyu Zhu",
        "Boyi Zhang",
        "Hongji Li",
        "Xiaowei Chi",
        "Kevin Zhang",
        "Yichen Wu",
        "Yangfan He",
        "Chun-Kai Fan",
        "Wentao Lu",
        "Kuangzhi Ge",
        "Xinyu Fang",
        "Hongyang He",
        "Kuan Lu",
        "Tianxiang Xu",
        "Li Zhang",
        "Yongxin Ni",
        "Youhua Li",
        "Shanghang Zhang"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2026-01-22 14:33:01+00:00",
      "link": "https://arxiv.org/pdf/2601.16007v1",
      "tags": [
        "query:sr-bench"
      ],
      "llm_score": 8.0,
      "llm_evidence": "Benchmark for physical reasoning in foundational multimodal large language models",
      "llm_tags": [
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.16172v1",
      "title": "Structured Hints for Sample-Efficient Lean Theorem Proving",
      "abstract": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.",
      "authors": [
        "Zachary Burton"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 18:16:46+00:00",
      "link": "https://arxiv.org/pdf/2601.16172v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "evaluates DeepSeek-Prover which uses LLMs and reinforcement learning",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    },
    {
      "id": "2601.16206v1",
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "abstract": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "authors": [
        "Daixuan Cheng",
        "Shaohan Huang",
        "Yuxian Gu",
        "Huatong Song",
        "Guoxin Chen",
        "Li Dong",
        "Wayne Xin Zhao",
        "Ji-Rong Wen",
        "Furu Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-22 18:57:09+00:00",
      "link": "https://arxiv.org/pdf/2601.16206v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 8.0,
      "llm_evidence": "combines LLM architectures with reinforcement learning for agentic intelligence",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2601.15668v1",
      "title": "EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning",
      "abstract": "Emotional information in speech plays a unique role in multimodal perception. However, current Speech Large Language Models (SpeechLLMs), similar to conventional speech emotion recognition (SER) systems, still treat emotion understanding as a simple classification problem. This provides limited interpretability of predictions, while leaving the LLMs' expressive and reasoning capabilities underutilized. In this work, we take the first step to reformulate SER as a deep reasoning problem through reinforcement learning (RL). We propose EmotionThinker, which is designed to generate accurate emotion predictions with interpretable explanations grounded in fine-grained acoustic cues. To achieve this, we first construct EmotionCoT-35K, an emotional reasoning dataset with Chain-of-Thought annotations and detailed captions. Second, we observe that current SpeechLLMs exhibit weak prosody perception, whereas prosodic cues constitute fundamental signals for interpreting emotions. To address this, we develop the prosody-enhanced foundation model EmotionThinker-Base, and demonstrate that prosody enhancement improves emotion understanding. Third, we introduce Group-Relative-Policy-Optimization with Progressive-Trust-aware-Reasoning-Reward (GRPO-PTR) for RL. Different from standard GRPO, which relies only on rule-based outcome rewards, GRPO-PTR progressively introduces reasoning reward, dynamically adjusts it with a trustworthiness weight reflecting the alignment between reasoning and outcome, and evaluates the overall reasoning quality with a reward model based on multi-dimensional criteria. EmotionThinker outperforms previous state-of-the-art evaluation models both in emotion accuracy and explanation quality, advancing SER toward interpretable multimodal reasoning. Project page: https://github.com/dingdongwang/EmotionThinker",
      "authors": [
        "Dingdong Wang",
        "Shujie Liu",
        "Tianhua Zhang",
        "Youjun Chen",
        "Jinyu Li",
        "Helen Meng"
      ],
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD"
      ],
      "published": "2026-01-22 05:51:53+00:00",
      "link": "https://arxiv.org/pdf/2601.15668v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Reformulates emotion reasoning as a reinforcement learning problem",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15624v1",
      "title": "Explainable Deepfake Detection with RL Enhanced Self-Blended Images",
      "abstract": "Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.",
      "authors": [
        "Ning Jiang",
        "Dingheng Zeng",
        "Yanhong Liu",
        "Haiyang Yi",
        "Shijie Yu",
        "Minghe Weng",
        "Haifeng Shen",
        "Ying Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22 03:55:46+00:00",
      "link": "https://arxiv.org/pdf/2601.15624v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Uses reinforcement learning (RL) to enhance multimodal large language model performance",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15690v1",
      "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models",
      "abstract": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.",
      "authors": [
        "Jiaxin Zhang",
        "Wendi Cui",
        "Zhuohang Li",
        "Lifu Huang",
        "Bradley Malin",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "published": "2026-01-22 06:21:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15690v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Discusses LLM uncertainty and reinforcement learning for self-improvement",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15727v1",
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "abstract": "The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.",
      "authors": [
        "Yang Yu",
        "Peiyu Zang",
        "Chi Hsu Tsai",
        "Haiming Wu",
        "Yixin Shen",
        "Jialing Zhang",
        "Haoyu Wang",
        "Zhiyou Xiao",
        "Jingze Shi",
        "Yuyu Luo",
        "Wentao Zhang",
        "Chunlei Men",
        "Guang Liu",
        "Yonghua Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-22 07:53:52+00:00",
      "link": "https://arxiv.org/pdf/2601.15727v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Discusses LLM-based agents for automated kernel generation and optimization",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15812v1",
      "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models",
      "abstract": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.",
      "authors": [
        "Shir Ashury-Tahan",
        "Yifan Mai",
        "Elron Bandel",
        "Michal Shmueli-Scheuer",
        "Leshem Choshen"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22 09:52:39+00:00",
      "link": "https://arxiv.org/pdf/2601.15812v1",
      "tags": [
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Evaluation framework for analyzing failure modes in LLM benchmarks",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15737v1",
      "title": "PhysProver: Advancing Automatic Theorem Proving for Physics",
      "abstract": "The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\\sim$5K training samples, PhysProver achieves an overall 2.4\\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.",
      "authors": [
        "Hanning Zhang",
        "Ruida Wang",
        "Rui Pan",
        "Wenyuan Wang",
        "Bingxu Meng",
        "Tong Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-22 08:05:32+00:00",
      "link": "https://arxiv.org/pdf/2601.15737v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Enhances formal reasoning and theorem proving using LLMs and dedicated datasets",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15995v1",
      "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour",
      "abstract": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.",
      "authors": [
        "Liang Wang",
        "Kanzhong Yao",
        "Yang Liu",
        "Weikai Qin",
        "Jun Wu",
        "Zhe Sun",
        "Qiuguo Zhu"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-22 14:16:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15995v1",
      "tags": [
        "keyword:RL",
        "query:sr-bench"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Uses reinforcement learning for quadruped locomotion benchmarks",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15793v1",
      "title": "HumanLLM: Towards Personalized Understanding and Simulation of Human Nature",
      "abstract": "Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.",
      "authors": [
        "Yuxuan Lei",
        "Tianfu Wang",
        "Jianxun Lian",
        "Zhengyu Hu",
        "Defu Lian",
        "Xing Xie"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 09:27:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15793v1",
      "tags": [
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "discusses LLM pretraining and behavioral simulation",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16061v1",
      "title": "Dynamic Tactile Sensing System and Soft Actor Critic Reinforcement Learning for Inclusion Characterization",
      "abstract": "This paper presents the Dynamic Tactile Sensing System that utilizes robotic tactile sensing in conjunction with reinforcement learning to locate and characterize embedded inclusions. A dual arm robot is integrated with an optical Tactile Imaging Sensor that utilizes the Soft Actor Critic Algorithm to acquire tactile data based on a pixel intensity reward. A Dynamic Interrogation procedure for tactile exploration is developed that enables the robot to first localize inclusion and refine their positions for precise imaging. Experimental validation conducted on Polydimethylsiloxane phantoms demonstrates that the robot using the Tactile Soft Actor Critic Model was able to achieve size estimation errors of 2.61% and 5.29% for soft and hard inclusions compared to 7.84% and 6.87% for expert human operators. Results also show that Dynamic Tactile Sensing System was able to locate embedded inclusions and autonomously determine their mechanical properties, useful in applications such as breast tumor characterization.",
      "authors": [
        "John Bannan",
        "Nazia Rahman",
        "Chang-Hee Won"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-01-22 15:57:15+00:00",
      "link": "https://arxiv.org/pdf/2601.16061v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 7.0,
      "llm_evidence": "utilizes reinforcement learning for robotic tactile sensing and characterization",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15808v1",
      "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
      "abstract": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.",
      "authors": [
        "Yuxuan Wan",
        "Tianqing Fang",
        "Zaitang Li",
        "Yintong Huo",
        "Wenxuan Wang",
        "Haitao Mi",
        "Dong Yu",
        "Michael R. Lyu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 09:47:31+00:00",
      "link": "https://arxiv.org/pdf/2601.15808v1",
      "tags": [
        "keyword:RL",
        "keyword:符号回归（示例）",
        "query:sr-bench"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Focuses on inference-time scaling and verification for research agents",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16093v1",
      "title": "SAMTok: Representing Any Mask with Two Words",
      "abstract": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
      "authors": [
        "Yikang Zhou",
        "Tao Zhang",
        "Dengxian Gong",
        "Yuanzheng Wu",
        "Ye Tian",
        "Haochen Wang",
        "Haobo Yuan",
        "Jiacong Wang",
        "Lu Qi",
        "Hao Fei",
        "Anran Wang",
        "Zhuochen Wang",
        "Yujing Wang",
        "Cheng Chen",
        "Shunping Ji",
        "Xiangtai Li"
      ],
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2026-01-22 16:44:09+00:00",
      "link": "https://arxiv.org/pdf/2601.16093v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Uses reinforcement learning and QwenVL series LLMs for pixel-wise capabilities",
      "llm_tags": [
        "keyword:RL",
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15871v1",
      "title": "Why Inference in Large Models Becomes Decomposable After Training",
      "abstract": "Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.",
      "authors": [
        "Jidong Jin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 11:20:57+00:00",
      "link": "https://arxiv.org/pdf/2601.15871v1",
      "tags": [
        "keyword:RL",
        "keyword:resnet",
        "keyword:符号回归（示例）"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Discusses internal structures and training methodologies of large-scale AI models",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16097v1",
      "title": "Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating",
      "abstract": "Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.",
      "authors": [
        "Makbule Gulcin Ozsoy"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 16:46:57+00:00",
      "link": "https://arxiv.org/pdf/2601.16097v1",
      "tags": [
        "keyword:ppo",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Multilingual Text2Cypher using LoRA adapters and Large Language Models",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15892v1",
      "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
      "abstract": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
      "authors": [
        "Chenghao Fan",
        "Wen Heng",
        "Bo Li",
        "Sichen Liu",
        "Yuxuan Song",
        "Jing Su",
        "Xiaoye Qu",
        "Kai Shen",
        "Wei Wei"
      ],
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2026-01-22 12:13:17+00:00",
      "link": "https://arxiv.org/pdf/2601.15892v1",
      "tags": [
        "keyword:resnet",
        "keyword:符号回归（示例）",
        "query:sr-bench",
        "query:大厂llm"
      ],
      "llm_score": 6.0,
      "llm_evidence": "Technical report on training methodologies and benchmarks for code LLMs",
      "llm_tags": [
        "query:大厂llm"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16109v1",
      "title": "Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision",
      "abstract": "We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.",
      "authors": [
        "Yashuai Yan",
        "Tobias Egle",
        "Christian Ott",
        "Dongheui Lee"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-01-22 16:56:52+00:00",
      "link": "https://arxiv.org/pdf/2601.16109v1",
      "tags": [
        "keyword:RL",
        "keyword:ppo",
        "keyword:resnet",
        "query:大厂llm"
      ],
      "llm_score": 7.0,
      "llm_evidence": "Integrates residual reinforcement learning (RL) for robust control and locomotion",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.16035v1",
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "abstract": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.",
      "authors": [
        "Han Xue",
        "Sikai Liang",
        "Zhikai Zhang",
        "Zicheng Zeng",
        "Yun Liu",
        "Yunrui Lian",
        "Jilong Wang",
        "Qingtao Liu",
        "Xuesong Shi",
        "Li Yi"
      ],
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2026-01-22 15:08:53+00:00",
      "link": "https://arxiv.org/pdf/2601.16035v1",
      "tags": [
        "keyword:RL"
      ],
      "llm_score": 6.0,
      "llm_evidence": "RL-based traversal skill learning for humanoids",
      "llm_tags": [
        "keyword:RL"
      ],
      "quick_tier": "6"
    }
  ]
}